{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "magma_dir = '/home/marco/epfl/magma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 58788,
     "status": "ok",
     "timestamp": 1610359570812,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "TAoI-Sm37yuM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, magma_dir)\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 59486,
     "status": "ok",
     "timestamp": 1610359571513,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "82WSp6khIcua"
   },
   "outputs": [],
   "source": [
    "MODEL = 'bart'\n",
    "\n",
    "RE_SPLITTER = '\\n'              # do we split sentences of paragraphs?\n",
    "                                # use '\\.(?!\\d)|\\n' or '\\n', respectively\n",
    "\n",
    "TOKEN_MAX_LEN = 99              # max length of a word\n",
    "PARA_MIN_LENGTH = 2             # minimum length for a sentence or\n",
    "                                # a paragraph, in tokens\n",
    "\n",
    "# Output path\n",
    "OUTPUT_PATH = magma_dir+'pipeline/karger_books_para_extraction/'+MODEL+'/'\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 59489,
     "status": "ok",
     "timestamp": 1610359571520,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "KJMpUw0Wl8NH"
   },
   "outputs": [],
   "source": [
    "# Topic modeling specific configurations\n",
    "\n",
    "REDUCTION_MAX_LEN = 1024        # maximum length of the LDA/LSI/TextRank reduction\n",
    "\n",
    "STOPWORDS_EXTENSION =\\\n",
    "    ['may', 'might',\n",
    "     'also', 'with',\n",
    "     'without', 'use',\n",
    "     'uses', 'used', 'using']\n",
    "\n",
    "STEMMER = 'snowball'            # name of the stemmer, might use 'porter'\n",
    "\n",
    "N_GRAM = 2                      # the length of n-gram we want to create\n",
    "N_GRAM_MIN_COUNT = 2            # there should be at least N_GRAM_MIN_COUNT\n",
    "                                # repetitions in the text\n",
    "N_GRAM_THRESHOLD = 20           # see gensim.Phrases documentation\n",
    "\n",
    "DIC_NO_BELOW = 3                # keep tokens present in DIC_NO_BELOW+ sentences/paragraphs\n",
    "DIC_NO_ABOVE = 1                # fraction of total corpus size (default: 1)\n",
    "\n",
    "TOP_N = 30                      # number of words to keep for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb7fAfzaK4es"
   },
   "source": [
    "### **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "3a0917c309734495885543d54e7bd8c2",
      "7aa26b43182d43058c2a04e7cec5cf71"
     ]
    },
    "executionInfo": {
     "elapsed": 66395,
     "status": "ok",
     "timestamp": 1610359578432,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "wvbMlPBxk45S",
    "outputId": "b4ac510d-34d9-4940-fa2c-38bdf017e9bd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from textwrap import fill\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "if 'pegasus' in MODEL:\n",
    "    from transformers import PegasusTokenizer\n",
    "    tokenizer =\\\n",
    "        PegasusTokenizer.from_pretrained('google/pegasus-large')\n",
    "elif 'bart' in MODEL:\n",
    "    from transformers import BartTokenizer\n",
    "    tokenizer =\\\n",
    "        BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "elif 't5' in MODEL:\n",
    "    from transformers import T5Tokenizer\n",
    "    tokenizer =\\\n",
    "        T5Tokenizer.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFd0ppeJyX1o"
   },
   "source": [
    "### **Karger Books Base Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "Z0lbkScg0a7j"
   },
   "outputs": [],
   "source": [
    "base_dataset = magma_dir+'datasets/karger_books_base/df.csv'\n",
    "df = pd.read_csv(base_dataset)\n",
    "df = df.set_index(['book', 'chapter', 'section', 'subsection'])\n",
    "df.bullets = df.bullets.map(eval, na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSd3WuiICKfQ"
   },
   "source": [
    "### **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fXJEMYaCKfn"
   },
   "source": [
    "#### Preprocessing\n",
    "\n",
    "* Split based on RE_SPLITTER\n",
    "* Explode the dataset\n",
    "* Remove unwanted chars at beginning or end of sentence\n",
    "* Remove multiple spaces\n",
    "* Remove long words (> TOKEN_MAX_LEN chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "aaYoqzcyCKfp"
   },
   "outputs": [],
   "source": [
    "# Split in sentences / paragraphs based on RE_SPLITTER\n",
    "df.text =\\\n",
    "    df.text.map(lambda x: [p.strip() for p in re.split(RE_SPLITTER, x) if p!=''],\n",
    "                na_action='ignore')\n",
    "    \n",
    "# explode to get one row for each paragraph /sentence\n",
    "df = df.explode('text')\n",
    "df = df.rename(columns={'text': 'para'})\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove unwanted chars at beginning or end of sentence\n",
    "df.para = df.para.map(lambda p: p.lstrip('.,;:-)] \\n'))\n",
    "df.para = df.para.map(lambda p: p.rstrip('.,;:-([ \\n'))\n",
    "\n",
    "# Remove multiple spaces\n",
    "df.para = df.para.map(lambda p:\n",
    "    re.sub('\\s+', ' ', p).strip())\n",
    "\n",
    "# Remove long words (> TOKEN_MAX_LEN chars)\n",
    "def para2words(para):\n",
    "    return gensim.utils.simple_preprocess(\n",
    "        para, deacc=True, max_len=TOKEN_MAX_LEN)\n",
    "df['para_proc'] = df.para.map(para2words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Preprocessing\n",
    "\n",
    "* Remove stop words\n",
    "* Remove short sentences / paragraphs (< PARA_MIN_LENGTH tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "df.para_proc = df.para_proc.map(lambda p:\n",
    "    [w for w in p if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove short sentences / paragraphs (< PARA_MIN_LENGTH tokens)\n",
    "df.loc[df.para_proc.map(len) <\\\n",
    "    PARA_MIN_LENGTH, 'para_proc'] = np.nan\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.para = df.para.map(lambda p: p+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpaitY9dlTM5"
   },
   "source": [
    "### **Topic Modeling Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "gKBuLTt5qQ1Z"
   },
   "outputs": [],
   "source": [
    "# Stem\n",
    "if 'port' in STEMMER:\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    st = PorterStemmer()\n",
    "elif 'snow' in STEMMER:\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    st = SnowballStemmer('english')\n",
    "\n",
    "df.para_proc = df.para_proc.map(lambda p:\n",
    "    [st.stem(w) for w in p], na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "QHKOznWOqQ1i"
   },
   "outputs": [],
   "source": [
    "# Create n-grams (N_GRAM)\n",
    "data_words = df.para_proc.dropna().values.tolist()\n",
    "\n",
    "if N_GRAM == 2:\n",
    "    bigram = gensim.models.Phrases(\n",
    "        data_words,\n",
    "        min_count=N_GRAM_MIN_COUNT,\n",
    "        threshold=N_GRAM_THRESHOLD)\n",
    "\n",
    "    df.para_proc = df.para_proc.map(lambda p:\n",
    "        [b for b in bigram[p]], na_action='ignore')\n",
    "    \n",
    "elif N_GRAM == 3:\n",
    "    trigram = gensim.models.Phrases(\n",
    "        bigram[data_words],\n",
    "        min_count=N_GRAM_MIN_COUNT,\n",
    "        threshold=N_GRAM_THRESHOLD)\n",
    "    \n",
    "    df.para_proc = df.para_proc.map(lambda p:\n",
    "        [b for b in trigram[bigram[p]]], na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMPw10hNljZ1"
   },
   "source": [
    "#### Dictionary (DIC_NO_BELOW, DIC_NO_ABOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14060,
     "status": "ok",
     "timestamp": 1610110852799,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "OaMCDRcNlW9M",
    "outputId": "3f9bb004-e958-42d8-9970-b2ac462fa6f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/ipykernel_launcher.py:11: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary for topic model (DIC_NO_BELOW, DIC_NO_ABOVE)\n",
    "book_ch_comb = set(zip(df.index.get_level_values(0),\n",
    "    df.index.get_level_values(1)))\n",
    "\n",
    "id2word = {}\n",
    "for book, ch in book_ch_comb:\n",
    "    if book not in id2word:\n",
    "        id2word[book] = {}\n",
    "\n",
    "    id2word[book][ch] = gensim.corpora.Dictionary(\n",
    "        df.loc[book, ch].para_proc.dropna().values.tolist() )\n",
    "\n",
    "    id2word[book][ch].filter_extremes(\n",
    "        no_below = DIC_NO_BELOW, no_above = DIC_NO_ABOVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhO2hirsvrcc"
   },
   "source": [
    "#### LDA\n",
    "\n",
    "https://radimrehurek.com/gensim_3.8.3/models/ldamodel.html\n",
    "\n",
    "https://www.di.ens.fr/~fbach/mdhnips2010.pdf\n",
    "\n",
    "Keep N_TOP words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "R1QAj1x72aqb"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "def get_lda_model(df, book, ch):\n",
    "    corpus = df.loc[book, ch].para_proc.map(id2word[book][ch].doc2bow,\n",
    "        na_action='ignore').dropna().values.tolist()\n",
    "\n",
    "    return gensim.models.ldamodel.LdaModel(\n",
    "        corpus = corpus,\n",
    "        num_topics = 1,\n",
    "        id2word = id2word[book][ch],\n",
    "        alpha = 'auto',\n",
    "        random_state = config.SEED)\n",
    "\n",
    "lda_word2prob = {}\n",
    "for book, ch in book_ch_comb:       \n",
    "    if book not in lda_word2prob:\n",
    "        lda_word2prob[book] = {}\n",
    "\n",
    "    lda_word2prob[book][ch] = dict(\\\n",
    "        get_lda_model(df, book, ch).show_topic(0, TOP_N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeDlmvn43RsI"
   },
   "source": [
    "### **Paragraph importance**\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{importance}(p) = \\frac{\\sum_{w \\in p} \\text{probability}(w)}{\\sqrt{\\text{length}(p)}}\\quad\\quad\\text{what about}\\quad\\frac{\\sum_{w \\in p} \\text{probability}(w)}{log(\\text{length}(p))}\\quad?\n",
    "\\end{equation}\n",
    "\n",
    "Where $p$ is a paragraph or a sentence, $w$ is a word (token) and $\\text{probability}$ is the probability assigned by LDA or LSI model to $w$ (0 if it is not present in the TOP_N words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42565,
     "status": "ok",
     "timestamp": 1610110906175,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "EnODsA484Zrr",
    "outputId": "0f9dbf85-cbb8-4e7b-a70b-07e3ec9fda5c"
   },
   "outputs": [],
   "source": [
    "def word_importance(model_word2prob, word):\n",
    "    return model_word2prob.get(word, 0)\n",
    "\n",
    "def para_importance(model_word2prob, para):\n",
    "    l_importance = [word_importance(model_word2prob, w) for w in para]\n",
    "    return  np.sum(l_importance) / np.sqrt(len(l_importance))\n",
    "\n",
    "df['lda_imp'] = np.nan\n",
    "df['lda_imp_norm'] = np.nan\n",
    "for book, ch in book_ch_comb:\n",
    "    idx_slice = pd.IndexSlice[book, ch, :, :]\n",
    "    # getting LDA and LSI importance\n",
    "    df.loc[idx_slice, 'lda_imp'] = df.loc[idx_slice, 'para_proc'].map(lambda p:\n",
    "        para_importance(lda_word2prob[book][ch], p), na_action='ignore')\n",
    "\n",
    "    # normalizing\n",
    "    s = df.loc[idx_slice, 'lda_imp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_sec_bul = df.groupby(['book', 'chapter', 'section'], sort=False).agg({\n",
    "    'para': lambda p: list(p),\n",
    "    'bullets': lambda b: list(b)[0]\n",
    "}).groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'para': lambda p: len(list(p)),\n",
    "    'bullets': lambda b: len(list(b)[0])\n",
    "}).rename(columns={'para':'num_sec', 'bullets':'num_bul'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_best_para = df.reset_index(level=[2, 3]).groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'section': lambda s: list(s),\n",
    "    'subsection': lambda ss: list(ss),\n",
    "    'para': lambda p: list(p),\n",
    "    'bullets': lambda b: list(b)[0],\n",
    "    'para_proc': lambda pp: list(pp),\n",
    "    'lda_imp': lambda lda: list(lda)\n",
    "})\n",
    "df_best_para.lda_imp = df_best_para.lda_imp.map(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_para['num_sec'] = 0\n",
    "for idx in df_best_para.index.tolist():\n",
    "    df_best_para.loc[idx, 'num_sec'] = int(df_num_sec_bul.loc[idx, 'num_sec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find Best Paragraph for each Book, Chapter (and Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_para['best_para'] = df_best_para.apply(lambda row: np.argsort(row.lda_imp)[::-1][:row.num_sec], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How Many Sections Are We Covering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.000000\n",
       "mean      58.253368\n",
       "std       16.016781\n",
       "min       16.666667\n",
       "25%       50.000000\n",
       "50%       60.000000\n",
       "75%       70.000000\n",
       "max      100.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_diversity(r):\n",
    "    best_idx = r.best_para\n",
    "    all_sections = set(r.section)\n",
    "    \n",
    "    selected_sections = set([r.section[i] for i in best_idx])\n",
    "    \n",
    "    return len(selected_sections.intersection(all_sections))/len(all_sections)*100\n",
    "    \n",
    "df_best_para.apply(calculate_diversity, axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expanding from Best Paragraph Based on Paragraph Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_para['para_num_tok'] =\\\n",
    "    df_best_para.para.map(lambda ps: np.array([len(tokenizer.tokenize(p)) for p in ps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:00<00:00, 847.14it/s]\n"
     ]
    }
   ],
   "source": [
    "def expand_based_on_importance(r):\n",
    "    max_length = len(r.para)\n",
    "    max_idx = max_length-1\n",
    "    \n",
    "    extracted_para = []\n",
    "    \n",
    "    # Calculate the fraction we need to extract\n",
    "    # based on total number of tokens in this chp\n",
    "    # and number of centroids (sections) in this chp\n",
    "    # do not go over the model max length\n",
    "    num_tok_tot = sum(r.para_num_tok)\n",
    "    num_tok_th = min(\n",
    "        int(0.8*num_tok_tot / len(r.best_para)),\n",
    "        0.9*tokenizer.model_max_length)\n",
    "    \n",
    "    for i, best in enumerate(r.best_para):\n",
    "        merged_para_idx = [best]\n",
    "        num_tok = r.para_num_tok[best]\n",
    "        \n",
    "        while num_tok < num_tok_th:\n",
    "            if len(merged_para_idx) == max_length : break\n",
    "            elif 0 in merged_para_idx:\n",
    "                merged_para_idx.append(max(merged_para_idx)+1)\n",
    "            elif max_idx in merged_para_idx:\n",
    "                merged_para_idx.append(min(merged_para_idx)-1)\n",
    "            else:\n",
    "                if (r.lda_imp)[min(merged_para_idx)-1] <\\\n",
    "                    (r.lda_imp)[max(merged_para_idx)+1]:\n",
    "                    merged_para_idx.append(max(merged_para_idx)+1)\n",
    "                else:\n",
    "                    merged_para_idx.append(min(merged_para_idx)-1)\n",
    "            num_tok = np.sum(r.para_num_tok[merged_para_idx])\n",
    "                  \n",
    "        extracted_para.append(sorted(merged_para_idx))\n",
    "        \n",
    "    return extracted_para\n",
    "\n",
    "df_best_para['selected_para'] =\\\n",
    "    df_best_para.progress_apply(expand_based_on_importance, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Study Overlap and Remove Useless (>90% overlap) Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_remove_overlap = df_best_para.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_overlap_matrix(r):\n",
    "    num_sec = len(r.selected_para)\n",
    "    overlap_matrix = np.zeros((num_sec,num_sec))\n",
    "    \n",
    "    def list_overlap(a, b):\n",
    "        return list( set(a).intersection(set(b)) )\n",
    "    \n",
    "    for i in range(num_sec):\n",
    "        for j in range(num_sec):\n",
    "            if i == j : continue\n",
    "            num_tok_i = np.sum(r.para_num_tok[r.selected_para[i]])\n",
    "            overlap = list_overlap(\n",
    "                r.selected_para[i], r.selected_para[j])\n",
    "            num_tok_overlap = np.sum(r.para_num_tok[overlap])\n",
    "            assert num_tok_overlap <= num_tok_i\n",
    "            \n",
    "            overlap_matrix[i, j] = round(num_tok_overlap/num_tok_i*100, 2)\n",
    "    \n",
    "    return overlap_matrix\n",
    "\n",
    "def remove_big_overlap(r, threshold):\n",
    "    om = r.overlap_matrix\n",
    "    big_overlap_idx = np.argwhere(om >= threshold)\n",
    "    to_be_removed = set()\n",
    "    for idx in big_overlap_idx:\n",
    "        i, j = idx[0], idx[1]\n",
    "        if om[i, j] == om[j, i]:\n",
    "            if i in to_be_removed or j in to_be_removed : continue\n",
    "            else : to_be_removed.add(i)\n",
    "        elif om[i, j] > om[j, i]:\n",
    "            to_be_removed.add(i)\n",
    "        else:\n",
    "            to_be_removed.add(j)\n",
    "    return [s for i, s in enumerate(r.selected_para) if i not in to_be_removed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.000000\n",
       "mean       3.653422\n",
       "std        3.084570\n",
       "min        0.000000\n",
       "25%        2.000000\n",
       "50%        2.000000\n",
       "75%        5.000000\n",
       "max       20.000000\n",
       "Name: overlap_matrix, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove_overlap['overlap_matrix'] = df_remove_overlap.apply(lambda row: create_overlap_matrix(row), axis=1)\n",
    "df_remove_overlap.overlap_matrix.map(lambda om: np.sum(om > 90)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.0\n",
       "mean       0.0\n",
       "std        0.0\n",
       "min        0.0\n",
       "25%        0.0\n",
       "50%        0.0\n",
       "75%        0.0\n",
       "max        0.0\n",
       "Name: overlap_matrix, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove_overlap.selected_para = df_remove_overlap.apply(lambda row: remove_big_overlap(row, 90), axis=1)\n",
    "\n",
    "df_remove_overlap['overlap_matrix'] = df_remove_overlap.apply(lambda row: create_overlap_matrix(row), axis=1)\n",
    "df_remove_overlap.overlap_matrix.map(lambda om: np.sum(om > 90)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge when >90% overlap Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_overlap = df_best_para.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_big_overlap(r, threshold):\n",
    "    om = r.overlap_matrix\n",
    "    big_overlap_idx = np.argwhere(om >= threshold)\n",
    "    big_overlap_idx = set([frozenset(t) for t in big_overlap_idx])\n",
    "    merged = set()\n",
    "    to_be_merged = set()\n",
    "    for idx in big_overlap_idx:\n",
    "        idx = tuple(idx)\n",
    "        i, j = idx[0], idx[1]\n",
    "        if i not in merged and j not in merged:\n",
    "            to_be_merged.add(idx)\n",
    "            merged.add(i)\n",
    "            merged.add(j)\n",
    "    return to_be_merged\n",
    "\n",
    "def merge_para(r):\n",
    "    new_selected_para = np.empty((len(r.selected_para),), dtype=object)\n",
    "    for i, j in r.to_be_merged:\n",
    "        new_selected_para[i] = np.array(list(set(\n",
    "            np.concatenate((r.selected_para[i], r.selected_para[j])))))\n",
    "        \n",
    "        new_selected_para[j] = []\n",
    "        \n",
    "    selected_para = []\n",
    "    for i, x in enumerate(new_selected_para):\n",
    "        if x is None:\n",
    "            selected_para.append(np.array(r.selected_para[i]))\n",
    "        elif len(x) == 0 : continue\n",
    "        else:\n",
    "            selected_para.append(x)\n",
    "    return selected_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para to be merged: 643\n",
      "Para to be merged: 110\n",
      "Para to be merged: 4\n",
      "Para to be merged: 0\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    df_merge_overlap['overlap_matrix'] = df_merge_overlap.apply(create_overlap_matrix, axis=1)\n",
    "\n",
    "    df_merge_overlap['to_be_merged'] = df_merge_overlap.apply(lambda row: find_big_overlap(row, 90), axis=1)\n",
    "\n",
    "    num_to_be_merged = df_merge_overlap.to_be_merged.map(len).sum()\n",
    "    print('Para to be merged: %d'%num_to_be_merged)\n",
    "    if (num_to_be_merged <= 0) : break\n",
    "\n",
    "    df_merge_overlap.selected_para = df_merge_overlap.apply(merge_para, axis=1)\n",
    "    \n",
    "df_merge_overlap.selected_para = df_merge_overlap.selected_para.map(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize Results Remove Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove_overlap_tobesaved = df_remove_overlap.explode('selected_para')\n",
    "df_remove_overlap_tobesaved = df_remove_overlap_tobesaved.drop(\n",
    "    columns=['best_para', 'lda_imp', 'overlap_matrix'])\n",
    "\n",
    "df_remove_overlap_tobesaved['selected_para_lda'] = df_remove_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para) if i in row.selected_para], axis=1)\n",
    "\n",
    "df_remove_overlap_tobesaved['para_num_tok'] = df_remove_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para_num_tok) if i in row.selected_para], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1887.000000\n",
       "mean      461.736089\n",
       "std       207.686768\n",
       "min       112.000000\n",
       "25%       309.000000\n",
       "50%       412.000000\n",
       "75%       565.000000\n",
       "max      1255.000000\n",
       "Name: para_num_tok, dtype: float64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove_overlap_tobesaved.para_num_tok.map(sum).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove_overlap_tobesaved.to_csv(OUTPUT_PATH+'df_lda_remove.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare to Para Wordembed ST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(pred, ref):\n",
    "    return round(100*len(pred.intersection(ref)) / len(pred), 2)\n",
    "    \n",
    "def recall(pred, ref):\n",
    "    return round(100*len(pred.intersection(ref)) / len(ref), 2)\n",
    "\n",
    "def fmeasure(prec, rec):\n",
    "    if prec + rec == 0 : return 0\n",
    "    return round(2*prec*rec/(prec+rec), 2)\n",
    "\n",
    "flatten = lambda t: [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n",
      "\n",
      "       unsup_coverage  sup_coverage  intersection   precision      recall\n",
      "count      453.000000    453.000000    453.000000  453.000000  453.000000\n",
      "mean        49.705078     29.231305     17.228806   34.619536   59.719360\n",
      "std         13.343276     15.454056     11.213288   20.162173   24.551033\n",
      "min         11.250000      2.564103      0.000000    0.000000    0.000000\n",
      "25%         40.000000     18.000000      9.090909   19.050000   42.860000\n",
      "50%         50.000000     25.714286     15.000000   31.030000   58.820000\n",
      "75%         57.894737     38.095238     23.076923   47.620000   77.780000\n",
      "max         88.888889    100.000000     75.000000  100.000000  100.000000\n",
      "\n",
      "\n",
      "th\n",
      "\n",
      "       unsup_coverage  sup_coverage  intersection   precision      recall\n",
      "count      453.000000    453.000000    453.000000  453.000000  453.000000\n",
      "mean        49.705078     63.226644     35.402266   71.756380   56.502958\n",
      "std         13.343276     15.442000     13.306320   20.265302   17.773207\n",
      "min         11.250000     15.384615      1.428571    5.880000    7.690000\n",
      "25%         40.000000     54.166667     26.027397   60.000000   44.830000\n",
      "50%         50.000000     63.414634     35.135135   73.680000   57.140000\n",
      "75%         57.894737     75.000000     44.444444   85.710000   68.090000\n",
      "max         88.888889    100.000000     81.250000  100.000000  100.000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in ['base', 'th']:\n",
    "    df_para_wordembed_st =\\\n",
    "        pd.read_csv(magma_dir+'datasets/karger_books_para_wordembed/'+MODEL+'/st/df_'+t+'_selected_para.csv')\\\n",
    "        .set_index(['book', 'chapter'])\n",
    "    df_para_wordembed_st.para = df_para_wordembed_st.para.map(eval)\n",
    "    df_para_wordembed_st.best_match = df_para_wordembed_st.best_match.map(eval)\n",
    "\n",
    "    num_para = df_para_wordembed_st.para.map(len)\n",
    "\n",
    "    df_remove_overlap_tobesaved = pd.read_csv(OUTPUT_PATH+'df_lda_remove.csv').set_index(['book', 'chapter'])\n",
    "    df_remove_overlap_tobesaved.selected_para = df_remove_overlap_tobesaved.selected_para.map(eval)\n",
    "    df_remove_overlap_tobesaved = df_remove_overlap_tobesaved.groupby(['book', 'chapter'], sort=False).agg({\n",
    "        'selected_para': lambda p: list(p)\n",
    "    })\n",
    "    df_remove_overlap_tobesaved.selected_para = df_remove_overlap_tobesaved.selected_para\n",
    "\n",
    "    selected_para = df_remove_overlap_tobesaved.selected_para.map(lambda pp: set(flatten(pp)))\n",
    "    best_match = df_para_wordembed_st.best_match.map(set)\n",
    "\n",
    "    df_comparison = pd.concat([num_para, selected_para, best_match], axis=1).rename(\n",
    "        columns={'para': 'num_para', 'selected_para': 'unsup_selected', 'best_match': 'sup_selected'})\n",
    "    df_comparison['unsup_coverage'] = 100*df_comparison.unsup_selected.map(len) / df_comparison.num_para\n",
    "    df_comparison['sup_coverage'] = 100*df_comparison.sup_selected.map(len) / df_comparison.num_para\n",
    "    \n",
    "    df_comparison['intersection'] = 100*df_comparison.apply(lambda r:\n",
    "        len(r.unsup_selected.intersection(r.sup_selected)) / r.num_para, axis=1)\n",
    "\n",
    "    df_comparison['precision'] = df_comparison.apply(lambda r:\n",
    "        precision(r.unsup_selected, r.sup_selected), axis=1)\n",
    "    df_comparison['recall'] = df_comparison.apply(lambda r:\n",
    "        recall(r.unsup_selected, r.sup_selected), axis=1)\n",
    "    \n",
    "    df_comparison.drop(columns='num_para', inplace=True)\n",
    "    \n",
    "    print(t+'\\n')\n",
    "    print(df_comparison.describe())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize Results Merge Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_overlap_tobesaved = df_merge_overlap.explode('selected_para')\n",
    "df_merge_overlap_tobesaved.selected_para = df_merge_overlap_tobesaved.selected_para.map(list)\n",
    "df_merge_overlap_tobesaved = df_merge_overlap_tobesaved.drop(\n",
    "    columns=['best_para', 'lda_imp', 'overlap_matrix', 'to_be_merged'])\n",
    "\n",
    "df_merge_overlap_tobesaved['selected_para_lda'] = df_merge_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para) if i in row.selected_para], axis=1)\n",
    "\n",
    "df_merge_overlap_tobesaved['para_num_tok'] = df_merge_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para_num_tok) if i in row.selected_para], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1887.000000\n",
       "mean      462.031797\n",
       "std       208.060483\n",
       "min       112.000000\n",
       "25%       309.000000\n",
       "50%       412.000000\n",
       "75%       566.000000\n",
       "max      1255.000000\n",
       "Name: para_num_tok, dtype: float64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_overlap_tobesaved.para_num_tok.map(sum).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_overlap_tobesaved.to_csv(OUTPUT_PATH+'df_lda_merge.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare to Para Wordembed ST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n",
      "\n",
      "       unsup_coverage  sup_coverage  intersection   precision      recall\n",
      "count      453.000000    453.000000    453.000000  453.000000  453.000000\n",
      "mean        49.859914     29.231305     17.270216   34.561435   59.819912\n",
      "std         13.326727     15.454056     11.251706   20.107712   24.554122\n",
      "min         11.250000      2.564103      0.000000    0.000000    0.000000\n",
      "25%         40.243902     18.000000      9.090909   19.050000   42.860000\n",
      "50%         50.000000     25.714286     15.000000   31.030000   60.000000\n",
      "75%         58.333333     38.095238     23.076923   47.620000   77.780000\n",
      "max         88.888889    100.000000     75.000000  100.000000  100.000000\n",
      "\n",
      "\n",
      "th\n",
      "\n",
      "       unsup_coverage  sup_coverage  intersection   precision      recall\n",
      "count      453.000000    453.000000    453.000000  453.000000  453.000000\n",
      "mean        49.859914     63.226644     35.511742   71.746623   56.676777\n",
      "std         13.326727     15.442000     13.324318   20.271281   17.780393\n",
      "min         11.250000     15.384615      1.428571    5.880000    7.690000\n",
      "25%         40.243902     54.166667     26.190476   60.000000   45.450000\n",
      "50%         50.000000     63.414634     35.294118   73.680000   57.140000\n",
      "75%         58.333333     75.000000     44.444444   85.710000   68.420000\n",
      "max         88.888889    100.000000     81.250000  100.000000  100.000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in ['base', 'th']:\n",
    "    df_para_wordembed_st =\\\n",
    "        pd.read_csv(magma_dir+'datasets/karger_books_para_wordembed/'+MODEL+'/st/df_'+t+'_selected_para.csv')\\\n",
    "        .set_index(['book', 'chapter'])\n",
    "    df_para_wordembed_st.para = df_para_wordembed_st.para.map(eval)\n",
    "    df_para_wordembed_st.best_match = df_para_wordembed_st.best_match.map(eval)\n",
    "\n",
    "    num_para = df_para_wordembed_st.para.map(len)\n",
    "\n",
    "    df_merge_overlap_tobesaved = pd.read_csv(OUTPUT_PATH+'df_lda_merge.csv').set_index(['book', 'chapter'])\n",
    "    df_merge_overlap_tobesaved.selected_para = df_merge_overlap_tobesaved.selected_para.map(eval)\n",
    "    df_merge_overlap_tobesaved = df_merge_overlap_tobesaved.groupby(['book', 'chapter'], sort=False).agg({\n",
    "        'selected_para': lambda p: list(p)\n",
    "    })\n",
    "    df_merge_overlap_tobesaved.selected_para = df_merge_overlap_tobesaved.selected_para\n",
    "\n",
    "    selected_para = df_merge_overlap_tobesaved.selected_para.map(lambda pp: set(flatten(pp)))\n",
    "    best_match = df_para_wordembed_st.best_match.map(set)\n",
    "\n",
    "    df_comparison = pd.concat([num_para, selected_para, best_match], axis=1).rename(\n",
    "        columns={'para': 'num_para', 'selected_para': 'unsup_selected', 'best_match': 'sup_selected'})\n",
    "    df_comparison['unsup_coverage'] = 100*df_comparison.unsup_selected.map(len) / df_comparison.num_para\n",
    "    df_comparison['sup_coverage'] = 100*df_comparison.sup_selected.map(len) / df_comparison.num_para\n",
    "    \n",
    "    df_comparison['intersection'] = 100*df_comparison.apply(lambda r:\n",
    "        len(r.unsup_selected.intersection(r.sup_selected)) / r.num_para, axis=1)\n",
    "\n",
    "    df_comparison['precision'] = df_comparison.apply(lambda r:\n",
    "        precision(r.unsup_selected, r.sup_selected), axis=1)\n",
    "    df_comparison['recall'] = df_comparison.apply(lambda r:\n",
    "        recall(r.unsup_selected, r.sup_selected), axis=1)\n",
    "    \n",
    "    df_comparison.drop(columns='num_para', inplace=True)\n",
    "    \n",
    "    print(t+'\\n')\n",
    "    print(df_comparison.describe())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "Yib4JoG59ZhY",
    "S0FByNNOIRvG",
    "tb7fAfzaK4es",
    "JFd0ppeJyX1o",
    "SSd3WuiICKfQ",
    "8fXJEMYaCKfn",
    "z1by1i56qQ0q",
    "UMPw10hNljZ1",
    "WhO2hirsvrcc",
    "W4K-CTF-vtha",
    "IeDlmvn43RsI",
    "z2es0TkqG7Q8",
    "NfsBzMt1OoAP",
    "1x886q72Ggs0",
    "L9AIlf42vH_v",
    "X2xp7jJNwB6b",
    "VndEUBoDjjkV"
   ],
   "name": "topic_modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3a0917c309734495885543d54e7bd8c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d2951197dc524cf398449229119fc731",
       "IPY_MODEL_3607591535e04a18851f3ef1cb96afa1"
      ],
      "layout": "IPY_MODEL_e605902f06844a85be28925d0fc208e4"
     }
    },
    "7aa26b43182d43058c2a04e7cec5cf71": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cf6df9999a93469ab081ff01db781b25",
       "IPY_MODEL_5d28e3032abe4568bc298405b0677d1c"
      ],
      "layout": "IPY_MODEL_b63c21b93cb846999844233e6bdea0c3"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
