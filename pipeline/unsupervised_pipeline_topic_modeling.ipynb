{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 58788,
     "status": "ok",
     "timestamp": 1610359570812,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "TAoI-Sm37yuM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/home/marco/epfl/magma/')\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 59486,
     "status": "ok",
     "timestamp": 1610359571513,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "82WSp6khIcua"
   },
   "outputs": [],
   "source": [
    "MODEL = 'pegasus'\n",
    "\n",
    "RE_SPLITTER = '\\n'              # do we split sentences of paragraphs?\n",
    "                                # use '\\.(?!\\d)|\\n' or '\\n', respectively\n",
    "\n",
    "# Output path\n",
    "OUTPUT_PATH = config.MAGMA_DIR+'pipeline/unsupervised_pipeline_topic_modeling/'+MODEL+'/'\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 59489,
     "status": "ok",
     "timestamp": 1610359571520,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "KJMpUw0Wl8NH"
   },
   "outputs": [],
   "source": [
    "# Topic modeling specific configurations\n",
    "\n",
    "REDUCTION_MAX_LEN = 1024        # maximum length of the LDA/LSI/TextRank reduction\n",
    "\n",
    "STOPWORDS_EXTENSION =\\\n",
    "    ['may', 'might',\n",
    "     'also', 'with',\n",
    "     'without', 'use',\n",
    "     'uses', 'used', 'using']\n",
    "\n",
    "STEMMER = 'snowball'            # name of the stemmer, might use 'porter'\n",
    "\n",
    "N_GRAM = 2                      # the length of n-gram we want to create\n",
    "N_GRAM_MIN_COUNT = 2            # there should be at least N_GRAM_MIN_COUNT\n",
    "                                # repetitions in the text\n",
    "N_GRAM_THRESHOLD = 20           # see gensim.Phrases documentation\n",
    "\n",
    "DIC_NO_BELOW = 3                # keep tokens present in DIC_NO_BELOW+ sentences/paragraphs\n",
    "DIC_NO_ABOVE = 1                # fraction of total corpus size (default: 1)\n",
    "\n",
    "TOP_N = 30                      # number of words to keep for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb7fAfzaK4es"
   },
   "source": [
    "### **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "3a0917c309734495885543d54e7bd8c2",
      "7aa26b43182d43058c2a04e7cec5cf71"
     ]
    },
    "executionInfo": {
     "elapsed": 66395,
     "status": "ok",
     "timestamp": 1610359578432,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "wvbMlPBxk45S",
    "outputId": "b4ac510d-34d9-4940-fa2c-38bdf017e9bd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from textwrap import fill\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "if 'pegasus' in MODEL:\n",
    "    from transformers import PegasusTokenizer\n",
    "    tokenizer =\\\n",
    "        PegasusTokenizer.from_pretrained('google/pegasus-large')\n",
    "elif 'bart' in MODEL:\n",
    "    from transformers import BartTokenizer\n",
    "    tokenizer =\\\n",
    "        BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "elif 't5' in MODEL:\n",
    "    from transformers import T5Tokenizer\n",
    "    tokenizer =\\\n",
    "        T5Tokenizer.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFd0ppeJyX1o"
   },
   "source": [
    "### **Karger Books Base Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Z0lbkScg0a7j"
   },
   "outputs": [],
   "source": [
    "base_dataset = config.MAGMA_DIR+'datasets/karger_books_base/df.csv'\n",
    "df = pd.read_csv(base_dataset)\n",
    "df = df.set_index(['book', 'chapter', 'section', 'subsection'])\n",
    "df.bullets = df.bullets.map(eval, na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSd3WuiICKfQ"
   },
   "source": [
    "### **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "8fXJEMYaCKfn"
   },
   "source": [
    "#### Preprocessing\n",
    "\n",
    "* Split based on RE_SPLITTER\n",
    "* Explode the dataset\n",
    "* Remove unwanted chars at beginning or end of sentence\n",
    "* Remove multiple spaces\n",
    "* Remove long words (> config.TOKEN_MAX_LEN chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "id": "aaYoqzcyCKfp"
   },
   "outputs": [],
   "source": [
    "# Split in sentences / paragraphs based on RE_SPLITTER\n",
    "df.text =\\\n",
    "    df.text.map(lambda x: [p.strip() for p in re.split(RE_SPLITTER, x) if p!=''],\n",
    "                na_action='ignore')\n",
    "    \n",
    "# explode to get one row for each paragraph /sentence\n",
    "df = df.explode('text')\n",
    "df = df.rename(columns={'text': 'para'})\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove unwanted chars at beginning or end of sentence\n",
    "df.para = df.para.map(lambda p: p.lstrip('.,;:-)] \\n'))\n",
    "df.para = df.para.map(lambda p: p.rstrip('.,;:-([ \\n'))\n",
    "\n",
    "# Remove multiple spaces\n",
    "df.para = df.para.map(lambda p:\n",
    "    re.sub('\\s+', ' ', p).strip())\n",
    "\n",
    "# Remove long words (> config.TOKEN_MAX_LEN chars)\n",
    "def para2words(para):\n",
    "    return gensim.utils.simple_preprocess(\n",
    "        para, deacc=True, max_len=config.TOKEN_MAX_LEN)\n",
    "df['para_proc'] = df.para.map(para2words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Further Preprocessing\n",
    "\n",
    "* Remove stop words\n",
    "* Remove short sentences / paragraphs (< config.PARA_MIN_LEN tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "df.para_proc = df.para_proc.map(lambda p:\n",
    "    [w for w in p if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove short sentences / paragraphs (< config.PARA_MIN_LEN tokens)\n",
    "df.loc[df.para_proc.map(len) <\\\n",
    "    config.PARA_MIN_LEN, 'para_proc'] = np.nan\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.para = df.para.map(lambda p: p+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpaitY9dlTM5"
   },
   "source": [
    "### **Topic Modeling Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gKBuLTt5qQ1Z"
   },
   "outputs": [],
   "source": [
    "# Stem\n",
    "if 'port' in STEMMER:\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    st = PorterStemmer()\n",
    "elif 'snow' in STEMMER:\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    st = SnowballStemmer('english')\n",
    "\n",
    "df.para_proc = df.para_proc.map(lambda p:\n",
    "    [st.stem(w) for w in p], na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QHKOznWOqQ1i"
   },
   "outputs": [],
   "source": [
    "# Create n-grams (N_GRAM)\n",
    "data_words = df.para_proc.dropna().values.tolist()\n",
    "\n",
    "if N_GRAM == 2:\n",
    "    bigram = gensim.models.Phrases(\n",
    "        data_words,\n",
    "        min_count=N_GRAM_MIN_COUNT,\n",
    "        threshold=N_GRAM_THRESHOLD)\n",
    "\n",
    "    df.para_proc = df.para_proc.map(lambda p:\n",
    "        [b for b in bigram[p]], na_action='ignore')\n",
    "    \n",
    "elif N_GRAM == 3:\n",
    "    trigram = gensim.models.Phrases(\n",
    "        bigram[data_words],\n",
    "        min_count=N_GRAM_MIN_COUNT,\n",
    "        threshold=N_GRAM_THRESHOLD)\n",
    "    \n",
    "    df.para_proc = df.para_proc.map(lambda p:\n",
    "        [b for b in trigram[bigram[p]]], na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMPw10hNljZ1"
   },
   "source": [
    "#### Dictionary (DIC_NO_BELOW, DIC_NO_ABOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14060,
     "status": "ok",
     "timestamp": 1610110852799,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "OaMCDRcNlW9M",
    "outputId": "3f9bb004-e958-42d8-9970-b2ac462fa6f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/ipykernel_launcher.py:11: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary for topic model (DIC_NO_BELOW, DIC_NO_ABOVE)\n",
    "book_ch_comb = set(zip(df.index.get_level_values(0),\n",
    "    df.index.get_level_values(1)))\n",
    "\n",
    "id2word = {}\n",
    "for book, ch in book_ch_comb:\n",
    "    if book not in id2word:\n",
    "        id2word[book] = {}\n",
    "\n",
    "    id2word[book][ch] = gensim.corpora.Dictionary(\n",
    "        df.loc[book, ch].para_proc.dropna().values.tolist() )\n",
    "\n",
    "    id2word[book][ch].filter_extremes(\n",
    "        no_below = DIC_NO_BELOW, no_above = DIC_NO_ABOVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhO2hirsvrcc"
   },
   "source": [
    "#### LDA\n",
    "\n",
    "https://radimrehurek.com/gensim_3.8.3/models/ldamodel.html\n",
    "\n",
    "https://www.di.ens.fr/~fbach/mdhnips2010.pdf\n",
    "\n",
    "Keep N_TOP words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "R1QAj1x72aqb"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "def get_lda_model(df, book, ch):\n",
    "    corpus = df.loc[book, ch].para_proc.map(id2word[book][ch].doc2bow,\n",
    "        na_action='ignore').dropna().values.tolist()\n",
    "\n",
    "    return gensim.models.ldamodel.LdaModel(\n",
    "        corpus = corpus,\n",
    "        num_topics = 1,\n",
    "        id2word = id2word[book][ch],\n",
    "        alpha = 'auto',\n",
    "        random_state = config.SEED)\n",
    "\n",
    "lda_word2prob = {}\n",
    "for book, ch in book_ch_comb:       \n",
    "    if book not in lda_word2prob:\n",
    "        lda_word2prob[book] = {}\n",
    "\n",
    "    lda_word2prob[book][ch] = dict(\\\n",
    "        get_lda_model(df, book, ch).show_topic(0, TOP_N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeDlmvn43RsI"
   },
   "source": [
    "### **Paragraph importance**\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{importance}(p) = \\frac{\\sum_{w \\in p} \\text{probability}(w)}{\\sum_{w \\in p} 1}\n",
    "\\end{equation}\n",
    "\n",
    "Where $p$ is a paragraph or a sentence, $w$ is a word (token) and $\\text{probability}$ is the probability assigned by LDA or LSI model to $w$ (0 if it is not present in the TOP_N words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42565,
     "status": "ok",
     "timestamp": 1610110906175,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "EnODsA484Zrr",
    "outputId": "0f9dbf85-cbb8-4e7b-a70b-07e3ec9fda5c"
   },
   "outputs": [],
   "source": [
    "def word_importance(model_word2prob, word):\n",
    "    return model_word2prob.get(word, 0)\n",
    "\n",
    "def para_importance(model_word2prob, para):\n",
    "    l_importance = [word_importance(model_word2prob, w) for w in para]\n",
    "    return  np.sum(l_importance) / len(l_importance)\n",
    "\n",
    "df['lda_imp'] = np.nan\n",
    "df['lda_imp_norm'] = np.nan\n",
    "for book, ch in book_ch_comb:\n",
    "    idx_slice = pd.IndexSlice[book, ch, :, :]\n",
    "    # getting LDA and LSI importance\n",
    "    df.loc[idx_slice, 'lda_imp'] = df.loc[idx_slice, 'para_proc'].map(lambda p:\n",
    "        para_importance(lda_word2prob[book][ch], p), na_action='ignore')\n",
    "\n",
    "    # normalizing\n",
    "    s = df.loc[idx_slice, 'lda_imp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_sec_bul = df.groupby(['book', 'chapter', 'section'], sort=False).agg({\n",
    "    'para': lambda p: list(p),\n",
    "    'bullets': lambda b: list(b)[0]\n",
    "}).groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'para': lambda p: len(list(p)),\n",
    "    'bullets': lambda b: len(list(b)[0])\n",
    "}).rename(columns={'para':'num_sec', 'bullets':'num_bul'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_best_para = df.reset_index(level=[2, 3]).groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'section': lambda s: list(s),\n",
    "    'subsection': lambda ss: list(ss),\n",
    "    'para': lambda p: list(p),\n",
    "    'bullets': lambda b: list(b)[0],\n",
    "    'para_proc': lambda pp: list(pp),\n",
    "    'lda_imp': lambda lda: list(lda)\n",
    "})\n",
    "df_best_para.lda_imp = df_best_para.lda_imp.map(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_para['num_sec'] = 0\n",
    "for idx in df_best_para.index.tolist():\n",
    "    df_best_para.loc[idx, 'num_sec'] = int(df_num_sec_bul.loc[idx, 'num_sec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find Best Paragraph for each Book, Chapter (and Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_para['best_para'] = df_best_para.apply(lambda row: np.argsort(row.lda_imp)[::-1][:row.num_sec], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How Many Sections Are We Covering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.000000\n",
       "mean      62.602047\n",
       "std       17.849079\n",
       "min       20.000000\n",
       "25%       50.000000\n",
       "50%       62.500000\n",
       "75%       75.000000\n",
       "max      100.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_diversity(r):\n",
    "    best_idx = r.best_para\n",
    "    all_sections = set(r.section)\n",
    "    \n",
    "    selected_sections = set([r.section[i] for i in best_idx])\n",
    "    \n",
    "    return len(selected_sections.intersection(all_sections))/len(all_sections)*100\n",
    "    \n",
    "df_best_para.apply(calculate_diversity, axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expanding from Best Paragraph Based on Paragraph Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_para['para_num_tok'] =\\\n",
    "    df_best_para.para.map(lambda ps: np.array([len(tokenizer.tokenize(p)) for p in ps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:01<00:00, 298.35it/s]\n"
     ]
    }
   ],
   "source": [
    "def expand_based_on_importance(r):\n",
    "    max_length = len(r.para)\n",
    "    max_idx = max_length-1\n",
    "    \n",
    "    extracted_para = []\n",
    "    \n",
    "    # Calculate the fraction we need to extract\n",
    "    # based on total number of tokens in this chp\n",
    "    # and number of centroids (sections) in this chp\n",
    "    # do not go over the model max length\n",
    "    num_tok_tot = sum(r.para_num_tok)\n",
    "    num_tok_th = min(\n",
    "        int(0.8*num_tok_tot / len(r.best_para)),\n",
    "        0.9*tokenizer.model_max_length)\n",
    "    \n",
    "    for i, best in enumerate(r.best_para):\n",
    "        merged_para_idx = [best]\n",
    "        num_tok = r.para_num_tok[best]\n",
    "        \n",
    "        while num_tok < num_tok_th:\n",
    "            if len(merged_para_idx) == max_length : break\n",
    "            elif 0 in merged_para_idx:\n",
    "                merged_para_idx.append(max(merged_para_idx)+1)\n",
    "            elif max_idx in merged_para_idx:\n",
    "                merged_para_idx.append(min(merged_para_idx)-1)\n",
    "            else:\n",
    "                if (r.lda_imp)[min(merged_para_idx)-1] <\\\n",
    "                    (r.lda_imp)[max(merged_para_idx)+1]:\n",
    "                    merged_para_idx.append(max(merged_para_idx)+1)\n",
    "                else:\n",
    "                    merged_para_idx.append(min(merged_para_idx)-1)\n",
    "            num_tok = np.sum(r.para_num_tok[merged_para_idx])\n",
    "                  \n",
    "        extracted_para.append(sorted(merged_para_idx))\n",
    "        \n",
    "    return extracted_para\n",
    "\n",
    "df_best_para['selected_para'] =\\\n",
    "    df_best_para.progress_apply(expand_based_on_importance, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Study Overlap and Remove Useless (>90% overlap) Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_remove_overlap = df_best_para.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_overlap_matrix(r):\n",
    "    num_sec = len(r.selected_para)\n",
    "    overlap_matrix = np.zeros((num_sec,num_sec))\n",
    "    \n",
    "    def list_overlap(a, b):\n",
    "        return list( set(a).intersection(set(b)) )\n",
    "    \n",
    "    for i in range(num_sec):\n",
    "        for j in range(num_sec):\n",
    "            if i == j : continue\n",
    "            num_tok_i = np.sum(r.para_num_tok[r.selected_para[i]])\n",
    "            overlap = list_overlap(\n",
    "                r.selected_para[i], r.selected_para[j])\n",
    "            num_tok_overlap = np.sum(r.para_num_tok[overlap])\n",
    "            assert num_tok_overlap <= num_tok_i\n",
    "            \n",
    "            overlap_matrix[i, j] = round(num_tok_overlap/num_tok_i*100, 2)\n",
    "    \n",
    "    return overlap_matrix\n",
    "\n",
    "def remove_big_overlap(r, threshold):\n",
    "    om = r.overlap_matrix\n",
    "    big_overlap_idx = np.argwhere(om >= threshold)\n",
    "    to_be_removed = set()\n",
    "    for idx in big_overlap_idx:\n",
    "        i, j = idx[0], idx[1]\n",
    "        if om[i, j] == om[j, i]:\n",
    "            if i in to_be_removed or j in to_be_removed : continue\n",
    "            else : to_be_removed.add(i)\n",
    "        elif om[i, j] > om[j, i]:\n",
    "            to_be_removed.add(i)\n",
    "        else:\n",
    "            to_be_removed.add(j)\n",
    "    return [s for i, s in enumerate(r.selected_para) if i not in to_be_removed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.000000\n",
       "mean       4.942605\n",
       "std        4.443477\n",
       "min        0.000000\n",
       "25%        2.000000\n",
       "50%        4.000000\n",
       "75%        6.000000\n",
       "max       32.000000\n",
       "Name: overlap_matrix, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove_overlap['overlap_matrix'] = df_remove_overlap.apply(lambda row: create_overlap_matrix(row), axis=1)\n",
    "df_remove_overlap.overlap_matrix.map(lambda om: np.sum(om > 90)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.0\n",
       "mean       0.0\n",
       "std        0.0\n",
       "min        0.0\n",
       "25%        0.0\n",
       "50%        0.0\n",
       "75%        0.0\n",
       "max        0.0\n",
       "Name: overlap_matrix, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove_overlap.selected_para = df_remove_overlap.apply(lambda row: remove_big_overlap(row, 90), axis=1)\n",
    "\n",
    "df_remove_overlap['overlap_matrix'] = df_remove_overlap.apply(lambda row: create_overlap_matrix(row), axis=1)\n",
    "df_remove_overlap.overlap_matrix.map(lambda om: np.sum(om > 90)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Merge when >90% overlap Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_merge_overlap = df_best_para.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_big_overlap(r, threshold):\n",
    "    om = r.overlap_matrix\n",
    "    big_overlap_idx = np.argwhere(om >= threshold)\n",
    "    big_overlap_idx = set([frozenset(t) for t in big_overlap_idx])\n",
    "    merged = set()\n",
    "    to_be_merged = set()\n",
    "    for idx in big_overlap_idx:\n",
    "        idx = tuple(idx)\n",
    "        i, j = idx[0], idx[1]\n",
    "        if i not in merged and j not in merged:\n",
    "            to_be_merged.add(idx)\n",
    "            merged.add(i)\n",
    "            merged.add(j)\n",
    "    return to_be_merged\n",
    "\n",
    "def merge_para(r):\n",
    "    new_selected_para = np.empty((len(r.selected_para),), dtype=object)\n",
    "    for i, j in r.to_be_merged:\n",
    "        new_selected_para[i] = np.array(list(set(\n",
    "            np.concatenate((r.selected_para[i], r.selected_para[j])))))\n",
    "        \n",
    "        new_selected_para[j] = []\n",
    "        \n",
    "    selected_para = []\n",
    "    for i, x in enumerate(new_selected_para):\n",
    "        if x is None:\n",
    "            selected_para.append(np.array(r.selected_para[i]))\n",
    "        elif len(x) == 0 : continue\n",
    "        else:\n",
    "            selected_para.append(x)\n",
    "    return selected_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para to be merged: 684\n",
      "Para to be merged: 182\n",
      "Para to be merged: 11\n",
      "Para to be merged: 0\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    df_merge_overlap['overlap_matrix'] = df_merge_overlap.apply(create_overlap_matrix, axis=1)\n",
    "\n",
    "    df_merge_overlap['to_be_merged'] = df_merge_overlap.apply(lambda row: find_big_overlap(row, 90), axis=1)\n",
    "\n",
    "    num_to_be_merged = df_merge_overlap.to_be_merged.map(len).sum()\n",
    "    print('Para to be merged: %d'%num_to_be_merged)\n",
    "    if (num_to_be_merged <= 0) : break\n",
    "\n",
    "    df_merge_overlap.selected_para = df_merge_overlap.apply(merge_para, axis=1)\n",
    "    \n",
    "df_merge_overlap.selected_para = df_merge_overlap.selected_para.map(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize Results Remove Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove_overlap_tobesaved = df_remove_overlap.explode('selected_para')\n",
    "df_remove_overlap_tobesaved = df_remove_overlap_tobesaved.drop(\n",
    "    columns=['best_para', 'lda_imp', 'overlap_matrix'])\n",
    "\n",
    "df_remove_overlap_tobesaved['selected_para_lda'] = df_remove_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para) if i in row.selected_para], axis=1)\n",
    "\n",
    "df_remove_overlap_tobesaved['para_num_tok'] = df_remove_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para_num_tok) if i in row.selected_para], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1767.000000\n",
       "mean      424.793435\n",
       "std       193.049125\n",
       "min        87.000000\n",
       "25%       285.000000\n",
       "50%       378.000000\n",
       "75%       520.000000\n",
       "max      1172.000000\n",
       "Name: para_num_tok, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove_overlap_tobesaved.para_num_tok.map(sum).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove_overlap_tobesaved.to_csv(OUTPUT_PATH+'df_lda_remove.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare to Para Wordembed ST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(pred, ref):\n",
    "    return round(100*len(pred.intersection(ref)) / len(pred), 2)\n",
    "    \n",
    "def recall(pred, ref):\n",
    "    return round(100*len(pred.intersection(ref)) / len(ref), 2)\n",
    "\n",
    "def fmeasure(prec, rec):\n",
    "    if prec + rec == 0 : return 0\n",
    "    return round(2*prec*rec/(prec+rec), 2)\n",
    "\n",
    "flatten = lambda t: [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       unsup_coverage  sup_coverage  intersection   precision      recall\n",
      "count      453.000000    453.000000    453.000000  453.000000  453.000000\n",
      "mean        57.335688     29.168566     18.719701   32.210905   62.856247\n",
      "std         12.989680     15.464656     12.314824   19.255536   23.063081\n",
      "min         14.925373      2.564103      0.000000    0.000000    0.000000\n",
      "25%         48.888889     18.095238      9.722222   17.650000   50.000000\n",
      "50%         57.142857     25.925926     16.666667   29.410000   64.290000\n",
      "75%         65.384615     37.837838     24.390244   42.860000   80.000000\n",
      "max        100.000000    100.000000     68.750000  100.000000  100.000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_para_wordembed_st =\\\n",
    "    pd.read_csv(config.MAGMA_DIR+'datasets/bullet_paragraph_embeddings/'+MODEL+'/st/df_base_selected_para.csv')\\\n",
    "    .set_index(['book', 'chapter'])\n",
    "df_para_wordembed_st.para = df_para_wordembed_st.para.map(eval)\n",
    "df_para_wordembed_st.best_match = df_para_wordembed_st.best_match.map(eval)\n",
    "\n",
    "num_para = df_para_wordembed_st.para.map(len)\n",
    "\n",
    "df_remove_overlap_tobesaved = pd.read_csv(OUTPUT_PATH+'df_lda_remove.csv').set_index(['book', 'chapter'])\n",
    "df_remove_overlap_tobesaved.selected_para = df_remove_overlap_tobesaved.selected_para.map(eval)\n",
    "df_remove_overlap_tobesaved = df_remove_overlap_tobesaved.groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'selected_para': lambda p: list(p)\n",
    "})\n",
    "df_remove_overlap_tobesaved.selected_para = df_remove_overlap_tobesaved.selected_para\n",
    "\n",
    "selected_para = df_remove_overlap_tobesaved.selected_para.map(lambda pp: set(flatten(pp)))\n",
    "best_match = df_para_wordembed_st.best_match.map(set)\n",
    "\n",
    "df_comparison = pd.concat([num_para, selected_para, best_match], axis=1).rename(\n",
    "    columns={'para': 'num_para', 'selected_para': 'unsup_selected', 'best_match': 'sup_selected'})\n",
    "df_comparison['unsup_coverage'] = 100*df_comparison.unsup_selected.map(len) / df_comparison.num_para\n",
    "df_comparison['sup_coverage'] = 100*df_comparison.sup_selected.map(len) / df_comparison.num_para\n",
    "\n",
    "df_comparison['intersection'] = 100*df_comparison.apply(lambda r:\n",
    "    len(r.unsup_selected.intersection(r.sup_selected)) / r.num_para, axis=1)\n",
    "\n",
    "df_comparison['precision'] = df_comparison.apply(lambda r:\n",
    "    precision(r.unsup_selected, r.sup_selected), axis=1)\n",
    "df_comparison['recall'] = df_comparison.apply(lambda r:\n",
    "    recall(r.unsup_selected, r.sup_selected), axis=1)\n",
    "\n",
    "df_comparison.drop(columns='num_para', inplace=True)\n",
    "\n",
    "print(df_comparison.describe())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize Results Merge Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_overlap_tobesaved = df_merge_overlap.explode('selected_para')\n",
    "df_merge_overlap_tobesaved.selected_para = df_merge_overlap_tobesaved.selected_para.map(list)\n",
    "df_merge_overlap_tobesaved = df_merge_overlap_tobesaved.drop(\n",
    "    columns=['best_para', 'lda_imp', 'overlap_matrix', 'to_be_merged'])\n",
    "\n",
    "df_merge_overlap_tobesaved['selected_para_lda'] = df_merge_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para) if i in row.selected_para], axis=1)\n",
    "\n",
    "df_merge_overlap_tobesaved['para_num_tok'] = df_merge_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para_num_tok) if i in row.selected_para], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1767.000000\n",
       "mean      425.120543\n",
       "std       193.163019\n",
       "min        87.000000\n",
       "25%       285.000000\n",
       "50%       378.000000\n",
       "75%       520.000000\n",
       "max      1172.000000\n",
       "Name: para_num_tok, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_overlap_tobesaved.para_num_tok.map(sum).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_overlap_tobesaved.to_csv(OUTPUT_PATH+'df_lda_merge.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare to Para Wordembed ST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       unsup_coverage  sup_coverage  intersection   precision      recall\n",
      "count      453.000000    453.000000    453.000000  453.000000  453.000000\n",
      "mean        57.647969     29.168566     18.815379   32.187837   63.166203\n",
      "std         13.002657     15.464656     12.414590   19.284932   23.238809\n",
      "min         14.925373      2.564103      0.000000    0.000000    0.000000\n",
      "25%         49.253731     18.095238     10.000000   17.650000   50.000000\n",
      "50%         57.377049     25.925926     16.666667   29.410000   64.290000\n",
      "75%         65.740741     37.837838     25.000000   42.860000   80.000000\n",
      "max        100.000000    100.000000     68.750000  100.000000  100.000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_para_wordembed_st =\\\n",
    "    pd.read_csv(config.MAGMA_DIR+'datasets/bullet_paragraph_embeddings/'+MODEL+'/st/df_base_selected_para.csv')\\\n",
    "    .set_index(['book', 'chapter'])\n",
    "df_para_wordembed_st.para = df_para_wordembed_st.para.map(eval)\n",
    "df_para_wordembed_st.best_match = df_para_wordembed_st.best_match.map(eval)\n",
    "\n",
    "num_para = df_para_wordembed_st.para.map(len)\n",
    "\n",
    "df_merge_overlap_tobesaved = pd.read_csv(OUTPUT_PATH+'df_lda_merge.csv').set_index(['book', 'chapter'])\n",
    "df_merge_overlap_tobesaved.selected_para = df_merge_overlap_tobesaved.selected_para.map(eval)\n",
    "df_merge_overlap_tobesaved = df_merge_overlap_tobesaved.groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'selected_para': lambda p: list(p)\n",
    "})\n",
    "df_merge_overlap_tobesaved.selected_para = df_merge_overlap_tobesaved.selected_para\n",
    "\n",
    "selected_para = df_merge_overlap_tobesaved.selected_para.map(lambda pp: set(flatten(pp)))\n",
    "best_match = df_para_wordembed_st.best_match.map(set)\n",
    "\n",
    "df_comparison = pd.concat([num_para, selected_para, best_match], axis=1).rename(\n",
    "    columns={'para': 'num_para', 'selected_para': 'unsup_selected', 'best_match': 'sup_selected'})\n",
    "df_comparison['unsup_coverage'] = 100*df_comparison.unsup_selected.map(len) / df_comparison.num_para\n",
    "df_comparison['sup_coverage'] = 100*df_comparison.sup_selected.map(len) / df_comparison.num_para\n",
    "\n",
    "df_comparison['intersection'] = 100*df_comparison.apply(lambda r:\n",
    "    len(r.unsup_selected.intersection(r.sup_selected)) / r.num_para, axis=1)\n",
    "\n",
    "df_comparison['precision'] = df_comparison.apply(lambda r:\n",
    "    precision(r.unsup_selected, r.sup_selected), axis=1)\n",
    "df_comparison['recall'] = df_comparison.apply(lambda r:\n",
    "    recall(r.unsup_selected, r.sup_selected), axis=1)\n",
    "\n",
    "df_comparison.drop(columns='num_para', inplace=True)\n",
    "\n",
    "print(df_comparison.describe())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "Yib4JoG59ZhY",
    "S0FByNNOIRvG",
    "tb7fAfzaK4es",
    "JFd0ppeJyX1o",
    "SSd3WuiICKfQ",
    "8fXJEMYaCKfn",
    "z1by1i56qQ0q",
    "UMPw10hNljZ1",
    "WhO2hirsvrcc",
    "W4K-CTF-vtha",
    "IeDlmvn43RsI",
    "z2es0TkqG7Q8",
    "NfsBzMt1OoAP",
    "1x886q72Ggs0",
    "L9AIlf42vH_v",
    "X2xp7jJNwB6b",
    "VndEUBoDjjkV"
   ],
   "name": "topic_modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3a0917c309734495885543d54e7bd8c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d2951197dc524cf398449229119fc731",
       "IPY_MODEL_3607591535e04a18851f3ef1cb96afa1"
      ],
      "layout": "IPY_MODEL_e605902f06844a85be28925d0fc208e4"
     }
    },
    "7aa26b43182d43058c2a04e7cec5cf71": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cf6df9999a93469ab081ff01db781b25",
       "IPY_MODEL_5d28e3032abe4568bc298405b0677d1c"
      ],
      "layout": "IPY_MODEL_b63c21b93cb846999844233e6bdea0c3"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
