{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 16184,
     "status": "ok",
     "timestamp": 1610463826089,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "ClE5D523OTZG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/home/marco/epfl/magma/')\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 16184,
     "status": "ok",
     "timestamp": 1610463826092,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "82WSp6khIcua"
   },
   "outputs": [],
   "source": [
    "MODEL = 'pegasus'\n",
    "\n",
    "# Output path\n",
    "OUTPUT_PATH = config.MAGMA_DIR+'pipeline/bullet_points_creation_thesis/'+MODEL+'/'\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb7fAfzaK4es"
   },
   "source": [
    "### **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "executionInfo": {
     "elapsed": 16443,
     "status": "ok",
     "timestamp": 1610463826354,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "wvbMlPBxk45S"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from textwrap import fill\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "if 'pegasus' in MODEL:\n",
    "    from transformers import PegasusTokenizer\n",
    "    tokenizer =\\\n",
    "        PegasusTokenizer.from_pretrained('google/pegasus-large')\n",
    "elif 'bart' in MODEL:\n",
    "    from transformers import BartTokenizer\n",
    "    tokenizer =\\\n",
    "        BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "elif 't5' in MODEL:\n",
    "    from transformers import T5Tokenizer\n",
    "    tokenizer =\\\n",
    "        T5Tokenizer.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQGq4WLu3Gei"
   },
   "source": [
    "### **Thesis Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "Z0lbkScg0a7j"
   },
   "outputs": [],
   "source": [
    "text_thesis = [\n",
    "\"\"\"\n",
    "Introdution. This thesis is about the Master's Project in industry I conducted at Magma\n",
    "Learning Sàrl from the 28th of September 2020 to the 26th of March 2021. The Master's Project\n",
    "was supervised by Dr. Maxime Gabella, Founder and CEO of Magma Learning,\n",
    "and Professor Martin Jaggi, Tenure Track Assistant Professor at the Machine\n",
    "Learning and Optimization Laboratory (MLO) at the École polytechnique fédérale\n",
    "de Lausanne (EPFL).\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Background. Magma Learning Sàrl is a young start-up created in 2019 in Switzerland, it has\n",
    "the mission to radically enhance learning thanks to artificial intelligence. It is\n",
    "set up as a multidisciplinary research project to understand how humans learn,\n",
    "how machines learn, and how they can learn from each other. The company is\n",
    "currently active in enhancing both corporate and education learning through\n",
    "an AI tutor called ARI 9000 which comes in the form of a mobile and web\n",
    "application. It automatically generates micro-learning content based on training\n",
    "material; adapts to the interests, knowledge level and memory abilities of the user; consolidates long-term retention and gives\n",
    "visual feedbacks on the learning progress. ARI 9000 exploits machine learning\n",
    "techniques, in particular Natural Language Processing (NLP), to create exercises,\n",
    "puzzles and topic modeling visualizations.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The MLO is active in the field of machine learning, optimization algorithms and\n",
    "text understanding, as well as several other application domains. The alignment\n",
    "of the research interests of the laboratory with those of Magma Learning Sàrl in\n",
    "the field of NLP is the starting point for their collaboration to supervise the Master's Project.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "NLP is a discipline spanning from linguistic to computer science. It concerns\n",
    "the interaction between computers and human language, in particular how to\n",
    "automatically process, analyze and generate natural language dataII . The result is\n",
    "a computer capable of understanding the lexical, syntactic and semantic levels of\n",
    "language. Some examples of application are summarization, automatic speech\n",
    "recognition, grammatical error correction, machine translation, part-of-speech\n",
    "tagging. The most studied tongue in NLP is English.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Problem Statement. ARI 9000 produces personalized exercises thanks to a variety of algorithms.\n",
    "Figure 1.1 presents an exercise taken directly from the app. The starting material\n",
    "is either input by the user or scraped from the net and it comes in the form of\n",
    "natural language. A good example is a textbook. One key component in the\n",
    "creation of such exercises is the extraction of key passages from the material,\n",
    "which is often long and complex. This can be achieved with Automatic Text\n",
    "Summarization (ATS).\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The main objective of an ATS system is to produce a summary that includes\n",
    "the main concepts in the input document in less words and to keep repetition to\n",
    "a minimum (Moratanch & Chitrakala, 2017; Radev et al, 2002). According to El-Kassas et al (2020), ATS systems are designed by applying extractive, abstractive\n",
    "or hybrid summarization. The extractive approach selects the most important\n",
    "sentences from the input text and uses them to generate the summary. The\n",
    "abstractive approach represents the input text in an intermediate form then\n",
    "generates the summary with words and sentences that differ from the original\n",
    "text. The hybrid approach combines both extractive and abstractive approaches.\n",
    "ATS poses many challenges to the research and industry communities, such as\n",
    "identification of the most informative segments in the input text to be included in\n",
    "the generated summary, summarization of long documents like books, evaluation\n",
    "of the computer-generated summary, generation of an abstractive summary\n",
    "similar to a human-produced one (El-Kassas et al, 2020).\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Deep learning has been successfully applied to various NLP tasks. The ATS\n",
    "problem is commonly solved by a Sequence-to-Sequence (Seq2Seq) model, this\n",
    "type of model takes a sequence of items (e.g. words, letters, tokens, ...) as input\n",
    "and outputs another sequence of items which makes up the summary. Recurrent\n",
    "Neural Network (RNN), Long Short-Term Memory (LSTM) and gated recurrent\n",
    "neural networks have been firmly established as state of the art approaches\n",
    "in sequence modeling such as ATS. A revolutionary new approach has been\n",
    "introduced by Vaswani et al (2017), who proposed the Transformer, a model\n",
    "architecture avoiding recurrence and instead relying entirely on an attention\n",
    "mechanism to draw global dependencies between input and output, which allows\n",
    "for significantly more parallelization. Transformers are pervasive and have made\n",
    "tremendous impact in many fields such as NLP and image processing. The\n",
    "attention mechanism is a key defining characteristic of Transformer models.\n",
    "However, a well-known concern with attention is the quadratic time and memory\n",
    "complexity, which can mine model scalability in many settings, such as ATS of\n",
    "long documents (Tay et al, 2020).\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The evaluation of the computer-generated summaries is another important\n",
    "point in ATS. Even simple manual evaluation of summaries on a large scale\n",
    "over a few linguistic quality questions and content coverage as in the Document\n",
    "Understanding Conference (DUC) (Over, 2003) would require over 3,000 hours of\n",
    "human efforts (Lin, 2004). Therefore, how to evaluate summaries automatically\n",
    "is a key research aspect of ATS.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The Master's Project has the objective to explore the application of ATS systems, with an accent on Transformer models, to automatically summarize and evaluate long text\n",
    "document in industrial environment. In particular, it tackles the main problem of\n",
    "Automatic Text Summarization while facing the issues of long input documents,\n",
    "quadratic time and space complexity of the attention mechanism and automatic\n",
    "evaluation of computer-generated summaries.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Literature Review. In this Chapter the theoretical concepts which make up the foundations of the\n",
    "Master's Project are explained. Section 2.1 introduces the self-attention mechanism, Section\n",
    "2.2 presents the most popular Transformer models in ATS and Section 2.3 focuses\n",
    "on the evaluation metrics in ATS.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Self-Attention and Transformer. The attention mechanism introduced by Vaswani et al (2017) is the self-attention,\n",
    "also known as intra-attention. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual\n",
    "entailment and learning task-independent sentence representations (Vaswani\n",
    "et al, 2017). The Transformer is the first model relying entirely on self-attention\n",
    "to compute representations of its input and output without using RNN or convolution.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "In the Transformer, the encoder maps an input sequence of symbol representations (x1 , ..., xn) to a sequence of continuous representations z = (z1, ..., zn).\n",
    "Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols\n",
    "one element at a time. At each step the model is auto-regressive, consuming\n",
    "the previously generated symbols as additional input when generating the next.\n",
    "The Transformer follows this overall architecture using stacked self-attention and\n",
    "point-wise, fully connected layers for both the encoder and decoder, shown in\n",
    "the left and right halves of Figure 2.1, respectively.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Encoder and Decoder Stacks\n",
    "The encoder is composed of a stack of N identical layers. Each layer has two\n",
    "sub-layers. The first is a multi-head self-attention mechanism, and the second\n",
    "is a simple, position-wise fully connected feed-forward network. This consist\n",
    "of two linear transformations with an activation function in between, which is\n",
    "generally ReLU:\n",
    "FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "The linear transformations use different parameters from layer to layer. Residual\n",
    "connection is employed around each of the two sub-layers, followed by layer\n",
    "normalization.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The decoder is also composed of a stack of N identical layers. In addition to the\n",
    "two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which\n",
    "performs multi-head attention over the output of the encoder stack. Residual\n",
    "connections around each of the sub-layers, followed by layer normalization, are\n",
    "used. The self-attention sub-layer in the decoder stack is masked to prevent\n",
    "positions from attending to subsequent positions. This masking, combined with\n",
    "the fact that the output embeddings are offset by one position, ensures that the\n",
    "predictions for position i can depend only on the known outputs at positions\n",
    "less than i . All sub-layers in the model, as well as the embedding layers, produce\n",
    "outputs of dimension dmodel , which is set to 512 by Vaswani et al (2017).\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Multi-Head Attention.\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values and output are vectors.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The most popular attention mechanism is the one introduced by Vaswani et al.\n",
    "(2017) and it is called Scaled Dot-Product Attention. The input consists of queries\n",
    "and keys of dimension dk and values of dimension dv . The weights on the values\n",
    "are computed by applying softmax to the dot products of the query with all keys,\n",
    "each divided by sqrt(dk) . In practice, the attention function is calculated on a set of\n",
    "queries simultaneously, stacked together into a matrix Q. The keys and values\n",
    "are also stacked into matrices K and V :\n",
    "The matrix multiplication QK T must be integrally computed and stored. Thus,\n",
    "the time and memory complexity is O(n2)\n",
    ", where n is the input length (e.g.\n",
    "number of tokens in the input text document). This is the source of time and\n",
    "GPU memory constraints encountered during the Master's Project, as in Section\n",
    "3.4.4.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The query, key and value vectors are abstractions that are useful for calculating\n",
    "and thinking about attention. As an example, the process is described and visualized in Figure 2.3 for the sentence \"Thinking Machines\". First, a query, a key and\n",
    "a value vector are created from the words' embeddings. Now, the self-attention is\n",
    "calculated for each word. Focusing on the \"Thinking\" word, all words of the input\n",
    "sentence are scored against this word. The score is calculated by taking the dot\n",
    "product of the query vector with the key vector of the respective word to score\n",
    "(q1*kk, q1*k2). The score determines how much focus to place on other parts of\n",
    "the\n",
    " input\n",
    " while \"Thinking\" is encoded. Then, the results are divided by a constant\n",
    "3p  ́dk and passed through softmax. Then, the softmax result is multiplied by\n",
    "the value vector. The intuition here is to keep intact the values of the important\n",
    "words and drown-out irrelevant word. Figure 2.3 shows how the value vector of\n",
    "\"Machines\" is kept less into consideration because its softmax value is only 0.12,\n",
    "thus vector v2 is more transparent than v1. Finally, the results are summed.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Instead of performing a single attention function with dmodel-dimensional keys,\n",
    "values and queries, Vaswani et al (2017) found it beneficial to linearly project\n",
    "the queries, keys and values h times with different, learned linear projections to\n",
    "dk , dk and dv dimensions, respectively. On each of these projected versions of\n",
    "queries, keys and values the attention function is performed in parallel, yielding\n",
    "dv-dimensional output values. These are concatenated and once again projected,\n",
    "resulting in the final values, as depicted in the left side of Figure 2.2. The right side\n",
    "of Figure 2.2 shows multi-head attention, which allows the model to jointly attend\n",
    "to information from different representation subspaces at different positions.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The Transformer uses multi-head attention in three different ways:\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "In encoder-decoder attention layers, depicted in the top right orange box\n",
    "of Figure 2.1, the queries come from the previous decoder layer, and the\n",
    "keys and values come from the output of the encoder. This allows every\n",
    "position in the decoder to attend over all positions in the input sequence.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The encoder contains self-attention layers, depicted in the left orange box\n",
    "of Figure 2.1. In a self-attention layer all of the keys, values and queries\n",
    "come from the same place, in this case, the output of the previous layer in\n",
    "the encoder.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Similarly, self-attention layers in the decoder, depicted in the bottom right\n",
    "orange box of Figure 2.1, allow each position in the decoder to attend to\n",
    "all positions in the decoder up to and including that position. Leftward\n",
    "information flow is prevented in the decoder to preserve the auto-regressive\n",
    "property. This is implemented inside of scaled dot-product attention by\n",
    "masking out (setting to −inf) all values in the input of the softmax which\n",
    "correspond to illegal connections.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Embeddings and Positional Encodings.\n",
    "The Transformer uses learned embeddings to convert the input tokens and\n",
    "output tokens to vectors of dimension dmodel. It also uses the usual learned\n",
    "linear transformation and softmax function to convert the decoder output to\n",
    "predicted next-token probabilities.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Since the Transformer does not contain recurrence and convolution, in order\n",
    "for the model to make use of the order of the sequence, positional encoding is\n",
    "added to the input embeddings before the encoder and decoder stack, as seen\n",
    "in Figure 2.1. The positional encodings have the same dimension dmodel as\n",
    "the embeddings, so that the two can be summed. Sine and cosine functions of\n",
    "different frequencies are typically used.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Unsupervised Pre-Training and Fine-Tuning\n",
    "Training a Transformer model to perform NLP tasks often requires that the\n",
    "model can process text in a way that is similar to downstream learning. This\n",
    "can be viewed as developing a general-purpose knowledge that allows the model\n",
    "to understand the text. To this end, Transformer models are pre-trained on a\n",
    "huge number (see Section 2.2.1) of language tasks to develop abilities which\n",
    "can then be transferred, thanks to fine-tuning, to downstream tasks, such as\n",
    "summarization.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Pre-training is typically done using unsupervised learning on unlabeled data. A\n",
    "Transformer model is pre-trained by corrupting documents and then optimizing\n",
    "the cross-entropy between the decoder's output and the original document. The\n",
    "original document is both the training reference and the input of the decoder. To\n",
    "remember is that in the decoder leftward information flow is prevented, as introduced in Section 2.1.2. If leftward information flow is not prevented, the decoder\n",
    "would just learn to copy the text from its input. A visual example of a generalized\n",
    "unsupervised pre-training can be seen in Figure 2.4, where a corrupted document is bidirectionally encoded and then the likelihood of the original document\n",
    "is calculated with the auto-regressive decoder. Transformer models also differ\n",
    "based on the pre-training technique they employ, as Section 2.2.1 explains later.\n",
    "The representations produced with pre-training can be used in several ways\n",
    "for downstream applications. Fine-tuning the model to perform a downstream\n",
    "task is supervised and requires a labeled dataset. In ATS the label is the human-generated summary. Because Transformers have an auto-regressive decoder, they\n",
    "can be directly fine-tuned for sequence generation tasks such as summarization.\n",
    "Here, the encoder input is the text document, and the decoder generates outputs\n",
    "auto-regressively. In other words, the decoder generates one token at a time. This\n",
    "process is closely related to the denoising pre-training objective. During pre-training the decoder's input and the training reference is the original document,\n",
    "while during fine-tuning the decoder's input and the training reference is the\n",
    "human-generated summary.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Transformer From Input to Output.\n",
    "Following the concrete example of Figure 2.1, this Section explains the step-by-step operations of a Transformer model. The input is a text document and it has\n",
    "length constraints due to the self-attention complexity.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The text is tokenized with either Byte Pair Encoding (BPE) (Sennrich et al, 2015)\n",
    "or SentencePiece, which implements both BPE and unigram language model\n",
    "(Kudo, 2018). The tokens vocabulary, which is made up of words and sub-words\n",
    "and depends on the tokenization technique, keeps spaces and punctuation into\n",
    "account. The vocabulary dimension dvoc is in the order of 104 entries. The\n",
    "tokenized text is a list of numbers where each number corresponds to a token\n",
    "and the length is greater than or equal to the number of words in the input text.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Firstly, the text is tokenized. The tokenized text is a list of numbers where each\n",
    "number corresponds to a token and the length is greater than or equal to the\n",
    "number of words in the input text. The tokens vocabulary, which is made up of\n",
    "words and sub-words and depends on the tokenization technique, keeps spaces\n",
    "and punctuation into account. Its dimension dvoc is in the order of 104 entries.\n",
    "The tokenization technique is either Byte Pair Encoding (BPE) (Sennrich et al.,\n",
    "2015) or SentencePiece, which implements both BPE and unigram language\n",
    "model (Kudo, 2018).\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Next, the tokens are embedded to vectors of dimension dmodel and summed\n",
    "to the positional encodings. While Vaswani et al. (2017) used sinusoidal position\n",
    "signal, it has recently become more common to use relative position encodings\n",
    "(Raffel et al., 2019).\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "At this point, the encoder stack is applied to the embeddings and it outputs\n",
    "vectors of the same dimension dmodel . Also the decoder needs some input to\n",
    "function properly. The first input given to the decoder is a special token called\n",
    "Begin of Sentence (BoS), which communicates to the decoder to start generating text. The BoS token is embedded and fed to the decoder. The second\n",
    "self-attention block in the decoder stack receives also the encoded embeddings\n",
    "of the input text from the encoder. The decoder stack produces the first output of\n",
    "dimension dmodel . The linear transformation and softmax function are used to\n",
    "convert the decoder output to next-token probabilities.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "During fine-tuning, these probabilities are compared to the ground-truth with\n",
    "the cross-entropy loss function. Then, the next inputs of the decoder are the\n",
    "tokens of the reference summary, one at a time. On the other hand, during\n",
    "generation, the most probable token is the first output of the model. This token\n",
    "is then concatenated to the previous tokens (only the BoS token at first) and fed\n",
    "back to the decoder stack. The decoder loop continues as described until the\n",
    "special token End of Sentence (EoS) is generated. The final output of the model\n",
    "is a list of tokens, starting with BoS and ending with EoS.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Finally, to obtain the summary, the tokens are converted into text with the inverse transformation of tokenization. The described generation technique where\n",
    "at each step the output is the most probable token is called greedy decoding. A\n",
    "more sophisticated and powerful method is beam search and it is presented in\n",
    "Section 3.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Lastly, Figure 2.5 shows the complete architecture of the Transformer model in\n",
    "details.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "T5 (Raffel et al., 2019), BART (Lewis et al., 2019) and PEGASUS (Zhang et al.,\n",
    "2020) are the three main Transformer models on which the Master's Project\n",
    "focuses. They are full Seq2Seq generative Transformer models, as opposed to\n",
    "other Transformers which are composed by only one encoder (e.g. BERT) or\n",
    "one decoder (e.g. GPT-3) and are generally employed for other purposes. T5,\n",
    "BART and PEGASUS are the current state-of-the-art on the most common sum-\n",
    "marization datasets, such as CNN/DailyMail (CNN/DM) (Hermann et al., 2015),\n",
    "XSum (Narayan et al., 2018) and arXiv / PubMed (Cohan et al., 2018). T5 can\n",
    "perform a wide variety of English-based NLP problems, including question answering, classification and translation, to name a few. BART is focused on text\n",
    "generation tasks, such as question answering and summarization. PEGASUS\n",
    "only performs summarization and achieves state-of-the-art on 12 renowed downstream datasets.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Due to its increasing ubiquity, all of the presented models are based on the\n",
    "Transformer architecture introduced by Vaswani et al (2017) and do not deviate\n",
    "significantly from it.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Training.\n",
    "All three Transformer models are pre-trained on a huge amount of data with\n",
    "an unsupervised objective. The pre-trained models are available to the public,\n",
    "which makes leveraging transfer learning possible. Generally, pre-training has\n",
    "two stages. First, the text is corrupted with an arbitrary noising function, then\n",
    "a Seq2Seq model is learned to reconstruct the original text thanks to the crossentropy loss.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "T5 is pre-trained on the Colossal Clean Crawled Corpus (C4), a dataset consisting of 750 gigabytes of clean English text scraped from the web (Raffel et al,\n",
    "2019). The objective specifically corrupts contiguous, randomly-spaced spans of\n",
    "tokens. A visual example is shown in Figure 2.3. The mean span length is 3 and\n",
    "the corruption rate is 15% of the original sequence. The model is pre-trained for\n",
    "1 million steps on a batch size of 211 sequences of length 512, corresponding to a\n",
    "total of about 1 trillion tokens.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "BART is pre-trained on the same pre-training data as Liu et al (2019), consisting\n",
    "of 160 gigabytes of news, books, stories and web text. The model is pre-trained\n",
    "by corrupting the input text and then optimizing the cross-entropy between the\n",
    "decoder's output and the original input. The corruption process masks 30% of\n",
    "tokens and permute all sentences. BART is pre-trained for 500,000 steps with a\n",
    "batch size of 8,000 (≈ 213 ).\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "PEGASUS is pre-trained on the mixture of C4 and HugeNews, weighted by\n",
    "their number of examples. HugeNews is a datasets of 1.5 billion articles (3.8\n",
    "terabytes) collected from news and news-like websites. The objective selects\n",
    "and masks whole sentences from the input documents, and concatenate the\n",
    "gap-sentences into a pseudo-summary. The selected sentences are the ones\n",
    "that appear to be important to the document. In more details, the strategy is\n",
    "to select top scored sentences according to importance. Sentence's importance\n",
    "is calculated as the ROUGE-1 metric between the sentence and the rest of the\n",
    "input document. Zhang et al. (2020) showed that this pre-training objective leads\n",
    "to better and faster fine-tuning performance for summarization because more\n",
    "closely resembles the downstream task. The model dynamically chooses gap\n",
    "sentences ratio (the number of selected gap sentences to the total number of\n",
    "sentences in the document) uniformly between 15% and 45%, and importance\n",
    "sentences are stochastically samples with 20% uniform noise on their scores.\n",
    "PEGASUS is pre-trained for 1.5 million steps with a batch size of 213 .\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Low-Resource Summarization.\n",
    "In real-world practice, it is often difficult to collect a large number of supervised\n",
    "examples to train or fine-tune a summarization model. This is the case of the Master's Project,\n",
    "as explained in Section 3.3. To simulate the low-resource summarization setting,\n",
    "Zhang et al (2020) picked the first 10k (k = 1, 2, 3, 4) training examples from each\n",
    "downstream dataset to fine-tune PEGASUS. In 8 out of 12 datasets, with just 100\n",
    "examples, the model could be fine-tuned to generate summaries at comparable\n",
    "quality to a base Transformer model trained on the full supervised datasets (from\n",
    "20,000 to 200,000 examples). PEGASUS also beats previous state-of-the-art results\n",
    "on 6 out of 12 datasets with only 1,000 fine-tuning examples.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    " Long Document Summarization.\n",
    "The Transformer success is partly due to the self-attention component which\n",
    "enables the network to capture contextual information from the entire sequence.\n",
    "While powerful, the memory and computational requirements of self-attention\n",
    "grow quadratically with sequence length, making it infeasible to process long\n",
    "sequences. Many recent models try to address this problem by introducing new\n",
    "attention mechanisms.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "These new attention techniques can be clustered in three main groups. The\n",
    "data-independent patterns, employed by models such BigBird (Zaheer et al,\n",
    "2020) and Longformer (Beltagy et al, 2020), make the attention matrix sparse; the\n",
    "data-dependent patterns, implemented in the Linformer (Wang et al, 2020) and\n",
    "Reformer (Kitaev et al, 2020), compress the attention matrix thanks to Locality-Sensitive Hashing (LSH), pooling and convolution; other mechanisms simplify\n",
    "the attention dot product with a decomposable kernel, such as Linear Transformer (Katharopoulos et al, 2020) and Performer (Choromanski et al, 2020).\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Of particular interest to the Master's Project is the Longformer Encoder Decoder\n",
    "(LED) (Beltagy et al., 2020), which is easily accessible online and it is initialized\n",
    "from BART, since both models share the exact same architecture. The difference is\n",
    "that LED uses the Longformer sparse variant of self-attention, presented in Figure\n",
    "2.7, in the encoder. This self-attention technique has linear memory complexity\n",
    "with respect to the input length because the dilated sliding window computes\n",
    "only a fixed number of the diagonals. The decoder uses the full self-attention\n",
    "because the summary is by definition much shorter than the input text.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "ROUGE Evaluation Metric.\n",
    "Even simple manual evaluation of summaries on a large scale would require incredibly long hours of human efforts. Thus, finding an automatic summarization\n",
    "method is central to the research community and to the Master's Project.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "ROUGE-N.\n",
    "ROUGE-N, where N is typically 1 or 2, is an N-gram measure between a candidate summary and the reference summary. The ROUGE-N precision and recall,\n",
    "respectively pN and rN , are computed as follows:\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Where N stands for the length of the N-gram gramN , S is the reference summary,\n",
    "C is a candidate summary and Countmatch (gramN) is the maximum number of\n",
    "N-grams co-occurring in C and S. Then, the F score is.\n",
    "Higher ROUGE-N precision, recall or F score measures translate into a better\n",
    "quality summary.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "A sequence Z = [z1 , z2 , ..., zn ] is a subsequence of another sequence X =\n",
    "[x1 , x2, ..., xm ] if there exists a strict increasing sequence [i1 , i2 , ..., ik ] of indices of\n",
    "X such that for all j = 1, 2, ..., k, we have xij = zj. Given two sequences X and Y ,\n",
    "the Longest Common Subsequence (LCS) of X and Y is a common subsequence\n",
    "with maximum length.\n",
    "Given a candidate summary and the reference summary, the union of the LCS\n",
    "is given by.\n",
    "As for ROUGE-N, higher ROUGE-L means better quality in summaries. Throughout the thesis, the terms ROUGE-1, ROUGE-2 and ROUGE-L always refer to the F\n",
    "score measure of the corresponding metric.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Methods.\n",
    "In this chapter, the methods applied in the Master's Project are explained in details. While\n",
    "reading the chapter, of particular interest are Appendix A, where the code written\n",
    "throughout the Master's Project is presented and the functionalities of the files are described,\n",
    "and Appendix B, where one can find the instructions to set up a virtual environment compatible with the code. Moreover, Section 3.1 presents the most\n",
    "common generation technique used by Transformer models, Section 3.2 delineates the computer environment, Section 3.3 introduces the data and Section 3.4\n",
    "unfolds the methodologies for using and fine-tuning the Transformer models.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Beam Search.\n",
    "As quickly introduced in Section 2.1.4, there are several techniques for generating a summary with a Transformer. The most common one in recent literature is\n",
    "beam search. Beam search reduces the risk of missing hidden high probability\n",
    "tokens by keeping the most likely num_beams of hypotheses at each time step\n",
    "and eventually choosing the one that has the overall highest probability.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "An example with num_beams = 2 is shown in Figure 3.1. At time step 1,\n",
    "besides the most likely hypothesis (The, nice), beam search also keeps track of\n",
    "the second most likely one (The, dog). At time step 2, beam search finds that the\n",
    "word sequence (The, dog, has) with 0.36 has a higher probability than (The, nice,\n",
    "woman), which has 0.2. Beam search will always find an output sequence with\n",
    "higher probability than greedy search, but is not guaranteed to find the most\n",
    "likely output.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "In addition to the num_beams parameter, other generative parameters can\n",
    "influence the output summary. The mi n_l en can be used to force the model to\n",
    "not produce an EoS token before mi n_l en, the max_l en forces the summary\n",
    "length to be lower than max_l en, the length_penalty is a penalty to the length\n",
    "of the summary, the no_r epeat _ng r am number forbids the generation of n-gram tokens that are already present in the input.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Environment.\n",
    "Python 3.6.9 is the main programming language used in the Master's Project. Miniconda is\n",
    "the package manager. A virtual environment is created to facilitate reproducibility.\n",
    "GitHub is the version control management system. Jupyter Notebooks are used\n",
    "to present the code and the results. Weights and Biases is the platform employed\n",
    "to visualize fine-tuning progresses and results. Other important libraries that are\n",
    "used are Pandas for handling datasets, Gensim and Nltk for solving common NLP\n",
    "problems and Matplotlib and Plotly for plots. A complete list of the libraries, with\n",
    "the corresponding versions, can be found in the file env.yml of the Master's Project repository.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "HuggingFace.\n",
    "HuggingFace is an online community that helps build, train and deploy state-of-the-art models powered by the reference open source in NLP. In particular,\n",
    "HuggingFace TransformersI is the library used in the Master's Project to interact with the\n",
    "Transformer models. In more details, it is a GitHub repository that works as a\n",
    "container for multiple Transformer models. All the models are coded in PyTorch\n",
    "following a common template and they are accessible in a standardized fashion.\n",
    "The repository is forked and the code is personalized based on the needs of the\n",
    "Master's Project.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The library is continuously updated due to bugs and new releases. To give\n",
    "an idea, during the period of the Master's Project there has been an average of around 60\n",
    "commits per week on the master branch, and 3,086 issues have been created.\n",
    "To keep pace with the dynamics of the repository and to benefit from the newly\n",
    "implemented features, the code has been changed multiple times. Around the\n",
    "end of the Master's Project, the code is made compliant with the latest available release.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "On the other hand, the community contributing to the repository is very active. The documentation is thorough and the forum is a useful communication\n",
    "channel, particularly thanks to the developers who participate in most of the discussions. While working at the Master's Project, I have read more than 850 posts and written\n",
    "26 posts, 12 of which were liked by the developers.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Cloud Computing.\n",
    "Fine-tuning a Transformer model requires high memory and time resources.\n",
    "To tackle these bottlenecks, Google Colab and Amazon Web Services (AWS) EC2\n",
    "and S3 are exploited. Both are cloud computing providers. The former works\n",
    "with Jupyter Notebooks only, while the latter is a complete virtual machine.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "To access Google Colab's GPUs, a Jupyter Notebook is simply uploaded to the\n",
    "service and run. The system provides one random GPU with a memory ranging\n",
    "from 7 to 16 gigabytes, depending on the availability.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Although AWS is more complex to set up, the GPU is chosen by the user. For the\n",
    "purpose of the Master's Project the AWS EC2 g4dn.xlarge instance, with 16 gigabytes of GPU\n",
    "memory, and AWS S3 for data storage are chosen. In particular, the AWS machine\n",
    "is reachable via SSH and by accessing Jupyter Notebook remotely through a\n",
    "browser.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Data.\n",
    "The data is provided by Karger Publishers and consists of 77 books in Extensible\n",
    "Markup Language (XML) format, belonging to the Fast Facts series, a collection\n",
    "of books spanning multiple topics across the field of medicine. Each book has\n",
    "several chapters. At the end of each chapter there is a key facts section, consisting\n",
    "of a list of bullet points with the most important information of the chapter. These\n",
    "key fact bullet points are taken to be the summary of the chapter. The sayings\n",
    "\"bullet point\", \"key fact\" and \"summary\" refers to one single bullet point in the\n",
    "key facts section of a chapter. Ideally, the input of a Transformer model is a full\n",
    "chapter and the output is the list of bullet points of that chapter. As explained\n",
    "later, achieving this result is problematic.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Parsing and Pre-processing.\n",
    "The XML books are parsed using the ElementTree XML API module in Python.\n",
    "In total, there are 77 books. However, 8 are in Spanish, 3 in German, 3 in Italian, 1\n",
    "in French, 5 do not have key facts sections and 4 are a newer edition of an already\n",
    "present book. Thus, 53 books are kept because they are written in English, they\n",
    "have one key facts section each chapter and they are the newest version available.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The XML files are parsed following the tree configuration. When a chapter root\n",
    "is encountered, the sub-trees containing sections and sub-sections are explored.\n",
    "The structure of the book is maintained during the parsing process. The key facts\n",
    "are retrieved using a regular expression.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "At this point, some pre-processing is applied to all books. Special and Unicode\n",
    "characters, which can not be easily tokenized are replaced by normal characters\n",
    "with similar meaning (e.g. • replaced by -, ± replaced by +-, β replaced by beta).\n",
    "Moreover, to take care of wrongly parsed phrases, characters which should not\n",
    "be found at the beginning of a sentence are removed (e.g. ;, ], }), and the same is\n",
    "done for the end of a sentence. Then, multiple spaces or new lines are merged\n",
    "to a single space or a single new line, respectively. Finally, words longer than 45\n",
    "characters and sentences shorter than 3 words are removed.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Analysis\n",
    "On average, there are 8.5 chapters per book and 5.6 bullet points per chapter.\n",
    "In total, there are 453 chapters and 2,556 bullet points. Figure 3.2a shows the\n",
    "distribution of the number of tokens per bullet point. The orange line shows the\n",
    "median, which is exactly 25 tokens. Although some outliers are present, these\n",
    "are not removed from the data since the model should learn when to generate\n",
    "shorter or longer summaries.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The number of tokens per chapter is 2,717 on average and has a median of\n",
    "2,287. This is a problem because the maximum input length is 512 tokens for T5\n",
    "and 1,024 tokens for BART and PEGASUS, due to the self-attention memory and\n",
    "time constraints.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The data is made up of 18,822 paragraphs in total. Figure 3.2b shows the\n",
    "paragraphs number of tokens distribution, which has a mean and a median\n",
    "of around 65 tokens. This information is useful when creating the datasets for\n",
    "fine-tuning.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Dataset Generation.\n",
    "Problems arise regarding the length in number of tokens of the chapters and\n",
    "the variability in number of bullet points per chapter. Thus, the data needs to be\n",
    "further processed to create a suitable dataset for the Transformer models. The\n",
    "only model which can handle the data out of the box is LED, Section 3.4 studies\n",
    "this alternative in details.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "However, the other models need a specifically crafted dataset. To keep in mind\n",
    "is that the goal of the Master's Project is to automatically generate the list of bullet points of\n",
    "a chapter. During the Master's Project, different ideas to create suitable datasets have been\n",
    "implemented and tried out. Table 3.1 lists the datasets along with an explanation\n",
    "of the input text and the reference summary. Hereafter, the generated datasets\n",
    "are presented in chronological order:\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Chunk Chapter: The chapter is split in chunks, which are portions of the\n",
    "text, based on the number of tokens which can fit into the model. This\n",
    "number is 512 for T5 and 1,024 for BART and PEGASUS. The split is made\n",
    "as uniform as possible, with the constraint of never dividing in the middle of a sentence. For example, a chapter made of 2,300 tokens would\n",
    "be divided in 5 chunks of around 450 tokens to fit into T5. The reference\n",
    "summary of each entry is the key facts section of the chapter. Using this\n",
    "dataset for fine-tuning is not trivial because the model is given the same key\n",
    "facts as reference for different input text documents. Moreover, although\n",
    "the model would generate one summary for each chunk, the number of\n",
    "chunks does not reflect the number of bullet points. Moreover, some important information might be split and ignored in the computer-generated\n",
    "summary.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Merge or Chunk: As in the Chunk Chapter dataset, the chapter is divided\n",
    "into chunks. However, in this dataset the structure of the sections is preserved as much as possible. An important drawback is that sometimes the\n",
    "sections are too long and must be chunked to fit into the model. As a result,\n",
    "this method preserves on average only 30% of the sections. Although the\n",
    "input and reference are the same as in the Chunk Chapter dataset, this time\n",
    "the chunks should be more meaningful. Also in this case the fine-tuning\n",
    "is not trivial due to the same reasons explained for the Chunk Chapter dataset.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Assign Bullets: To make the fine-tuning easier, a method is implemented\n",
    "to assign each bullet to a specific chunk, in order to avoid the case where\n",
    "the same reference, i.e. the key facts section of the chapter is associated\n",
    "with multiple chunks. In particular, a bullet point is assigned to a chunk\n",
    "if the ROUGE-L recall is maximal. Using this technique, around 30% of\n",
    "the chunks are not matched with any bullet point and they are discarded.\n",
    "Regarding the remaining entries, on average there are 2 bullet points per\n",
    "chunk, with a maximum of 11. Also in this case the fine-tuning is hard\n",
    "because the model would need to learn how many bullet points to output\n",
    "for each chunk.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Topic Modeling: For the above reasons, another approach is carried out\n",
    "to generate a suitable dataset. For this dataset, Latent Dirichlet Allocation\n",
    "(LDA) and Latent Semantic Indexing (LSI) topic modeling techniques and\n",
    "the TextRank algorithm are employed to create an extractive summary of\n",
    "the chapter, called reduction. The reduction is crafted so that it can fit\n",
    "into the Transformer models, which then are in charge of producing the\n",
    "abstractive summary (i.e. the bullet points). In particular, each paragraph\n",
    "of the chapter is assigned an importance value based on the number of key\n",
    "words it contains. The final reduction is made solving a fixed-size knapsack\n",
    "problem, where the items are paragraphs, the weights are their number of\n",
    "tokens, the values are their importance and the maximum weight is set to\n",
    "the maximum input length of the model.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Bullet-Paragraph:  Fine-tuning a Transformer model to generate a single\n",
    "bullet point needs a dataset different from the above. When crafting the\n",
    "Bullet-Paragraph dataset, a single bullet point is matched with the passage\n",
    "in the chapter that represents it the most. Section 3.3.4 is entirely dedicated\n",
    "to the creation of this dataset, which is the one finally used for fine-tuning.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    " Bullet-Paragraph Dataset.\n",
    "The craft of this dataset comes from the need of having a cleaner dataset, with\n",
    "a text passage that does not exceed the input length constraint as input, and one\n",
    "bullet point as reference. As the chapters are treated independently, only one\n",
    "chapter is considered in the explanations.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "To create this dataset, a metric (e.g. ROUGE-L recall) is computed between\n",
    "every paragraph and every bullet point in the chapter. Then, each bullet point is\n",
    "associated with the paragraph with the maximal similarity, based on the implemented metric. As can be noted in Figure 3.2b, one paragraph, compared to a\n",
    "full chapter, is on average too short to generate a meaningful summary. To solve\n",
    "this issue, each entry is expanded merging above or below paragraphs, forming\n",
    "a passage. This operation is done maximizing the similarity between the bullet\n",
    "point and the paragraph to be merged. Paragraphs are merged until the input's\n",
    "number of tokens is greater than or equal to 4 times the number of tokens of the\n",
    "matched bullet point. At this stage, each entry of the dataset is composed of a\n",
    "passage of merged paragraphs as input and one bullet point as reference.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Another issue arises now because 16% of the entries have an overlap of more\n",
    "than 90% of tokens with another entry. Thus, one dataset (Base) is kept as is\n",
    "with the overlaps, and a different dataset (Merged Overlaps) is created merging\n",
    "paragraphs which overlap more than 90% and the corresponding bullet points.\n",
    "Some statistics on these two datasets is presented in Table 3.2.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "To conclude, two different approaches to match paragraphs to bullet points are\n",
    "tried. One exploits the ROUGE-L recall and the other focuses on cosine similarity\n",
    "between embeddings. Three approaches are followed to create meaningful embeddings: Word2Vec, Doc2Vec and Sentence-Transformer (Reimers & Gurevych,\n",
    "2019). The latter, which is based on the Transformer architecture, is the one with\n",
    "overall best performance. In more details, Sentence-Transformer adds a pooling\n",
    "operation to the output of an encoder-only Transformer to derive a fixed-size\n",
    "sentence embedding. The pre-trained model paraphrase-distilroberta-base-v1II\n",
    "is employed in the Master's Project. Finally, ROUGE-L recall and cosine similarity between\n",
    "embeddings are compared empirically by reading 20 random bullet points and\n",
    "the corresponding matched passages. The first is correct 11 times out of 20 and\n",
    "the second 17 times out of 20. Thus, Sentence-Transformer cosine similarity is\n",
    "chosen as the metric to be employed for matching.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The Base and Merged Overlap datasets are then shuffled and split using train/validation/test ratio of 80/10/10. Two different splitting techniques are used. The\n",
    "first ensures that bullet points belonging to the same section in the same chapter\n",
    "cannot belong to two different splits. The second has a stronger constraint because it ensures that bullet points belonging to the same book can not belong to\n",
    "two different splits. Chapter 4 presents the outcomes of choosing one technique\n",
    "rather than the other.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Transformer Models.\n",
    "Throughout the Master's Project, different Transformer models have been tested and fine-tuned. The focus has been simultaneously on two different topics. The first,\n",
    "explored in the following Section 3.4.1 and 3.4.2, is about summarizing an entire\n",
    "chapter with the problem of having a very long input. The second is about\n",
    "investigating different datasets, as described in Section 3.3.3 and 3.3.4, applying\n",
    "and fine-tuning state-of-the-art Transformer models that generally have input\n",
    "constraints, as outlined hereafter in Section 3.4.3 and 3.4.4.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Recurrent Decoder.\n",
    "To overcome the long input problem, the recurrent architecture of the Transformer's decoder can be exploited. This technique, called Recurrent Decoder, is\n",
    "applied to the Chunk Chapter and Merge or Chunk datasets, introduced in Section 3.3.3. Since these datasets are composed of chunked chapters, a traditional\n",
    "model would generate a summary for one chunk at a time without information\n",
    "from the others. This is a problem because the information in a chapter is usually\n",
    "homogeneous and the generation of the bullet points should take all chunks of\n",
    "the chapter into account.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "In the Recurrent Decoder method the first chunk C1 is fed to the model, which\n",
    "generates the first summary S 1 starting from the Begin of Sentence (BoS) special\n",
    "token. From the second chunk onwards {C2, ...,Cn}, although the input of the\n",
    "model is still the chunk only, the decoder is made to start the generation from the\n",
    "concatenation of the previously generated summaries. In mathematical formulas:\n",
    "S 1 = decode (encode(C1), BoS)\n",
    "S n = decode (encode(Cn), concat(S1, ..., Sn−1))\n",
    "Where BoS is the Begin of Sentence token, introduced in Section 2.1.4. A schematized visualization of the procedure is presented in Figure 3.3.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Using this method, the final summary is compared to the concatenation of the\n",
    "bullet points of the chapter. Although this techniques works better than an usual\n",
    "Transformer, it cannot reproduce the original bullet points structure. Moreover,\n",
    "the generative parameters, such as the mi n_l en, must be changed every time a\n",
    "summary is created. This last point also makes the fine-tuning hard.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    " Longformer Encoder Decoder.\n",
    "The LED, introduced in Section 2.2.3, exploits sparse self-attention in the\n",
    "encoder to process long inputs up to around 8,000 tokens. In the Master's Project, it is applied\n",
    "to the data as is, after parsing and the general pre-processing described in Section\n",
    "3.3.1.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "LED is the only model on the HuggingFace Transformers repository which can\n",
    "handle such long inputs. It was added to the platform on 13th of January and\n",
    "multiple bugs were fixed on 8th of February. Thus, the time to experiment with\n",
    "the model was limited.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Although the model is not very powerful out of the box, it reaches better performance after fine-tuning. During fine-tuning, the input of the model is a chapter\n",
    "and the reference is the concatenation of bullet points of the chapter with a\n",
    "special token <BUL> added at the beginning of each bullet point. Thanks to fine-tuning, the model should learn to output this special token in its summary, which\n",
    "is then interpreted as a list of bullet points. Although the results are promising, the\n",
    "number of bullet points generated by the model is far from the ground truth and\n",
    "the implementation of an appropriate evaluation metric is not straightforward.\n",
    "Hence, a different approach is followed.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "T5, BART and PEGASUS.\n",
    "T5, BART and PEGASUS are introduced in Section 2.2. They are the Transformer\n",
    "models applied to the Bullet-Paragraph dataset described in Section 3.3.4. Since\n",
    "T5 has an input length of maximum 512 tokens, a special dataset where the\n",
    "paragraphs are merged up to the model's maximum length is prepared. On the\n",
    "other hand, BART and PEGASUS' maximum input length is 1,024 and the entry\n",
    "with the longest input in the dataset has 647 tokens. Due to memory problems, a\n",
    "distilled version (Sanh et al, 2019) of BART is used. The distilled version of the\n",
    "model is referred to as BART.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    " Fine-tuning.\n",
    "The first step towards fine-tuning the model is understanding the best environment settings. The HuggingFace Transformers library gives a diverse variety of\n",
    "tools for fine-tuning a model. To make the process as smooth as possible, the\n",
    "fine-tuning techniques for each model are copied from the respective paper. For\n",
    "example T5 and PEGASUS are fine-tuned using the Adafactor optimizer while\n",
    "BART uses AdamW, T5's learning rate is constant while BART and PEGASUS'\n",
    "ones decay over time. The performance outcome is very much dependent on\n",
    "the dataset in play when changing the batch size and learning rate parameters.\n",
    "Hence, a hyperparameter search is needed.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The batch size defines the number of samples that will be propagated through\n",
    "the network. Since higher batch size means more input text, this number cannot\n",
    "be raised to values higher than 2, due to GPU memory constraints on Google\n",
    "Colab and the AWS instance. However, most of the Transformer models are\n",
    "trained and fine-tuned with much higher batch sizes, as seen in Section 2.2.1.\n",
    "For this reason, the Gradient Accumulation Steps (GAS) parameter must be employed. Gradient accumulation is a mechanism to split the batch of samples,\n",
    "used for training a neural network, into several mini-batches of samples that run\n",
    "sequentially. The GAS parameter is the number of steps the model runs without\n",
    "updating its variables while accumulating (summing) gradients of those steps\n",
    "and then using them to compute the variable updates. For example setting batch\n",
    "size to 2 and GAS to 64 simulates a batch size of 2·64 = 128. This technique implemented on the Transformer models is highly corroborated by the HuggingFace\n",
    "community.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Thanks to the hyperparameter search, the best learning rate and GAS are\n",
    "selected. Then, the models are fine-tuned for a number of steps aligned to\n",
    "the one found in the respective paper or until over-fitting the dataset. A list\n",
    "of the parameters used for each model is presented in Appendix C. During the\n",
    "process, the Weights and Biases platform is employed to monitor the system\n",
    "performances, such as GPU memory occupation; the training values, such as\n",
    "loss and learning rate's decay; and the evaluation results. The last step is to\n",
    "execute a second hyperparameter search on the fine-tuned model to find the best\n",
    "generative parameters for the beam search, described in Section 3.1. In particular,\n",
    "the space of generative parameters is search for the best results on the validation\n",
    "split.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Results.\n",
    "In this chapter, the fine-tuning results are presented. Section 4.1 shows the\n",
    "reproduction of BART's fine-tuning carried out originally by Lewis et al (2019);\n",
    "Section 4.2 compares the fine-tuned Transformer models and Section 4.3 explores\n",
    "a fully automated summarization pipeline that exploits PEGASUS, which is the\n",
    "model with overall best performances.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "All hyperparameter searches, fine-tuning projects and plots are publicly accessible on my Weights and Biases profileI .\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "BART Results on XSum.\n",
    "To gain confidence with the HuggingFace Transformers library, in particular\n",
    "with the fine-tuning code, BART is fine-tuned to reproduce the results published\n",
    "by Lewis et al (2019) on the XSum dataset. The XSum dataset is chosen because\n",
    "on average the input length of the entries is shorter than the other datasets on\n",
    "which BART was fine-tuned in the paper. This translates into less GPU memory\n",
    "consumption during fine-tuning. On the other hand, the XSum dataset counts\n",
    "around 200,000 entries in the training split and around 11,000 in the validation\n",
    "split. The large amount of data is synonym of a long fine-tuning time. In particular, the whole process takes around 30 hours.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "In the paper, the model is fine-tuned for 20,000 steps while in the Master's Project for 4,500\n",
    "steps only due to time constraints. Moreover, the GAS is set to 128 since only\n",
    "one batch at a time can fit in the GPU; the embeddings are freezed, which means\n",
    "they are not updated during fine-tuning; the process is stopped and started again\n",
    "from the latest available checkpoint three times; and the beam search generative\n",
    "parameters space is not searched for the optimal combination, as opposed to the\n",
    "paper.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Nonetheless, on average the results of the Master's Project are only 0.61 points lower than\n",
    "the results reported by Lewis et al (2019), as one can see in Table 4.1 along with\n",
    "the complete evaluation outcome.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    " Models Comparison.\n",
    "After a thorough examination of the fine-tuning code, T5, BART and PEGASUS\n",
    "are studied on the Bullet-Paragraph dataset introduced in Section 3.3.4. To start,\n",
    "a hyperparameter search to find the best GAS and learning rate combination is\n",
    "executed for all models. The hyperparameter search carried out for PEGASUS\n",
    "is shown in the parallel coordinate plot of Figure 4.1. Lower GAS numbers are\n",
    "not present in Figure 4.1 because already discarded previously. A similar plot is\n",
    "analyzed for the other models. A complete list of the Transformers' parameters\n",
    "used during fine-tuning can be found in Appendix C.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "After this first step, the models are fine-tuned on the Bullet-Paragraph dataset\n",
    "using the optimal parameters. For now, the validation split refers to the one with\n",
    "weaker constraints. A comparison of the evaluation results of the models on the\n",
    "validation split during fine-tuning can be seen in Figure 4.2. The metric to the\n",
    "bottom right is the cosine similarity between Sentence-Transformer embeddings\n",
    "of the generated bullet point and the reference bullet point. This metric is included because it is also employed in the creation of the Bullet-Paragraph dataset\n",
    "and shows great results when comparing summaries.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "It is clear from the plots of Figure 4.2 that PEGASUS, in purple, performs better\n",
    "than the other models. In particular, BART and T5's evaluation functions, in\n",
    "blue and green respectively, are noisy. This translates into the fact that choosing a checkpoint instead of another would change the quality of the summary\n",
    "accordingly. T5 is probably disadvantaged because of the halved input length\n",
    "with respect to the other models. At this point, for each model, the checkpoint\n",
    "achieving the best results on the validation split is chosen. Figure 4.3 shows,\n",
    "on the top plots and bottom left plot, the comparison of the evaluation metrics\n",
    "on the validation split for the models out of the box versus the best fine-tuned\n",
    "checkpoints. The bottom right plot of Figure 4.3 presents, in the same bar plot,\n",
    "the evaluation results on the validation split for T5, BART and PEGASUS' best\n",
    "fine-tuned checkpoints.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    " PEGASUS.\n",
    "After analyzing the fine-tuning results, PEGASUS is selected as the best alternative to generate summaries for the Bullet-Paragraph dataset. During the\n",
    "fine-tuning process and while choosing the best model, multiple summaries\n",
    "were read and compared to the input paragraphs and to the reference summary\n",
    "by hand. As an additional proof that the fine-tuning amazingly contributes to\n",
    "generate better summaries, an empirical example is selected at random from the\n",
    "validation split. In particular, the first entry of the split is presented hereafter. The\n",
    "reference bullet point is:\n",
    "The immune system has two components: innate immunity, involving mechanisms present throughout life, and adaptive (acquired)\n",
    "immunity, which is conferred by immune responses following exposure to an antigen, and is specific to that antigen.\n",
    "PEGASUS model out of the box generates the following summary:\n",
    "A hallmark of cancer is that tumor cells - which would normally be\n",
    "recognized by the immune system as abnormal - acquire the ability\n",
    "to evade the immune system.\n",
    "And the best fine-tuned checkpoint of PEGASUS produces the following summary:\n",
    "The immune system consists of two components: innate immunity\n",
    "and adaptive immunity. Innate immunity is conferred by mechanisms that are present throughout life, such as the physical barriers\n",
    "to infection provided by the skin and mucous membranes, white\n",
    "blood cells that remove foreign material, and serum proteins such as\n",
    "lysozymes and kinins.\n",
    "One can notice that, although the second sentence of the fine-tuned model\n",
    "diverges a bit, the topic of the summary is much more aligned to the reference\n",
    "summary than the one generated by the model out of the box.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The last step is to execute a generative parameter search to find the best parameters for the beam search algorithm, introduced in Section 3.1. This process\n",
    "is not improving the results already obtained thanks to fine-tuning. However,\n",
    "choosing the right parameters makes it possible to adapt the generated summary\n",
    "to the preferences of the user. An example of how changing the length_penalty\n",
    "can change the evaluation results on the validation split is shown in the parallel\n",
    "coordinate plot of Figure 4.4. For example, one can choose to maximize the\n",
    "Sentence-Transformer metric while suffering a reduction of the ROUGE metrics.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "To conclude, Figure 4.5 shows the fine-tuning improvements on the test split\n",
    "for PEGASUS.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "To make the Transformer as general as possible, the fine-tuning process is\n",
    "repeated on the train, validation and test splits with the stronger constraints, i.e.\n",
    "where bullet points belonging to the same book cannot belong to two different\n",
    "splits. Figure 4.6 shows that PEGASUS results on the validation and test splits\n",
    "with stronger constraints are comparable to the results obtained on the splits\n",
    "with weaker constraints. Although the results are similar, the model trained on\n",
    "the splits with stronger constraints should be preferred because it will generalize\n",
    "better for unseen books.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "A final experiment tests the Merged Overlaps version of the Bullet-Paragraph\n",
    "dataset, introduced in Section 3.3.4. This dataset is created by merging paragraphs which overlap for more than 90% of tokens and the corresponding bullet\n",
    "points. Figure 4.7 shows in orange the evaluation results on this dataset, compared to the results previously obtained in blue. One can clearly see that PEGASUS achieves better results on the Merged Overlaps version. This is probably\n",
    "due to the fact that redundant entries are not present in the new version of the\n",
    "dataset.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Random summarization examples picked from the test split of the Merged\n",
    "Overlaps Bullet-Paragraph dataset and generated by the best fine-tuned checkpoint of PEGASUS along with the reference summary and the evaluation metrics\n",
    "can be read in Appendix D.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Summarization Pipeline.\n",
    "This section explores a fully automated summarization pipeline which can\n",
    "be used on new materials, such as textbooks. The pipeline heavily relies on the\n",
    "Sentence-Transformer metric, previously described to extract the most meaningful passages of a chapter, and on the fine-tuned PEGASUS model to create the\n",
    "bullet points.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "To simulate a real setting, only the books belonging to the validation and test\n",
    "splits of the Bullet-Paragraph Merged Overlaps dataset are considered. This is\n",
    "done because PEGASUS is fine-tuned on the train split and otherwise the results\n",
    "would be biased. In this setting, the bullet points section is not employed in any\n",
    "way. Thus, the pipeline is an unsupervised method which can theoretically be\n",
    "applied to any book. Hereafter, the full process is described for a single chapter\n",
    "because the technique is identical for the others.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "To start, one parsed and pre-processed chapter C of a Karger Publishers book\n",
    "belonging either to the test or validation split is retrieved. Then, the number of\n",
    "sections S of chapter C is considered as the most representative figure for the\n",
    "number of bullet points in the key facts section of the chapter. At this point, each\n",
    "paragraph is embedded with the Sentence-Transformer embeddings technique.\n",
    "In the new space, the K-Means clustering method is applied, where K equals the\n",
    "number of sections S of chapter C. If the chapter does not contain sections or it\n",
    "is impossible to parse them, the elbow technique might be employed to find the\n",
    "most suitable K.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Now, chapter C has S centroids. Cosine similarity is computed between paragraphs and centroids. For each centroid, the closest paragraph is extracted and\n",
    "selected as key paragraph. Thus, S key paragraphs are extracted from chapter\n",
    "C. Although these key paragraphs might be considered as the generated bullet\n",
    "points, PEGASUS abstractive potential is not used yet.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "At this point, the key paragraphs are merged with above or below paragraphs\n",
    "in the text to create key passages. This process is done maximizing the cosine\n",
    "similarity to the associated centroid until the number of tokens of the passage\n",
    "is close to the maximum number of tokens that can be fed into PEGASUS. Afterwards, chapter C is associated to S key passages, which are generally made up of\n",
    "multiple paragraphs.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Similarly to the Merged Overlaps version of the Bullet-Paragraph dataset, passages which overlap for more than 90% of the tokens are merged. Finally, PEGASUS is applied to generate a bullet point for each key passage. As one can imagine,\n",
    "the number of generated bullet points with this unsupervised method diverges\n",
    "from the true number of bullet points. In more details, the Mean Absolute Error\n",
    "over the chapters in the test and validation splits is 1.7, with a variance of 1.4.\n",
    "However, an evaluation of this technique is carried out by concatenating both\n",
    "reference and prediction bullet points to create a reference and prediction summary, respectively. Table 4.2 shows ROUGE and Sentence-Transformer metrics\n",
    "calculated between the unsupervised prediction and reference summaries for\n",
    "the test and validation splits. The results, although obtained on different data,\n",
    "are comparable to PEGASUS fine-tuned results on the Bullet-Paragraph dataset,\n",
    "which means the unsupervised summaries are of similar quality.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Conclusion.\n",
    "In the Master's Project, the application of Automatic Text Summarization Transformer models was explored in depth. First, the HuggingFace Transformers\n",
    "library was tested by reproducing BART fine-tuning results on the XSum dataset.\n",
    "Then, the Recurrent Decoder method was tried and the LED efficient Transformer was fine-tuned on the full data. At this point, considering the poor results,\n",
    "a more traditional approach was taken by applying and fine-tuning T5, BART\n",
    "and PEGASUS on a specifically crafted dataset. The effectiveness of fine-tuning\n",
    "a Transformer model was corroborated by the ROUGE evaluation metrics and\n",
    "the Sentence-Transformer cosine similarity between computer-generated bullet\n",
    "points and reference bullets points. An empirical evaluation was also carried out\n",
    "by me to finally choose the model with overall best performances. Then, the best\n",
    "fine-tuned checkpoint of PEGASUS was used to summarize the entries of the test\n",
    "split. Finally, a summarization pipeline was experimented to generate a list of\n",
    "bullet points for an unseen generic chapter, in a fully unsupervised manner.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Future Work.\n",
    "The new PEGASUS model fine-tuned on the Karger Publishers books will likely\n",
    "perform similarly with unseen Karger Publishers books or any medicine-related\n",
    "book. However, the model is not fine-tuned nor tested on other unrelated material. Thus, it is probable that it will perform poorly in other domains. To avoid this\n",
    "situation, one might add entries from different fields to the fine-tuning dataset.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "The Bullet-Paragraph dataset starts from the assumption that the Sentence-Transformer cosine similarity used for matching passages to bullet points does\n",
    "not fail. However, there is a small possibility that the wrong passage is considered\n",
    "and this would negatively affect the fine-tuning process. To circumvent this problem, one might look more into efficient Transformer models. These models, not\n",
    "yet present on the HuggingFace Transformers library except for the Longformer\n",
    "Encoder Decoder, are capable of taking an entire chapter as input, without the\n",
    "need of manually matching bullet points to shorter passages. The ones that stand\n",
    "out in my opinion are BigBird1 , Performer2 and Linformer3, which are soon to be\n",
    "added to the library, as you can browse from the footnotes.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Regarding the metrics, the understanding of how precise and good these metrics are when comparing summaries is still a big question for the entire community. Although the use of Sentence-Transformer cosine similarity is a little\n",
    "step towards a more reliable metric, this method makes absolutely impossible to\n",
    "understand how the metric works since the embeddings are created by a neural\n",
    "network with millions of parameters. The implementation of a better metric is an\n",
    "intricate discussion and it is outside the scope of this thesis. On the other hand,\n",
    "to assess how close the fine-tuned model is to human performance, a human\n",
    "evaluation can be conducted. For example, the Amazon Mechanical Turk service\n",
    "might be exploited to compare model summaries with reference summaries by\n",
    "asking workers to rate the summaries on a scale, as done by Zhang et al. (2020).\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "To conclude, if I had continued to work at Magma Learning, I would have\n",
    "focused more on making the code faster, lighter and easier to use for the summarization pipeline. In my humble opinion, although the Master's Project puts\n",
    "a solid background for Automatic Text Summarization with Transformers in an\n",
    "industrial environment, the process lacks velocity and consumes too much GPU\n",
    "memory, which results in a negative experience for the user and an expensive\n",
    "setting for the company, respectively.\n",
    "\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thesis Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Introdution. This thesis is about the Master's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Background. Magma Learning Sàrl is a young sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The MLO is active in the field of machine lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLP is a discipline spanning from linguistic t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Problem Statement. ARI 9000 produces personali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Conclusion. In the Master's Project, the appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Future Work. The new PEGASUS model fine-tuned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>The Bullet-Paragraph dataset starts from the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Regarding the metrics, the understanding of ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>To conclude, if I had continued to work at Mag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  para\n",
       "0    Introdution. This thesis is about the Master's...\n",
       "1    Background. Magma Learning Sàrl is a young sta...\n",
       "2    The MLO is active in the field of machine lear...\n",
       "3    NLP is a discipline spanning from linguistic t...\n",
       "4    Problem Statement. ARI 9000 produces personali...\n",
       "..                                                 ...\n",
       "108  Conclusion. In the Master's Project, the appli...\n",
       "109  Future Work. The new PEGASUS model fine-tuned ...\n",
       "110  The Bullet-Paragraph dataset starts from the a...\n",
       "111  Regarding the metrics, the understanding of ho...\n",
       "112  To conclude, if I had continued to work at Mag...\n",
       "\n",
       "[113 rows x 1 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data={'para': text_thesis})\n",
    "df.para = df.para.map(lambda p: p.replace('\\n', ' ').strip())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSHT0mxuvkEp"
   },
   "source": [
    "### **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eRnW74aH95b"
   },
   "source": [
    "#### Preprocessing\n",
    "\n",
    "* Remove unwanted chars at beginning or end of sentence\n",
    "* Remove multiple spaces\n",
    "* Remove long words (> config.TOKEN_MAX_LEN chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "CDsT33j-wPCw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove unwanted chars at beginning or end of sentence\n",
    "df.para = df.para.map(lambda p: p.lstrip('.,;:-)] \\n'))\n",
    "df.para = df.para.map(lambda p: p.rstrip('.,;:-([ \\n'))\n",
    "\n",
    "# Remove multiple spaces\n",
    "df.para = df.para.map(lambda p:\n",
    "    re.sub('\\s+', ' ', p).strip())\n",
    "\n",
    "# Remove long words (> config.TOKEN_MAX_LEN chars)\n",
    "def para2words(para):\n",
    "    return gensim.utils.simple_preprocess(\n",
    "        para, deacc=True, max_len=config.TOKEN_MAX_LEN)\n",
    "df['para_proc'] = df.para.map(para2words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Preprocessing\n",
    "\n",
    "* Remove stop words\n",
    "* Remove short sentences / paragraphs (< config.PARA_MIN_LEN tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "df.para_proc = df.para_proc.map(lambda p:\n",
    "    [w for w in p if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove short sentences / paragraphs (< config.PARA_MIN_LEN tokens)\n",
    "df.loc[df.para_proc.map(len) <\\\n",
    "    config.PARA_MIN_LEN, 'para_proc'] = np.nan\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.para = df.para.map(lambda p: p+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sentence-Transformers Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:06<00:00, 16.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# might want to try 'msmarco-distilbert-base-v2' too\n",
    "st_model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "df['para_enc'] = df.para.progress_map(st_model.encode)\n",
    "df.para_enc = df.para_enc.map(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cluster Based on Number of Sections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.2565575e-01, -4.7261547e-02, -6.8232477e-02, ...,\n",
       "         3.2487008e-01,  1.3825442e-01, -3.1990733e-02],\n",
       "       [-1.4868605e-01, -2.7556273e-01,  4.1686401e-02, ...,\n",
       "         1.4344034e-01, -8.7471336e-02,  2.6399034e-01],\n",
       "       [-3.6144126e-02,  2.3763007e-02, -2.1090892e-01, ...,\n",
       "         4.7304592e-01, -3.3370119e-02,  2.9228203e-02],\n",
       "       ...,\n",
       "       [ 3.7123200e-02,  2.1992660e-01, -3.8950145e-04, ...,\n",
       "        -1.2614240e-01,  1.9563973e-01,  8.5724086e-02],\n",
       "       [-2.8973350e-02,  3.0258974e-01, -2.5085211e-03, ...,\n",
       "         5.8089353e-02,  4.4488209e-01,  1.6201158e-01],\n",
       "       [-2.3737235e-01,  2.0165345e-01, -3.5204262e-02, ...,\n",
       "        -4.3637085e-01, -1.7685018e-01, -2.0491444e-01]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thesis_para_enc = np.vstack(df.para_enc.tolist())\n",
    "thesis_para_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sec = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_centroids =\\\n",
    "    KMeans(\n",
    "        num_sec,\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        random_state=config.SEED).fit(thesis_para_enc).cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Low Dimensional Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "para_enc_2dim = PCA(2, random_state=config.SEED).fit_transform(thesis_para_enc)\n",
    "kmeans_centroids_2dim = PCA(2, random_state=config.SEED).fit_transform(kmeans_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    x = [x[0] for x in para_enc_2dim],\n",
    "    y = [x[1] for x in para_enc_2dim],\n",
    "    s = 20\n",
    ")\n",
    "plt.scatter(\n",
    "    x = [x[0] for x in kmeans_centroids_2dim],\n",
    "    y = [x[1] for x in kmeans_centroids_2dim],\n",
    "    marker='*',\n",
    "    s = 50\n",
    ")\n",
    "plt.xlabel('PCA-1')\n",
    "plt.ylabel('PCA-2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:00<00:00, 7711.00it/s]\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "def row_cosine_sim(r):\n",
    "    centroids = kmeans_centroids\n",
    "    return [cosine_sim(r.para_enc, c) for c in centroids]\n",
    "\n",
    "df['cosine_sim'] = df.progress_apply(lambda row: row_cosine_sim(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find Best Paragraph for each Book, Chapter, Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cosine_sim = df.cosine_sim.map(lambda c: np.array(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([95, 23, 22, 73])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_para_cosine = np.argmax(np.vstack(df.cosine_sim.tolist()), axis=0)\n",
    "best_para_cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expanding from Best Paragraph Based on Cosine Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['para_num_tok'] =\\\n",
    "    df.para.map(lambda p: np.array(len(tokenizer.tokenize(p))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def expand_based_on_cosine(df, best_para_cosine):\n",
    "    max_length = len(df)\n",
    "    max_idx = max_length-1\n",
    "    \n",
    "    extracted_para = []\n",
    "    \n",
    "    # Calculate the fraction we need to extract\n",
    "    # based on total number of tokens in this chp\n",
    "    # and number of centroids (sections) in this chp\n",
    "    # do not go over the model max length\n",
    "    cosine_sim = np.vstack(df.cosine_sim.tolist())\n",
    "    para_num_tok = np.array(df.para_num_tok.tolist())\n",
    "    num_tok_tot = sum(para_num_tok)\n",
    "    num_tok_th = min(\n",
    "        int(0.8*num_tok_tot / len(best_para_cosine)),\n",
    "        0.9*tokenizer.model_max_length)\n",
    "    \n",
    "    for i, best in enumerate(best_para_cosine):\n",
    "        merged_para_idx = [best]\n",
    "        num_tok = para_num_tok[best]\n",
    "        \n",
    "        while num_tok < num_tok_th:\n",
    "            if len(merged_para_idx) == max_length : break\n",
    "            elif 0 in merged_para_idx:\n",
    "                merged_para_idx.append(max(merged_para_idx)+1)\n",
    "            elif max_idx in merged_para_idx:\n",
    "                merged_para_idx.append(min(merged_para_idx)-1)\n",
    "            else:\n",
    "                if cosine_sim[min(merged_para_idx)-1, i] <\\\n",
    "                    cosine_sim[max(merged_para_idx)+1, i]:\n",
    "                    merged_para_idx.append(max(merged_para_idx)+1)\n",
    "                else:\n",
    "                    merged_para_idx.append(min(merged_para_idx)-1)\n",
    "            num_tok = np.sum(para_num_tok[merged_para_idx])\n",
    "                  \n",
    "        extracted_para.append(sorted(merged_para_idx))\n",
    "        \n",
    "    return extracted_para\n",
    "\n",
    "selected_para_cosine = expand_based_on_cosine(df, best_para_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[93, 94, 95, 96, 97, 98, 99],\n",
       " [20, 21, 22, 23, 24, 25, 26, 27],\n",
       " [18, 19, 20, 21, 22, 23, 24, 25, 26],\n",
       " [67, 68, 69, 70, 71, 72, 73]]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_para_cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Overlaps Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_para_cosine[1] += selected_para_cosine[2]\n",
    "selected_para_cosine[1] = sorted(list(set(selected_para_cosine[1])))\n",
    "selected_para_cosine[2] = selected_para_cosine[3]\n",
    "selected_para_cosine = selected_para_cosine[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[93, 94, 95, 96, 97, 98, 99],\n",
       " [18, 19, 20, 21, 22, 23, 24, 25, 26, 27],\n",
       " [67, 68, 69, 70, 71, 72, 73]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_para_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_passage = []\n",
    "for para_idx in selected_para_cosine:\n",
    "    passage = ' '.join(df.para.iloc[para_idx].tolist())\n",
    "    selected_passage.append(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(config.MAGMA_DIR+\\\n",
    "#    'fine-tuning/ft_pegasus_bull_para_embed_merged_overlaps_bybook_gas64_lr5e-05/checkpoint-539/')\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained('google/pegasus-large')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(config.MAGMA_DIR+\\\n",
    "#    'fine-tuning/ft_pegasus_bull_para_embed_merged_overlaps_bybook_gas64_lr5e-05/checkpoint-539/')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('google/pegasus-large')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1047 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1047\n",
      "1147\n",
      "953\n"
     ]
    }
   ],
   "source": [
    "for s in selected_passage:\n",
    "    print(len(tokenizer.tokenize(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = model.config.task_specific_params.get('summarization', {})\n",
    "model.config.update(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:50<00:00, 16.69s/it]\n"
     ]
    }
   ],
   "source": [
    "selected_passage_summ = []\n",
    "for passage in tqdm(selected_passage):\n",
    "    summ = tokenizer.batch_decode(\n",
    "        model.generate(\n",
    "            tokenizer(passage, return_tensors='pt', truncation=True, padding='longest').input_ids,\n",
    "            min_length = config.ONE_BULLET_MIN_LEN,\n",
    "            num_beams = 6,\n",
    "            early_stopping = True\n",
    "        ), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    selected_passage_summ.append(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models T5, BART and PEGASUS are studied on the Bullet-Paragraph dataset. A hyperparameter search to find the best GAS and learning rate combination is executed for all models. After this first step, the models are fine-tuned using the optimal parameters. For each model, the checkpoint achieving the best results is chosen.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "Transformer models are pre-trained on a huge number of language tasks to develop abilities which can then be transferred to downstream tasks. Pre-training is typically done using unsupervised learning on unlabeled data. Because Transformers have an auto-regressive decoder, they can be directly fine-tuned for sequence generation tasks.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "Chunk Chapter: The chapter is split in chunks, which are portions of the text, based on the number of tokens which can fit into the model. Bullet-Paragraph: A single bullet point is matched with the passage in the chapter that represents it the most.\n",
      "\n",
      "####################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for passage, summ in zip(selected_passage, selected_passage_summ):\n",
    "    print(summ)\n",
    "    print()\n",
    "    print(''.join(['#']*100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "mFAC31paODFl",
    "S0FByNNOIRvG",
    "tb7fAfzaK4es",
    "eQGq4WLu3Gei",
    "tSHT0mxuvkEp",
    "-eRnW74aH95b",
    "X2xp7jJNwB6b",
    "2Eb-_Ud3vxeY",
    "VndEUBoDjjkV",
    "8_li_hFKF_Ws"
   ],
   "name": "paragraph_assign_bullets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
