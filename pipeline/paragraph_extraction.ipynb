{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "magma_dir = '/home/marco/epfl/magma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 16184,
     "status": "ok",
     "timestamp": 1610463826089,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "ClE5D523OTZG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, magma_dir)\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 16184,
     "status": "ok",
     "timestamp": 1610463826092,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "82WSp6khIcua"
   },
   "outputs": [],
   "source": [
    "MODEL = 'bart'\n",
    "\n",
    "RE_SPLITTER = '\\n'              # do we split sentences of paragraphs?\n",
    "                                # use '\\.(?!\\d)|\\n' or '\\n', respectively\n",
    "\n",
    "TOKEN_MAX_LEN = 99              # max length of a word\n",
    "PARA_MIN_LENGTH = 2             # minimum length for a sentence or\n",
    "                                # a paragraph, in tokens\n",
    "\n",
    "# Output path\n",
    "OUTPUT_PATH = magma_dir+'pipeline/karger_books_para_extraction/'+MODEL+'/'\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb7fAfzaK4es"
   },
   "source": [
    "### **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 16443,
     "status": "ok",
     "timestamp": 1610463826354,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "wvbMlPBxk45S"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from textwrap import fill\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "if 'pegasus' in MODEL:\n",
    "    from transformers import PegasusTokenizer\n",
    "    tokenizer =\\\n",
    "        PegasusTokenizer.from_pretrained('google/pegasus-large')\n",
    "elif 'bart' in MODEL:\n",
    "    from transformers import BartTokenizer\n",
    "    tokenizer =\\\n",
    "        BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "elif 't5' in MODEL:\n",
    "    from transformers import T5Tokenizer\n",
    "    tokenizer =\\\n",
    "        T5Tokenizer.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQGq4WLu3Gei"
   },
   "source": [
    "### **Karger Books Base Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Z0lbkScg0a7j"
   },
   "outputs": [],
   "source": [
    "base_dataset = magma_dir+'datasets/karger_books_base/df.csv'\n",
    "df = pd.read_csv(base_dataset)\n",
    "df = df.set_index(['book', 'chapter', 'section', 'subsection'])\n",
    "df.bullets = df.bullets.map(eval, na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "tSHT0mxuvkEp"
   },
   "source": [
    "### **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "-eRnW74aH95b"
   },
   "source": [
    "#### Preprocessing\n",
    "\n",
    "* Split based on RE_SPLITTER\n",
    "* Explode the dataset\n",
    "* Remove unwanted chars at beginning or end of sentence\n",
    "* Remove multiple spaces\n",
    "* Remove long words (> TOKEN_MAX_LEN chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "id": "CDsT33j-wPCw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split in sentences / paragraphs based on RE_SPLITTER\n",
    "df.text =\\\n",
    "    df.text.map(lambda x: [p.strip() for p in re.split(RE_SPLITTER, x) if p!=''],\n",
    "                na_action='ignore')\n",
    "    \n",
    "# explode to get one row for each paragraph /sentence\n",
    "df = df.explode('text')\n",
    "df = df.rename(columns={'text': 'para'})\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove unwanted chars at beginning or end of sentence\n",
    "df.para = df.para.map(lambda p: p.lstrip('.,;:-)] \\n'))\n",
    "df.para = df.para.map(lambda p: p.rstrip('.,;:-([ \\n'))\n",
    "\n",
    "# Remove multiple spaces\n",
    "df.para = df.para.map(lambda p:\n",
    "    re.sub('\\s+', ' ', p).strip())\n",
    "\n",
    "# Remove long words (> TOKEN_MAX_LEN chars)\n",
    "def para2words(para):\n",
    "    return gensim.utils.simple_preprocess(\n",
    "        para, deacc=True, max_len=TOKEN_MAX_LEN)\n",
    "df['para_proc'] = df.para.map(para2words)\n",
    "df['bullets_proc'] = df.bullets.map(lambda bs: [para2words(b) for b in bs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Further Preprocessing\n",
    "\n",
    "* Remove stop words\n",
    "* Remove short sentences / paragraphs (< PARA_MIN_LENGTH tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "df.para_proc = df.para_proc.map(lambda p:\n",
    "    [w for w in p if w not in stop_words])\n",
    "df.bullets_proc = df.bullets_proc.map(lambda bs:\n",
    "    [[w for w in b if w not in stop_words] for b in bs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove short sentences / paragraphs (< PARA_MIN_LENGTH tokens)\n",
    "df.loc[df.para_proc.map(len) <\\\n",
    "    PARA_MIN_LENGTH, 'para_proc'] = np.nan\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.para = df.para.map(lambda p: p+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Number of Sections vs Bullets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.000000\n",
       "mean       2.070640\n",
       "std        1.928902\n",
       "min        0.000000\n",
       "25%        1.000000\n",
       "50%        2.000000\n",
       "75%        3.000000\n",
       "max       13.000000\n",
       "Name: absolute_error, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num_sec_bul = df.groupby(['book', 'chapter', 'section'], sort=False).agg({\n",
    "    'para': lambda p: list(p),\n",
    "    'bullets': lambda b: list(b)[0]\n",
    "}).groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'para': lambda p: len(list(p)),\n",
    "    'bullets': lambda b: len(list(b)[0])\n",
    "}).rename(columns={'para':'num_sec', 'bullets':'num_bul'})\n",
    "\n",
    "df_num_sec_bul['absolute_error'] = np.abs(df_num_sec_bul['num_sec'] - df_num_sec_bul['num_bul'])\n",
    "\n",
    "df_num_sec_bul['absolute_error'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.000000\n",
       "mean      39.709757\n",
       "std       42.101261\n",
       "min        0.000000\n",
       "25%       14.290000\n",
       "50%       28.570000\n",
       "75%       50.000000\n",
       "max      300.000000\n",
       "Name: relative_error, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num_sec_bul['relative_error'] = round(100*df_num_sec_bul['absolute_error'] / df_num_sec_bul['num_bul'], 2)\n",
    "df_num_sec_bul['relative_error'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sentence-Transformers Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18773/18773 [21:12<00:00, 14.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# might want to try 'msmarco-distilbert-base-v2' too\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "df['para_enc'] = df.para.progress_map(model.encode)\n",
    "df.para_enc = df.para_enc.map(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cluster Based on Number of Sections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df.groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'para': lambda p: list(p),\n",
    "    'bullets': lambda b: list(b)[0],\n",
    "    'para_enc': lambda pe: list(pe)\n",
    "})\n",
    "df_group.para_enc = df_group.para_enc.map(np.vstack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group['num_sec'] = 0\n",
    "for idx in df_group.index.tolist():\n",
    "    df_group.loc[idx, 'num_sec'] = int(df_num_sec_bul.loc[idx, 'num_sec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:26<00:00, 16.99it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df_group['kmeans_centroids'] = df_group.progress_apply(lambda row:\\\n",
    "    KMeans(\n",
    "        row.num_sec,\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        random_state=config.SEED).fit(row.para_enc).cluster_centers_, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Low Dimensional Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "para_enc_2dim = df_group.para_enc.map(lambda pe: PCA(2, random_state=config.SEED).fit_transform(pe))\n",
    "kmeans_centroids_2dim =\\\n",
    "    df_group.kmeans_centroids.map(lambda pe: PCA(2, random_state=config.SEED).fit_transform(pe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sections for (9783318067095, 'ch5'): 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(len(df_group))\n",
    "idx = df_group.index.tolist()[i]\n",
    "print('Number of sections for %s: %d'%(idx, df_group.loc[idx, 'num_sec']))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    x = [x[0] for x in para_enc_2dim.loc[idx].tolist()],\n",
    "    y = [x[1] for x in para_enc_2dim.loc[idx].tolist()],\n",
    "    s = 20\n",
    ")\n",
    "plt.scatter(\n",
    "    x = [x[0] for x in kmeans_centroids_2dim.loc[idx].tolist()],\n",
    "    y = [x[1] for x in kmeans_centroids_2dim.loc[idx].tolist()],\n",
    "    marker='*',\n",
    "    s = 50\n",
    ")\n",
    "plt.xlabel('PCA-1')\n",
    "plt.ylabel('PCA-2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Cosine Similarity / Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18773/18773 [00:13<00:00, 1358.03it/s]\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "def row_cosine_sim(r):\n",
    "    idx = r.name[:2]\n",
    "    centroids = df_group.loc[idx, 'kmeans_centroids']\n",
    "    return [cosine_sim(r.para_enc, c) for c in centroids]\n",
    "\n",
    "df['cosine_sim'] = df.progress_apply(lambda row: row_cosine_sim(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18773/18773 [00:12<00:00, 1468.79it/s]\n"
     ]
    }
   ],
   "source": [
    "def euclidean_dist(a, b):\n",
    "    return np.sqrt(np.sum((a-b)**2))\n",
    "\n",
    "def row_euclidean_dist(r):\n",
    "    idx = r.name[:2]\n",
    "    centroids = df_group.loc[idx, 'kmeans_centroids']\n",
    "    return [euclidean_dist(r.para_enc, c) for c in centroids]\n",
    "\n",
    "df['euclidean_dist'] = df.progress_apply(lambda row: row_euclidean_dist(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find Best Paragraph for each Book, Chapter, Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_para = df.reset_index(level=[2, 3]).groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'section': lambda s: list(s),\n",
    "    'subsection': lambda ss: list(ss),\n",
    "    'para': lambda p: list(p),\n",
    "    'bullets': lambda b: list(b)[0],\n",
    "    'cosine_sim': lambda c: list(c),\n",
    "    'euclidean_dist': lambda e: list(e)\n",
    "})\n",
    "df_best_para.cosine_sim = df_best_para.cosine_sim.map(lambda c: np.array(c))\n",
    "df_best_para.euclidean_dist = df_best_para.euclidean_dist.map(lambda e: np.array(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_para['best_para_cosine'] = df_best_para.cosine_sim.map(lambda c: np.argmax(c, axis=0))\n",
    "df_best_para['best_para_euclidean'] = df_best_para.euclidean_dist.map(lambda e: np.argmin(e, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How Many Sections Are We Covering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.000000\n",
       "mean      65.742854\n",
       "std       15.920651\n",
       "min       20.000000\n",
       "25%       54.545455\n",
       "50%       66.666667\n",
       "75%       75.000000\n",
       "max      100.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_diversity(r):\n",
    "    best_idx = r.best_para_cosine\n",
    "    all_sections = set(r.section)\n",
    "    \n",
    "    selected_sections = set([r.section[i] for i in best_idx])\n",
    "    \n",
    "    return len(selected_sections.intersection(all_sections))/len(all_sections)*100\n",
    "    \n",
    "df_best_para.apply(lambda row: calculate_diversity(row), axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expanding from Best Paragraph Based on Cosine Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_para['para_num_tok'] =\\\n",
    "    df_best_para.para.map(lambda ps: np.array([len(tokenizer.tokenize(p)) for p in ps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:00<00:00, 561.38it/s]\n"
     ]
    }
   ],
   "source": [
    "def expand_based_on_cosine(r):\n",
    "    max_length = len(r.para)\n",
    "    max_idx = max_length-1\n",
    "    \n",
    "    extracted_para = []\n",
    "    \n",
    "    # Calculate the fraction we need to extract\n",
    "    # based on total number of tokens in this chp\n",
    "    # and number of centroids (sections) in this chp\n",
    "    # do not go over the model max length\n",
    "    num_tok_tot = sum(r.para_num_tok)\n",
    "    num_tok_th = min(\n",
    "        int(0.8*num_tok_tot / len(r.best_para_cosine)),\n",
    "        0.9*tokenizer.model_max_length)\n",
    "    \n",
    "    for i, best in enumerate(r.best_para_cosine):\n",
    "        merged_para_idx = [best]\n",
    "        num_tok = r.para_num_tok[best]\n",
    "        \n",
    "        while num_tok < num_tok_th:\n",
    "            if len(merged_para_idx) == max_length : break\n",
    "            elif 0 in merged_para_idx:\n",
    "                merged_para_idx.append(max(merged_para_idx)+1)\n",
    "            elif max_idx in merged_para_idx:\n",
    "                merged_para_idx.append(min(merged_para_idx)-1)\n",
    "            else:\n",
    "                if (r.cosine_sim)[min(merged_para_idx)-1, i] <\\\n",
    "                    (r.cosine_sim)[max(merged_para_idx)+1, i]:\n",
    "                    merged_para_idx.append(max(merged_para_idx)+1)\n",
    "                else:\n",
    "                    merged_para_idx.append(min(merged_para_idx)-1)\n",
    "            num_tok = np.sum(r.para_num_tok[merged_para_idx])\n",
    "                  \n",
    "        extracted_para.append(sorted(merged_para_idx))\n",
    "        \n",
    "    return extracted_para\n",
    "\n",
    "df_best_para['selected_para_cosine'] =\\\n",
    "    df_best_para.progress_apply(lambda row: expand_based_on_cosine(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Study Overlap and Remove Useless (>90% overlap) Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove_overlap = df_best_para.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_overlap_matrix(r):\n",
    "    num_centr = len(r.selected_para_cosine)\n",
    "    overlap_matrix = np.zeros((num_centr,num_centr))\n",
    "    \n",
    "    def list_overlap(a, b):\n",
    "        return list( set(a).intersection(set(b)) )\n",
    "    \n",
    "    for i in range(num_centr):\n",
    "        for j in range(num_centr):\n",
    "            if i == j : continue\n",
    "            num_tok_i = np.sum(r.para_num_tok[r.selected_para_cosine[i]])\n",
    "            overlap = list_overlap(\n",
    "                r.selected_para_cosine[i], r.selected_para_cosine[j])\n",
    "            num_tok_overlap = np.sum(r.para_num_tok[overlap])\n",
    "            assert num_tok_overlap <= num_tok_i\n",
    "            \n",
    "            overlap_matrix[i, j] = round(num_tok_overlap/num_tok_i*100, 2)\n",
    "    \n",
    "    return overlap_matrix\n",
    "\n",
    "def remove_big_overlap(r, threshold):\n",
    "    om = r.overlap_matrix\n",
    "    big_overlap_idx = np.argwhere(om >= threshold)\n",
    "    to_be_removed = set()\n",
    "    for idx in big_overlap_idx:\n",
    "        i, j = idx[0], idx[1]\n",
    "        if om[i, j] == om[j, i]:\n",
    "            if i in to_be_removed or j in to_be_removed : continue\n",
    "            else : to_be_removed.add(i)\n",
    "        elif om[i, j] > om[j, i]:\n",
    "            to_be_removed.add(i)\n",
    "        else:\n",
    "            to_be_removed.add(j)\n",
    "    return [s for i, s in enumerate(r.selected_para_cosine) if i not in to_be_removed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.000000\n",
       "mean       1.668874\n",
       "std        1.874568\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        2.000000\n",
       "75%        2.000000\n",
       "max        8.000000\n",
       "Name: overlap_matrix, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove_overlap['overlap_matrix'] = df_remove_overlap.apply(lambda row: create_overlap_matrix(row), axis=1)\n",
    "df_remove_overlap.overlap_matrix.map(lambda om: np.sum(om > 90)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.0\n",
       "mean       0.0\n",
       "std        0.0\n",
       "min        0.0\n",
       "25%        0.0\n",
       "50%        0.0\n",
       "75%        0.0\n",
       "max        0.0\n",
       "Name: overlap_matrix, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove_overlap.selected_para_cosine = df_remove_overlap.apply(lambda row: remove_big_overlap(row, 90), axis=1)\n",
    "\n",
    "df_remove_overlap['overlap_matrix'] = df_remove_overlap.apply(lambda row: create_overlap_matrix(row), axis=1)\n",
    "df_remove_overlap.overlap_matrix.map(lambda om: np.sum(om > 90)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge when >90% overlap Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_overlap = df_best_para.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_big_overlap(r, threshold):\n",
    "    om = r.overlap_matrix\n",
    "    big_overlap_idx = np.argwhere(om >= threshold)\n",
    "    big_overlap_idx = set([frozenset(t) for t in big_overlap_idx])\n",
    "    merged = set()\n",
    "    to_be_merged = set()\n",
    "    for idx in big_overlap_idx:\n",
    "        idx = tuple(idx)\n",
    "        i, j = idx[0], idx[1]\n",
    "        if i not in merged and j not in merged:\n",
    "            to_be_merged.add(idx)\n",
    "            merged.add(i)\n",
    "            merged.add(j)\n",
    "    return to_be_merged\n",
    "\n",
    "def merge_para(r):\n",
    "    for i, j in r.to_be_merged:\n",
    "        r.selected_para_cosine[i] = np.array(list(set(\n",
    "            np.concatenate((r.selected_para_cosine[i], r.selected_para_cosine[j])))))\n",
    "        \n",
    "        r.selected_para_cosine[j] = None\n",
    "    r.selected_para_cosine = [sp for sp in r.selected_para_cosine if sp is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para to be merged: 370\n",
      "Para to be merged: 39\n",
      "Para to be merged: 0\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    df_merge_overlap['overlap_matrix'] = df_merge_overlap.apply(create_overlap_matrix, axis=1)\n",
    "\n",
    "    df_merge_overlap['to_be_merged'] = df_merge_overlap.apply(lambda row: find_big_overlap(row, 90), axis=1)\n",
    "\n",
    "    num_to_be_merged = df_merge_overlap.to_be_merged.map(len).sum()\n",
    "    print('Para to be merged: %d'%num_to_be_merged)\n",
    "    if (num_to_be_merged <= 0) : break\n",
    "\n",
    "    df_merge_overlap.apply(merge_para, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize Results Remove Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_remove_overlap_tobesaved = df_remove_overlap.explode('selected_para_cosine')\n",
    "df_remove_overlap_tobesaved = df_remove_overlap_tobesaved.drop(\n",
    "    columns=['best_para_cosine', 'best_para_euclidean', 'cosine_sim', 'euclidean_dist', 'overlap_matrix'])\n",
    "\n",
    "df_remove_overlap_tobesaved['selected_para'] = df_remove_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para) if i in row.selected_para_cosine], axis=1)\n",
    "\n",
    "df_remove_overlap_tobesaved['para_num_tok'] = df_remove_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para_num_tok) if i in row.selected_para_cosine], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2235.000000\n",
       "mean      463.276063\n",
       "std       206.719927\n",
       "min        97.000000\n",
       "25%       315.000000\n",
       "50%       413.000000\n",
       "75%       574.000000\n",
       "max      1210.000000\n",
       "Name: para_num_tok, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove_overlap_tobesaved.para_num_tok.map(sum).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_remove_overlap_tobesaved.to_csv(OUTPUT_PATH+'df_cosine_remove.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare to Para Wordembed ST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(pred, ref):\n",
    "    return round(100*len(pred.intersection(ref)) / len(pred), 2)\n",
    "    \n",
    "def recall(pred, ref):\n",
    "    return round(100*len(pred.intersection(ref)) / len(ref), 2)\n",
    "\n",
    "def fmeasure(prec, rec):\n",
    "    if prec + rec == 0 : return 0\n",
    "    return round(2*prec*rec/(prec+rec), 2)\n",
    "\n",
    "flatten = lambda t: [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n",
      "\n",
      "       unsup_coverage  sup_coverage  intersection   precision      recall\n",
      "count      453.000000    453.000000    453.000000  453.000000  453.000000\n",
      "mean        66.222847     29.231305     20.627977   30.734967   69.446291\n",
      "std         11.215380     15.454056     13.061869   17.927027   21.818957\n",
      "min         19.285714      2.564103      0.000000    0.000000    0.000000\n",
      "25%         59.677419     18.000000     11.111111   17.240000   57.140000\n",
      "50%         66.666667     25.714286     18.181818   27.270000   70.000000\n",
      "75%         73.076923     38.095238     28.571429   41.670000   85.710000\n",
      "max         93.333333    100.000000     80.000000  100.000000  100.000000\n",
      "\n",
      "\n",
      "th\n",
      "\n",
      "       unsup_coverage  sup_coverage  intersection   precision      recall\n",
      "count      453.000000    453.000000    453.000000  453.000000  453.000000\n",
      "mean        66.222847     63.226644     43.387649   65.453422   68.358587\n",
      "std         11.215380     15.442000     14.890276   19.728711   16.783563\n",
      "min         19.285714     15.384615      0.000000    0.000000    0.000000\n",
      "25%         59.677419     54.166667     33.333333   53.190000   58.330000\n",
      "50%         66.666667     63.414634     44.186047   66.670000   69.230000\n",
      "75%         73.076923     75.000000     53.846154   79.310000   80.000000\n",
      "max         93.333333    100.000000     86.666667  100.000000  100.000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in ['base', 'th']:\n",
    "    df_para_wordembed_st =\\\n",
    "        pd.read_csv(magma_dir+'datasets/karger_books_para_wordembed/'+MODEL+'/st/df_'+t+'_selected_para.csv')\\\n",
    "        .set_index(['book', 'chapter'])\n",
    "    df_para_wordembed_st.para = df_para_wordembed_st.para.map(eval)\n",
    "    df_para_wordembed_st.best_match = df_para_wordembed_st.best_match.map(eval)\n",
    "\n",
    "    num_para = df_para_wordembed_st.para.map(len)\n",
    "\n",
    "    df_remove_overlap_tobesaved = pd.read_csv(OUTPUT_PATH+'df_cosine_remove.csv').set_index(['book', 'chapter'])\n",
    "    df_remove_overlap_tobesaved.selected_para_cosine = df_remove_overlap_tobesaved.selected_para_cosine.map(eval)\n",
    "    df_remove_overlap_tobesaved = df_remove_overlap_tobesaved.groupby(['book', 'chapter'], sort=False).agg({\n",
    "        'selected_para_cosine': lambda p: list(p)\n",
    "    })\n",
    "    df_remove_overlap_tobesaved.selected_para_cosine = df_remove_overlap_tobesaved.selected_para_cosine\n",
    "\n",
    "    selected_para = df_remove_overlap_tobesaved.selected_para_cosine.map(lambda pp: set(flatten(pp)))\n",
    "    best_match = df_para_wordembed_st.best_match.map(set)\n",
    "\n",
    "    df_comparison = pd.concat([num_para, selected_para, best_match], axis=1).rename(\n",
    "        columns={'para': 'num_para', 'selected_para_cosine': 'unsup_selected', 'best_match': 'sup_selected'})\n",
    "    df_comparison['unsup_coverage'] = 100*df_comparison.unsup_selected.map(len) / df_comparison.num_para\n",
    "    df_comparison['sup_coverage'] = 100*df_comparison.sup_selected.map(len) / df_comparison.num_para\n",
    "    \n",
    "    df_comparison['intersection'] = 100*df_comparison.apply(lambda r:\n",
    "        len(r.unsup_selected.intersection(r.sup_selected)) / r.num_para, axis=1)\n",
    "\n",
    "    df_comparison['precision'] = df_comparison.apply(lambda r:\n",
    "        precision(r.unsup_selected, r.sup_selected), axis=1)\n",
    "    df_comparison['recall'] = df_comparison.apply(lambda r:\n",
    "        recall(r.unsup_selected, r.sup_selected), axis=1)\n",
    "    \n",
    "    df_comparison.drop(columns='num_para', inplace=True)\n",
    "    \n",
    "    print(t+'\\n')\n",
    "    print(df_comparison.describe())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize Results Merge Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merge_overlap_tobesaved = df_merge_overlap.explode('selected_para_cosine')\n",
    "df_merge_overlap_tobesaved.selected_para_cosine = df_merge_overlap_tobesaved.selected_para_cosine.map(list)\n",
    "df_merge_overlap_tobesaved = df_merge_overlap_tobesaved.drop(\n",
    "    columns=['best_para_cosine', 'best_para_euclidean', 'cosine_sim',\n",
    "             'euclidean_dist', 'overlap_matrix', 'to_be_merged'])\n",
    "\n",
    "df_merge_overlap_tobesaved['selected_para'] = df_merge_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para) if i in row.selected_para_cosine], axis=1)\n",
    "\n",
    "df_merge_overlap_tobesaved['para_num_tok'] = df_merge_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para_num_tok) if i in row.selected_para_cosine], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2235.000000\n",
       "mean      463.687248\n",
       "std       207.006395\n",
       "min        97.000000\n",
       "25%       316.000000\n",
       "50%       413.000000\n",
       "75%       576.000000\n",
       "max      1210.000000\n",
       "Name: para_num_tok, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_overlap_tobesaved.para_num_tok.map(sum).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merge_overlap_tobesaved.to_csv(OUTPUT_PATH+'df_cosine_merge.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare to Para Wordembed ST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n",
      "\n",
      "       unsup_coverage  sup_coverage  intersection   precision      recall\n",
      "count      453.000000    453.000000    453.000000  453.000000  453.000000\n",
      "mean        66.537857     29.231305     20.727682   30.699448   69.708035\n",
      "std         11.170373     15.454056     13.166065   17.880305   21.806849\n",
      "min         19.285714      2.564103      0.000000    0.000000    0.000000\n",
      "25%         60.000000     18.000000     11.111111   17.240000   57.140000\n",
      "50%         66.666667     25.714286     18.181818   26.830000   70.000000\n",
      "75%         73.333333     38.095238     28.571429   41.670000   85.710000\n",
      "max         93.333333    100.000000     80.000000  100.000000  100.000000\n",
      "\n",
      "\n",
      "th\n",
      "\n",
      "       unsup_coverage  sup_coverage  intersection   precision      recall\n",
      "count      453.000000    453.000000    453.000000  453.000000  453.000000\n",
      "mean        66.537857     63.226644     43.559783   65.374415   68.635011\n",
      "std         11.170373     15.442000     14.935850   19.660060   16.835372\n",
      "min         19.285714     15.384615      0.000000    0.000000    0.000000\n",
      "25%         60.000000     54.166667     33.333333   52.940000   58.330000\n",
      "50%         66.666667     63.414634     44.354839   66.670000   70.000000\n",
      "75%         73.333333     75.000000     53.846154   79.310000   80.000000\n",
      "max         93.333333    100.000000     86.666667  100.000000  100.000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in ['base', 'th']:\n",
    "    df_para_wordembed_st =\\\n",
    "        pd.read_csv(magma_dir+'datasets/karger_books_para_wordembed/'+MODEL+'/st/df_'+t+'_selected_para.csv')\\\n",
    "        .set_index(['book', 'chapter'])\n",
    "    df_para_wordembed_st.para = df_para_wordembed_st.para.map(eval)\n",
    "    df_para_wordembed_st.best_match = df_para_wordembed_st.best_match.map(eval)\n",
    "\n",
    "    num_para = df_para_wordembed_st.para.map(len)\n",
    "\n",
    "    df_remove_overlap_tobesaved = pd.read_csv(OUTPUT_PATH+'df_cosine_merge.csv').set_index(['book', 'chapter'])\n",
    "    df_remove_overlap_tobesaved.selected_para_cosine = df_remove_overlap_tobesaved.selected_para_cosine.map(eval)\n",
    "    df_remove_overlap_tobesaved = df_remove_overlap_tobesaved.groupby(['book', 'chapter'], sort=False).agg({\n",
    "        'selected_para_cosine': lambda p: list(p)\n",
    "    })\n",
    "    df_remove_overlap_tobesaved.selected_para_cosine = df_remove_overlap_tobesaved.selected_para_cosine\n",
    "\n",
    "    selected_para = df_remove_overlap_tobesaved.selected_para_cosine.map(lambda pp: set(flatten(pp)))\n",
    "    best_match = df_para_wordembed_st.best_match.map(set)\n",
    "\n",
    "    df_comparison = pd.concat([num_para, selected_para, best_match], axis=1).rename(\n",
    "        columns={'para': 'num_para', 'selected_para_cosine': 'unsup_selected', 'best_match': 'sup_selected'})\n",
    "    df_comparison['unsup_coverage'] = 100*df_comparison.unsup_selected.map(len) / df_comparison.num_para\n",
    "    df_comparison['sup_coverage'] = 100*df_comparison.sup_selected.map(len) / df_comparison.num_para\n",
    "    \n",
    "    df_comparison['intersection'] = 100*df_comparison.apply(lambda r:\n",
    "        len(r.unsup_selected.intersection(r.sup_selected)) / r.num_para, axis=1)\n",
    "\n",
    "    df_comparison['precision'] = df_comparison.apply(lambda r:\n",
    "        precision(r.unsup_selected, r.sup_selected), axis=1)\n",
    "    df_comparison['recall'] = df_comparison.apply(lambda r:\n",
    "        recall(r.unsup_selected, r.sup_selected), axis=1)\n",
    "    \n",
    "    df_comparison.drop(columns='num_para', inplace=True)\n",
    "    \n",
    "    print(t+'\\n')\n",
    "    print(df_comparison.describe())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Expanding from Best Paragraph Based on Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_best_para['para_num_tok'] =\\\n",
    "    df_best_para.para.map(lambda ps: np.array([len(tokenizer.tokenize(p)) for p in ps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def expand_based_on_euclidean(r):\n",
    "    max_length = len(r.para)\n",
    "    max_idx = max_length-1\n",
    "    \n",
    "    extracted_para = []\n",
    "    \n",
    "    # Calculate the fraction we need to extract\n",
    "    # based on total number of tokens in this chp\n",
    "    # and number of centroids (sections) in this chp\n",
    "    # do not go over the model max length\n",
    "    num_tok_tot = sum(r.para_num_tok)\n",
    "    num_tok_th = min(\n",
    "        int(0.8*num_tok_tot / len(r.best_para_euclidean)),\n",
    "        0.9*tokenizer.model_max_length)\n",
    "    \n",
    "    for i, best in enumerate(r.best_para_euclidean):\n",
    "        merged_para_idx = [best]\n",
    "        num_tok = r.para_num_tok[best]\n",
    "        \n",
    "        while num_tok < num_tok_th:\n",
    "            if len(merged_para_idx) == max_length : break\n",
    "            elif 0 in merged_para_idx:\n",
    "                merged_para_idx.append(max(merged_para_idx)+1)\n",
    "            elif max_idx in merged_para_idx:\n",
    "                merged_para_idx.append(min(merged_para_idx)-1)\n",
    "            else:\n",
    "                if (r.euclidean_dist)[min(merged_para_idx)-1, i] >\\\n",
    "                    (r.euclidean_dist)[max(merged_para_idx)+1, i]:\n",
    "                    merged_para_idx.append(max(merged_para_idx)+1)\n",
    "                else:\n",
    "                    merged_para_idx.append(min(merged_para_idx)-1)\n",
    "            num_tok = np.sum(r.para_num_tok[merged_para_idx])\n",
    "                  \n",
    "        extracted_para.append(sorted(merged_para_idx))\n",
    "        \n",
    "    return extracted_para\n",
    "\n",
    "df_best_para['selected_para_euclidean'] =\\\n",
    "    df_best_para.progress_apply(lambda row: expand_based_on_euclidean(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Study Overlap and Remove Useless (>90% overlap) Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def study_overlap(r):\n",
    "    num_centr = len(r.selected_para_euclidean)\n",
    "    overlap_matrix = np.zeros((num_centr,num_centr))\n",
    "    \n",
    "    def list_overlap(a, b):\n",
    "        return list( set(a).intersection(set(b)) )\n",
    "    \n",
    "    for i in range(num_centr):\n",
    "        for j in range(num_centr):\n",
    "            if i == j : continue\n",
    "            num_tok_i = np.sum(r.para_num_tok[r.selected_para_euclidean[i]])\n",
    "            overlap = list_overlap(\n",
    "                r.selected_para_euclidean[i], r.selected_para_euclidean[j])\n",
    "            num_tok_overlap = np.sum(r.para_num_tok[overlap])\n",
    "            assert num_tok_overlap <= num_tok_i\n",
    "            \n",
    "            overlap_matrix[i, j] = num_tok_overlap/num_tok_i*100\n",
    "    \n",
    "    return overlap_matrix\n",
    "    \n",
    "df_best_para['overlap_matrix'] = df_best_para.apply(lambda row: study_overlap(row), axis=1)\n",
    "df_best_para.overlap_matrix.map(lambda om: np.sum(om > 90)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_big_overlap(r, threshold):\n",
    "    om = r.overlap_matrix\n",
    "    big_overlap_idx = np.argwhere(om >= threshold)\n",
    "    to_be_removed = set()\n",
    "    for idx in big_overlap_idx:\n",
    "        i, j = idx[0], idx[1]\n",
    "        if om[i, j] == om[j, i]:\n",
    "            if i in to_be_removed or j in to_be_removed : continue\n",
    "            else : to_be_removed.add(i)\n",
    "        elif om[i, j] > om[j, i]:\n",
    "            to_be_removed.add(i)\n",
    "        else:\n",
    "            to_be_removed.add(j)\n",
    "    return [s for i, s in enumerate(r.selected_para_euclidean) if i not in to_be_removed]\n",
    "\n",
    "df_best_para.selected_para_euclidean = df_best_para.apply(lambda row: remove_big_overlap(row, 90), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_best_para['overlap_matrix'] = df_best_para.apply(lambda row: study_overlap(row), axis=1)\n",
    "df_best_para.overlap_matrix.map(lambda om: np.sum(om > 90)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Finalize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_selected_para_euclidean = df_best_para.explode('selected_para_euclidean')\n",
    "df_selected_para_euclidean = df_selected_para_euclidean.drop(\n",
    "    columns=['best_para_cosine', 'best_para_euclidean', 'cosine_sim', 'euclidean_dist', 'overlap_matrix'])\n",
    "\n",
    "df_selected_para_euclidean['selected_para'] = df_selected_para_euclidean.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para) if i in row.selected_para_euclidean], axis=1)\n",
    "\n",
    "df_selected_para_euclidean['para_num_tok'] = df_selected_para_euclidean.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para_num_tok) if i in row.selected_para_euclidean], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_selected_para_euclidean.para_num_tok.map(sum).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_selected_para_euclidean.to_csv(OUTPUT_PATH+'df_euclidean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Compare to Para Wordembed ST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_para_wordembed_st =\\\n",
    "    pd.read_csv(magma_dir+'datasets/karger_books_para_wordembed/'+MODEL+'/st/df_selected_para.csv')\\\n",
    "    .set_index(['book', 'chapter'])\n",
    "df_para_wordembed_st.para = df_para_wordembed_st.para.map(eval)\n",
    "df_para_wordembed_st.best_match = df_para_wordembed_st.best_match.map(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "\n",
    "selected_para = df_best_para.selected_para_euclidean.map(lambda l: set(flatten(l)))\n",
    "\n",
    "best_match = df_para_wordembed_st.best_match.map(set)\n",
    "\n",
    "df_comparison = pd.concat([selected_para, best_match], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def precision(pred, ref):\n",
    "    return round(100*len(pred.intersection(ref)) / len(pred), 2)\n",
    "    \n",
    "def recall(pred, ref):\n",
    "    return round(100*len(pred.intersection(ref)) / len(ref), 2)\n",
    "\n",
    "def fmeasure(prec, rec):\n",
    "    if prec + rec == 0 : return 0\n",
    "    return round(2*prec*rec/(prec+rec), 2)\n",
    "\n",
    "df_comparison['precision'] = df_comparison.apply(\n",
    "    lambda row: precision(row.selected_para_euclidean, row.best_match), axis=1)\n",
    "df_comparison['recall'] = df_comparison.apply(\n",
    "    lambda row: recall(row.selected_para_euclidean, row.best_match), axis=1)\n",
    "df_comparison['fmeasure'] = df_comparison.apply(\n",
    "    lambda row: fmeasure(row.precision, row.recall), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_comparison.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "mFAC31paODFl",
    "S0FByNNOIRvG",
    "tb7fAfzaK4es",
    "eQGq4WLu3Gei",
    "tSHT0mxuvkEp",
    "-eRnW74aH95b",
    "X2xp7jJNwB6b",
    "2Eb-_Ud3vxeY",
    "VndEUBoDjjkV",
    "8_li_hFKF_Ws"
   ],
   "name": "paragraph_assign_bullets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
