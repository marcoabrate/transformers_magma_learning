{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 16184,
     "status": "ok",
     "timestamp": 1610463826089,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "ClE5D523OTZG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/home/marco/epfl/magma/')\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 16184,
     "status": "ok",
     "timestamp": 1610463826092,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "82WSp6khIcua"
   },
   "outputs": [],
   "source": [
    "MODEL = 'pegasus'\n",
    "\n",
    "RE_SPLITTER = '\\n'              # do we split sentences of paragraphs?\n",
    "                                # use '\\.(?!\\d)|\\n' or '\\n', respectively\n",
    "\n",
    "# Output path\n",
    "OUTPUT_PATH = config.MAGMA_DIR+'pipeline/paragraph_extraction_embeddings/'+MODEL+'/'\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb7fAfzaK4es"
   },
   "source": [
    "### **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 16443,
     "status": "ok",
     "timestamp": 1610463826354,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "wvbMlPBxk45S"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from textwrap import fill\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "if 'pegasus' in MODEL:\n",
    "    from transformers import PegasusTokenizer\n",
    "    tokenizer =\\\n",
    "        PegasusTokenizer.from_pretrained('google/pegasus-large')\n",
    "elif 'bart' in MODEL:\n",
    "    from transformers import BartTokenizer\n",
    "    tokenizer =\\\n",
    "        BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "elif 't5' in MODEL:\n",
    "    from transformers import T5Tokenizer\n",
    "    tokenizer =\\\n",
    "        T5Tokenizer.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQGq4WLu3Gei"
   },
   "source": [
    "### **Karger Books Base Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z0lbkScg0a7j"
   },
   "outputs": [],
   "source": [
    "base_dataset = config.MAGMA_DIR+'datasets/karger_books_base/df.csv'\n",
    "df = pd.read_csv(base_dataset)\n",
    "df = df.set_index(['book', 'chapter', 'section', 'subsection'])\n",
    "df.bullets = df.bullets.map(eval, na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSHT0mxuvkEp"
   },
   "source": [
    "### **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eRnW74aH95b"
   },
   "source": [
    "#### Preprocessing\n",
    "\n",
    "* Split based on RE_SPLITTER\n",
    "* Explode the dataset\n",
    "* Remove unwanted chars at beginning or end of sentence\n",
    "* Remove multiple spaces\n",
    "* Remove long words (> config.TOKEN_MAX_LEN chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CDsT33j-wPCw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split in sentences / paragraphs based on RE_SPLITTER\n",
    "df.text =\\\n",
    "    df.text.map(lambda x: [p.strip() for p in re.split(RE_SPLITTER, x) if p!=''],\n",
    "                na_action='ignore')\n",
    "    \n",
    "# explode to get one row for each paragraph /sentence\n",
    "df = df.explode('text')\n",
    "df = df.rename(columns={'text': 'para'})\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove unwanted chars at beginning or end of sentence\n",
    "df.para = df.para.map(lambda p: p.lstrip('.,;:-)] \\n'))\n",
    "df.para = df.para.map(lambda p: p.rstrip('.,;:-([ \\n'))\n",
    "\n",
    "# Remove multiple spaces\n",
    "df.para = df.para.map(lambda p:\n",
    "    re.sub('\\s+', ' ', p).strip())\n",
    "\n",
    "# Remove long words (> config.TOKEN_MAX_LEN chars)\n",
    "def para2words(para):\n",
    "    return gensim.utils.simple_preprocess(\n",
    "        para, deacc=True, max_len=config.TOKEN_MAX_LEN)\n",
    "df['para_proc'] = df.para.map(para2words)\n",
    "df['bullets_proc'] = df.bullets.map(lambda bs: [para2words(b) for b in bs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Preprocessing\n",
    "\n",
    "* Remove stop words\n",
    "* Remove short sentences / paragraphs (< config.PARA_MIN_LEN tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "df.para_proc = df.para_proc.map(lambda p:\n",
    "    [w for w in p if w not in stop_words])\n",
    "df.bullets_proc = df.bullets_proc.map(lambda bs:\n",
    "    [[w for w in b if w not in stop_words] for b in bs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove short sentences / paragraphs (< config.PARA_MIN_LEN tokens)\n",
    "df.loc[df.para_proc.map(len) <\\\n",
    "    config.PARA_MIN_LEN, 'para_proc'] = np.nan\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.para = df.para.map(lambda p: p+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Number of Sections vs Bullets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.000000\n",
       "mean       2.070640\n",
       "std        1.928902\n",
       "min        0.000000\n",
       "25%        1.000000\n",
       "50%        2.000000\n",
       "75%        3.000000\n",
       "max       13.000000\n",
       "Name: absolute_error, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num_sec_bul = df.groupby(['book', 'chapter', 'section'], sort=False).agg({\n",
    "    'para': lambda p: list(p),\n",
    "    'bullets': lambda b: list(b)[0]\n",
    "}).groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'para': lambda p: len(list(p)),\n",
    "    'bullets': lambda b: len(list(b)[0])\n",
    "}).rename(columns={'para':'num_sec', 'bullets':'num_bul'})\n",
    "\n",
    "df_num_sec_bul['absolute_error'] = np.abs(df_num_sec_bul['num_sec'] - df_num_sec_bul['num_bul'])\n",
    "\n",
    "df_num_sec_bul['absolute_error'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.000000\n",
       "mean      39.709757\n",
       "std       42.101261\n",
       "min        0.000000\n",
       "25%       14.290000\n",
       "50%       28.570000\n",
       "75%       50.000000\n",
       "max      300.000000\n",
       "Name: relative_error, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num_sec_bul['relative_error'] = round(100*df_num_sec_bul['absolute_error'] / df_num_sec_bul['num_bul'], 2)\n",
    "df_num_sec_bul['relative_error'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sentence-Transformers Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18773/18773 [24:51<00:00, 12.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# might want to try 'msmarco-distilbert-base-v2' too\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "df['para_enc'] = df.para.progress_map(model.encode)\n",
    "df.para_enc = df.para_enc.map(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cluster Based on Number of Sections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df.groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'para': lambda p: list(p),\n",
    "    'bullets': lambda b: list(b)[0],\n",
    "    'para_enc': lambda pe: list(pe)\n",
    "})\n",
    "df_group.para_enc = df_group.para_enc.map(np.vstack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group['num_sec'] = 0\n",
    "for idx in df_group.index.tolist():\n",
    "    df_group.loc[idx, 'num_sec'] = int(df_num_sec_bul.loc[idx, 'num_sec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:21<00:00, 21.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df_group['kmeans_centroids'] = df_group.progress_apply(lambda row:\\\n",
    "    KMeans(\n",
    "        row.num_sec,\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        random_state=config.SEED).fit(row.para_enc).cluster_centers_, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Low Dimensional Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "para_enc_2dim = df_group.para_enc.map(lambda pe: PCA(2, random_state=config.SEED).fit_transform(pe))\n",
    "kmeans_centroids_2dim =\\\n",
    "    df_group.kmeans_centroids.map(lambda pe: PCA(2, random_state=config.SEED).fit_transform(pe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sections for (9783318066241, 'ch4'): 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(len(df_group))\n",
    "idx = df_group.index.tolist()[i]\n",
    "print('Number of sections for %s: %d'%(idx, df_group.loc[idx, 'num_sec']))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    x = [x[0] for x in para_enc_2dim.loc[idx].tolist()],\n",
    "    y = [x[1] for x in para_enc_2dim.loc[idx].tolist()],\n",
    "    s = 20\n",
    ")\n",
    "plt.scatter(\n",
    "    x = [x[0] for x in kmeans_centroids_2dim.loc[idx].tolist()],\n",
    "    y = [x[1] for x in kmeans_centroids_2dim.loc[idx].tolist()],\n",
    "    marker='*',\n",
    "    s = 50\n",
    ")\n",
    "plt.xlabel('PCA-1')\n",
    "plt.ylabel('PCA-2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Cosine Similarity / Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18773/18773 [00:17<00:00, 1060.44it/s]\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "def row_cosine_sim(r):\n",
    "    idx = r.name[:2]\n",
    "    centroids = df_group.loc[idx, 'kmeans_centroids']\n",
    "    return [cosine_sim(r.para_enc, c) for c in centroids]\n",
    "\n",
    "df['cosine_sim'] = df.progress_apply(lambda row: row_cosine_sim(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18773/18773 [00:10<00:00, 1785.01it/s]\n"
     ]
    }
   ],
   "source": [
    "def euclidean_dist(a, b):\n",
    "    return np.sqrt(np.sum((a-b)**2))\n",
    "\n",
    "def row_euclidean_dist(r):\n",
    "    idx = r.name[:2]\n",
    "    centroids = df_group.loc[idx, 'kmeans_centroids']\n",
    "    return [euclidean_dist(r.para_enc, c) for c in centroids]\n",
    "\n",
    "df['euclidean_dist'] = df.progress_apply(lambda row: row_euclidean_dist(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find Best Paragraph for each Book, Chapter, Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_para = df.reset_index(level=[2, 3]).groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'section': lambda s: list(s),\n",
    "    'subsection': lambda ss: list(ss),\n",
    "    'para': lambda p: list(p),\n",
    "    'bullets': lambda b: list(b)[0],\n",
    "    'cosine_sim': lambda c: list(c),\n",
    "    'euclidean_dist': lambda e: list(e)\n",
    "})\n",
    "df_best_para.cosine_sim = df_best_para.cosine_sim.map(lambda c: np.array(c))\n",
    "df_best_para.euclidean_dist = df_best_para.euclidean_dist.map(lambda e: np.array(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_para['best_para_cosine'] = df_best_para.cosine_sim.map(lambda c: np.argmax(c, axis=0))\n",
    "df_best_para['best_para_euclidean'] = df_best_para.euclidean_dist.map(lambda e: np.argmin(e, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How Many Sections Are We Covering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.000000\n",
       "mean      65.742854\n",
       "std       15.920651\n",
       "min       20.000000\n",
       "25%       54.545455\n",
       "50%       66.666667\n",
       "75%       75.000000\n",
       "max      100.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_diversity(r):\n",
    "    best_idx = r.best_para_cosine\n",
    "    all_sections = set(r.section)\n",
    "    \n",
    "    selected_sections = set([r.section[i] for i in best_idx])\n",
    "    \n",
    "    return len(selected_sections.intersection(all_sections))/len(all_sections)*100\n",
    "    \n",
    "df_best_para.apply(lambda row: calculate_diversity(row), axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Expanding from Best Paragraph Based on Cosine Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_best_para['para_num_tok'] =\\\n",
    "    df_best_para.para.map(lambda ps: np.array([len(tokenizer.tokenize(p)) for p in ps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:00<00:00, 625.16it/s]\n"
     ]
    }
   ],
   "source": [
    "def expand_based_on_cosine(r):\n",
    "    max_length = len(r.para)\n",
    "    max_idx = max_length-1\n",
    "    \n",
    "    extracted_para = []\n",
    "    \n",
    "    # Calculate the fraction we need to extract\n",
    "    # based on total number of tokens in this chp\n",
    "    # and number of centroids (sections) in this chp\n",
    "    # do not go over the model max length\n",
    "    num_tok_tot = sum(r.para_num_tok)\n",
    "    num_tok_th = min(\n",
    "        int(0.8*num_tok_tot / len(r.best_para_cosine)),\n",
    "        0.9*tokenizer.model_max_length)\n",
    "    \n",
    "    for i, best in enumerate(r.best_para_cosine):\n",
    "        merged_para_idx = [best]\n",
    "        num_tok = r.para_num_tok[best]\n",
    "        \n",
    "        while num_tok < num_tok_th:\n",
    "            if len(merged_para_idx) == max_length : break\n",
    "            elif 0 in merged_para_idx:\n",
    "                merged_para_idx.append(max(merged_para_idx)+1)\n",
    "            elif max_idx in merged_para_idx:\n",
    "                merged_para_idx.append(min(merged_para_idx)-1)\n",
    "            else:\n",
    "                if (r.cosine_sim)[min(merged_para_idx)-1, i] <\\\n",
    "                    (r.cosine_sim)[max(merged_para_idx)+1, i]:\n",
    "                    merged_para_idx.append(max(merged_para_idx)+1)\n",
    "                else:\n",
    "                    merged_para_idx.append(min(merged_para_idx)-1)\n",
    "            num_tok = np.sum(r.para_num_tok[merged_para_idx])\n",
    "                  \n",
    "        extracted_para.append(sorted(merged_para_idx))\n",
    "        \n",
    "    return extracted_para\n",
    "\n",
    "df_best_para['selected_para_cosine'] =\\\n",
    "    df_best_para.progress_apply(lambda row: expand_based_on_cosine(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Study Overlap and Remove Useless (>90% overlap) Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_remove_overlap = df_best_para.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_overlap_matrix(r):\n",
    "    num_centr = len(r.selected_para_cosine)\n",
    "    overlap_matrix = np.zeros((num_centr,num_centr))\n",
    "    \n",
    "    def list_overlap(a, b):\n",
    "        return list( set(a).intersection(set(b)) )\n",
    "    \n",
    "    for i in range(num_centr):\n",
    "        for j in range(num_centr):\n",
    "            if i == j : continue\n",
    "            num_tok_i = np.sum(r.para_num_tok[r.selected_para_cosine[i]])\n",
    "            overlap = list_overlap(\n",
    "                r.selected_para_cosine[i], r.selected_para_cosine[j])\n",
    "            num_tok_overlap = np.sum(r.para_num_tok[overlap])\n",
    "            assert num_tok_overlap <= num_tok_i\n",
    "            \n",
    "            overlap_matrix[i, j] = round(num_tok_overlap/num_tok_i*100, 2)\n",
    "    \n",
    "    return overlap_matrix\n",
    "\n",
    "def remove_big_overlap(r, threshold):\n",
    "    om = r.overlap_matrix\n",
    "    big_overlap_idx = np.argwhere(om >= threshold)\n",
    "    to_be_removed = set()\n",
    "    for idx in big_overlap_idx:\n",
    "        i, j = idx[0], idx[1]\n",
    "        if om[i, j] == om[j, i]:\n",
    "            if i in to_be_removed or j in to_be_removed : continue\n",
    "            else : to_be_removed.add(i)\n",
    "        elif om[i, j] > om[j, i]:\n",
    "            to_be_removed.add(i)\n",
    "        else:\n",
    "            to_be_removed.add(j)\n",
    "    return [s for i, s in enumerate(r.selected_para_cosine) if i not in to_be_removed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.000000\n",
       "mean       1.699779\n",
       "std        1.934303\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        2.000000\n",
       "75%        2.000000\n",
       "max       10.000000\n",
       "Name: overlap_matrix, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove_overlap['overlap_matrix'] = df_remove_overlap.apply(lambda row: create_overlap_matrix(row), axis=1)\n",
    "df_remove_overlap.overlap_matrix.map(lambda om: np.sum(om > 90)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.0\n",
       "mean       0.0\n",
       "std        0.0\n",
       "min        0.0\n",
       "25%        0.0\n",
       "50%        0.0\n",
       "75%        0.0\n",
       "max        0.0\n",
       "Name: overlap_matrix, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove_overlap.selected_para_cosine = df_remove_overlap.apply(lambda row: remove_big_overlap(row, 90), axis=1)\n",
    "\n",
    "df_remove_overlap['overlap_matrix'] = df_remove_overlap.apply(lambda row: create_overlap_matrix(row), axis=1)\n",
    "df_remove_overlap.overlap_matrix.map(lambda om: np.sum(om > 90)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Merge when >90% overlap Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_merge_overlap = df_best_para.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_big_overlap(r, threshold):\n",
    "    om = r.overlap_matrix\n",
    "    big_overlap_idx = np.argwhere(om >= threshold)\n",
    "    big_overlap_idx = set([frozenset(t) for t in big_overlap_idx])\n",
    "    merged = set()\n",
    "    to_be_merged = set()\n",
    "    for idx in big_overlap_idx:\n",
    "        idx = tuple(idx)\n",
    "        i, j = idx[0], idx[1]\n",
    "        if i not in merged and j not in merged:\n",
    "            to_be_merged.add(idx)\n",
    "            merged.add(i)\n",
    "            merged.add(j)\n",
    "    return to_be_merged\n",
    "\n",
    "def merge_para(r):\n",
    "    for i, j in r.to_be_merged:\n",
    "        r.selected_para_cosine[i] = np.array(list(set(\n",
    "            np.concatenate((r.selected_para_cosine[i], r.selected_para_cosine[j])))))\n",
    "        \n",
    "        r.selected_para_cosine[j] = None\n",
    "    r.selected_para_cosine = [sp for sp in r.selected_para_cosine if sp is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para to be merged: 371\n",
      "Para to be merged: 41\n",
      "Para to be merged: 0\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    df_merge_overlap['overlap_matrix'] = df_merge_overlap.apply(create_overlap_matrix, axis=1)\n",
    "\n",
    "    df_merge_overlap['to_be_merged'] = df_merge_overlap.apply(lambda row: find_big_overlap(row, 90), axis=1)\n",
    "\n",
    "    num_to_be_merged = df_merge_overlap.to_be_merged.map(len).sum()\n",
    "    print('Para to be merged: %d'%num_to_be_merged)\n",
    "    if (num_to_be_merged <= 0) : break\n",
    "\n",
    "    df_merge_overlap.apply(merge_para, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize Results Remove Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_remove_overlap_tobesaved = df_remove_overlap.explode('selected_para_cosine')\n",
    "df_remove_overlap_tobesaved = df_remove_overlap_tobesaved.drop(\n",
    "    columns=['best_para_cosine', 'best_para_euclidean', 'cosine_sim', 'euclidean_dist', 'overlap_matrix'])\n",
    "\n",
    "df_remove_overlap_tobesaved['selected_para'] = df_remove_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para) if i in row.selected_para_cosine], axis=1)\n",
    "\n",
    "df_remove_overlap_tobesaved['para_num_tok'] = df_remove_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para_num_tok) if i in row.selected_para_cosine], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2232.000000\n",
       "mean      428.370968\n",
       "std       195.554295\n",
       "min        87.000000\n",
       "25%       290.000000\n",
       "50%       380.000000\n",
       "75%       528.000000\n",
       "max      1095.000000\n",
       "Name: para_num_tok, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remove_overlap_tobesaved.para_num_tok.map(sum).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_remove_overlap_tobesaved.to_csv(OUTPUT_PATH+'df_cosine_remove.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare to Para Wordembed ST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(pred, ref):\n",
    "    return round(100*len(pred.intersection(ref)) / len(pred), 2)\n",
    "    \n",
    "def recall(pred, ref):\n",
    "    return round(100*len(pred.intersection(ref)) / len(ref), 2)\n",
    "\n",
    "def fmeasure(prec, rec):\n",
    "    if prec + rec == 0 : return 0\n",
    "    return round(2*prec*rec/(prec+rec), 2)\n",
    "\n",
    "flatten = lambda t: [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       unsup_coverage  sup_coverage  intersection   precision      recall\n",
      "count      453.000000    453.000000    453.000000  453.000000  453.000000\n",
      "mean        66.366254     29.168566     20.579869   30.617241   69.447461\n",
      "std         11.169539     15.464656     13.066681   17.976710   21.781380\n",
      "min         20.000000      2.564103      0.000000    0.000000    0.000000\n",
      "25%         59.375000     18.095238     11.111111   17.240000   56.250000\n",
      "50%         66.666667     25.925926     18.333333   27.500000   70.000000\n",
      "75%         73.333333     37.837838     28.571429   40.000000   85.710000\n",
      "max        100.000000    100.000000     80.000000  100.000000  100.000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_para_wordembed_st =\\\n",
    "    pd.read_csv(config.MAGMA_DIR+'datasets/bullet_paragraph_embeddings/'+MODEL+'/st/df_base_selected_para.csv')\\\n",
    "    .set_index(['book', 'chapter'])\n",
    "df_para_wordembed_st.para = df_para_wordembed_st.para.map(eval)\n",
    "df_para_wordembed_st.best_match = df_para_wordembed_st.best_match.map(eval)\n",
    "\n",
    "num_para = df_para_wordembed_st.para.map(len)\n",
    "\n",
    "df_remove_overlap_tobesaved = pd.read_csv(OUTPUT_PATH+'df_cosine_remove.csv').set_index(['book', 'chapter'])\n",
    "df_remove_overlap_tobesaved.selected_para_cosine = df_remove_overlap_tobesaved.selected_para_cosine.map(eval)\n",
    "df_remove_overlap_tobesaved = df_remove_overlap_tobesaved.groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'selected_para_cosine': lambda p: list(p)\n",
    "})\n",
    "df_remove_overlap_tobesaved.selected_para_cosine = df_remove_overlap_tobesaved.selected_para_cosine\n",
    "\n",
    "selected_para = df_remove_overlap_tobesaved.selected_para_cosine.map(lambda pp: set(flatten(pp)))\n",
    "best_match = df_para_wordembed_st.best_match.map(set)\n",
    "\n",
    "df_comparison = pd.concat([num_para, selected_para, best_match], axis=1).rename(\n",
    "    columns={'para': 'num_para', 'selected_para_cosine': 'unsup_selected', 'best_match': 'sup_selected'})\n",
    "df_comparison['unsup_coverage'] = 100*df_comparison.unsup_selected.map(len) / df_comparison.num_para\n",
    "df_comparison['sup_coverage'] = 100*df_comparison.sup_selected.map(len) / df_comparison.num_para\n",
    "\n",
    "df_comparison['intersection'] = 100*df_comparison.apply(lambda r:\n",
    "    len(r.unsup_selected.intersection(r.sup_selected)) / r.num_para, axis=1)\n",
    "\n",
    "df_comparison['precision'] = df_comparison.apply(lambda r:\n",
    "    precision(r.unsup_selected, r.sup_selected), axis=1)\n",
    "df_comparison['recall'] = df_comparison.apply(lambda r:\n",
    "    recall(r.unsup_selected, r.sup_selected), axis=1)\n",
    "\n",
    "df_comparison.drop(columns='num_para', inplace=True)\n",
    "\n",
    "print(df_comparison.describe())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize Results Merge Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merge_overlap_tobesaved = df_merge_overlap.explode('selected_para_cosine')\n",
    "df_merge_overlap_tobesaved.selected_para_cosine = df_merge_overlap_tobesaved.selected_para_cosine.map(list)\n",
    "df_merge_overlap_tobesaved = df_merge_overlap_tobesaved.drop(\n",
    "    columns=['best_para_cosine', 'best_para_euclidean', 'cosine_sim',\n",
    "             'euclidean_dist', 'overlap_matrix', 'to_be_merged'])\n",
    "\n",
    "df_merge_overlap_tobesaved['selected_para'] = df_merge_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para) if i in row.selected_para_cosine], axis=1)\n",
    "\n",
    "df_merge_overlap_tobesaved['para_num_tok'] = df_merge_overlap_tobesaved.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para_num_tok) if i in row.selected_para_cosine], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2232.000000\n",
       "mean      428.812276\n",
       "std       195.895995\n",
       "min        87.000000\n",
       "25%       290.000000\n",
       "50%       380.000000\n",
       "75%       529.000000\n",
       "max      1103.000000\n",
       "Name: para_num_tok, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_overlap_tobesaved.para_num_tok.map(sum).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merge_overlap_tobesaved.to_csv(OUTPUT_PATH+'df_cosine_merge.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare to Para Wordembed ST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       unsup_coverage  sup_coverage  intersection   precision      recall\n",
      "count      453.000000    453.000000    453.000000  453.000000  453.000000\n",
      "mean        66.731598     29.168566     20.670585   30.522097   69.659426\n",
      "std         11.059131     15.464656     13.177231   17.880394   21.798778\n",
      "min         20.714286      2.564103      0.000000    0.000000    0.000000\n",
      "25%         60.000000     18.095238     11.111111   16.670000   56.250000\n",
      "50%         66.666667     25.925926     18.333333   27.270000   70.000000\n",
      "75%         73.437500     37.837838     28.571429   40.000000   85.710000\n",
      "max        100.000000    100.000000     80.000000  100.000000  100.000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_para_wordembed_st =\\\n",
    "    pd.read_csv(config.MAGMA_DIR+'datasets/bullet_paragraph_embeddings/'+MODEL+'/st/df_base_selected_para.csv')\\\n",
    "    .set_index(['book', 'chapter'])\n",
    "df_para_wordembed_st.para = df_para_wordembed_st.para.map(eval)\n",
    "df_para_wordembed_st.best_match = df_para_wordembed_st.best_match.map(eval)\n",
    "\n",
    "num_para = df_para_wordembed_st.para.map(len)\n",
    "\n",
    "df_remove_overlap_tobesaved = pd.read_csv(OUTPUT_PATH+'df_cosine_merge.csv').set_index(['book', 'chapter'])\n",
    "df_remove_overlap_tobesaved.selected_para_cosine = df_remove_overlap_tobesaved.selected_para_cosine.map(eval)\n",
    "df_remove_overlap_tobesaved = df_remove_overlap_tobesaved.groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'selected_para_cosine': lambda p: list(p)\n",
    "})\n",
    "df_remove_overlap_tobesaved.selected_para_cosine = df_remove_overlap_tobesaved.selected_para_cosine\n",
    "\n",
    "selected_para = df_remove_overlap_tobesaved.selected_para_cosine.map(lambda pp: set(flatten(pp)))\n",
    "best_match = df_para_wordembed_st.best_match.map(set)\n",
    "\n",
    "df_comparison = pd.concat([num_para, selected_para, best_match], axis=1).rename(\n",
    "    columns={'para': 'num_para', 'selected_para_cosine': 'unsup_selected', 'best_match': 'sup_selected'})\n",
    "df_comparison['unsup_coverage'] = 100*df_comparison.unsup_selected.map(len) / df_comparison.num_para\n",
    "df_comparison['sup_coverage'] = 100*df_comparison.sup_selected.map(len) / df_comparison.num_para\n",
    "\n",
    "df_comparison['intersection'] = 100*df_comparison.apply(lambda r:\n",
    "    len(r.unsup_selected.intersection(r.sup_selected)) / r.num_para, axis=1)\n",
    "\n",
    "df_comparison['precision'] = df_comparison.apply(lambda r:\n",
    "    precision(r.unsup_selected, r.sup_selected), axis=1)\n",
    "df_comparison['recall'] = df_comparison.apply(lambda r:\n",
    "    recall(r.unsup_selected, r.sup_selected), axis=1)\n",
    "\n",
    "df_comparison.drop(columns='num_para', inplace=True)\n",
    "\n",
    "print(df_comparison.describe())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Expanding from Best Paragraph Based on Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_best_para['para_num_tok'] =\\\n",
    "    df_best_para.para.map(lambda ps: np.array([len(tokenizer.tokenize(p)) for p in ps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [00:00<00:00, 792.67it/s]\n"
     ]
    }
   ],
   "source": [
    "def expand_based_on_euclidean(r):\n",
    "    max_length = len(r.para)\n",
    "    max_idx = max_length-1\n",
    "    \n",
    "    extracted_para = []\n",
    "    \n",
    "    # Calculate the fraction we need to extract\n",
    "    # based on total number of tokens in this chp\n",
    "    # and number of centroids (sections) in this chp\n",
    "    # do not go over the model max length\n",
    "    num_tok_tot = sum(r.para_num_tok)\n",
    "    num_tok_th = min(\n",
    "        int(0.8*num_tok_tot / len(r.best_para_euclidean)),\n",
    "        0.9*tokenizer.model_max_length)\n",
    "    \n",
    "    for i, best in enumerate(r.best_para_euclidean):\n",
    "        merged_para_idx = [best]\n",
    "        num_tok = r.para_num_tok[best]\n",
    "        \n",
    "        while num_tok < num_tok_th:\n",
    "            if len(merged_para_idx) == max_length : break\n",
    "            elif 0 in merged_para_idx:\n",
    "                merged_para_idx.append(max(merged_para_idx)+1)\n",
    "            elif max_idx in merged_para_idx:\n",
    "                merged_para_idx.append(min(merged_para_idx)-1)\n",
    "            else:\n",
    "                if (r.euclidean_dist)[min(merged_para_idx)-1, i] >\\\n",
    "                    (r.euclidean_dist)[max(merged_para_idx)+1, i]:\n",
    "                    merged_para_idx.append(max(merged_para_idx)+1)\n",
    "                else:\n",
    "                    merged_para_idx.append(min(merged_para_idx)-1)\n",
    "            num_tok = np.sum(r.para_num_tok[merged_para_idx])\n",
    "                  \n",
    "        extracted_para.append(sorted(merged_para_idx))\n",
    "        \n",
    "    return extracted_para\n",
    "\n",
    "df_best_para['selected_para_euclidean'] =\\\n",
    "    df_best_para.progress_apply(lambda row: expand_based_on_euclidean(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Study Overlap and Remove Useless (>90% overlap) Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.000000\n",
       "mean       1.763797\n",
       "std        2.021853\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        2.000000\n",
       "75%        2.000000\n",
       "max       11.000000\n",
       "Name: overlap_matrix, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def study_overlap(r):\n",
    "    num_centr = len(r.selected_para_euclidean)\n",
    "    overlap_matrix = np.zeros((num_centr,num_centr))\n",
    "    \n",
    "    def list_overlap(a, b):\n",
    "        return list( set(a).intersection(set(b)) )\n",
    "    \n",
    "    for i in range(num_centr):\n",
    "        for j in range(num_centr):\n",
    "            if i == j : continue\n",
    "            num_tok_i = np.sum(r.para_num_tok[r.selected_para_euclidean[i]])\n",
    "            overlap = list_overlap(\n",
    "                r.selected_para_euclidean[i], r.selected_para_euclidean[j])\n",
    "            num_tok_overlap = np.sum(r.para_num_tok[overlap])\n",
    "            assert num_tok_overlap <= num_tok_i\n",
    "            \n",
    "            overlap_matrix[i, j] = num_tok_overlap/num_tok_i*100\n",
    "    \n",
    "    return overlap_matrix\n",
    "    \n",
    "df_best_para['overlap_matrix'] = df_best_para.apply(lambda row: study_overlap(row), axis=1)\n",
    "df_best_para.overlap_matrix.map(lambda om: np.sum(om > 90)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_big_overlap(r, threshold):\n",
    "    om = r.overlap_matrix\n",
    "    big_overlap_idx = np.argwhere(om >= threshold)\n",
    "    to_be_removed = set()\n",
    "    for idx in big_overlap_idx:\n",
    "        i, j = idx[0], idx[1]\n",
    "        if om[i, j] == om[j, i]:\n",
    "            if i in to_be_removed or j in to_be_removed : continue\n",
    "            else : to_be_removed.add(i)\n",
    "        elif om[i, j] > om[j, i]:\n",
    "            to_be_removed.add(i)\n",
    "        else:\n",
    "            to_be_removed.add(j)\n",
    "    return [s for i, s in enumerate(r.selected_para_euclidean) if i not in to_be_removed]\n",
    "\n",
    "df_best_para.selected_para_euclidean = df_best_para.apply(lambda row: remove_big_overlap(row, 90), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    453.0\n",
       "mean       0.0\n",
       "std        0.0\n",
       "min        0.0\n",
       "25%        0.0\n",
       "50%        0.0\n",
       "75%        0.0\n",
       "max        0.0\n",
       "Name: overlap_matrix, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_best_para['overlap_matrix'] = df_best_para.apply(lambda row: study_overlap(row), axis=1)\n",
    "df_best_para.overlap_matrix.map(lambda om: np.sum(om > 90)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Finalize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_selected_para_euclidean = df_best_para.explode('selected_para_euclidean')\n",
    "df_selected_para_euclidean = df_selected_para_euclidean.drop(\n",
    "    columns=['best_para_cosine', 'best_para_euclidean', 'cosine_sim', 'euclidean_dist', 'overlap_matrix'])\n",
    "\n",
    "df_selected_para_euclidean['selected_para'] = df_selected_para_euclidean.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para) if i in row.selected_para_euclidean], axis=1)\n",
    "\n",
    "df_selected_para_euclidean['para_num_tok'] = df_selected_para_euclidean.apply(lambda row:\\\n",
    "    [p for i, p in enumerate(row.para_num_tok) if i in row.selected_para_euclidean], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2223.000000\n",
       "mean      426.573549\n",
       "std       194.769371\n",
       "min        87.000000\n",
       "25%       288.500000\n",
       "50%       381.000000\n",
       "75%       520.000000\n",
       "max      1094.000000\n",
       "Name: para_num_tok, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected_para_euclidean.para_num_tok.map(sum).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_selected_para_euclidean.to_csv(OUTPUT_PATH+'df_euclidean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Compare to Para Wordembed ST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_para_wordembed_st =\\\n",
    "    pd.read_csv(config.MAGMA_DIR+'datasets/bullet_paragraph_embeddings/'+MODEL+'/st/df_base_selected_para.csv')\\\n",
    "    .set_index(['book', 'chapter'])\n",
    "df_para_wordembed_st.para = df_para_wordembed_st.para.map(eval)\n",
    "df_para_wordembed_st.best_match = df_para_wordembed_st.best_match.map(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "\n",
    "selected_para = df_best_para.selected_para_euclidean.map(lambda l: set(flatten(l)))\n",
    "\n",
    "best_match = df_para_wordembed_st.best_match.map(set)\n",
    "\n",
    "df_comparison = pd.concat([selected_para, best_match], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def precision(pred, ref):\n",
    "    return round(100*len(pred.intersection(ref)) / len(pred), 2)\n",
    "    \n",
    "def recall(pred, ref):\n",
    "    return round(100*len(pred.intersection(ref)) / len(ref), 2)\n",
    "\n",
    "def fmeasure(prec, rec):\n",
    "    if prec + rec == 0 : return 0\n",
    "    return round(2*prec*rec/(prec+rec), 2)\n",
    "\n",
    "df_comparison['precision'] = df_comparison.apply(\n",
    "    lambda row: precision(row.selected_para_euclidean, row.best_match), axis=1)\n",
    "df_comparison['recall'] = df_comparison.apply(\n",
    "    lambda row: recall(row.selected_para_euclidean, row.best_match), axis=1)\n",
    "df_comparison['fmeasure'] = df_comparison.apply(\n",
    "    lambda row: fmeasure(row.precision, row.recall), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>453.000000</td>\n",
       "      <td>453.000000</td>\n",
       "      <td>453.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30.414680</td>\n",
       "      <td>69.348477</td>\n",
       "      <td>40.042627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.810506</td>\n",
       "      <td>21.361017</td>\n",
       "      <td>18.079504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.670000</td>\n",
       "      <td>55.560000</td>\n",
       "      <td>26.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26.670000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>38.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>85.710000</td>\n",
       "      <td>52.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>85.710000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        precision      recall    fmeasure\n",
       "count  453.000000  453.000000  453.000000\n",
       "mean    30.414680   69.348477   40.042627\n",
       "std     17.810506   21.361017   18.079504\n",
       "min      0.000000    0.000000    0.000000\n",
       "25%     16.670000   55.560000   26.670000\n",
       "50%     26.670000   70.000000   38.460000\n",
       "75%     40.000000   85.710000   52.630000\n",
       "max    100.000000  100.000000   85.710000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comparison.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "mFAC31paODFl",
    "S0FByNNOIRvG",
    "tb7fAfzaK4es",
    "eQGq4WLu3Gei",
    "tSHT0mxuvkEp",
    "-eRnW74aH95b",
    "X2xp7jJNwB6b",
    "2Eb-_Ud3vxeY",
    "VndEUBoDjjkV",
    "8_li_hFKF_Ws"
   ],
   "name": "paragraph_assign_bullets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
