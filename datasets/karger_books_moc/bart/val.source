2 Pathophysiology and classification. Normal neuromuscular transmission. At the normal neuromuscular junction (NMJ), acetylcholine (ACh) is released from the motor neuron terminal, diffuses across the synaptic space and binds to ACh receptors (AChR), which are densely clustered on the folded endplate membrane of the muscle fiber. The high concentration of AChRs is crucial for efficient neuromuscular transmission. The ACh depolarizes the muscle endplate region, ultimately causing the muscle to contract (Figure 2.1). Myasthenia gravis (MG) is the best characterized autoimmune disorder of the nervous system. The immune-mediated nature of MG was suspected as early as the 1960s when it was speculated to be caused by a dysregulated immune response, with antibodies directed against skeletal muscle. Acetylcholine receptor antibodies. A series of animal and human experiments in the 1970s confirmed the above hypothesis, and elevated titers of antibodies against the acetylcholine receptor (AChR) were ultimately discovered in the serum of patients with MG. Loss of AChRs results in impaired neuromuscular transmission and muscle weakness. Multiple pathological processes cause loss of functional AChRs in MG. complement-mediated lysis. accelerated internalization and degradation of AChRs. direct blockade of AChRs by antibodies. Complement-mediated lysis is thought to be the most important mode of loss of AChR, but the actual process that initiates the aberrant immune attack on the AChR is unknown. Both the immunoglobulin (Ig) G antibody and complement have been localized to the motor endplate in myasthenia models, indicating that circulating IgG antibodies directed against the AChR bind to the postsynaptic membrane and activate the terminal complement sequence (C5b-9), or membrane attack complex (MAC). This results in lysis of the postsynaptic membrane, causing loss of the AChR (Figure 2.2). In fact, elevated MAC levels have been demonstrated in the plasma of patients with MG. 'Binding' antibodies are the most common type found in patients with MG. Accelerated internalization and degradation of acetylcholine receptors. A key mechanism of disease pathology in MG is the modulation, internalization and eventual destruction of AChRs at the NMJ by the crosslinking of AChR-specific autoantibodies (Figure 2.3). This process is known as antigenic modulation. Direct blockade of acetylcholine receptors by antibodies. 'Blocking' antibodies are the second most common type found in patients with MG. These antibodies block the binding of ACh to AChRs on the muscle endplate (Figure 2.4). Impaired neuromuscular transmission. As a result of the pathological processes described above, the postsynaptic membrane becomes simplified, with a reduction in the number of AChRs and junctional folds and a reduced safety factor in neuromuscular transmission, ultimately leading to impaired neuromuscular transmission. Muscle-specific receptor tyrosine kinase antibodies. Although most patients with generalized MG have antibodies to AChR (see below), a minority of patients without AChR antibodies have antibodies to the muscle-specific receptor tyrosine kinase (MuSK). As discussed in Chapter 1, a higher percentage of patients with treatment-refractory MG have been found to have MuSK antibodies than those who respond to treatment. The disease can be particularly severe in patients with MuSK antibodies (see below). At present it is unclear how MuSK antibodies cause MG, but it is possible that MuSK antibodies alter AChR density or number. MuSK signaling is crucial for the development, long-term maintenance and stabilization of the postsynaptic portion of the NMJ. The antibodies in MuSK MG are largely of the IgG4 subclass and have been demonstrated in vitro to block the assembly and activation of MuSK. In animal models, administration of MuSK antibodies has resulted in reduced postsynaptic AChRs, disturbed synaptic alignment, reduced synaptic potentials and impaired muscle activation. Role of the thymus. Approximately 75% of patients who undergo thymectomy are found to have thymic pathology. Of these, 15% are discovered to have thymoma, and the remainder have evidence of lymphoid hyperplasia. Myoid cells are located in the thymus gland, and thymus tissue from patients with MG has been found to be enriched with AChR-reactive T cells. The close interaction between lymphocytes and myoid cells in the thymus, together with a yet-to-be-discovered stimulus that disrupts normal immune tolerance, may lead to the autoimmune response triggering MG. 
MG can be classified in a variety of ways. by disease type. by antibody status. by disease severity. Classification by disease type. Ocular MG, which causes only ptosis and/or diplopia, occurs in approximately 15% of cases; generalized disease occurs in the remainder. Classification by antibody status. Antibodies to AChR. As noted above, MG is most often caused by antibodies to the AChR. This is the case for 85% of cases of generalized disease and approximately 50% of cases of purely ocular MG. Antibodies to AChR may be of the binding, blocking or modulating type, with binding being the most common. AChR antibodies are highly specific for the diagnosis of MG, although the degree of elevation in titers has never been found to correlate with the severity of disease. Antibody titers may decline with immunosuppressive treatment within an individual, although the reliability of determining treatment effect is less robust. Antibodies to muscle receptor tyrosine kinase (MuSK) occur in about 7% of cases of generalized MG but are rarely reported in isolated ocular disease. MuSK-antibody-positive MG is a distinct clinical subset of the disease, almost always seen in adults, more frequently in women, and never in conjunction with thymic disease. There is predominant involvement of the bulbar muscles, often with concurrent neck and respiratory muscle weakness. Ocular and limb muscle weakness is less common than in AChR-antibody-related disease but is certainly seen in this population. Most patients can actually worsen on treatment with acetylcholinesterase inhibitors such as pyridostigmine. In general, treatment-refractory MG and myasthenic crises are more common in this subset of the disease. Seronegative for AChR or MuSK antibodies. When AChR or MuSK antibodies are not detected, patients are diagnosed with MG by clinical or electrodiagnostic means (typically with repetitive nerve stimulation or single fiber electromyography; see page 25 -) and are classified as 'double seronegative' or 'seronegative'. Included in this group are a small percentage of patients with antibodies only to 'clustered' AChRs. These antibodies can only be detected using cell-based assays, which, in contrast to standard diagnostic antibody tests, allow for the detection of antibodies that bind to AChRs that are clustered in a natural membrane environment, as they are at the NMJ. These patients are more likely to be younger and have milder disease than other patients with MG. Other antibodies. Antibodies to lipoprotein-related protein 4 (LRP4), agrin and cortactin have recently been found in a small percentage of patients with MG. Further discovery of pathogenic antibodies and improvements in antibody detection are likely to decrease the percentage of patients who are ultimately diagnosed with seronegative MG. Classification by disease severity. Osserman was the first to classify patients with MG according to disease severity. The most commonly used modification of the original classification scheme is as follows. Group I - ocular. Group IIA - mild generalized. Group IIB - moderate-to-severe generalized. Group III - acute, severe, developing over weeks to months. Group IV - late, severe, with marked bulbar involvement. This classification has several notable limitations, including the indistinct descriptive wording and lack of clear distinctions between groups. This classification system also fails to include a category for patients in clinical remission or those in crisis. A task force of the Myasthenia Gravis Foundation of America (MGFA) subsequently developed a refined classification system (Table 2.1) that is more clinically descriptive and provides a clearer distinction between groups. 
Morphological features. The initial step in the diagnosis of glioblastoma is morphological assessment of the tumor. High-grade gliomas are highly cellular, with varied nuclear morphology (Figure 3.1; also see Figure 1.3). Mitotic activity is usually noticeable, although its absence does not preclude malignant biological or clinical behavior. The matrix of these tumors is fibrillary, formed by processes of tumor cells and often involving underlying central nervous system (CNS) tissue that is overrun by tumor cells. The cytoplasm of tumor cells varies - some forms of high-grade astrocytic tumors show abundant eosinophilic cytoplasm, whereas others show a scanty perinuclear rim of cytoplasm. Overtly high-grade astrocytic tumors also have vessels with hyperplastic and often multilayered endothelium, termed microvascular proliferations. These vessels are formed by the underlying CNS tissue and respond to vascular growth stimuli, rather than being a neoplastic element of the tumors themselves. A further characteristic feature of high-grade gliomas is the presence of necrotic tumor cells of varying form and shape. Palisading necrosis is typical in glioblastoma, but larger areas of sharply demarcated necrotic tumor are also commonly seen. When assessing gliomas, it must be borne in mind that some tumors with the molecular profile of a glioblastoma (see below) may not have all high-grade features. It is increasingly apparent that diffuse infiltrating gliomas may not have high-grade features when diagnosed early. Likewise, the histone H3.3 K27M-mutant gliomas may not have any histological high-grade features but are defined as WHO grade IV. This is why a multidisciplinary approach is so important for the optimal diagnosis of patients with glioblastoma - the molecular analysis ensures a more robust classification. Identification of molecular subgroups. Following the initial histological assessment, a set of markers is used to identify molecular subgroups of high-grade gliomas (see Table 1.1) or, indeed, to refine the diagnosis where both low- and high-grade histology is present. Mutation-specific antibodies can be used to identify IDH1 R132H, histone H3.3 K27M and BRAF V600E. Staining can be used to identify loss of expression of ATRX, SMARCA4 or SMARCB1 (INI1). Some tumors can be robustly diagnosed with these antibodies alone, particularly tumors with IDH and ATRX mutations and H3.3 K27M mutated tumors. Other markers. As antibodies are not available for all known mutations (exempli gratia in the IDH1 and IDH2 genes), other methods, such as Sanger sequencing, next-generation sequencing or multiplex ligation-dependent probe amplification, are required. Other mutations, such as in the TERT promoter, require sequencing, and copy number assays are necessary to detect chromosome loss or gain (exempli gratia 7p gain and 10q loss in IDH wild-type glioblastoma; 1p/19q co-deletion in oligodendroglioma) or gene amplifications (exempli gratia EGFR, MYC) or deletions (exempli gratia CDKN2A/B; see Figure 1.3). Epigenetic changes. Many high-grade gliomas can be classified by a selection of the markers described above. However, some remain unclassifiable because of a lack of distinctive mutation patterns. Thus, a method has been developed that looks at epigenetic changes caused by a combination of mutations and the cell of origin. These epigenetic changes are identified from methylation patterns (which are a result of driver mutations), using a publicly accessible algorithm (www. This method is widely used in the diagnosis of brain tumors with unusual or ambiguous histology and when conventional molecular testing is not diagnostically informative. A diagnostically useful copy number profile is also generated, which elucidates chromosomal changes and gene amplifications or losses (see Figure 1.3). 
Imaging is performed at the time of presentation to localize and characterize mass lesions and to define the disease extent. This chapter focuses on the diagnostic use of imaging in patients with suspected glioma/glioblastoma. The role of imaging in the management of patients with glioblastoma is discussed in Chapter 4. Computed tomography has a low soft tissue contrast, therefore its role in diagnosing glioma is limited. However, it remains the most rapid and widely accessible imaging modality for patients with brain tumors in the emergency setting. CT is commonly the first imaging test to be carried out when patients newly present with symptoms of glioblastoma; it is often sufficient to exclude the need for immediate neurosurgical intervention. In many cases, CT will permit the distinction between a cortical ischemic infarct (cytotoxic edema) and a mass lesion (vasogenic edema). The accurate characterization of brain masses, however, requires MRI, as CT neither captures the lesion extent nor its morphology sufficiently. Of note, calcification on CT in an untreated tumor is rare in primary (IDH wild-type) glioblastoma and should prompt consideration of an alternative diagnosis (exempli gratia oligodendroglioma). Anatomic magnetic resonance imaging. All patients with suspected or confirmed glioblastoma should have an MRI scan. Clinical glioma MRI sequences almost always comprise T2-weighted, T2-FLAIR (fluid-attenuated inversion recovery) and T1-weighted pre- and post-contrast sequences. T2-weighted and FLAIR images are most valuable for estimating macroscopic tumor boundaries: FLAIR increases the overall contrast between the tumor and normal brain tissue, and may thus facilitate lesion detection, while the loss of grey-white matter detail, indicative of tumor, is well delineated on T2 images. For this reason, T2 imaging should always be reviewed when assessing the extent of glioma infiltration. Compared to IDH -mutant lower-grade gliomas, early (IDH wild-type) glioblastomas are variably smaller tumors that may manifest only as subtle signal abnormalities on T2/FLAIR imaging. Microscopic tumor infiltration, which is a typical feature of glioblastoma, remains occult on anatomic MRI. Similarly, imaging cannot reliably differentiate between peritumoral edema and infiltration. The typical post-contrast appearance of glioblastoma on MRI is that of a (usually large) nodular rim-enhancing mass surrounding a central necrotic core. However, the diagnosis of glioblastoma can be complex, as any type of enhancement pattern can occur, including solid-patchy enhancement, thin rim enhancement resembling a cyst or, indeed, a total lack of enhancement (Figure 3.2). Because the early stages of primary glioblastoma are frequently non-enhancing, radiological descriptions of non-enhancing gliomas as 'low grade' can be misleading and are best avoided given that the WHO 2016 classification is now based on an integrated diagnosis. Intratumoral susceptibility signals representing petechial hemorrhages are thought to be common in glioblastoma, so the inclusion of T2* or susceptibility weighted imaging (SWI) can be useful. SWI can also help delineate biopsy tracts. Numerous anatomic imaging features have been explored as biomarkers of glioma subtype, specifically glioblastoma. For example, the Visually Accessible Rembrandt (VASARI) criteria use a set of well-defined visual features (exempli gratia enhancement, necrosis, peritumoral edema) to provide an objective description of gliomas. However, the accuracy and reproducibility of such systems for glioma grading and genotyping are limited because of the many qualitative features that cannot be so readily incorporated. As mentioned in previous chapters, the location of the tumor can support the diagnosis, as it indicates the likelihood of certain genotypes.
For example, multifocality is thought to be more common in IDH wild-type gliomas than IDH -mutant low-grade gliomas. Given the propensity of glioblastoma for rapid spread along white matter tracts, it is often considered to be a multifocal disease, even though this is not consistently visible on imaging. Distant metastases, however, are virtually non-existent, so staging can be achieved from brain imaging only. Although spinal dissemination of glioblastoma is uncommon, if spinal drop metastases are suspected (exempli gratia on discovery of intracranial leptomeningeal disease), post-gadolinium spine imaging from the skull base to the sacrum (id est along the entire vertebral canal) should be considered as part of the diagnostic work-up. Distinguishing glioblastoma from other diseases is often, but not always, straightforward (Figure 3.3). Anatomic MRI is essential for the diagnosis of glioblastoma, but it has limited sensitivity and specificity for the characterization of morphologically atypical lesions; for example, CNS lymphoma with central necrosis or with solitary metastases. In these cases, body imaging is usually requested before brain biopsy. Perfusion imaging techniques. The recommendations for performing advanced imaging techniques are less prescriptive than anatomic MRI, as there is a lack of high-level evidence for many of the more complex modalities. However, to improve the identification of malignant gliomas, recent European guidance recommends the addition of perfusion imaging for newly diagnosed non-gadolinium-enhancing masses. Perfusion MRI aims to identify neovascularization in brain tumors by analyzing (contrast-enhanced) blood flow through a volume of tissue over time. Several perfusion methods exist (Table 3.1). ASL, arterial spin labeling; CBF, cerebral blood flow; DCE, dynamic T1-weighted contrast-enhanced (perfusion); DSC, dynamic susceptibility contrast-enhanced (perfusion); Ktrans, volume transfer constant; rCBV, relative cerebral blood volume; Ve, interstitial volume; Vp, plasma volume. Dynamic susceptibility contrast (DSC, T2*) perfusion is the most widely used and established technique. After obtaining a baseline image volume, gadolinium contrast is rapidly injected and the same image volume is repeatedly acquired at short (1-2 second) intervals for 1-2 minutes. The measured target parameter is relative cerebral blood volume (rCBV), which has been comprehensively validated as a biomarker of vascularity. Relative means that CBV values have undergone mathematical correction for gadolinium leakage effects, which typically occur in areas where the blood-brain-barrier is disrupted. rCBV quantification can be optimized by using a preload contrast bolus before the perfusion injection and by choosing the most suitable MRI settings (exempli gratia flip angle). rCBV increases (< 1.75 compared to normal white matter) in areas of neovascularization and is particularly elevated in glioblastoma. On T2-weighted imaging, intralesional flow voids may be inconsistently visible as a sign of (enlarged) tumor vessels, corresponding to high rCBV readings. rCBV can be used broadly to grade IDH -mutant astrocytomas, where serial DSC may improve the identification of anaplastic elements as a sign of 'transformation'. Blood volume is typically raised in aggressive IDH wild-type gliomas, but small (exempli gratia early) glioblastomas can elicit false-negative perfusion results. Conversely, 1p19q co-deleted oligodendrogliomas often demonstrate moderately raised perfusion, even those with a low grade and a relatively favorable prognosis. Therefore, elevated rCBV does not equal malignancy. Structural features on imaging, particularly calcification, and younger patient age can support the identification of oligodendrogliomas (Figure 3.4).
Other caveats to applying DSC perfusion are limited resolution and the inability to quantify rCBV in areas of susceptibility (hemorrhage, coarse calcification). Dynamic contrast-enhanced (DCE, T1) perfusion is an alternative technique, which is not confounded by susceptibility effects, but requires more complex mathematical modeling. The measured parameters of DCE include the volume transfer constant (Ktrans), plasma volume (Vp) and interstitial volume (Ve), as well as time signal intensity curves. Such curves typically differ between tumor (rapid contrast wash in and wash out) and necrotic tissue (delayed progressive contrast wash in). DCE is potentially useful for grading and differential diagnosis, but can be particularly valuable for follow-up after treatment if susceptibility effects hamper DSC assessment. Arterial spin labeling (ASL) is a new and promising modality that does not require a contrast injection. Instead, ASL uses endogenously labeled blood to directly measure cerebral blood flow. It is not prone to leakage errors or susceptibility effects, but does require substantial post-processing expertise. ASL may support glioma characterization, but published data on its use for glioma genotyping and differential diagnosis are, to date, limited. Examples of magnetic resonance perfusion imaging in glioblastoma are shown in Figure 3.5. Diffusion-weighted imaging (DWI) should always be interpreted within the context of structural MRI features, as many other diseases (exempli gratia ischemia, abscess, demyelination) can restrict diffusion. DWI uses a minimum of two diffusion gradients (usually b0 and b1000 mm /s) in three orthogonal directions and is widely integrated into clinical MRI protocols, including brain tumor imaging. From the b1000 image, an apparent diffusion coefficient (ADC) map is generated through mathematical subtraction of T2 effects. DWI (b1000) and ADC must be interpreted in conjunction and are often assessed qualitatively in clinical practice by reporting the presence or absence of restricted diffusion as a tumor feature. In general, diffusion decreases as tissue cellularity increases. Diffusion restriction is defined as reduced diffusivity (ADC signal) compared to normal brain parenchyma. This feature can occur in glioblastoma, but malignant gliomas are more likely to exhibit diffusion values similar to or slightly greater than normal brain tissue. Perilesional edema and infiltration may be seen as a focally increased ADC signal that makes the tumor core appear relatively darker, leading to overestimation of tumor restriction. This error can be avoided by quantifying ADC values (Figure 3.6). Lower ADC values have been shown to correlate with higher glioma grades, corresponding to an increase in proliferative indices. Numerous studies have observed lower ADC values in IDH wild-type gliomas than IDH -mutant WHO grade II/III gliomas, specifically IDH -mutant 1p19q intact astrocytomas, which may show markedly increased ADC values ('facilitated diffusion'). To a limited extent, ADC measurements may also be valuable for differential diagnosis; for example, primary central nervous system lymphoma typically has lower ADC values than glioblastoma, but results can overlap for individual tumors. Other diffusion techniques. It remains unclear whether more elaborate diffusion techniques (exempli gratia diffusion kurtosis imaging, and multiexponential and compartmental diffusion models) could offer a diagnostic advantage over standard DWI for radiomic predictions. Magnetic resonance spectroscopy (MRS) is a non-invasive modality that measures metabolites (including choline, N -acetyl aspartate, lactate, lipids) in a defined volume of brain tissue. Single voxel spectroscopy measures the average amount of metabolites within the chosen region. Multivoxel spectroscopy is technically more demanding but increasingly available for clinical use. MRS techniques, including chemical shift imaging, can depict intratumoral metabolite heterogeneity, which can be used for surgical targeting. Alterations in the normal metabolite distribution are interpreted as markers of disease processes.
For example, choline is a marker of cellular proliferation, which increases in neoplasms. Normally, the choline peak does not exceed the creatine peak (creatine is a relatively stable normal metabolite and thus suitable for comparison). N -acetyl aspartate is the principal marker of neuronal integrity, which non-specifically decreases in glioblastoma. Lactate is not present in normal tissues, but accumulates in rapidly proliferating tumors as an effect of tissue hypoxia (Figure 3.7). In general, the presence of lactate indicates a 'high grade' neoplasm, but as a single measurement it is not entirely specific and may occasionally be observed in IDH -mutant low-grade astrocytomas and non-neoplastic conditions. Lipids are markers of necrosis, which can also (non-specifically) increase in glioblastoma. MRS has limited accuracy for glioma grading and is preferentially used to distinguish tumors from non-neoplastic conditions. Recently, interest has grown in the use of spectroscopy to identify the oncometabolite 2-hydroxyglutarate (2HG), which is produced by IDH -mutant gliomas. A small number of institutions have published excellent results on 2HG-MRS, but false-positive and false-negative results have been reported. Positron emission tomography (PET) imaging techniques involve the administration of a radionuclide tracer, which preferentially accumulates in the target tissue. The most widely used PET tracer is F-fluorodeoxyglucose (FDG); this has been investigated with partial success for brain imaging, its main limitation being the relatively high 'background' uptake of normal brain parenchyma. For this reason, FDG is not widely used in clinical glioma imaging. A variety of amino acid (non-FDG) tracers have been tested for glioblastoma imaging, including C-methyl- L -methionine (C-MET), 18 F-fluoroethyl- L -tyrosine (F-FET), F-fluorothymidine (F-FLT), F-fluorocholine (F-CHO), C-choline (C-CHO) and F-fluoro- L -dihydroxy-phenylalanine (F-DOPA). Currently, no single amino acid tracer has shown clear superiority; false-negative and false-positive results have been reported with most tracers, and their accessibility may vary geographically for short half-life compounds. PET imaging is rarely indicated in clinical practice for untreated glioma characterization. It is mostly reserved for problem solving, specifically if advanced MRI techniques are unable to provide a clear distinction between recurrent glioblastoma and therapy effects (see page 50). Differential diagnosis. The presentation of patients with glioblastoma overlaps with a range of other conditions. Histological diagnosis of glioblastoma is therefore mandatory and is usually obtained through stereotactic biopsy or after tumor resection. It is imperative that the histology is reviewed by an expert neuropathologist and that a multidisciplinary team, including a neuroradiologist, is involved in the diagnostic work-up. Of the differential diagnoses that should be considered, listed in Table 3.2, all may present with neurological symptoms. In addition, cerebral abscess, encephalitis and toxoplasmosis may present with a fever. Toxoplasmosis is often asymptomatic, but it can present with flu-like symptoms (body aches, swollen lymph nodes, fever) in some people. If an infection is being considered as a cause of symptoms, then a full history and examination along with a septic work-up is advised, including, but not limited to, full blood count, blood cultures, lumbar puncture and imaging, as clinically appropriate.
Pancreatic adenocarcinoma. Etiology and pathogenesis. The term pancreatic cancer usually refers to common pancreatic ductal adenocarcinoma, although there is a large variety of other exocrine solid tumor types (Table 11.1) with varying prognoses. Pancreatic cancer usually arises in the head of the pancreas (80%) and less commonly in the body (15%) or tail (5%). Other tumors that arise in the pancreas, or in close proximity to it, are part of the differential diagnosis in a patient with suspected pancreatic adenocarcinoma. pancreatic neuroendocrine tumors. pancreatic lymphoma. metastatic tumors. adenocarcinoma of the ampulla of Vater. intrapancreatic bile duct adenocarcinoma. duodenal adenocarcinoma. Pancreatic adenocarcinoma usually has its origin in precursor lesions termed pancreatic intraepithelial neoplasia, which are graded from 1 to 3. Once the lesions progress to invasive carcinoma there is a strong desmoplastic reaction that impedes both diagnosis and treatment as a result of the decreased penetration of chemotherapeutic agents inside tumors. The KRAS oncogene is frequently altered in pancreatic adenocarcinomas. It can be activated by various factors and is involved in pancreatic cell transformation, tumor formation and progression. Other somatic mutations have been reported, including alterations of several tumor suppressor genes, such as CDKN2A (p16; cell cycle regulation), TP53 (p53; cellular stress response) and DPC4 (SMAD4), which modifies cellular signaling of the tumor growth factor-beta pathway. Epidemiology and risk factors. Pancreatic adenocarcinoma is one of the most common causes of death from cancer in westernized countries. Worldwide there are approximately 250 000 new cases each year: 70 000 in Europe and 32 000 in the USA. The overall crude incidence of pancreatic adenocarcinoma is approximately 10 per 100 000 people; the peak incidence is in the 65-75-year age group. Tobacco smoking is the major risk factor. The second most important risk factor is a familial background (5-10%). Familial pancreatic cancer is rare, although germline mutations of the BRCA2 gene are found in 10-20% of such families. Other families have a combination of pancreatic cancer and melanoma in which there are germline mutations of the gene p16 Ink4a. Furthermore, a variety of familial pancreatic syndromes have a significantly increased risk of pancreatic cancer. Peutz-Jeghers syndrome. familial breast and ovarian cancer. familial atypical multiple mole melanoma. hereditary non-polyposis colon cancer. ataxia telangiectasia. Li-Fraumeni syndrome. familial adenomatous polyposis. cystic fibrosis. The risk of pancreatic adenocarcinoma is increased about fivefold in chronic pancreatitis and 40-fold in hereditary pancreatitis. There is also an association with diabetes mellitus, especially in older patients. Symptoms and signs of pancreatic cancer are shown in Table 11.2. In the case of tumors of the head of the pancreas, painless obstructive jaundice (jaundice with dark urine and pale stools) commonly occurs but the presentation is usually insidious.
Weight loss, nausea, vomiting, and abdominal and back pain can also be suggestive of pancreatic head cancer, although they are more frequently encountered in body and tail tumors. As a result of the location, symptoms appear earlier in the head of the pancreas, while the evolution is often insidious until late stages in body and tail lesions. Pancreatic cancer should be suspected in any patient (over 40 years of age) with unresolving epigastric symptoms. Functioning pancreatic neuroendocrine tumors have their own particular modes of presentation (see Chapter 12). Laboratory tests. Blood tests should be carried out for anemia, clotting profile, proteins and liver function. Serum cancer antigen (CA) 19.9 is a useful tumor marker, but not all pancreatic adenocarcinomas have elevated CA 19.9. It can be elevated in benign disease and in the presence of obstructive jaundice or chronic pancreatitis. Cancers in the head of the pancreas will cause dilation of both the main bile duct and the main pancreatic duct (97%), giving rise to the classic 'double-duct' radiological sign (visualized by transabdominal ultrasound, CT or MRI). Pancreatic adenocarcinomas are relatively hypovascular, whereas neuroendocrine tumors are hypervascular. Transabdominal ultrasonography should be undertaken as an initial first step. Transabdominal ultrasound can be used to identify a dilated extrahepatic main bile duct as well as perhaps the primary pancreatic tumor (Figure 11.1) and large liver metastases, if present. This should never be used alone to exclude a pancreatic cancer. In developed countries, transabdominal ultrasound is being used less and less when a pancreatic cancer is strongly suspected, and physicians directly order a CT scan of the abdomen as the next step. There are certainly geographic variations in the use of transabdominal ultrasound as the initial test for imaging the pancreas when a pancreatic cancer is suspected. Contrast-enhanced computed tomography is the examination of choice for diagnosis and staging for resectability, provided that dedicated pancreatic protocols and latest state-of-the-art multidetector scanners are used (Figure 11.2). Enlargement of lymph nodes per se is a poor prognostic indicator. Portal or splenic venous involvement has become less relevant for surgical resection, while arterial encasement or involvement of the celiac artery and/or superior mesenteric artery is a very important factor in determining resectability. Although dynamic contrast-enhanced CT has less accuracy than endoscopic ultrasonography (EUS) for detecting small tumors (less than 2 cm), for staging purposes CT is still recommended. CT may be supplemented by EUS, MRI and laparoscopy. Magnetic resonance imaging has similar diagnostic and staging accuracies to CT and is frequently used in conjunction with magnetic resonance cholangiopancreatography. Magnetic resonance cholangiopancreatography may enhance diagnosis by revealing a double duct sign (dilation of both the common bile duct and pancreatic duct; Figure 11.3). Endoscopic ultrasonography is highly sensitive for the detection of small tumors (Figure 11.4), and even small lesions (< 10 mm) can be biopsied under EUS guidance.
EUS is also useful in staging and is helpful as the next step in evaluating a potentially resectable or borderline resectable lesion on CT scan. When a CT scan does not show a tumor or is borderline for the presence of a small pancreatic adenocarcinoma (< 1-2 cm), many studies have shown that EUS should be the next test to confirm or exclude the presence of a small pancreatic tumor. Endoscopic retrograde cholangiopancreatography (ERCP) is important for the diagnosis of ampullary and duodenal tumors as there is direct visualization of the major papilla, allowing the possibility of direct biopsy; brush cytology of the bile and pancreatic ducts may also be undertaken. Nevertheless, ERCP should no longer be used as a pure diagnostic technique (because of the risk of pancreatitis), and is mostly used for the preoperative drainage of the common bile duct in jaundiced patients. Laparoscopy, including laparoscopic ultrasonography, can detect occult metastatic lesions in the liver and peritoneal cavity and is an important adjunct to CT and EUS for staging at many centers. Positron-emission tomography (PET) is not routinely used in the diagnosis and staging of pancreatic cancer. However, PET in combination with CT or MRI may play an important role in the future for the staging of pancreatic cancer and patient stratification. Tissue diagnosis may be obtained by brushings during ERCP or fine needle aspirations (FNA) obtained percutaneously or by EUS. However, EUS-guided FNA is the preferred method for tissue diagnosis and has a higher accuracy than ERCP with brush cytology. Preoperative tissue diagnosis may not always be needed in a resectable pancreatic mass if the symptoms, signs, laboratory data and imaging are classic for pancreatic cancer in an otherwise surgically fit patient. However, when a preoperative tissue diagnosis is required in a potentially resectable pancreatic mass, EUS-guided FNA should be the preferred route on account of the lower potential risk of peritoneal seeding, the shorter needle tract and the inclusion of the needle track in the resection specimen (in pancreatic head tumors). Tissue diagnosis may be needed in borderline resectable pancreatic tumors if the patient is to undergo preoperative neoadjuvant chemotherapy or chemoradiation therapy before surgery. Differential diagnosis. In the case of solid lesions, the principal differential diagnoses are mass-forming (pseudotumoral) chronic pancreatitis and periampullary tumors (tumors of the intrapancreatic bile duct, ampulla of Vater and duodenum, and 'indeterminate' tumors). Secondary screening. None of the above diagnostic tests is routinely recommended for screening of the general population, as the overall incidence of pancreatic cancer is low and these tests are considered too expensive for mass screening. Emerging data show that EUS and MRI may be used for secondary screening of high-risk groups, such as those at risk of familial pancreatic cancer. Recent studies have suggested that EUS and MRI are the preferred tests for screening high-risk groups as they detect more lesions than CT and with no radiation involved.
Palliative treatment. The majority of patients present with advanced disease and have an overall median survival of less than 6 months, and a 5-year survival rate of up to 5%. Up to 10-20% of patients undergo pancreatic resection, with an overall median survival of 11-20 months, and a 5-year survival rate of 7-25%; virtually all patients die within 7 years of surgery. According to published guidelines, all patients with pancreatic cancer must be treated in a specialist high-volume center by a multidisciplinary team (Figure 11.5). Once the diagnosis is made or strongly suspected, the next objective is to stage the disease. If the tumor is resectable and there are no metastases then resection should be undertaken. If resection is not possible then non-surgical treatment should be undertaken. Stenting of the obstructed biliary tree. ERCP is the preferred method for drainage and stenting of an obstructed biliary tree to relieve jaundice. The stent is retrogradely inserted through the biliary stricture; expandable metallic stents are preferable. If ERCP fails, or is not considered feasible because of the anatomy of the obstruction (or obstruction from the tumor), percutaneous transhepatic cholangiography is a viable option (antegrade insertion into the stricture). EUS-guided biliary drainage has also become a viable option for failed biliary drainage via ERCP. Surgical bypass. In younger, fitter patients, biliary bypass may be preferred as blockage of stents is avoided. If duodenal obstruction is determined by tumor invasion, the duodenum may also be bypassed with a gastrojejunostomy. Pain relief is not easy. A pain team may be needed to advise on the use of opiates, celiac plexus block by an EUS-guided route or bilateral transthoracic sympathectomy. Enzyme supplements are essential, as the main pancreatic duct is usually blocked, leading to pancreatic exocrine insufficiency. Current treatment options for patients with locally advanced or metastatic cancer include either FOLFIRINOX (a combination regimen consisting of folinic acid [leucovorin], fluorouracil [5-FU], irinotecan and oxaliplatin), gemcitabine, or gemcitabine and albumin-bound nab-paclitaxel. Further options include gemcitabine and erlotinib (an epidermal growth factor inhibitor) or capecitabine. For second-line therapy, a fluoropyrimidine-based regimen is used for patients previously treated with a gemcitabine-based regimen, or a gemcitabine-based regimen is used for patients previously treated with a fluoropyrimidine-based regimen. Many trials are under way to find chemotherapy treatments that will improve survival. Radiotherapy in conjunction with chemotherapy may be helpful in controlling pain in patients with locally advanced disease.
However, it is not clear if survival is better than with chemotherapy alone. Some centers have pioneered and established the approach of preoperative chemoradiation followed by surgery in borderline resectable patients to improve survival. This approach is not universally adopted at this time. Curative surgical treatment. The overall mortality for major pancreatic resections is less than 5% in large centers but is much higher otherwise. Postoperative morbidity is high (30-40%), and patients require high-dependency care for at least the first 24 hours after surgery. Preoperative biliary drainage may be used before surgery to relieve jaundice in cases with cholangitis and fever. There is increasing evidence of the benefit of neoadjuvant therapy in borderline resectable patients. Current regimens include chemotherapy with FOLFIRINOX, or gemcitabine and albumin-bound paclitaxel, as well as chemoradiotherapy in selected reference centers. A number of different types of surgery have been described. Whipple partial pancreatoduodenectomy was once the standard procedure for tumors in the head of the pancreas and is still appropriate for large tumors and those close to the pylorus. Pylorus-preserving partial pancreatoduodenectomy is largely replacing the Whipple procedure as it does not involve removal of the gastric antrum and pylorus. Left (distal) partial pancreatectomy includes a splenectomy and is reserved for tumors in the tail of the pancreas. Total pancreatectomy is reserved for large tumors. Enucleation is a local resection and is used for small benign neuroendocrine tumors (without metastases) - essentially insulinoma. Complications following major pancreatic surgery occur in around 30% of patients and require close monitoring (Table 11.3). Adjuvant treatment. Adjuvant treatment following pancreatic resection is frequently used and consists of chemotherapy with or without the use of radiation. Treatment should be started within 12 weeks after surgery and should include either fluoropyrimidine- or gemcitabine-based chemoradiation, or chemotherapy alone, followed by maintenance 5-FU or gemcitabine. Metastatic sites may also be palliated with radiotherapy. Future trends. Improved methods for early diagnosis to improve survival in this very lethal cancer. Improved chemotherapy regimens to improve survival in locally advanced and metastatic patients. Centralization of treatment in reference centers with multidisciplinary teams. Better definition of high-risk patients for inclusion in screening programs using MRI and EUS. Use of PET in conjunction with CT/MRI (fusion PET-CT/MRI). This is an evolving technique that measures the metabolism in tumor cells. Individual treatment based on the molecular characteristics of each tumor and the pharmacogenomic constitution of the patient. Novel biological therapies and cancer vaccination. Anticachexia drugs. Improved methods of pain control.
Ethics in practice. Because of past harms associated with research involving human participants, there is an expectation and, in many cases, a regulatory requirement that an ethics committee review will take place in advance of the research commencing. In research supported by the US Department of Health and Human Services (HHS) or under US Food and Drug Administration (FDA) oversight, this is carried out by an institutional review board (IRB) registered with the federal Office for Human Research Protections (OHRP). These regulations were initiated in 1974 as part of the National Research Act. The involvement of an IRB in behavioral and biomedical research is common globally, though often by other names, such as a research ethics board (REB) or 'research ethics committee'. An IRB can be a part of the organization conducting the research (id est medical center or university) or operate as an independent fee-for-service entity. In the US, an IRB is required to have a minimum of five people, including scientists, non-scientists and someone who is unaffiliated with the organization. This membership convention has been adopted globally for organizations conducting FDA-regulated research under the International Council for Harmonisation of Technical Requirements for Pharmaceuticals for Human Use (ICH) Good Clinical Practice guidelines. The IRB is responsible for reviewing research that involves human participants to evaluate compliance with federal regulations and application of ethical principles. This review includes evaluating the probability and magnitude of potential harms to research participants and weighing these risks against the potential benefits of knowledge to be gained. The IRB also reviews the proposed research to make sure that participants selected to participate represent those most likely to benefit from its results. Moreover, the IRB wants to make sure that people who are invited to participate in research have a good understanding of the study purpose and what they will be asked to do. This process of sharing study information with a prospective participant is called informed consent and is a central tenet of biomedical research. Federal regulations and accepted ethical principles are in place to guide the conduct of research so that the science is rigorous and the participants are protected. In any research, an important step is to determine if people involved in the testing phase are considered to be human participants in the research. The federal regulations include definitions for what qualifies as 'research' and 'human subject' and address the responsibilities of the organization and research team. Rather than go into detail here, we suggest that you contact the IRB affiliated with your organization to discuss the process for obtaining approval to test a product on humans. The IRB review and approval are usually needed if the activity is considered to be research and the people involved with testing meet the definition of a human subject (exempli gratia clinical or non-clinical research). This is true regardless of whether the product is seeking FDA clearance or approval. IRB review criteria and pathways. Depending on the risk level (exempli gratia minimal or greater than minimal risk of harm) and type (exempli gratia psychological, physical, economic), there are three review pathways.
exempt from the Common Rule. expedited review. convened committee. Exempt from the Common Rule. The exempt classification is appropriate if the study procedures pose no more than a minimal risk of harm (exempli gratia observation of public activities, survey of adults, analysis of existing data). The concept of minimal risk is defined in the federal regulations and means that the risk to a participant, whether it be physical or psychological, is no greater than what they encounter in normal daily life. When a study is exempt, it means that the Common Rule does not apply to the research. Normally, the IRB decides whether a study meets the criteria for exemption, expedited or full committee review. Expedited review. To qualify for an expedited review, the study procedures may not exceed minimal risk of harm and must align with one of the criteria described in the regulations. For example, if the research involves collection of biosamples, non-invasive clinical testing (exempli gratia sensory acuity, moderate exercise by healthy volunteers) or examination of existing data like electronic health records (EHRs), it may be eligible for an expedited review. However, studies that are designed to carry out safety and efficacy testing of a medical device are probably not eligible for an expedited review and will be reviewed by a convened gathering of IRB members. The only difference between an expedited and convened committee review is the number of people involved in evaluating the research plan. An expedited review can be conducted by a subset of the IRB membership, which is usually the IRB chair and one other member. Convened committee. Any study that does not qualify for exempt or expedited review is evaluated by a convened group of IRB members. For research covered by the Common Rule, documentation of informed consent is required, though this may be waived. IRB application. Once the type of review is known, an IRB application is developed by the research team that includes a detailed research protocol and a draft of the informed consent document. The protocol will briefly describe the scientific literature that the study is building from, as well as the study aims, procedures, participant inclusion criteria, risks, benefits, risk management, data management, investigator qualifications and informed consent details. The IRB will review this protocol application to evaluate whether the risks are appropriate in relation to the potential contribution to science and benefits to people like those who participate in the study. Application of ethical principles. Researchers have applied these principles and relied on IRBs to help shape ethical research practices for nearly half a century. However, as digital products are increasingly used in health research and clinical care, all relevant stakeholders have a collective responsibility to think proactively about how to conduct digital health research ethically and responsibly. While IRB approval is an important step in the process for identifying and mitigating risk in studies, it is truly the responsibility of developers, researchers and clinicians to be a part of the ethical decision-making process. Simply stated, we cannot outsource ethics and hope for the best.
Of course, these regulations and ethical principles are sometimes difficult to put into practice. Because the use of digital methods is relatively new, accessing resources at the protocol development phase is important. Over the past few years, several initiatives have begun to address the ethical, legal and social implications (ELSI) of emerging technologies. A few focus specifically on artificial intelligence (AI) broadly (exempli gratia autonomous vehicles, facial recognition, city planning, future of work). AI initiatives presently underway (exempli gratia AI Now, A-100) are well-funded and global collaborative programs. Others addressing digital medicine technologies more specifically include the Connected and Open Research Ethics (CORE) initiative, MobileELSI research project, Sage Bionetworks and the Clinical Trials Transformation Initiative (CTTI), which are described in the following paragraphs. CORE initiative. Launched in 2015 at the University of California, San Diego, USA, the CORE initiative is a learning 'ethics' resource developed to support the digital medicine research community, including researchers and IRBs. The CORE features a QandA forum and a resource library with over 100 IRB-approved protocols and consent snippets that have been shared among 700+ members of the CORE Network. All resources are tagged for ease of access. For example, you can search the library to find protocols that have used a digital tool in clinical research involving Latino middle-schoolers or breast cancer survivors (see Figure 5.1). In addition, the CORE is creating checklists to assist the community in proactive decision-making. One checklist was inspired by a psychiatrist who had recommended to a patient that he use a mobile app to help with managing his daily patterns and mood. On closer inspection of the app's terms of service (ToS) and privacy policy, the clinician realized she was inadvertently putting her patient at increased risk because the app was sharing his personal information with third parties. The checklist prompts researchers to think about ethics, privacy, risks and benefits, access and usability, and data management (see) (Figure 5.2). The MobileELSI project, led by investigators from Sage Bionetworks and the University of Louisville, has a goal of understanding the scope of unregulated mobile health research to inform the development of a governance model. The increase in public access to technology has led to everyday citizens becoming involved in self-experimentation, a form of 'citizen science', which is largely unregulated as it falls outside of traditional regulatory requirements. In addition, technology companies are increasingly involved in biomedical research. Neither are obligated to apply the federal regulations to protect research participants unless, of course, they are developing an FDA-covered product or are conducting federally funded research. The MobileELSI project will develop recommendations to guide the conduct of unregulated digital medicine research. Sage Bionetworks and its governance team have led the charge in creating accessible informed consent templates for use on a smartphone that enable digital medicine research. For example, the 'Elements of Informed Consent' toolkit (Figure 5.3) is available to researchers to help them think through developing an effective informed consent process on a mobile device.
The CTTI is another source for guidance, with recommendations, resources and practical solutions to support responsible practices in mobile clinical trials. Ethics when an IRB review is not required. When research involves retrospective analysis of existing data or prospective observation, testing or experimenting with people to generate 'generalizable' knowledge, an IRB review is needed. The term 'generalizable' typically means that the results will be shared through peer-reviewed publication or presentations. The need for IRB review is relatively clear in the world of premarket clinical trials, but the lines defining human research in the postmarket, commercial world have been less obvious. Some instances of A/B testing may be considered human subject research, depending on whether an organization intends to share knowledge broadly or use it internally to improve its product or service. For example, Facebook found itself in hot water in 2014 after testing different versions of its News Feed with users to study emotional contagion. Had the results been kept internal to Facebook for product improvement, it would have flown under the radar for needing an IRB consult. However, those involved decided to publish the study results to share knowledge produced with the public. Sharing knowledge is believed by research ethicists and the scientific community to be a responsibility to society - which is certainly a good thing. In this case, though, many users were outraged about being involved in research that they did not consent to. In effect, more than 800 000 Facebook users had become inadvertent research participants. The takeaway message here is that ToS and end user license agreements (EULAs) are not a substitute for informed consent. People want the right to opt-in to being involved in biomedical research, and that is a clear call for respecting the ethical principle of 'respect for persons'. Yet, when we are doing work that is technically not research, what is our ethical obligation? In software development, the way user data have been treated has an emerging history of malfeasance. This practice likely results from the lack of universally agreed guidelines and standards. We strongly recommend the adoption of ethical principles to guide responsible practice when guidelines are lacking. In response to the lack of guidelines and exploitation of consumer data, new regulations have emerged that speak to consent and privacy concerns. The General Data Protection Regulations (GDPR) were passed by the European Parliament in 2016 and took effect in April 2018. The GDPR were designed to harmonize European Union (EU) privacy laws, protect EU citizens' data privacy and change how organizations, regardless of where they are located, process and manage EU citizen data. An important change that the GDPR introduced was the need for companies to obtain explicit informed consent separate from a ToS or EULA. This shift from consumers being helpless data subjects to empowered actors in the digital data economy is moving to the US. In 2018, California passed the California Consumer Privacy Act (CCPA) which, when implemented in 2020, gives consumers control over their data and requires that companies like Facebook and Google explain what data they collect, what they do with the data, and who the data are shared with.
Patients with glioblastoma should be cared for by a multidisciplinary team that includes a neurosurgeon, neuropathologist, neuroradiologist, medical and radiation oncologists, and possibly a neurologist, together with allied health professionals such as physiotherapists, occupational therapists and social workers. Involvement of a supportive care team including specialist nursing care is also mandatory in the palliative setting. Patients with glioblastoma have specific information needs. While the amount of information that patients want varies from person to person, and in some cases understanding may be impaired by cognitive deficits, patients and their carers need frank but compassionate information on diagnosis, prognosis, treatment options, recurrence and end-of-life care. News of a poor prognosis can be difficult to process initially. Patients need time to ask questions and receive honest answers. A combination of written and verbal information over several consultations and ongoing access to a well-resourced specialist nurse and psychologist or counselor are all important in this respect. Standard of care for newly diagnosed glioblastoma. The current standard of care for newly diagnosed glioblastoma is maximum surgical resection followed by radiotherapy and concomitant temozolomide and adjuvant temozolomide. Surgery is the initial therapeutic approach for tumor debulking and to obtain tissue for diagnosis. The aim of surgery is maximum tumor resection while maintaining neurological function. Risk versus benefit. The neurosurgeon's preoperative assessment of the patient's prognosis - which will depend, among other things, on the location of the glioblastoma and the patient's performance status - will determine the extent of resection. For example, if the tumor is near or involves eloquent (exempli gratia language, motor, vision) areas of the brain, a subtotal resection or tissue biopsy only may be considered a more appropriate option than aggressive total resection. The consequences of surgically acquired neurological deficits (exempli gratia language, cognitive, motor, coordination and visual deficits) can be significant. If surgical resection is not safe because of the tumor location or impaired performance status, a biopsy should be obtained if feasible. Surgical planning. MRI helps to estimate the operability of tumors and facilitates surgical planning. High-resolution 3D volumetric imaging sequences can help to assess tumor resectability and the optimal operative trajectory. Anatomic MRI can be integrated into surgical navigation software, where appropriate, together with functional and tractography sequences. Functional MRI (fMRI) measures dynamic blood oxygen level-dependent variations in brain activity. For the purpose of surgical planning, this usually requires the patient to perform specific tasks (task-based fMRI). Most commonly, fMRI can assess language lateralization or motor cortical activation in relation to the tumor. fMRI is considered less reliable for accurate language localization, because the activation of language regions is complex, multifocal and not comprehensively explored by basic fMRI tasks. Diffusion tensor imaging (DTI) builds on standard diffusion-weighted imaging (see page 36) by including additional (a minimum of six) diffusion directions. These multidirectional data enable both the volume and the directionality of diffusion to be calculated. DTI tractography provides an approximation of the relations between the most eloquent white matter tracts and the tumor. Distances are usually expressed as estimates rather than exact (exempli gratia millimeter) measurements, because the depiction of white matter fibers can vary depending on the segmentation technique and threshold settings. Task-based fMRI and DTI tractography are valuable tools for localizing eloquent cortical and subcortical areas before glioma surgery (Figure 4.1), providing better accuracy than intraoperative stimulation mapping data. Complete tumor resection is not curative, but is associated with longer overall survival (OS) than partial resection; the worst outcomes are in patients with unresectable tumors.
In a comparison of patients who had complete (100%) resection of a contrast-enhancing glioblastoma with those who had 78% to less than 100% resection, median OS was found to be 15.2 months versus 9.8 months, respectively (p < 0.001). More recently, it has been shown that IDH -mutant astrocytomas in particular benefit from near-complete resection. As tumors commonly recur around the resection cavity, especially after subtotal resection, depletion of postoperative tumor remnants may improve outcomes. In the study above, median survival was further increased to 20.7 months in patients who had more than 53% of the surrounding FLAIR (fluid-attenuated inversion recovery) abnormality region resected, beyond the 100% resection. An ongoing study is examining the efficacy and safety of stereotactic gamma knife radiosurgery to residual tumors immediately after excision of newly diagnosed glioblastoma. Postoperative care. Patients typically spend the night after surgery in an intensive care unit followed by an in-patient stay of 3-10 days. Recovery will depend on the location of the resected tumor and the patient's age and overall health. Further imaging will confirm the extent of surgical resection. CT has excellent sensitivity for the detection of blood, which makes it a useful initial investigation in patients who deteriorate after surgery. Furthermore, CT is useful for identifying and/or monitoring development of hydrocephalus. Antibiotics are usually given for 24 hours after surgery, along with anticoagulants until the patient is ambulatory. Anticonvulsants and steroids should be prescribed and tapered according to the patient's clinical status. Patients are usually advised to restrict the level of exercise in the immediate days after they leave hospital. Postoperative radiotherapy alone was standard treatment until it was demonstrated that external beam radiotherapy with concomitant temozolomide chemotherapy was more effective than radiotherapy alone (see below). This treatment is not usually administered until about 4 weeks after surgery when the craniotomy wound has healed. Three-dimensional conformal beam or intensity-modulated radiotherapy is then delivered as 60 Gy in 1.8-2 Gy fractions 5 days a week for 6 weeks. Doses above this are associated with increased toxicity with no additional survival benefit. Temozolomide is an imidazotetrazine-derived alkylating agent that is 100% bioavailable when taken orally, reaching peak plasma concentrations within 1 hour. Because of its small size and lipophilic properties, it can cross the blood-brain barrier. Temozolomide is administered orally at 75 mg/m daily during radiotherapy. After a duration of approximately 1 month, this is followed by six cycles of adjuvant temozolomide at 150-200 mg/m on days 1-5 of every 28-day cycle. Temozolomide acts by preventing DNA replication through alkyl group-mediated crosslinking of DNA. Clinical data. The standard of care described above is based on the results of the 2005 European Organisation for Research and Treatment of Cancer (EORTC) 26981-22981/National Cancer Institute of Canada (NCIC) Clinical Trials Group CE3 randomized Phase III trial of radiotherapy with concomitant and adjuvant temozolomide versus radiotherapy alone in 573 patients with glioblastoma. In this trial, median OS was 14.6 months with radiotherapy plus temozolomide, and 12.1 months with radiotherapy alone. The unadjusted hazard ratio (HR) for death in the radiotherapy/temozolomide group was 0.63 (95% CI 0.52-0.75, p < 0.001). In patients whose tumors contained a methylated MGMT promoter (see page 18), median OS was significantly longer with temozolomide and radiotherapy than with radiotherapy alone (21.7 [95% CI 17.4-30.4] versus 15.3 [13.0-20.9] months; p > 0.007).
The survival benefit in patients who received adjuvant temozolomide with radiotherapy continued to the 5-year follow-up, including in patients aged 60-70 years. Survival did not plateau in the EORTC-NCIC CE3 study. Indeed, the conditional probability of survival may offer relevant information for patients with glioblastoma who survive for some time. A study published in 2011 involving 498 patients with glioblastoma who received radiotherapy and chemotherapy, enrolled into seven Phase II protocols between 1975 and 2007, reported that the probability of surviving an additional year given survival to 1, 2, 3 and 4 years was 35%, 49%, 69% and 93%, respectively. This finding was corroborated in a later retrospective study of 882 consecutive patients with a diagnosis of glioblastoma in the temozolomide treatment era. A pooled analysis of individual patient data from four randomized trials involving patients with newly diagnosed glioblastoma treated with radiotherapy with concurrent and adjuvant temozolomide has been reported. The analysis included patients who received six cycles of adjuvant temozolomide and were progression free 28 days later (n > 624), with patients grouped into those who received up to six or more than six cycles. After adjusting for prognostic factors (age, performance status, extent of resection and MGMT methylation), treatment with more than six cycles of temozolomide was associated with improved progression-free survival (PFS) (HR 0.80 [95% CI 0.65-0.98]; p > 0.03), particularly in patients with methylated MGMT (n > 342; HR 0.65 [0.50-0.85]; p < 0.01). However, there was no significant improvement in OS. Thus, six cycles of adjuvant temozolomide remains the standard of care. Dose intensification of adjuvant temozolomide has not been shown to prolong OS in patients with newly diagnosed glioblastoma, and survival has not been improved by the addition of cilengitide, the peptide vaccine rindopepimut or bevacizumab, a monoclonal antibody targeted against vascular endothelial growth factor (VEGF) A (see page 53). A recent randomized Phase III trial investigated the addition of the alkylating nitrosourea lomustine to temozolomide plus radiotherapy in 129 patients with newly diagnosed glioblastoma with methylation of the MGMT promoter. The addition of lomustine significantly increased median OS to 48.1 months (95% CI 32.6 months to not assessable), compared with 31.4 months (27.7-47.1) with temozolomide alone (HR 0.60 [95% CI 0.35-1.03]; p > 0.0492) However, caution is warranted, as this trial was small and therefore underpowered. Managing treatment-related adverse events. The side effects of radiotherapy and concomitant temozolomide can affect each patient differently, but they are not unique to patients with glioblastoma and can be medically managed. The most common radiotherapy-related effects include fatigue, alopecia, radiation dermatitis, headaches, dysgeusia and anorexia, while the main chemotherapy-induced events include myelosuppression, fatigue, nausea and constipation. Cytopenias, pneumonitis and opportunistic infections have also been reported. As survival times improve, long-term monitoring of late toxic effects will be needed to guide treatment recommendations in the future.
Prognostic factors. MGMT methylation status. In 2019, Hegi and colleagues reported a study that examined the optimal cut-off for definition of MGMT methylation based on four randomized trials involving 1725 patients with newly diagnosed glioblastoma who received standard radiotherapy and temozolomide. A cut-off of greater than 1.27 was identified as high MGMT methylation that would render a tumor more sensitive to temozolomide. The cut-off for 'truly unmethylated' tumors was -0.28 or lower; approximately 10% of tumors were in the 'gray zone' (< 0.28, <= 1.27). The authors concluded that low MGMT methylation status within the 'gray zone' may confer some sensitivity to temozolomide therapy and recommended that the lower safety margin should be used to select patients with unmethylated tumors for inclusion in trials that do not include temozolomide. At present, clinicians form their own opinions as to how to apply this in clinical practice. Other prognostic factors associated with improved survival in patients with glioblastoma receiving radiotherapy with concomitant and adjuvant temozolomide are younger age, better performance status, Mini-Mental State Examination score of 27 or higher and no steroid treatment at baseline. Follow-up. Serial magnetic resonance imaging is the preferred method of response assessment and long-term monitoring after surgery and/or adjuvant therapy. MRI scans are commonly performed every 3-4 months unless the clinical status of the patient (exempli gratia deteriorating symptoms) dictates more frequent monitoring. Conversely, some clinicians may wait several months longer to image (well patients) to minimize any confusion with the transient worsening associated with pseudoprogression or radiation necrosis or, indeed, the misleading improvement of a pseudoresponse to therapy. This process of imaging is fraught with complexity. Many technical parameters such as scanner hardware, multiple software settings, patient positioning, contrast dose and timing all may influence lesion conspicuity over time. Serial tumor measurements can vary according to scan angle and slice selection. For this reason, the technical parameters of serial MRI scans should be kept as constant as practicable to avoid missing or overestimating tumor changes over time. Volumetric (3D) imaging can be useful in support of serial tumor measurements, as this provides image reconstructions in all planes. Glioblastoma recurrence versus therapy effects. A problematic issue is the overlap that exists between structural and advanced imaging features of viable tumor and necrotic tissue. On conventional MRI sequences, the early (pseudoprogression; Figure 4.2) and late (radiation necrosis) effects of adjuvant therapy are often indistinguishable from a growing tumor, as both may appear as new or enlarging enhancing lesions due to breakdown of the blood-brain barrier. The true incidence of therapy effects is debatable, as it depends on the definition of a progressive lesion. This complexity is acknowledged in the Response Assessment in Neuro-Oncology (RANO) criteria, which take into consideration the timing of adjuvant therapy for glioblastoma serial monitoring. To date, the challenge of distinguishing between glioblastoma recurrence and therapy effects has not been completely resolved, not least because the two may coexist. However, several advanced imaging techniques are valuable in this context. Perfusion MRI can help to identify neovascular (viable) tumor (Figure 4.3), but difficulty may arise from intermediate values, and short-interval monitoring may be necessary unless a histological diagnosis is pursued. Magnetic resonance spectroscopy (MRS) may have potential for identifying therapy effects, but it is infrequently used for this indication and there are conflicting data with regards to its accuracy. Diffusion imaging, using apparent diffusion coefficient (ADC) values and advanced methods (see page 36), has shown success in lesion follow-up, with tumor typically exhibiting lower diffusivity values than tissue inflammation/necrosis. However, overlap in diffusivity values and movement of tumor structures over time can hamper serial diffusion measurements in clinical practice. PET imaging may also be useful if advanced MRI techniques are unable to provide a clear distinction between recurrent glioblastoma and therapy effects. Other follow-up tests. Routine blood tests are not required, other than for monitoring during chemotherapy, monitoring antiepileptic use (where indicated) and measurement of blood glucose in patients taking steroids (see page 63). A potential role has been suggested for MRS for monitoring the levels of the oncometabolite 2-hydroxyglutarate (see page 11) in patients with IDH -mutant gliomas who are receiving adjuvant therapy, but this is not currently applied in routine clinical care. 
Treatment of progressive glioblastoma. Tumor recurrence after initial standard of care therapy is virtually inevitable. A standard of care has not been established for glioblastoma that progresses after chemotherapy. For patients who meet the inclusion criteria and are considered fit enough for further treatment, enrolment into clinical trials is recommended. Surgery may be considered, particularly if the tumor exerts a mass effect. Repeat radiotherapy may be considered for recurrent small tumors, although there is a lack of prospective comparative trials. The evidence supporting use of stereotactic radiotherapy in the recurrent setting is minimal. A Phase I multicenter dose-escalation study in 15 patients with recurrent glioblastoma or anaplastic astrocytoma showed that re-irradiation using hypofractionated stereotactic radiotherapy with concomitant bevacizumab (see below) was feasible and reasonably well tolerated; dose escalation up to 11 Gy x 3 was possible. Median OS was 13 months in the intent-to-treat population. Chemotherapy with procarbazine, lomustine and vincristine (PCV) or single-agent nitrosourea therapy may also be considered. Favorable response rates and a steroid-sparing effect have been reported with bevacizumab (+- irinotecan) in recurrent glioblastoma, but the effect is often short-lived and may be due to alterations in vasculature. A recent trial found that the addition of bevacizumab to lomustine in patients with progressive glioblastoma after chemoradiotherapy did not improve survival: median OS was 9.1 months (95% CI 8.1-10.1) with the combination treatment compared with 8.6 months (7.6-10.4) with lomustine monotherapy. Bevacizumab is a humanized monoclonal antibody that selectively binds to circulating VEGF, thus reducing the microvascular growth of tumor blood vessels. As well as its use in the combinations described above, in the USA, bevacizumab is licensed as a single agent for the treatment of recurrent glioblastoma; however, it has not been approved for this indication in Europe. The National Comprehensive Cancer Network (NCCN) guidelines support the use of bevacizumab in this setting, the rationale being that the drug reduces vasogenic edema, which results in less associated neurological loss, providing patients with non-enhancing progressive disease and stable functional status with a better quality of life. Antiangiogenic therapies such as bevacizumab rapidly restore the blood-brain barrier. Despite the presence of viable tumor, this may markedly reduce lesion contrast enhancement on MRI ('pseudoresponse'). This must be taken into account when considering serial scans of non-enhancing signal abnormalities after treatment with bevacizumab (exempli gratia RANO criteria, see page 51). The complexity of glioblastoma imaging follow-up after experimental treatments such as immunotherapy is also acknowledged in specific iRANO criteria. Immunotherapies may result in a transient lesion worsening, or apparent non-response, which may be distinguished from tumor through a similar combination of imaging techniques as other pseudoprogression phenomena (see page 50). Palliative care. Patients with glioblastoma experience a range of uncontrolled symptoms at different stages of the disease, including pain, seizures and fatigue.
Mood and cognitive disturbances are also common. Palliative care encompasses the management of disease- and treatment-related symptoms (the management of specific disease-related conditions is discussed in Chapter 5) and practical physical and psychosocial support. Improving quality of life at all stages of the disease is an important goal for the multidisciplinary team. Early involvement of a palliative care team can reduce the patient's symptom burden, improve quality of life and extend survival. In patients with metastatic non-small-cell lung cancer, the institution of early palliative care led to significant improvements in quality of life and mood, and median OS was longer in patients receiving early palliative care (11.6 vs 8.9 months, p > 0.02). Access to speech therapy and physical rehabilitation are also important. Patients may also utilize complementary therapies (exempli gratia meditation, aromatherapy, massage) to manage stress and anxiety and for general well-being. As many patients lose the ability to make decisions about their care at the end of life, advance directives are essential to ensure that the patient's wishes are respected. End-of-life treatment decisions include discussion/documentation on tube feeding, hydration, steroid interruption and palliative sedation. Valuable patient outcomes include minimal hospitalization and interventional care, early hospice enrolment and death outside of hospital. It should be noted that most patients with glioblastoma experience good symptom control with progressive loss of consciousness in the end stages of the disease, without the need for pharmacological sedation. Elderly patients. Population studies have reported that survival declines with age in patients with glioblastoma, and the incidence of glioblastoma is increasing, especially among the elderly. A single-center retrospective study of consecutive patients from 2004 to 2008 reported that elderly patients (>= 70 years) with glioblastoma were most likely to be treated with best supportive care or palliative radiotherapy. The pivotal EORTC-NCIC study that demonstrated the survival benefit of adding concomitant and adjuvant temozolomide to radiotherapy in patients with newly diagnosed glioblastoma only recruited patients aged 70 years and younger. Management of glioblastoma in older patients (>= 65 years) can be challenging, given the poor prognosis, the likelihood of comorbidities and an increased risk of toxicity from radiotherapy on the aging brain. A recent trial in older patients (>= 65 years; n > 562) with newly diagnosed glioblastoma compared radiotherapy (40 Gy in 15 fractions) alone and with concomitant and adjuvant temozolomide (75 mg/m daily for 21 days, followed by 150-200 mg/m on days 1-5 of each 28-day cycle for up to 12 cycles or until disease progression). The addition of temozolomide to radiotherapy increased median OS from 7.6 to 9.3 months (HR for death 0.67 [95% CI 0.56-0.8]; p < 0.001), without compromising quality of life and with manageable chemotherapy-related side effects. Subgroup analysis showed that the benefit of chemoradiotherapy was particularly evident in patients with methylated MGMT status. Thus, combination therapy should be considered for fit elderly patients with glioblastoma.
Principles of management. Prevention versus treatment. Current therapy of osteoarthritis (OA) largely aims to treat existing disease by controlling its major symptom - pain - and maintain or improve joint and limb function and improve quality of life. While there are numerous therapies targeted against symptoms, at present no treatments have been definitively shown to modify the structural progression of OA. Currently, the best advice we can give on preventing OA relates to the following lifestyle modifications. avoid joint trauma. avoid high-impact loading of joints (through sport or occupation). maintain a body mass index within the normal range for size. maintain aerobic fitness, which aids periarticular muscle strength and weight control. A healthy diet is important to aid weight control, but no diets or dietary supplements have been shown to prevent, or to modify the progression of, OA. Management of OA. There are many evidence-based guidelines from a variety of specialty societies for managing OA (see Key references, page 72). These guidelines agree on several main principles for the treatment of the disease. People with OA should be involved in their own management, and should receive education about their condition and the range and safety of treatment options available. Optimum treatment involves a combination of non-pharmacological and pharmacological approaches. Therapies, particularly pharmacological ones, need to be tailored according to an individual person's comorbidities and risk factors. Exercise and weight management should be recommended to all patients. Table 6.1 summarizes the hierarchy of options that should be considered for managing individuals with OA. In general, all non-pharmacological options should be considered early, with some sequence or 'step-up' when using pharmacological interventions. Remember, none of these therapies is mutually exclusive and patients who are not responding should be offered as many options as possible before surgery is considered. Therapies will need to be tailored according to an individual patient's comorbidities and preferences. There are a few areas of controversy where guidelines diverge (discussed further in later chapters). glucosamine and chondroitin, alone or in combination (see Chapter 7). intra-articular therapy, particularly intra-articular hyaluronans (see Chapter 9). some surgical procedures (see Chapter 10). Very importantly, we should not confuse what clinicians advise or prescribe with how patients manage their own OA. There is little research on what patients actually do, but certainly compliance, perhaps more so with self-directed therapies, such as exercise, is often low. Clinicians therefore need to adapt the consultation to the needs of the individual patient to reach agreement on a suitable and sustainable management plan. The subsequent chapters give an overview of the rationale, evidence and clinical hints for using a particular therapy. However, it is worth first reviewing some of the limitations of the evidence base. Problems with interpreting the OA therapy literature. The practicing clinician may well be overwhelmed at times with the number of publications on a given therapy.
However, some critical analysis of trial data is important. Some of the common problems in interpreting OA trials include. the use of different definitions (exempli gratia of OA). the use of different outcome measures. unrealistic inclusion criteria. the degree of clinical significance. Different definitions. Studies use different definitions of OA, focusing on symptoms, clinical features, radiographic findings, or a combination of these. Inclusion of a wide range of pain severity (and presumably a wide range of structural pathology) means that the effects of a therapy on subgroups may be missed. For example, many studies of knee OA do not evaluate the patellofemoral joint, which is a common site of pain in OA of the knee, and few studies consider multiple joint involvement. Use of different outcome measures. The Outcome Measures in Rheumatology Core Set (OMERACT) for OA recommends collecting data on pain, function and quality of life; however, there are many instruments for measuring these domains, and they have different performance metrics. For example, modern studies use patient-reported generic pain visual analog or numeric rating scales, or the more responsive disease-specific multi-domain instruments such as the Western Ontario MacMaster questionnaire (commonly known as WOMAC). As these tools may be used as averaged daily measures or at single time points, care needs to be taken when comparing studies. Non-generalizable inclusion criteria. Most trials (of drugs in particular) tend to include relatively healthy patients and do not reflect the usual older OA population with a high frequency of comorbidities. In addition, trial participants may have only one painful joint, whereas large community samples suggest that, in practice, patients over 55 years of age may have an average of four painful joints. Degree of clinical significance. Many studies have reported a statistically significant difference between two trial therapies without reporting whether the result is important clinically. This may sometimes be expressed as the minimal clinically important difference (MCID). The MCID may vary depending on the measure used to estimate a patient's symptoms. Although the reader should look at specific data for individual outcome measures, in general, a treatment effect of at least 20% is desirable. Structured care pathways. With the massive rise of symptomatic OA due to the aging 'baby boomer' population and the increasing prevalence of obesity, healthcare systems are being placed under increasing stress to manage OA. In the UK, Department of Health directives for the planning of appropriate care pathways are in place, based on patient self-management with aid from primary care physicians and physical therapists, with much smaller numbers of patients expected to use secondary care and surgical interventions if managed appropriately. Similarly, in the USA, a number of initiatives, supported by the National Institutes of Health (NIH) and the Centers for Disease Control and Prevention, are addressing the rising burden of OA at the public health level. Certainly, public and private systems will increasingly look at the structure and cost-effectiveness of OA care regimens.
Management of myeloproliferative neoplasm blast phase. Most myeloproliferative neoplasms (MPNs) are chronic conditions that remain clinically stable for many years; however, the condition may progress to acute leukemia in a minority of patients, referred to as MPN blast phase (MPN-BP). This transformation most commonly occurs in patients with myelofibrosis, in whom transformation rates of 15-30% have been reported, compared with 1-4% for patients with essential thrombocythemia (ET) and 4-9% for those with polycythemia vera (PV). Typically, patients with ET or PV initially develop post-ET or post-PV myelofibrosis, which may subsequently undergo transformation to MPN-BP, although in rare cases these conditions may transform directly into MPN-BP without an intermediate myelofibrotic stage. Risk factors for the development of MPN-BP include leukocytosis, thrombocytopenia and unfavorable karyotype (Table 9.1). There are no specific diagnostic criteria for MPN-BP. The principal criterion is the same as for de novo acute myeloid leukemia (AML): more than 20% blasts in bone marrow or peripheral blood. Persistence is also important if only peripheral blood blasts are considered. Importantly, acceleration or progression of myelofibrosis is usually apparent before the development of overt leukemia. Signs of this include. worsening cytopenia or anemia. worsening of constitutional symptoms. progressive and resistant splenomegaly. increasing numbers of blasts in bone marrow and peripheral blood. rising lactate dehydrogenase. new cytogenetic or molecular marker. In addition, recent evidence suggests that progression during treatment with Janus kinase inhibitors is associated with a risk of MPN-BP. MPN-BP is associated with a poor prognosis: median survival is 2-6 months, although this may be prolonged by intensive treatment and prompt recourse to stem cell transplantation (SCT). There is no standard of care for MPN-BP, and many of the current treatment regimens (Table 9.2) are derived from experience in AML. Induction chemotherapy and SCT. In general, MPN-BP should only be treated with induction chemotherapy when allogeneic SCT is planned in eligible patients. The induction chemotherapy regimens are the same as those used in AML, and response rates are similar to those seen in poor-risk AML patients: typically, complete response (CR) and CR with incomplete count recovery (CRi) rates are 40-50%, although rates of approximately 70% have been reported with a combination of cytosine arabinoside and anthracycline. However, these responses are not durable, having a median duration of approximately 5 months. Eligible patients should therefore undergo SCT as soon as possible. The importance of this is highlighted by a study in which median overall survival in patients who completed allogeneic SCT following CR/CRi with induction chemotherapy was 47 months, compared with 9.4 months in those who achieved a CR/CRi but did not go on to SCT. Two-year overall survival rates were 47% and 17%, respectively. Achieving a CR is essential for successful SCT. In one study, 8 of 13 patients proceeded to SCT after induction chemotherapy and were followed for a median of 20 months. All five who achieved CR had returned to chronic MPN at the time of SCT but remained in remission, whereas 2 of 3 patients with persistent marrow or circulating blasts died. There is growing interest in post-transplant immunomodulation with donor lymphocyte infusion or drugs such as hypomethylating agents. Less-intensive chemotherapy. Patients with MPN-BP who are not eligible for induction chemotherapy or SCT may benefit from treatment with hypomethylating agents. In one study involving 54 patients with MPN-BP or a condition suggestive of myelodysplastic syndrome, treatment with azacitidine resulted in an overall response rate (CR + CRi + partial response) of 28% after a median of 20 months' follow-up, and median overall survival was 8 months in the MPN-BP group. However, support for this approach currently comes mainly from small case series. 
Management of metastatic prostate cancer. Although there is a trend towards earlier detection of prostate cancer, many men worldwide still present with widespread metastatic disease. In countries where prostate-specific antigen (PSA) testing is not widely used, about 30% of patients present with localized disease, 40% with locally advanced disease and the remaining 30% with metastases. In contrast to localized or locally advanced disease, metastatic prostate cancer is associated with high mortality - approximately 70% within 5 years. Androgen deprivation, which has become the mainstay of treatment, effectively reduces the intratumoral dihydrotestosterone (DHT) concentration by 70-80%, reducing stimulation of androgen receptors and increasing apoptosis of prostate cancer cells (Table 7.1). Androgen deprivation can be achieved by orchidectomy (surgical removal of the testes) or treatment with luteinizing hormone-releasing hormone (LHRH) analogs/antagonists; the value of adding an antiandrogen (maximal androgen blockade; see page 90) is still debated. Some of the trials with important results for the treatment of metastatic prostate cancer are summarized at the end of this chapter (see Table 7.4). Bilateral orchidectomy or bilateral subcapsular orchidectomy is performed through a midline scrotal incision (Figure 7.1) under local, regional or light general anesthesia. The procedure is simple and is associated with little morbidity. The principal adverse events are local complications such as hematoma and wound infections, together with the usual effects of androgen deprivation such as loss of libido, erectile dysfunction and hot flashes (Table 7.2). Clinical responses (decreased bone pain and reduced PSA concentration) are obtained in more than 75% of patients. Because of the psychological/cosmetic impact of orchidectomy and its irreversibility, however, most patients and their partners prefer non-surgical treatment with LHRH analogs/antagonists. LHRH analogs. LHRH analogs, such as goserelin acetate, buserelin and leuprorelin (leuprolide acetate), are highly potent LHRH agonists (superagonists). Administration is followed by an initial transient increase in the secretion of luteinizing hormone (LH), and hence in testosterone, which lasts about 3 weeks, but this is followed by desensitization (downregulation), resulting in decreased secretion of LH and testosterone (Figure 7.2). These agents can be delivered via 1-, 3- or 6-monthly depot preparations, administered subcutaneously or intramuscularly. A potential side effect is tumor 'flare' resulting from the initial transient increase (140-170%) in testosterone, experienced by 8-32% of patients. This may result in increased bone pain or worsening symptoms of bladder outflow or ureteric obstruction; spinal metastases may also be stimulated to expand, increasing the risk of spinal cord compression. Tumor flare can be avoided by prior and concomitant administration of an antiandrogen during the first 4-6 weeks of LHRH analog treatment. Comparative trials have shown that the response rates obtained with LHRH analogs are equivalent to those obtained after orchidectomy in terms of time to progression and OS. However, the reduction in testosterone may induce features of the metabolic syndrome (see page 126) and reduce bone mineral density (BMD; see page 125). LHRH antagonists. Pure LHRH (or gonadotropin-releasing hormone) antagonists block pituitary receptors and thereby inhibit LHRH release without causing the initial stimulation in LH and testosterone seen with the LHRH analogs; thus, they are not associated with a surge in testosterone (flare). The LHRH antagonist degarelix has shown positive clinical results in controlled clinical studies involving men with hormone-sensitive prostate cancer (Figure 7.3). A rapid reduction in testosterone without flare was achieved with this antagonist, compared with the significant flare associated with an LHRH analog. PSA decrease was rapid in both cases and was maintained in the long term. LHRH antagonists are particularly beneficial for patients with bony metastases, spinal cord compression or bladder neck obstruction, for whom rapid tumor control without testosterone surge is important. An additional potential benefit could be in men receiving intermittent hormonal therapy, in whom the rapid return to normal LHRH receptor function following withdrawal of the drug results in an accompanying rapid return of testosterone. Recently, it has been suggested that LHRH antagonists carry a lower risk of cardiovascular side effects than the LHRH analogs, possibly because they maintain consistently low testosterone levels. 
Antiandrogens (taken in tablet form) do not alter the levels of circulating androgens. Instead, they inhibit the androgen receptor where testosterone or DHT binds. They may be steroidal or non-steroidal. The steroidal antiandrogens (exempli gratia cyproterone acetate) also have a central testosterone-lowering effect and can be taken as monotherapy instead of castration, although they are not as effective. The non-steroidal antiandrogens inhibit the androgen receptor only and should not be taken as monotherapy for metastatic disease as the results are inferior to those achieved with LHRH analogs. Maximal androgen blockade. Although both orchidectomy and LHRH treatment produce dramatic initial responses in 70-80% of men, remission is not usually maintained in the long term. Androgen-independent cancer cell clones are selected out, and the mean time to tumor progression is less than 18 months and mean overall survival (OS) is 28-36 months. Persistent adrenal androgen secretion may contribute to this poor prognosis; there is evidence that adrenal androgens account for up to 15-20% of total androgen concentrations within the prostate. This has led to the concept of 'maximal androgen blockade', in which androgen deprivation by orchidectomy or LHRH treatment is accompanied by treatment with an antiandrogen to block the effects of adrenal androgens in the prostate. Several trials have shown that maximal androgen blockade using an LHRH analog in combination with an antiandrogen improves survival compared with either LHRH analog treatment alone or orchidectomy. However, other trials have not shown significant improvements in tumor progression and survival, and a meta-analysis of all studies demonstrated little or no advantage with the combined therapy. This discrepancy may arise from steroidal and non-steroidal antiandrogens being evaluated together - a subgroup analysis of combination treatment with non-steroidal antiandrogens showed a small survival advantage of 2.9% for combination therapy compared with monotherapy. In addition, maximal androgen blockade may offer a slight advantage over monotherapy in a subgroup of patients with good performance status (id est those who are generally well) and a relatively restricted metastatic burden. Such treatment should therefore be considered in younger and fitter patients who are more likely to die from the prostate cancer rather than from a comorbid condition. However, the relatively modest benefits need to be weighed against the increased costs and the small but significant incidence in side effects from the antiandrogens. The timing of hormonal therapy. The appropriate timing of hormonal therapy has been the subject of vigorous debate and the evidence now favors earlier therapy rather than waiting for symptoms. This evidence includes a re-analysis of the cooperative studies (USA) in which men receiving diethylstilbestrol (DES), 1 mg, had a survival advantage. The Medical Research Council (UK) study showed that men with locally advanced or metastatic disease treated with castration at diagnosis had better outcomes than those in whom therapy was deferred (Table 7.3), and a US trial reported by Messing et al. in 2006 found that delayed hormonal treatment in men with pelvic lymph node metastases was associated with a sevenfold increase in deaths from prostate cancer compared with those who had immediate androgen ablation therapy. A study that explored the timing of androgen deprivation therapy (ADT) in men who experienced a rising PSA after primary treatment of prostate cancer reported a 45% decrease in OS in those who delayed therapy (by >= 2 years) compared with those who had immediate treatment. It is clear that early initiation of hormone therapy in men with locally advanced or metastatic disease improves survival and decreases complications. It should be borne in mind, however, that earlier treatment with hormonal therapy increases the risk of side effects such as osteoporosis. 
Intermittent hormonal therapy. It has been suggested that continuous androgen ablation therapy may, in fact, increase the rate of progression of prostate cancer to a castrate-resistant state (see Chapter 8), prompting the exploration of the use of intermittent hormonal therapy, which also has the potential advantage of decreasing the side effects of therapy. In this approach, hormone therapy is initially given for 6-9 months. If PSA levels become normalized, the LHRH analog/antagonist is temporarily discontinued. Hormone therapy is resumed when serum PSA returns to pretreatment levels in those with a PSA below 20 ng/mL at diagnosis, or when PSA exceeds 20 ng/mL in patients with an initial PSA above this. Such a regimen allows serum testosterone to return to normal, thereby stimulating atrophic cells and rendering them more sensitive to androgen ablation (Figure 7.4). The use of a pure LHRH antagonist, which blocks the receptor without initial stimulation (see page 89), could be advantageous in this setting given the absence of flare and, potentially, a more rapid restoration of testosterone level after cessation of therapy. In some studies of intermittent hormonal therapy, up to five treatment cycles were given before evidence of castrate resistance appeared; men spent approximately 50% of the time off therapy. Two randomized trials have compared intermittent versus continuous treatment in men with advanced prostate cancer. The first study, from Canada, included 1386 men with a rising PSA after failure of radiotherapy, who were randomized to continuous or intermittent hormone therapy. Survival was approximately 9 years, and was similar between the groups; however, the group receiving intermittent treatment had better quality of life in the areas of physical function, fatigue, hot flashes, libido and erectile function. The results from a US study of 1535 men with metastatic prostate cancer who were randomized to continuous or intermittent hormone treatment were statistically inconclusive, although there was a trend for longer survival in men receiving continuous treatment (5.8 years, vs 5.1 years in men receiving intermittent treatment). It appears that intermittent hormone therapy is safe and preferable in men with a rising PSA level, whereas men with confirmed metastatic disease may be better treated with continuous therapy. Management of the adverse effects of ADT is described in Chapter 9. Androgen deprivation in combination with chemotherapy. In the CHAARTED study, 790 men with hormone-sensitive metastatic prostate cancer were randomized to ADT, either alone or in combination with six cycles of docetaxel chemotherapy. The combination therapy was associated with a 33% improvement in OS (median 58 vs 44 months). Men who had a high volume of metastases seemed to benefit the most. In the STAMPEDE study, a significant difference in OS was reported in men with advanced hormone-sensitive prostate cancer who received ADT in combination with docetaxel chemotherapy compared with those who received androgen deprivation alone. In a subgroup analysis of men with metastatic prostate cancer, OS was improved by 27% with combination therapy. There is now good evidence that, at least in men with high-volume hormone-sensitive metastatic prostate cancer, giving early docetaxel chemotherapy in addition to androgen deprivation therapy improves survival. Spinal cord compression and pathological fractures. Sudden onset of low back pain and weakness in the lower limbs, with or without voiding difficulty, in a patient with metastatic prostate cancer is a urologic/neurosurgical emergency. Spinal cord compression due to pathological fracture or collapse of the lumbar vertebrae is the most common reason for these symptoms (Figure 7.5). The diagnosis may be confirmed by urgent spinal MRI. Early neurosurgical decompression is often advised, usually followed by external-beam radiotherapy (EBRT) and corticosteroids. Pathological fractures caused by prostate cancer metastases may also occur elsewhere, such as in the femur or humerus. Fixation by an orthopedic specialist is often required and should usually be followed by EBRT and androgen ablation. 
Prevention and treatment of chemotherapy-induced nausea. Pharmacological management. Conventional antiemetics are more successful at preventing emesis than nausea. Current data from multiple large studies suggest that neither first- nor second-generation 5-hydroxytryptamine-3 (5-HT) receptor antagonists (RAs) are effective in the control of nausea in patients receiving either moderately or highly emetogenic chemotherapy (MEC or HEC, respectively), despite marked improvement in the control of emesis. Multiple large phase III trials have also shown that although the neurokinin (NK)-1 RA aprepitant is effective for the control of emesis, it is not effective in controling nausea in patients receiving MEC or HEC. Similarly, phase III clinical trial data indicate that the new neurokinin (NK)-1 RAs netupitant and rolapitant are not effective antinausea agents,, although rolapitant may have some effect in patients receiving cisplatin HEC. These studies suggest that the serotonin (5-HT) and substance P (NK-1) receptors may not be important in the mediation of nausea, despite their important role in chemotherapy-induced emesis (see page 11). New studies using novel agents and using nausea as the primary endpoint need to be performed. Olanzapine appears to have high potential for the control of both emesis and nausea in patients receiving MEC or HEC (Table 6.1). Phase III studies suggest that olanzapine may be an important agent in the control of chemotherapy-induced nausea. Olanzapine is known to affect a wide variety of receptors including dopamine D, 5-HT 2C, histaminic and muscarinic receptors. At present, there is still a substantial lack of understanding of the pathogenesis of chemotherapy-induced nausea, and any or all of these receptors may be important mediators. The addition of olanzapine to the 5-HT RA azasetron and dexamethasone has been shown to improve nausea and emesis compared with azasetron and dexamethasone alone in patients receiving MEC and HEC. Olanzapine, palonosetron and dexamethasone has been shown to improve the control of nausea compared with aprepitant, palonosetron and dexamethasone in patients receiving HEC. Breakthrough nausea and emesis was controlled with olanzapine in patients receiving HEC and guideline-directed prophylactic antiemetics. The addition of olanzapine to aprepitant, a 5-HT RA and dexamethasone significantly improved nausea and emesis compared with aprepitant, a 5-HT RA and dexamethasone alone in patients receiving HEC. Olanzapine is available as a generic. Other agents. Preliminary small studies with gabapentin have demonstrated some effectiveness in the control of chemotherapy-induced emesis, but the control of nausea has yet to be determined. More studies with the use of cannabinoids need to be performed before it is known whether this class of agent is clinically efficacious in the control of CINV. The studies performed to date do not support the use of ginger as an effective agent in the prevention of CINV. Non-pharmacological management. Few randomized clinical trials have demonstrated efficacy of non-pharmacological measures to prevent or treat chemotherapy-induced nausea. Table 6.2 summarizes measures suggested by case reports. 
The diagnosis of acne involves taking a history and physical examination to determine the type of acne (see clinical features of the different types of acne below), an assessment of severity (see page 38), and an assessment of psychosocial effect (see Chapter 4). Other investigations are not normally necessary; however, occasionally an underlying endocrinological disorder may necessitate further tests (see Adult female acne, pages 26-9). Table 3.1 outlines the key elements in developing an accurate acne history. Acne vulgaris. Acne vulgaris is the most common type of acne. The individual lesions of acne vulgaris (Figure 3.1) can be characterized as. non-inflammatory lesions. inflammatory lesions. postinflammatory erythema and/or pigmentary changes. Most patients have a mixture of non-inflammatory and inflammatory lesions. Non-inflammatory lesions are called comedones (see Chapter 2). Comedones may be microscopic (microcomedones) or visible as blackheads (open comedones) or whiteheads (closed comedones) (Figure 3.2). Microcomedones, the precursors of all acne lesions, may develop into whiteheads or blackheads. Blackheads (open comedones) are of similar size and really need no description. The reason for the black appearance is the presence of the skin pigment melanin, which has undergone oxidation. Whiteheads (closed comedones) are small spots ( around 1 mm) and are usually white or cream in color. Macrocomedones are large closed comedones (usually > 2 mm) and are usually white in color and palpable (Figure 3.3). In closed comedones, the contents of the pore cannot escape as easily as from an open comedone (as their external orifice is very small), so closed comedones, like microcomedones and macrocomedones, are more likely to become inflamed. Inflammatory lesions may be superficial (papules, pustules) or deep (deep pustules or nodules). Papules are small raised red spots (< 5 mm) that persist for 7-10 days (Figure 3.4). Pustules (Figure 3.5) are of a similar size to papules but are predominantly yellow. Nodules (Figure 3.6) are larger lesions (> 5 mm) and persist much longer (2-3 weeks). They are often firm initially and may be tender; as inflammation develops they frequently soften. Nodules are often associated with scarring, but more superficial inflammatory papules or pustules can lead to scars. Increased accumulation of dermal collagen may produce firm hypertrophic or keloid scars, especially on the angle of the jaw and the upper back and chest (see Figure 2.4, page 16). Hypertrophic scars do not extend beyond the initial area of inflammation, while keloid scars do. Loss of dermal tissue results in large atrophic scars - especially seen on the upper trunk - or smaller but deeper ice-pick scars, especially seen on the cheek (see Figure 2.3, page 16). The best solution to the problem of acne scarring is prevention, by instituting appropriate therapy early in the course of the condition. Postinflammatory erythema and/or pigmentary changes. In some patients, particularly those with skin of color and Fitzpatrick skin types IV-VI, hyper- (Figure 3.7) or hypopigmented macules may persist following resolution of inflammatory acne lesions. Patients may think that these resolving lesions are active acne lesions and may have the erroneous impression that their acne is not improving. It is important to reassure patients that these dark areas are healing lesions and not active acne. Postinflammatory hyperpigmentation generally resolves slowly with time but may take up to a year or longer in many cases. As with scarring, early appropriate therapy is the best means of avoiding this complication. 
Other forms of acne. Acne conglobata is a very severe form of inflammatory acne characterized by grouped comedones, cysts, abscesses, draining sinus tracts and scars (Figure 3.8). The majority of affected patients are males who present with lesions on the back, buttocks, chest and face. The axilla and inguinal areas can also be involved. The grouped comedones often have multiple openings. The inflammatory lesions are large, tender and red to violaceous in color; they often drain a serous or purulent material. Deep-seated sinus tracts often develop, as does keloidal scarring. Secondary infection with staphylococci or streptococci can occur, although many lesions are colonized by Propionibacterium acnes (P. acnes) only. Patients with this very severe form of acne require expert care with oral isotretinoin, oral and intralesional corticosteroids and surgical excision of sinus tracts. Chronicity for many years is a feature, as is poor response to therapy. Acne fulminans is a very severe form of inflammatory acne associated with systemic signs and symptoms, including fever, arthralgias and/or osteolytic lesions of the clavicles or ribs. It usually occurs in boys aged 13-18 years and can be very acute in its onset. Investigations frequently demonstrate leukocytosis, elevated erythrocyte sedimentation rate and/or proteinuria. Clinically, acne fulminans is characterized by multiple intensely inflamed nodules, cysts and plaques (Figure 3.9). Large nodules can ulcerate, drain and become necrotic. Hemorrhagic crusting is common. A polyarthritis of large joints such as the sacroiliac, hips, knees, shoulders, elbows and ankles may be present. The etiology of acne fulminans is unknown. Patients with this disorder should be urgently referred to a dermatologist for management with oral corticosteroids and isotretinoin (Table 3.2). Acne with solid facial edema. In rare cases, acne can be accompanied by a firm non-pitting edema of the midportion of the face and forehead. The etiology of this phenomenon is unknown but may relate to lymphatic obstruction secondary to the inflammatory response associated with the acne. A primary hypoplastic abnormality of the lymphatics is the likely cause. This condition is unresponsive to oral antibiotics but has been treated successfully at times with oral isotretinoin and corticosteroids. Adult female acne. Adult acne, particularly in women, is worthy of special note; many adults have a prolonged course of acne beyond teenage years and in some cases acne may present for the first time in adulthood. Although it is generally less severe than teenage acne, acne in adult women is challenging to treat. In some cases, this form of acne is characterized by tender inflammatory papules or nodules involving the lower third of the face and neck (Figure 3.10). There may also be comedonal acne involving the forehead or lateral margins of the face. In other cases, widespread disease may affect the trunk. If an adult woman has sudden-onset severe acne, acne accompanied by signs of hyperandrogenism or acne that is refractory to conventional therapy, a medical history and physical examination directed towards eliciting symptoms or signs of hyperandrogenism should be performed. Investigations to rule out an underlying endocrine abnormality should also be carried out in these cases (Table 3.3); note, however, that most cases of adult female acne do not require an endocrine work-up. Screening tests for hyperandrogenism include serum dehydroepiandrosterone sulfate (DHEAS), total testosterone, free testosterone, and luteinizing hormone and follicle-stimulating hormone.
These tests should be obtained in the luteal phase of the menstrual cycle. Women should be advised not to take oral contraceptives for at least 1 month before laboratory testing, as these drugs can mask an underlying endocrine abnormality. Excess androgens may be produced by either the adrenal gland or the ovary. Serum DHEAS can be used to screen for an adrenal source of excess androgen production. An ovarian source of excess androgens may be suspected if the serum total testosterone is elevated. Women with hyperandrogenism may also have insulin resistance; they are at risk of developing diabetes and cardiovascular disease. It is therefore important for the long-term health of these patients to identify hyperandrogenism so that appropriate therapy from an endocrinologist or gynecologist can be initiated. Childhood endocrinopathy. Acute onset, persistent or severe acne in the presence of virilization in children aged 1-7 years should always raise the possibility of an underlying endocrinopathy including premature adrenarche, precocious puberty, congenital adrenal hyperplasia due to 11beta-hydroxylase deficiency or the presence of an adrenocortical tumor. A focused history and examination for signs of accelerated growth, precocious puberty (Table 3.4) and hirsutism or other signs of hyperandrogenism should be adopted, and referral to a pediatric endocrinologist should be considered. Follow up and anticipatory guidance is indicated in these children. In prepubertal acne, predictive factors for severity and persistence include a high number of comedones, mid-facial distribution and early development of comedones, high to normal levels of DHEAS and high total testosterone. Earlier menarche in girls has also been described as a predictive factor. Acne excorie (also known as acne excorie or 'acne de jeune filles') is described on pages 46-7. Acne mechanica. Repetitive rubbing or friction can sometimes exacerbate acne. This is most commonly observed with sports equipment such as football helmets, shoulder pads and chin straps. It can also occur in response to habits of rubbing the face or resting the head on the hands. Acne mechanica tends to occur in cases of moderate-to-severe inflammatory acne and less often in cases of mild acne. Treatment is aimed at controlling the underlying acne and minimizing the mechanical stress on the skin. Occupational acne. Exposure to certain industrial agents can lead to the development of acne. Coal tar derivatives and insoluble cutting oils can produce an inflammatory acne characterized by large comedones, papules, pustules, cysts and nodules. Lesions are most commonly noted in areas covered by clothing that was saturated with the offending agent. Chloracne is caused by exposure to halogenated hydrocarbons via ingestion, inhalation or contact with the skin. Most cases have been reported as a result of accidental industrial exposure, ingestion of contaminated food products, chemical warfare or exposure to herbicides. Implicated chemicals include polyhalogenated naphthalenes, biphenyls, dibenzofurans, dioxins and azobenzenes. Chloracne is characterized by the development of dense collections of comedones (Figure 3.11) on the face, retroauricular skin, neck, axilla and scrotum. These comedones can eventually develop into tender inflamed cysts. Outbreaks of severe inflammatory lesions, which heal with scarring, can occur for years following exposure to the offending agent. Treatments include topical tretinoin and oral isotretinoin; physical treatment with gentle cautery after application of local anesthesia can be particularly helpful.
Differential diagnosis. The differential diagnosis of acne includes drug-induced acneiform eruptions, rosacea, pyoderma faciale, gram-negative folliculitis and perioral dermatitis. Drug-induced acne. A number of drugs cause or worsen acne or can induce an acneiform eruption (Table 3.5). The latter account for about 1% of all drug-induced skin eruptions. A diagnosis of acne vulgaris is defined by the presence of closed comedones, papules and pustules. In contrast, most drug-induced 'acnes' represent acne-like eruptions embracing monomorphic inflammatory lesions in the absence of comedones, often presenting abruptly outside the most frequent age range for acne and extending over areas not commonly affected by acne. The face and upper trunk are most frequently affected. The interval between the onset of the acneiform eruption and administration of the implicated drug depends very much on the agent provoking the response. Psychotropic drugs. Rarely, some psychoactive drugs including lithium and amineptine, an atypical tricyclic antidepressant, may induce true acne. Lithium can worsen existing acne or cause acne de novo. The acne presents 2-3 months after starting therapy. Papules, pustules and nodules occur and severe forms such as acne conglobata have been described. Most reported cases involving amineptine have been in adult women. Comedonal lesions are the most frequently seen lesions; inflammatory lesions are usually sparse. The mechanism has been postulated to be via a selective decrease in the uptake of dopamine followed by an inhibitory effect of elevated dopamine on prolactin, with a subsequent increase in testosterone output. If 'acne' is triggered the drug should be withdrawn; response is variable and systemic therapy and physical treatment of the comedones may be required. Corticosteroids may provoke an acneiform reaction regardless of their route of administration. The precise mechanism is uncertain. Steroid acne is usually, but not invariably, more monomorphic than true acne vulgaris; however, both inflammatory and non-inflammatory lesions may be present on the face, back and chest (Figure 3.12). Abuse of androgenic anabolic steroids (AAS), synthetic derivatives of testosterone and testosterone salts by body builders and others can exacerbate true acne vulgaris and induce acne fulminans or acne conglobata. Usage is increasing in women. Induction of acne is due, at least in part, to androgen receptor binding leading to hypertrophy of the sebaceous glands with consequent increased sebum output and a concomitant increase in the population density of P. Estimates suggest that acne is a side effect in as many as 43% of users. Acne may be exacerbated by progestins and has been reported with use of the Mirena coil. Third-generation progestins in combined oral contraceptives have a lower androgenic effect. Antiepileptic drugs, especially phenytoin, have been incriminated in case reports in the past. Iodides and bromides commonly cause follicular pustules. Iodides may be found in non-prescription preparations for asthma, expectorants, kelp-containing substances and teas. Sedatives and cold remedies often contain bromides. Chloracne (see page 31) is caused by systemic poisoning, most frequently in occupational settings. Vitamins B, B and B. Exacerbation or new onset of acne have been described with vitamin B, 5-10 mg/week. Women are almost exclusively affected, with acne developing within the first 2 weeks after the injection. The acne-inducing dose of vitamin B has not been established. Iodine is present in some ampules of vitamin B and may be the trigger. Recently, a study showed that vitamin B supplementation in P. acnes cultures promoted the production of porphyrins, providing a potential mechanism of action, as porphyrins induce inflammation in acne.
The eruption is monomorphic, consisting of small follicular papules and pustules on the face, especially the forehead and chin, as well as the upper arms and upper trunk. Conventional acne therapies are usually unsuccessful but the acneiform rash settles within 8-10 days of withdrawing the vitamin B therapy. The severity of the reaction also appears to correlate with the response to therapy. EGFR inhibitors. A follicular acneiform eruption has been reported in patients with cancer who are treated with epidermal growth factor receptor (EGFR) inhibitors. The consistency of lesion morphology and timing following monotherapy with these agents suggests a direct biological effect of the antibody. A drug-induced skin reaction occurs in more than 50% of patients, often within a few weeks of receiving therapy. The changes include follicular papules and sterile pustules on the face and upper trunk in a distribution similar to conventional acne, although in severe cases the limbs may be affected. Comedones are not seen. Most reports suggest that topical acne therapies, oral tetracyclines and topical or oral corticosteroids are effective. Oral isotretinoin has also been used with some success. Rosacea is most common in adults with fair skin and light hair and eye color. Comedones are notably absent. It is characterized by facial flushing and erythema of the cheeks, nose, forehead and chin (Figure 3.13). Inflammatory papules and pustules can develop within the areas of erythema. In late stages of rosacea, a bulbous hypertrophy of the nose, termed rhinophyma, may occur (Figure 3.14). Pyoderma faciale (Figure 3.15), synonymous with rosacea fulminans, is deemed to be an explosive form of rosacea. This disorder is most common in young women, often in the context of stress, with a phenotype typical of rosacea patients. Treatment with oral isotretinoin and corticosteroids is indicated (see Table 3.2). Gram-negative folliculitis is characterized by the sudden development of superficial pustules in patients who have been treated for acne with antibiotics (Figure 3.16). It may seem to masquerade as a flare of the underlying acne, but it is actually a folliculitis caused by gram-negative bacteria including Pseudomonas species and Enterobacteriaceae. Cultures of pustules should be obtained. If gram-negative organisms are present the patient should be referred to a dermatologist for consultation regarding treatment with isotretinoin (see Table 3.2). High-dose trimethoprim has been used successfully but relapse rate is much higher than with oral isotretinoin. Perioral (periorificial) dermatitis (Figure 3.17) is characterized by erythema, scaling and small papules and pustules, most commonly around the mouth and on the chin. It often occurs in adult women, especially in the context of stress. Topical corticosteroids can cause or exacerbate the condition and should be avoided. Oral tetracycline is the treatment of choice. Assessing acne severity. Defining acne severity helps with the selection of the most suitable initial therapy and enables response to treatment to be monitored. There are many scales available for assessing the severity of acne, but no consensus on a gold standard. Severity can be determined according to the type, number, distribution or location of lesions, or a combination of each of these. Acne can be classified as comedonal, mild, moderate or severe based on the type of lesions (Table 3.6). Alternatively, an assessment tool such as the Leeds Revised Acne Grading System allows grading of severity based on photographic comparisons of acne on the face (Figure 3.18), back and chest.
Causes of low back pain. Mechanisms of pain. The precise pathophysiological mechanisms involved in pain perception in low back pain disorders are not well understood. When musculoskeletal dysfunction and tissue damage occur, a number of neurohumoral factors may be activated. In the intervertebral discs, a normal annulus fibrosus (but not a normal non-traumatized nucleus pulposus) is highly enervated by pain fibers and capable of provoking a pain response. The same is true of the posterior longitudinal ligament of the spine. Afferent neurons in the dorsal root ganglia stimulate production of substance P, a chemoattractant and vasodilator, and somatostatin, together with other neuropeptides including derivatives of cell membrane arachidonic acid including prostaglandins and leukotrienes. Prostaglandins and leukotrienes are released by activated nociceptors when there is local tissue injury to the annulus and/or adjacent neural, ligamentous and synovial tissues. These chemical factors play important roles in facilitating pain signal transmission, sustaining an inflammatory response and participating in an orderly healing process. This pain-inflammation reaction can be inhibited, in part, by corticosteroids and non-steroidal anti-inflammatory drugs (NSAIDs). Our current understanding of pain is that of multiple nerve interconnections modified by neuropeptides and other chemokines produced by nerves and surrounding tissues. The signals travel to the central nervous system where they are experienced as pain. Simultaneously, pain-reducing signals travel along the pain inhibitory pathway to reduce pain transmission. Endorphins, neurochemicals stimulating the pain inhibitory pathway, decrease pain and promote a feeling of well-being. Duration of pain is an important characteristic. Acute pain is a warning of acute injury. With acute injury, a specific array of prostaglandins, leukotrienes and other chemoattractants are released to alert the nervous system to damage and to initiate an inflammatory process to heal the traumatized structure. Therapies directed at control of acute pain, such as simple analgesics and NSAIDs, are effective. Importantly, the structure of the central nervous system has not been modified. When the injury has healed, the affected area returns to its usual status. Chronic pain is not acute pain that goes beyond the usual time for healing. Chronic pain is mediated through a different set of neurochemicals. Chronic pain causes modifications in nerve cell receptors, membrane thresholds and pathway interactions. These alterations result in increased sensitivity to painful stimuli. With these changes in the nervous system, chronic pain may persist even after the original injury has healed. Chronic pain also has effects on the centers of the brain involving mood and emotions. Chronic pain can add to stress, anxiety and depression, and vice versa. For more information on definitions and mechanisms of pain see Fast Facts: Chronic and Cancer Pain. Psychological factors affecting pain. Any painful stimulus affecting the body is ultimately experienced in the central nervous system. There, it is modulated by genetically determined pain transmission and reception capabilities, familial and social conditioning, and increasingly by medicolegal and work-related socioeconomic factors and the possibility of malingering. All of these factors can be further modified by experiences of previous trauma, surgery and medication, as well as by general and specific health issues. Back pain occurs in an individual with a combination of physical, social and emotional concerns. All of these factors must be considered when evaluating the person who presents with the complaint of low back pain. Back pain is, in the truest sense, psychosomatic. Psychological issues must be addressed concurrently with the anatomic and pathological somatic aspects of low back pain-related disorders. Psychosocial issues must be prioritized, though they can often be readily handled by the patient and the practitioner. If this is not the case, they should be addressed promptly with appropriate psychiatric, psychological and/or social counseling according to need. Issues relating to drug habituation, depression, symptom amplification and chronic pain syndromes must be identified and addressed. As often as possible, acute pain syndromes must be reversed in a timely manner to limit the possibilities of developing a chronic pain condition, and pain-relieving strategies must be structured and coordinated to implement a prompt return to family, workplace and social activities. 
Anatomic structures involved in generating pain. Many anatomic factors can play a primary or a secondary role in the development and progression of low back pain syndromes. The most common causes of low back pain are mechanical in origin. Mechanical disorders of the lumbar spine are related to injury, overuse or deformity of a spinal structure. The most important traumatic factors in low back pain relate to soft tissue structures. Precise identification of the injured tissue, and the role of that injury in the consequent pain and dysfunction, can be frustratingly difficult. Problems occur in muscles, intervertebral discs, facet joints, ligaments or spinal nerves. Aging causes modification of these structures over time, and different parts of the spine tend to be at greater risk for change or injury during different decades of life. Early in life, muscle injuries are more frequent, while joint problems occur in the sixth decade of life (Table 3.1). Paraspinal muscles are essential to postural stabilization and mobilization of the trunk, and consist of two main types. The small muscle group consists of two short intersegmental muscles: the interspinalis muscles that bridge the spinous processes and the intertransversarii that bridge the transverse processes. The multifidus and erector spinae form two large, polysegmental muscle groups. The multifidus is the most medial. Its origin is on the spinous processes, with insertions on the vertebral mamillary processes, the posterior superior iliac spine, the dorsal aspect of the sacrum and the posterior sacroiliac ligament. The erector spinae consists primarily of the longissimus thoracis and the ileocostalis lumborum (Figure 3.1) with a third small component, the spinalis dorsi, which only attaches to the spinous processes of L1, L2 and L3. The erector spinae muscles originate on the lower ribs and thoracic transverse processes and insert on the lumbar vertebrae, the iliac crests and adjacent to the posterior superior iliac spines. Although muscle 'spasm' of the erector spinae in association with acute pain is common, pain referral and muscle spasm with focal tenderness (a trigger point) in the mid-portion of the quadratus lumborum muscle and/or a tender point on the mid-iliac crest is more often noted. This can be a consequence of pain referral from vertebral facet or discal strains at either the dorsolumbar origins or the iliolumbar insertions of the erector spinae muscles. The gluteal and piriformis muscles are shown in Figure 3.1. The primary functions of the gluteal muscles are to support, stabilize and mobilize the hips and lower extremities in relation to the pelvis and the trunk. Possibly the most common manifestation of lumbar and lumbosacral pain referral is to the mid-portion of the gluteal muscles (buttocks) that overlie the piriformis muscle. This is accompanied by a palpable localized region of deep, tender muscle induration. Because of the thickness of the overlying gluteus maximus muscle, the fibers of which are parallel to (and indistinguishable from) the piriformis muscle fibers, it is not possible to determine by palpation if the piriformis muscle per se is the source of the localized muscle tenderness. A 'piriformis syndrome' is therefore somewhat problematic to identify and must be carefully distinguished from the more common, spinal discogenic basis for sciatic radiculopathy. A piriformis syndrome may result from scarring after a fall on the buttocks or as a consequence of pelvic or hip surgeries. MRI can be used to visualize the tender area in the buttocks and help to identify selective piriformis entrapments of the sciatic nerve. Piriformis syndrome consists of symptoms of sciatic radiculopathy associated with tenderness in the piriformis muscle region on deep palpation, and the absence of MRI or electrodiagnostic evidence of a disc protrusion. Because tenderness in the sciatic outlet/piriformis region is the most common pain referral manifestation of a lumbar strain with or without any nerve root impingement, the diagnosis of a piriformis syndrome is too often made.
There are no negative consequences of this, however, unless unnecessary surgery is performed. If an MRI of the lumbar spine shows no disc abnormalities, and if pathological enlargement of the piriformis muscle and displacement or entrapment is visualized by MRI of the sciatic nerve, therapy can be directed to relaxing and stretching the piriformis muscle or to the surgical release of the entrapped sciatic nerve tissue. Muscle strain of the lumbar region is the most common cause of back pain in the 20 to 40 age group (see Table 3.1). These events occur when an individual lifts an object too heavy for the paraspinous muscles to support. The reflex signal from the brain tells the injured muscle to contract to facilitate healing. Simultaneously, reflex signals are sent to surrounding muscles to contract to protect the injured muscle. The severity of the spasm can vary, but the associated pain can be overwhelming. The diagnosis is suspected when mobility of the spine is decreased and physical examination reveals increased muscle contraction, unilateral or bilateral. Treatment for muscle strain is gradual movement, NSAIDs and muscle relaxants. Discs and their associated ligaments are designed to absorb shock and simultaneously to transmit imposed forces in many directions. This is made possible by the resistive quality of the fluid contained in the nucleus pulposus, coupled with the elasticity of the surrounding annulus fibrosus. The fluidity of the nucleus pulposus resists primarily vertical compressive forces, and distributes them radially into the annulus fibrosus, which is uniquely designed with alternating bands of fibers to resist torsional strains (Figure 3.2a). Despite these design features, discs do wear, tear, rupture, protrude and extrude, though fortunately they usually heal. The consequences of disc herniations (Figure 3.2b) range from none (asymptomatic) to agonizing pain and paresis. The posterior longitudinal ligamentous constraints are weaker posterolaterally, and so most disc protrusions occur away from the midline and may impact the laterally placed nerve roots. Because of the more vertical alignment of the lower nerve roots in the cauda equina, the fifth lumbar nerve roots will be vulnerable to impaction by a posterolateral protrusion of the fourth lumbar disc (see posterior longitudinal ligament, 3. Other ligaments (Figure 3.3) are the following. The ligamenta flava consist of yellowish elastic fibers distributed vertically between the laminar arches, permitting extension without in-folding and pinching compression of the nerve tissue. The intertransverse ligaments lie deep to the tendinous insertions of the segmental muscles on the transverse processes. The interspinous ligaments bridge between the spinous processes beneath the longer and heavier, overlying, supraspinous ligament. The anterior longitudinal ligament is a heavy ligament that blends firmly with the periosteum of the vertebral body, and is loosely attached to the anterior aspect of the annulus. It is most firmly attached to the articular lip, or rim, of each vertebral body, thus accounting for the occurrence of so-called traction spurs and rim osteophytes (spurs) as a consequence of discal dysfunctions and ligamentous strains. The posterior longitudinal ligament, in contrast, is not attached to the vertebral body. It is bow-strung over the concave posterior surface of the vertebra, leaving space for the vascular elements to enter and leave the medullary sinuses. The strong mid-portion of the posterior longitudinal ligament tends to resist annular protrusion but, as noted above, disc protrusions can more easily penetrate laterally. Disc herniation occurs when the nucleus pulposus escapes the confines of the annulus fibrosus. Disc protrusion or bulging occurs when the nucleus remains within the outermost wall of the annulus. Disc extrusion or herniation occurs when the gel escapes the annulus entirely and enters the spinal canal. In flexion, the anterior annular fibers are compressed and the posterior fibers stretched. The annulus and the facet joints share 80% of torsional stress. The facet joints are separated in flexion, placing more strain on the annulus and increasing the risk of disc rupture. With age, or after a previous injury, the nucleus becomes stiffer, the annulus loses elasticity and the susceptibility to disc injury is increased.
When a neurological dysfunction is detected, the affected nerve root level can usually be identified, but the exact source of the neural impingement and/or trauma may be more elusive. When the gel comes in contact with the spinal nerve, an inflammatory response occurs that attempts to resorb the gel while simultaneously inflaming the corresponding nerve root. The inflammation of the nerve can result in corresponding dermatomal leg pain, numbness, loss of reflex and/or weakness. Healing of any injured tissue is a three-phase process. resolution of the acutely traumatized tissue site, exempli gratia reabsorption of local edema, hemorrhage and damaged tissue debris. repair of the injured tissues. remodeling of the damaged tissue, to the extent that this is possible. A majority of individuals (80% or more) can resolve a disc herniation without surgical intervention. Over a number of months, the gel can be resorbed and the nerve returned to normal function. Non-surgical therapy can include NSAIDs, muscle relaxants and analgesics (see Chapter 4). Vertebrae (cervical, thoracic and lumbar) are designed to perform various functional tasks. The lumbar spine supports the torso and permits flexion, lateral flexion and extension. The facet joint alignments restrict rotation except at the lumbosacral junction, where the heavy iliolumbar ligaments provide restraint. At the junction of the fifth lumbar vertebra and the sacrum, the sacrovertebral angle slopes downward, causing the truncal center of gravity, after passing through the fifth lumbar vertebra, to press on the thick anteriorly wedged fifth lumbar disc. This in turn leads to increased wear and tear on the lower lumbar discs, the adjacent vertebrae and their facet joints, and hence to interarticular laminar deficiencies which predispose the lumbosacral junction and lower lumbar vertebrae to degenerative spondylolisthesis. It should be remembered that a 'pain in the neck' can place a painful burden on the low back. Tenderness and pain at the cervicothoracic junction may often cause the heavy head (5-7 kg) to tilt forward, allowing the center of gravity to change. This adds to the vicious cycle of more pain, worsening posture, more mechanical dysfunction, more muscle spasm and more pain, not only in the neck, but in the low back as well. Degenerative spondylolisthesis is perhaps in the truest sense a 'slipped disc' (although this term is colloquially applied more generally to any disc protrusion or extrusion). The vertebrae slip over each other, and so the disc must also slip. Pronounced discal slippage may contribute to adjacent nerve root impingement, adding to the spinal canal narrowing already resulting from vertebral overlap, and thereby contributing, along with accompanying facet and degenerative spondylosis, to the potential development of spinal stenosis and to recurrent and chronic low back pain syndromes. Fortunately, the symptoms of spinal stenosis (neurogenic claudication) tend to stabilize and often do not progress. The most common type of spondylolisthesis is anterolisthesis, with the superior vertebra anteriorly overriding the one below, but retrolisthesis and lateral listhesis are also seen, usually as a consequence of intradiscal and secondary facet degenerative pathology in the upper (L2-L3) lumbar vertebrae. Spondylolisthesis is usually graded in four stages (Figure 3.4), depending on the degree of vertebral slippage. grade 1: 25% or less slippage. grade 2: 25-50% slippage or overlap. grade 3: 50-75% overlap. grade 4: the superior vertebra almost completely or completely overrides the lower one. Even though spondylolisthesis is often not accompanied by low back pain, characteristic findings on examination become increasingly apparent as the graded severity of the spondylolisthesis increases from 1 to 3. Developmental spondylolisthesis is typically a consequence of bilateral defects (spondylolysis) in the vertebral pars interarticularis, which lead to their separation and hence vertebral slippage.
Spondylolysis alone or accompanied by mild spondylolisthesis occurs in about 5% of the adult population, and is generally asymptomatic or associated with only mild low back pain. These lytic defects are attributed to a developmental failure of bone fusion during maturation, for which there can be a strong familial predisposition. They occur between 5 and 20 years of age, but are usually asymptomatic until later adult life. They may also be a consequence of stress fractures, as they are more common in female gymnasts, male weight-lifters and college football linemen. Spondylolisthesis of the lower lumbar vertebrae (most common at L5-S1 and less common at L4-L5) can also result from discal and secondary facet degeneration. Spondylolysis is also used to describe stages of age- and trauma-related disc deterioration and, like spondylolisthesis, is graded 1-4. In association with spondylolisthesis, hamstring tightness, another predisposing factor for low back pain, is commonly found. Leg muscle tightness may be associated with varying degrees of flexion of the hips and knees on standing, as well as shortening and flattening of the trunk. As the trunk shortens, the waistline widens and side bending is restricted, and when truncal shortening is more pronounced the abdomen protrudes. Even in milder cases, a telltale marker of spondylolisthesis in the form of a protruding hard lump in the midline at the base of the spine may be found. This is the spinous process of the fifth lumbar vertebra, which gains prominence due to forward slippage of the fourth lumbar vertebra caused by anterior spondylolisthesis of L4 over L5, the most common location for this abnormality. As spondylolisthesis predisposes to low back stress and strain and the resultant low back pain disorders, confirmation of its presence in symptomatic patients should prompt appropriate conservative treatment; only occasionally is surgical fusion appropriate. It should be remembered, however, that radiological evidence of spondylolisthesis may not be the explanation for the back pain, as major degrees of listhesis can occur without symptoms. Other bone pathologies. A number of of systemic or localized inflammatory pathologies affect vertebrae and may cause back pain. They include. infective causes, exempli gratia osteomyelitis and tuberculosis. neoplastic causes, exempli gratia primary and secondary tumors, myeloma. metabolic causes, exempli gratia osteoporosis and osteomalacia. idiopathic causes, exempli gratia Paget's disease and Scheuermann's disease (see 3. Facet (zygapophyseal) joints (Figure 3.5) are synovial-lined, cartilage-surfaced diarthrodial joints that are connected by a ligamentous joint capsule. Facet joint malalignments and associated degenerative osteoarthritic changes (Figure 3.6) are commonly noted radiographically with or without, and before and after, any accompanying low back pain disorders. This does not preclude an abnormal facet joint being a causal factor of a low back pain syndrome. This can be confirmed by a selective joint injection of hypertonic saline under fluoroscopic visualization to attempt to reproduce the pain, followed by a local anesthetic injection to provide specific relief of the induced pain. The facet joint pain referral pattern thus elicited may not be limited to the lumbar region, and may radiate into the buttock and/or down the leg, mimicking a sciatic nerve root pain (facet syndrome). Osteoarthritic vertebral and facet joint osteophytes, plus facet malalignments and discal protrusions (see Figure 3.2b), can all predispose to and contribute to a nerve root impingement and radicular symptoms. Occasionally, a synovial cyst may protrude from an arthritic facet joint and cause compression of an adjacent nerve root. Some, but not all lumbar facet joints may have intra-articular meniscoid synovial thickenings that are potentially capable of being impinged during stressful lumbar movements, and thereby cause pain.
It has been suggested that the release of a meniscal impingement could possibly explain the prompt relief that sometimes occurs with manual manipulation therapies. The audible cracking sound that can be elicited with the instantaneous abrupt facet joint displacement during a manipulation may be attributable to the snapping of an abrupt ligamentous displacement or to the creation of an intra-articular relative vacuum, similar to when a knuckle-cracking manipulation is performed. Sacroiliac joints. The sacroiliac joint (see Figure 3.1) was once thought to be the major factor in low back pain and sciatica, but discovery of the role of disc protrusions has dramatically changed that perception. Although there is no question about the role of the sacroiliac joint in the spondyloarthropathies - ankylosing spondylitis, reactive arthritis, psoriatic arthritis - and, much less commonly, in infection, tumor or trauma, its role in low back pain and/or sciatic pain referral is questionable. Radiographic evidence of degenerative or malalignment abnormalities of the sacroiliac joints are commonly found, often in the absence of back pain. Evidence supporting the pain referral patterns of lumbar discs and facet joints to the sacroiliac region is irrefutable, but this is not evidence of a causative role for the sacroiliac joint in a low back pain syndrome. Despite the variety of examination procedures designed to confirm the presence of sacroiliac pathology, most notably subluxations potentially amenable to manipulation or other therapy, there is no consistent reproducibility of the various methods when careful assessments have been made by blinded experts. Even in ankylosing spondylitis, invariably associated with sacroiliitis, the localization of sacroiliac pain remains unclear. This uncertainty about the role of the sacroiliac joint and the difficulty in establishing a firm diagnosis of sacroiliac pathology is reflected in the scant mention made of it in current texts on low back pain. Conservative management of lumbar and lumbosacral painful disorders can usually be successfully undertaken without specifically addressing the sacroiliac joint per se. Nevertheless, non-inflammatory symptomatic sacroiliac derangements do occur, and can be confirmed by fluoroscopically guided local injections. A local corticosteroid injection can provide a useful therapeutic option in the few patients in whom the diagnosis is confirmed. The role of manipulation therapy in resolving a specific sacroiliac derangement is questionable, but to discount it totally would cause great consternation to the many who have been convinced by their experience (placebo or otherwise) of its benefits. Sacralization/lumbaralization of the lumbosacral vertebrae, and/or various degrees of fusion of transverse processes, are not per se typically associated with low back pain, though they may contribute to restricted lumbar movement and secondary strain. The function of all bursae (see Figure 3.1) is to minimize the potential for friction as a consequence of a muscle rubbing normally on a bony prominence. The bursae can be overloaded, however, and become inflamed. Bursitis associated with low back pain and hip joint arthritis usually originates at the submaximus trochanteric bursa and, less often, at the obturator internis bursa. The submaximus bursa is of particular interest because it is not uncommonly associated with a sciatic-like, neuralgic pain referred to the lateral thigh. The precise basis for the development of trochanteric bursitis is not always apparent, though it appears to be related to reflex-induced tension of the muscles that pass over the bursa due to referred lumbar disc-related pain, or to torsional strain as a consequence of gait alterations in the case of hip and/or knee joint disorders, or leg length asymmetry. Bursitis is associated with focal tenderness on palpation with moderate pressure over the anatomic location of the bursa in question. Bursal tenderness of the subgluteus-medius bursa is less common. Ischiogluteal bursitis, which usually results from trauma to, or pressure on, and irritation of an ischial tuberosity, is otherwise unrelated to low back discogenic disorders.
Lumbar nerves transmit the stimuli for all sensory and motor functions in the lower extremities, though previous or concomitant central or peripheral nervous system disorders may also affect sensation, balance and coordination. The spinal cord terminates in the conus medullaris at the upper border of the second lumbar vertebra. Nerve roots in the lumbosacral spine consist of a posterior or dorsal root with afferent sensory fibers and the dorsal root ganglia, and an anterior or ventral root with efferent motor fibers. These anterior and posterior roots fuse, and the nerve roots thus formed in the dorsolumbar region descend almost vertically to form the cauda equina before exiting through their respective nerve root canals. The sensory fibers consist of. large, rapidly conducting type A fibers that transmit touch, pressure and proprioception. smaller, more slowly conducting type B fibers that transmit pain and temperature. small unmyelinated type C pain-transmitting fibers. The sensory neural components of the nerve roots enervate their respective peripheral nerves, sclerotomes and dermatomes (see Figure 1.13). Each dermatome is innervated by a specific nerve root, but there is considerable overlap between the dermatomes and considerable variation between individuals. Most radiculopathies, and particularly those associated with low back pain and sciatica, are the result of disc-related nerve root compressions (Figure 3.7). The extent of clinical manifestations of radiculopathy related to apparently similar nerve root compressions, exempli gratia the same disc levels and apparently similar herniations, is also variable. Nerve root compression can cause sensory impairment, less often both sensory and motor nerve root impairment, and rarely only motor nerve root dysfunction, exempli gratia a foot drop gait or demonstrable weakness on physical examination. This relative frequency of sensory to motor abnormalities corresponds to the anatomic location of the sensory fibers in the outer portion of the nerve root. The extent and severity of neural dysfunction is related to the degree of neural compression and its duration. Cells within the dorsal root ganglia process painful stimuli with the resultant release of substance P and other neuropeptides within the dorsal column of the spinal cord. The sympathetic afferent fibers in the lumbar region enter the four to six sensory sympathetic ganglia along each side of the anterior vertebral column and subsequently enter the spinal cord via the ventral nerve root. Afferent sympathetic fibers interface with cells in the dorsal root ganglia, while efferent fibers emanate from the sympathetic ganglia and are then distributed in conjunction with the sensory and motor nerve fibers to the various sympathetic effectors, exempli gratia blood vessels and sweat glands. Axons of the post-ganglionic sympathetic fibers are the only autonomic adrenergic fibers that interact with alpha-receptors (skin and muscle vasoconstriction with cold, stress or Raynaud's phenomenon) and beta-receptors (sweating to cool and vasodilation to enhance vascular flow to maintain muscular function and body temperature during exercise). Compression of nerve roots or ganglia per se does not cause pain, but does impair both sensory and motor functioning. Compression in association with inflammation and congestion of the nerve roots and/or ganglia, as well as adjacent ligamentous and articular tissues, however, does cause release of substance P and other pain-stimulating cytokines. Sensory dysfunctions resulting from disc-nerve-root disorders usually manifest in a fairly specific dermatomal pattern. Sclerotomal, or deep pain referral patterns are more diffuse and do not follow the classic dermatomes. Concomitant specific nerve impairment from previous or associated disorders can make the clinical assessment of nerve root impairment on a lumbar discogenic basis more difficult and may necessitate additional electrodiagnostic and other testing. The L1-L2 disc is seldom affected in low back disc disorders, so compression of the conus medullaris is a 'red flag' (see Table 1.1), not only because of the severe nature of the neurological complications but also because of the likelihood of malignancy as the cause.
Spinal stenosis. Morphological spinal stenosis (Figure 3.8) and/or root foraminal stenosis, with consequent disturbance of neural function, can be congenital, associated with spondylolisthesis (see 3. iii), secondary to discal protrusions, ligamentous thickening and ossifications, or any combination of these factors. Not uncommonly, spinal stenosis identified radiographically during differential diagnosis of a low back pain syndrome is considered to be the cause of the pain. Although this is a possibility, the stenosis is more likely to be simply an associated phenomenon, comparable to noting the presence of an osteophyte in the same or adjacent area. Clinically significant spinal stenosis is characterized by neurogenic (pseudo) claudication, but the symptoms of neurogenic claudication are not invariably associated with low back pain. Similar but unilateral symptoms may be caused by nerve root canal stenosis. The classic clinical symptoms of neurogenic claudication are heaviness and aching, with or without dysesthesias or numbness, in the thighs and lower legs brought on by standing erect or walking for varying periods of time. They are promptly relieved (within 2-5 minutes) by sitting or lying down, and sometimes by just leaning forward. Vascular claudication, in contrast, is typically relieved by simply standing still, as well as by sitting or lying down, to restore adequate circulation. The upright posture decreases volume in the spinal canal. The symptoms of neurogenic claudication that result from spinal stenosis are caused by neural compression in the cauda equina or specific nerve root canals when the spinal canal or specific root canals are constricted in the upright posture. Neurogenic claudication depends on time and posture, but not on activity or energy utilization. Although patients with spinal stenosis may complain of back discomfort and/or stiffness or pain with changing position or with activity, these are manifestations of associated lumbar or lumbosacral pathologies including facet arthritis, and are not due to the spinal stenosis per se. Examination of lower extremity pulses, particularly the dorsalis pedis and posterior tibial pulses, can help diagnose vascular claudication, but both spinal stenosis and arteriosclerosis can occur in the same patient. Spinal stenosis is most often a geriatric problem, and it can be symptomatically compounded by or confused with a number of conditions, including. peripheral arteriosclerosis. current or previous sciatica. osteoarthritis of the hips or knees. peripheral neuropathies. Paget's disease. spinal cord tumors. A clear differential diagnosis of clinical lumbar spinal stenosis is therefore dependent on more than radiographic confirmation. Clinical features relating to pain and functional impairment, and careful neurological assessment, should be taken into consideration before proceeding with aggressive surgical interventions, unless the development of bowel, bladder, sexual or motor dysfunction dictates a need for urgent surgical intervention (see Traction, 4. Vascular components. For practical purposes, the circulation to the spine is not a concern unless a surgical procedure is undertaken, or possibly if a clot in an artery causes avascular necrosis. The segmental arterial supply to the lumbar vertebrae and adjacent muscles consists of paired arteries that branch off the posterior surface of the aorta. The dorsum of each vertebra is supplied by the four arteries from two adjacent vertebrae that have entered through the intervertebral foramina. The venous system consists largely of a complex of valveless, cross-connected, epidural channels that drain into the intercostal and lumbar veins. In addition to venous drainage, these epidural venous channels can provide a shock-absorbing cushion to protect the spinal cord and cauda equina. 
Congenital and developmental abnormalities. Although the spine is very cleverly designed and very adaptable, it contains many structural subtleties that predispose it to the consequences of wear and tear and trauma. In addition, it is not always congenitally fabricated properly, nor does it always develop correctly during growth. Ligaments, tendons and fascias. Our genes dictate our individual human identities, including our musculoskeletal and neural structures, vascular tissues, immune systems and responsiveness to inflammation and trauma. The ligaments and tendons of the musculoskeletal system are highly varied structures, each individually designed to perform a specific task. The universal assignment for the ligaments is to hold the various skeletal structures together. Some ligaments, such as the anterior longitudinal ligaments, are thick, firm and unyielding. Others, such as the ligamenta flava, contain elastic fibers designed to permit a wide range of motion and suppleness. All ligaments, tendons and fascias, however, are designed to impose specific constraints while facilitating purposeful movements of a specific anatomic region. Normal ligaments, tendons and fascias respond to physical stresses without undue strain, though there is a considerable range of flexibility within what is considered normal in these tissues. For example, both increased ligamentous laxity (simple double-jointedness or benign joint hypermobility syndrome) and more profound collagen defective disorders (exempli gratia Ehlers-Danlos syndrome or Marfan's syndrome) increase vulnerability to joint and spinal wear and tear, subluxations and degenerative changes, presumably as a consequence of the less restrictive ligaments permitting joints to be more readily traumatized during activities of daily living. Often this is compounded by the vigorous stretching maneuvers, exempli gratia yoga, ballet or gymnastics, encouraged by the joint laxity. Such ligamentous laxity is associated with a tendency to scoliosis that may be complicated by vertebral abnormalities, and to spondylolisthesis with consequent facet arthropathies and/or spinal stenosis. Scheuermann's disease (osteochondromatosis or juvenile kyphosis) (Figure 3.9) occurs spontaneously during adolescence at the time of rapid growth and development. It is more common in females than in males. Pathological alteration and necrosis of the cartilage and bone of the vertebral endplate result in endplate irregularities during the healing phase. Schmorl's nodes, disc space narrowing, anterior wedging and occasional intervertebral fusions are readily demonstrated by radiography. Scheuermann's disease may be asymptomatic or associated with varying degrees of discomfort and kyphotic deformity. The apex of the kyphotic curve is typically at T7-T9, and it is therefore sometimes confused with, or can occur in association with, kyphosis resulting from osteoporotic fractures. Spina bifida is seldom associated with low back pain, but it has been associated with compression of the unprotected nerve roots in the cauda equina, which can occur during vigorous lumbar extension in the absence of the bony laminae that normally provide a protective cover. Spinal rigidity. Any structural alteration or congenital abnormality that restricts intervertebral movement inevitably imposes a greater mechanical stress on higher and lower discal and vertebral segments that have retained movement capability.
This predisposes the movable disc's annular and adjacent ligaments, and the endplates and facet joints, to greater wear and tear, more rapid degeneration and secondary arthrosis. Rigidity of vertebral segments can result from congenital, post-traumatic or surgical fusions (Figure 3.10), or from degenerative disc shrinkage and secondary osteophyte formation. Ankylosing spondylitis and diffuse idiopathic skeletal hyperostosis (DISH) can restrict spinal movement segmentally as well as diffusely, depending on the extent of the longitudinal and periannular ligamentous ossification and fusion and any associated muscle spasm. Where movement between vertebrae does occur, additional wear and tear strains are imposed on the remaining mobile osseous vertebral and ligamentous segments. Examination will reveal restricted movement and a flattened lumbar curve. Traumatic factors. Lumbar fractures are usually related to trauma and are quite straightforward to diagnose. Atraumatic vertebral compression fractures due to severe osteoporosis are not uncommon, however, and may be associated with minimal or no pain. Such non-malignant osteoporotic fractures seldom occur in the cervical spine and are also less often observed in the lower lumbar vertebrae than in the thoracic and upper lumbar spine. See Fast Facts: Osteoporosis. Coccygodynia, pain in the coccygeal region, can be caused by fracture or sprain of the coccyx (see Figure 3.1), resulting from trauma, usually a hard fall in a half-seated position with a significant impact on the sacrococcygeal tissues rather than just the ischial tuberosities. Coccygodynia can also result from childbirth trauma or perianal dysfunction, and can be referred pain from lower midline disc protrusions. The onset of pain can be acute, with tenderness primarily localized to the coccyx and lower sacrum. Pain is typically brought on by sitting or changing position. Tenderness on palpation of the coccygeal area may also be due to inflammation of the small overlying subcutaneous coccygeal bursa. Sacrococcygeal pain can often be very simply relieved with a single, well-placed injection of corticosteroid with an anesthetic. Non-traumatic coccygodynia, particularly when elicited not by palpation but by motion of the coccyx, can be a manifestation of inflammation of the sacral plexus or of diseases or tumors of the pelvic tissues. Other pathologies. Pain may also be referred to the spine from soft tissue injuries or pathology in other organ systems, such as. vascular injuries, exempli gratia aortic aneurysm or aortic dissection. renal disorders, exempli gratia carcinoma or calculus. gynecologic disorders, exempli gratia uterine carcinoma, pelvic inflammatory disease or endometriosis. pancreatic disorders, exempli gratia tumor or pancreatitis.
Long-term complications. Long-term complications develop in virtually all patients taking continued levodopa therapy. Although levodopa is the most effective drug for the treatment of Parkinson's disease, the initial benefit begins to diminish over time with dyskinesias and fluctuations in motor response (Figure 7.1). It is likely that two factors determine the development of fluctuations and dyskinesias. disease severity. chronic pulsatile stimulation of postsynaptic dopamine receptors by the use of dopaminergic drugs with a short half-life. Table 7.1 shows the clinically most important indicators, both motor and non-motor, for defining advanced Parkinson's disease, and the functional impact those symptoms are likely to have on the patient. Motor fluctuations. Initial treatment with levodopa will reverse parkinsonian symptoms consistently during the day without fluctuations. This occurs despite the short plasma half-life of levodopa (2-3 hours), and presumably reflects the ability of nigrostriatal dopaminergic neurons to store dopamine before its release into the synapse. Coadministration of a decarboxylase inhibitor, such as carbidopa or benserazide, prevents metabolism of levodopa to dopamine in the bloodstream. These drugs do not cross the blood-brain barrier, and so dopa is decarboxylated to dopamine within the neurons without rate limitation. Established/unstable Parkinson's disease. Within 2-3 years, most patients begin to experience fluctuations (Table 7.2). In advanced disease, the improvement in parkinsonism correlates with the level of levodopa in the bloodstream (Table 7.3). One factor underlying the development of fluctuations is the progressive fallout of dopamine neurons within the striatum, which leads to a reduced capacity for dopamine storage: levodopa arriving in the neuron is metabolized immediately to dopamine and released into the synapse. Postsynaptic factors may be at work, as shown by a rapidly waning response to apomorphine in fluctuating patients. Another factor is pharmacokinetic: plasma levels of levodopa become increasingly erratic due to variable absorption from the stomach and duodenum (involving some dietary interaction), together with a greater capacity for levodopa metabolism in the periphery. If low doses of combination therapy are used, decarboxylase may not be blocked effectively: carbidopa, 75 mg daily, is required. Of more importance is the peripheral O -methylation of dopa by catechol- O -methyl transferase (COMT) to 3- O -methyl-dopa, an inactive metabolite that does not cross the blood-brain barrier (see Figure 4.2).
Advanced disease. Fluctuations become increasingly unpredictable over a period of years and are difficult to relate to dopa dosage. 'Dose failures' may be related to poor absorption of levodopa or due to the recently described phenomenon of 'internalization' of dopamine receptors, so that drugs become ineffective for a period. Patients are significantly disabled and find it hard to plan their lives, not knowing whether they will be mobile ('on') or frozen ('off'). 'On/off' syndrome becomes the focus of their lives and is a challenging problem for clinicians (Table 7.4). Its appearance is related to the severity of disease and the underlying death of dopaminergic neurons. Dyskinesias are hyperkinetic states mostly seen in patients with levodopa-treated Parkinson's disease. In the early stage of the disease, these states are often not noticed by patients and are a good sign of dopa responsiveness. A large body of research in animal models suggests that dyskinesias arise from alteration of dopaminergic tone in the denervated striatum, together with delivery of treatment by non-physiological pulsatile stimulation of dopamine receptors. This leads to cellular adaptations such as activation of transcription factors and alteration of downstream gene expression such as that of the immediate early genes. Dyskinesias may be related to 'off' or 'on' periods, but as the disease advances they become a continuum and difficult to classify. 'Peak-dose' dyskinesias are the most common. They involve choreic or ballistic movements, and may be associated with a variety of non-motor symptoms such as pain, mood alterations and cognitive changes. Dyskinesias may be socially unacceptable to carers, while patients may prefer to be dyskinetic when 'on'. Most dyskinesias progress and may lead to a reduced quality of life and weight loss. Reducing dopaminergic drugs will diminish dyskinesia, but may lead to a return of Parkinson's disease symptoms. Smaller doses of dopa given more frequently, or a combination of the drug with a dopamine agonist, may help, but problems are likely to reappear (see Table 7.4). The findings of a large number of preclinical and clinical studies indicate that continuous dopaminergic stimulation (CDS) may be the most desirable way to combat dyskinesias.
Dementia is an important cause of comorbidity in parkinsonism; when cognition declines, mortality increases. Dementia is a bad sign and will often lead to the breakdown of support networks. Patients who are confused and hallucinating become difficult to manage at home and are frequently admitted to nursing homes. At this stage, life expectancy may be only 1-2 years. Estimation of the incidence of dementia has proved difficult. Most parkinsonian patients have minor cognitive problems that are not apparent to spouses or relatives; 90% have deficits in frontal lobe tests, such as shifting sets (Wisconsin card sorting test). This reflects a rigidity of thought that may be apparent premorbidly for several years. Characteristically, patients have difficulty 'changing their mind' and slowness of thought, with impairment of central reaction times. This can lead to a false estimation of cognitive function unless time is spent carefully assessing patients. Global intellectual decline (dementia) is usually a feature of parkinsonism in the elderly, with more than 30% of patients over 70 years affected. Memory disturbance may be a warning sign, although younger patients may complain of short-term memory problems for many years. This may be due in part to drug therapy, particularly with anticholinergics, or to depression. Hallucinosis is particularly worrying and often presages dementia (Table 7.5). Excessive dopaminergic therapy or anticholinergic drugs can cause hallucinosis in the early stages of disease, but later hallucinosis may occur without drugs and is difficult to control. Often the next stage involves confusional episodes characterized by disorientation. Dementia associated with Parkinson's disease has been termed subcortical, indicating predominant memory disorders, bradyphrenia and disorientation. This is in contrast to Alzheimer's disease, in which language difficulties and other cortical problems such as dyspraxia are predominant (Table 7.6). The distinction, however, is largely theoretical and is not substantiated by postmortem studies. The cause of parkinsonian dementia is unclear. Autopsy studies have indicated a substantial contribution from Alzheimer-type pathology. Lewy bodies in the substantia nigra lead to a parkinsonian motor syndrome in life, and it is thought that they spread into the cortex and cause hallucinosis (occipital and temporal cortex) and other cognitive problems. Diffuse Lewy body disease (now termed 'dementia with Lewy bodies', DLB) can present with parkinsonian characteristics, but cognitive decline and hallucinosis will be early features: 60% of Alzheimer's disease patients have extrapyramidal/parkinsonian signs in the later stages. Management of dementia is difficult and requires the involvement of a multidisciplinary team (see Fast Facts: Dementia). The drug regimen used to treat Parkinson's disease should be kept as simple as possible, with the aim of achieving maximum mobility with minimum psychological disturbance (particularly hallucinosis). Anticholinergics, amantadine and even dopamine agonists may have to be stopped and the patient treated with the lowest possible dose of levodopa. Atypical antipsychotics such as quetiapine and clozapine may be employed in low dosages. Carers often prefer patients who are less mobile and less confused, because they are easier to care for. Wandering can be a problem; if confused, the patient will need constant supervision. Support for the carer is crucial in this situation and, if it cannot be provided, the patient will need residential care. 
Depression and anxiety. Depression and anxiety are common accompaniments of Parkinson's disease (>= 50%, 2.7-70% depression, 50-66% anxiety), which may occur with the shock of the diagnosis or later on as a result of increasing disability. There is also evidence that depression is endogenous, intrinsic to the condition and part of the neurochemical profile. Depression/anxiety is a marker for dementia in older patients. However, depression must not be mistaken for dementia. Pseudo-dementia can be mistaken for Parkinson's disease dementia. It is important to seek out depressive features and treat with antidepressants (tricyclic drugs or serotonin-reuptake inhibitors) if there is any suspicion of depression. The debate as to the advantages of the various antidepressants is ongoing (see Fast Facts: Depression). A syndrome of apathy distinct from depression, anxiety and fatigue is increasingly recognized in Parkinson's disease. Separate scales have been devised to assess apathy; it is important to recognize this syndrome, which may masquerade as depression. Psychosis causes gross impairment in reality testing. Psychotic patients evaluate their perceptions abnormally, leading to incorrect interpretation of external reality. Psychosis consists of delusions (false beliefs about external reality) and hallucinations (sensory perceptions in the absence of external stimuli), and has a variety of causes (Table 7.7). Drug-induced psychosis appears to result from altered function within dopamine projection neurons originating in the ventral tegmental area. Overstimulation of mesencephalic-limbic dopamine receptors causes limbic system dysfunction, leading to psychosis. The management of drug-induced psychosis is outlined in Figure 7.2. Typical antipsychotic drugs (exempli gratia haloperidol, chlorpromazine) antagonize dopamine D receptors and may cause extrapyramidal side effects such as tardive dyskinesias and parkinsonism. Atypical antipsychotics (exempli gratia quetiapine, clozapine) appear to control psychosis without significantly compromising motor function and so are more suitable for treating psychosis in Parkinson's disease (Table 7.8). These agents bind more selectively to serotonin and mesolimbic dopamine receptors than they do to striatal dopamine receptors. However, olanzapine should be avoided as it may significantly worsen Parkinson's disease even in small doses. Ondansetron, an antiemetic with action at the serotonin receptors, is also useful in some cases. Sleep disorders. Sleep problems in Parkinson's disease are common and may affect 60-98% of individuals at both early and late stages of the disease. Problems range from disease-related difficulties, such as rapid eye movement (REM) behavior disorder, sleep-maintenance insomnia, excessive daytime sleepiness and nocturia, to possibly drug-related problems, such as early-morning dystonia and night-time akinesia.
The cause of restless legs syndrome in Parkinson's disease remains unclear, although it occurs around twice as commonly as in the general population. Sleep problems are a key determinant of quality of life in Parkinson's disease, and sleep scales specific to the disease such as the Parkinson's Disease Sleep Scale (PDSS) or the SCale for Outcomes in PArkinson's disease (SCOPA) are important for regular clinical assessments. REM behavior disorder in particular has emerged as an important symptom that may predict the motor diagnosis of Parkinson's disease by years. The condition may cause self or partner injury, as the patient tends to act out violent dreams in REM sleep. The pathophysiology may involve brainstem nuclei, such as the pedunculopontine nucleus and locus ceruleus, which are involved in stage 2 disease (by Braak staging; see pages 20 -; Figure 2.3), with a motor diagnosis being made in stage 3. Autonomic problems. Autonomic problems occur with increasing frequency in advanced disease, as has been shown in the Non-Motor Symptoms Questionnaire (NMSQuest) study. The problems include gastrointestinal symptoms (constipation, dribbling, dysphagia), orthostatic hypotension, excessive sweating or hyperhidrosis, bladder dysfunction such as detrusor muscle hyperactivity, and sexual dysfunction. Sensory problems. Sensory problems associated with the disease mainly comprise pain syndromes and akathisia, usually in the 'off' state. Pain is a key unmet need in Parkinson's disease and can be of several types. The King's Parkinson's pain scale (KPP) has recently been validated as the first pain scale for Parkinson's disease; it allows classification of different types of pain in Parkinson's disease and can be used to devise targeted treatment strategies. Declaration of non-motor symptoms. Early recognition of non-motor symptoms is important, as studies show that the burden of non-motor symptoms as a whole is the key determinant to quality of life. An international study showed that large numbers of patients do not declare these symptoms unless specifically asked. This can lead to a shocking neglect of symptoms that may be treatable, even in well-established centers of Parkinson's disease care. The NMSQuest (see Figure 3.5) empowers patients to declare their non-motor symptoms to health professionals (see Figure 3.6).
Neuropsychology and functional imaging. The original name for schizophrenia, dementia praecox, reflects the appreciation from the outset of the primary role of neurocognitive impairments in schizophrenia, often referred to as CIAS (cognitive impairments associated with schizophrenia). However, fundamental questions remain regarding the relationship between neurocognitive impairments and other aspects of schizophrenia. Neurocognitive impairments. Neuropsychological assessment. People with schizophrenia present with a broad range of neurocognitive impairments. Many of these impairments may be detected through the use of traditional neuropsychological assessments and include abnormalities in processing speed, reasoning and problem solving, verbal and visual learning and memory, working memory and social cognition (Table 7.1). These domains of impairment may reflect abnormalities in more basic cognitive functions, which can be detected through the use of sophisticated neurocognitive paradigms. For example, studies of visual information processing have demonstrated subtle impairments in early sensory information processing, which may contribute to impaired performance on more complex neuropsychological tasks. Cognitive function in schizophrenia is typically one to two standard deviations below normal mean values. Computerized assessment. In addition to impairments on neuropsychological measures of cognitive function, people with schizophrenia also exhibit impairments in cognitive functions assessed through the use of sophisticated computerized procedures. These techniques are able to detect basic information processing (referred to above), eye-tracking and sensory gating abnormalities. Eye-tracking abnormalities. There are two major types of eye-tracking abnormality: one involves the smooth pursuit eye movement system and is reflected in an inability to track objects as they move through space; the other involves the saccadic eye movement system, which is used to correct the gaze of subjects when they have moved off target. People with schizophrenia are less able to inhibit inappropriate saccadic eye movement intrusions into their smooth pursuit eye movements, and are less able to produce saccadic eye movements in the direction opposite to a visual stimulus. Sensory gating abnormalities that have been extensively examined in people with schizophrenia are P50 and prepulse inhibition (PPI). Normal individuals are able to gradually ignore repetitive stimuli, whereas people with schizophrenia experience each stimulus as a novel stimulus and are unable to suppress their response to the repetitive sensory stimuli. This abnormality is thought to be related to the heightened distractibility seen in schizophrenia, and is detected by the P50 evoked-potential paradigm. PPI refers to the normal diminishment of a startle response to a stimulus, when a less intense form of the stimulus (the prepulse stimulus) is presented prior to the full stimulus. People with schizophrenia fail to respond to the prepulse stimulus with decreased startle response. P50 and PPI abnormalities may be reversed by some antipsychotic medications, nicotine and other pharmacological agents.0. Familial studies suggest that several of these neurocognitive impairments may not only be characteristic of schizophrenia, but may also be present in non-affected family members. Abnormalities of attention, verbal memory, eye-tracking and P50 have been shown to be more prevalent in family members than in the general population, leading to the supposition that these impairments may represent alternative phenotypic markers of the illness (see Chapter 4). Do cognitive deficits predate the illness? The results of high-risk and large-scale birth cohort studies and studies of military inductees who have developed schizophrenia provide compelling evidence that people with schizophrenia exhibit subtle neurocognitive impairments prior to the onset of more florid psychotic symptoms (Table 7.2). Studies of IQ test scores obtained routinely during childhood show that those children who later develop schizophrenia, but not bipolar disorder, have significantly lower mean scores than either age- or social-class-matched children or siblings who do not develop schizophrenia.
There is a dose-response relationship, such that for every point decrease in IQ, the risk of later schizophrenia increases by 4%. At the time of onset of positive psychotic symptoms, there is usually a further decline in IQ score. What is the pathophysiology? Neurocognitive impairments may reflect the involvement of specific brain regions or neural circuits, or a more global involvement of the brain. There is evidence for both hypotheses. The pattern of predominant impairments suggests the relatively selective involvement of frontotemporal areas. The results of functional imaging studies are also consistent with the selective involvement of frontotemporal areas, although this may be due in part to the nature of the cognitive tasks studied. These studies also implicate the hippocampus in the pathophysiology of these impairments. On the other hand, the pattern of predominant impairments may be more apparent than real. Several studies suggest that the cognitive impairments in patients are highly correlated with each other and that control differences between individuals with and without schizophrenia are largely due to differences in general intelligence. Whether there are a few basic cognitive abnormalities that underlie the broad range of observed cognitive manifestations or whether multiple cognitive processes are independently affected in schizophrenia remains an unresolved issue. Meta-analysis of the literature shows the main deficit areas to be executive function, attention and episodic memory. The first two deficit areas may both be manifestations of an underlying deficit in working memory, id est in the ability to 'hold information on-line'. How do deficits relate to symptoms and outcome? Patients with negative symptoms are more likely to exhibit clinically significant neurocognitive impairments than patients without these symptoms. Patients with negative symptoms have also exhibited the most impairment on tasks requiring self-generated, rather than stimulus-driven, activity. In contrast, disorganized patients are characterized by impaired performance on measures of distractibility, and they are also more likely to show an inability to inhibit inappropriate behavioral responses. Impairments of verbal memory, language, vigilance and executive function have been shown to be major determinants of poor social and community function in people with schizophrenia. Impairments of memory and executive and processing speed have been related to poor occupational outcome. The relationship of neurocognitive impairments to the broad range of symptom and functional outcomes suggests that the development of effective treatments for these impairments would have far-reaching beneficial effects on people with schizophrenia. However, despite early claims for second-generation drugs, no antipsychotic drug has been shown to improve cognition to any significant degree. Cognitive test batteries measuring response to drug or psychological interventions include the MATRICS Consensus Cognitive Battery (MCCB), developed by a panel of experts and viewed as the gold standard by regulatory agencies. However, it has two main drawbacks: administrators of the MCCB require training, and the battery is heavily language-based. Computerized test batteries are easier to deliver. CANTAB is a touchscreen-based battery which has been shown to have validity with animal models. Aberrant salience. Failure to discriminate between salient and non-salient stimuli contributes to psychotic symptoms such as delusions, where irrelevant stimuli appear relevant, for example in ideas of reference. Brain dopamine levels are key to the reward system that controls this discrimination of salient stimuli. Too much dopamine may lead to increased 'background noise' and altered reward processing whereby relevant signals are less distinguishable from irrelevant ones. This can be seen in the lower than expected activation of the ventral striatum on functional MRI. Drug treatment normalizes this deficit.
Functional imaging. Whatever else underlies the symptoms and deficits of schizophrenia, there is plainly a disturbance of brain function. Schizophrenia research has, therefore, been quick to use imaging techniques that demonstrate disturbances in brain function. However, it was not until the late 1970s that neuroscientists came to believe that, just as in other tissues (exempli gratia muscle), the blood flow and metabolism of brain tissue increased when a region was specifically active. The use of imaging techniques that are able to show regional cerebral blood flow or metabolism confirmed that the blood flow to regions of primary motor or sensory cortex increases by 5-10% a few seconds after movement or perception begins. Similar changes of lesser magnitude also occur in brain regions executing a complex cognitive task, such as solving a puzzle. These changes can be used to clarify which areas of the brain are being used to do specific tasks. Molecular imaging. Positron emission tomography (PET) and single-photon emission computed tomography (SPECT) were the first true functional brain-scanning techniques. They both rely on intravenous injection or inhalation of radioactive tracers, which are taken up into the brain in proportion to local blood flow and produce a map of glucose metabolism or regional cerebral blood flow. PET uses radioactive isotopes with short half-lives, which limits the amount of radiation received and enables repeated measurements in one session. PET isotopes decay to release two gamma rays, or photons, whereas SPECT isotopes emit just one photon at a time. In each case, the photons are recorded by detectors arrayed around the head. Functional imaging. The safest and most widely used technique is functional magnetic resonance (fMR) blood-oxygen-level-dependent (BOLD) imaging. This produces a map of regional cerebral blood flow and brain activity from the natural change in paramagnetic properties of hemoglobin as it deoxygenates. Image analysis involves two general approaches. In the top-down 'region of interest' approach, functional images have to be mapped precisely onto structural scans ('coregistration'). Bottom-up 'voxel-based morphometry' is more widely used. Using this approach, images from groups of subjects, or from the same subjects during and after a cognitive challenge, are transformed into identical brain shapes. The differences between the groups are then expressed as maps of significant p-value scores (exempli gratia those of p < 0.01) in a process known as statistical parametric mapping. Chemical imaging. Magnetic resonance spectroscopy (MRS) allows the assessment of brain biochemistry. Phosphorus MRS shows a series of peaks, which correspond to energy reactions in brain cells, involving adenosine triphosphate, as well as turnover of membranes in synapses and vesicles (Figure 7.1). In untreated schizophrenia, MRS of the prefrontal cortex has repeatedly shown a decrease in phosphomonoesters and an increase in phosphodiesters. This suggests a decreased synthesis and/or increased breakdown of cell membrane phospholipids, which could reflect disturbed pruning of synapses. Proton MRS visualizes a separate set of biochemical processes and suggests reduced frontal glutamate activity in early schizophrenia. Experimental design.
Imaging brain function is more difficult than imaging brain structure. Patients and controls need to be engaged in similar standardized cognitive activity to reduce chance variations and allow real differences to be seen. Unless they are properly controlled for, drug effects can mask all other effects. A common method of investigating the abnormal brain function underpinning a cognitive deficit is to first scan a control subject before and during the execution of a relevant task. By subtracting the brain activity map as it was before the task from the map during the task, the specific areas of the brain involved in the task will be highlighted. The same brain activity maps are then produced for people with schizophrenia before and during the same task, which can then be compared with the pattern in control subjects. A general finding is that people with schizophrenia tend to use aberrant networks of cortical activity, even during simple cognitive, perceptual or motor tasks and even when performance is apparently normal. Resting state fMR studies are increasingly being used, with advanced image analysis that detects correlations in activity between brain areas. Resting state fMR studies of brain connectivity have focused on the striatum, and have found reduced frontostriatal connectivity in the circuits linking dorsal, or associative, striatum with anterior thalamus and polymodal association areas such as the dorsolateral prefrontal cortex. This abnormal pattern has also been found in unaffected relatives. Mapping symptoms. Functional imaging has been informative at the level of understanding individual symptoms, symptom clusters and specific cognitive deficits in schizophrenia (Figure 7.2). The three overlapping syndromes outlined in Chapter 2 correlate with particular cognitive deficits and with distinct patterns of brain function (Tables 7.3 and 7.4). Negative symptoms, or the psychomotor poverty syndrome, correlate with decreased activity in the dorsolateral prefrontal cortex, particularly on the left. Interestingly, this pattern also appears in severe depressive illness with psychomotor retardation, thus indicating a final common cortical deficit underlying psychomotor poverty in schizophrenia and psychomotor retardation in depression. Positive psychotic symptoms correlate with hippocampal activation and with a distributed network of frontal, temporal and subcortical sites. Among positive symptoms, auditory hallucinations, rather than delusions, have been most extensively studied. This is partly because the normal cerebral mechanisms of auditory perception are well understood, whereas the normal mechanisms underlying the formation of beliefs are not. During auditory hallucinations, increased blood flow is seen in Broca's area in the left hemisphere. This is similar to the activity produced during normal speech and during normal 'inner' speech, when one is rehearsing something silently to oneself. In a healthy brain, a connected area of cortex in the superior temporal gyrus is deactivated during inner speech. Functional imaging shows that this does not occur in hallucinating patients, suggesting that hallucinations may actually be misinterpreted inner speech. In some cases, the link between a specific symptom and a functional abnormality emerges only under particular conditions. Compared with controls, patients with persecutory delusions show increased activation in the amygdala in situations of possible threat. Patients with auditory hallucinations show right-sided temporoparietal and parahippocampal deficits during inner speech tasks, which again suggest an internal monitoring problem. The disorganization syndrome correlates with increased activity in the dorsal anterior cingulate gyrus, a region involved in selective attention.
Clinical presentation. Patients with acute myeloid leukemia (AML) usually present with vague symptoms that are consequences of pancytopenia. Typically, the onset of symptoms is no more than 3 months before diagnosis. Fatigue is a common first symptom (Table 2.1), often accompanied by anorexia and weight loss. Fever or infection is the initial symptom in approximately 10% of patients, and 5% have signs of abnormal hemostasis (beyond minor bleeding and easy bruising). Bone pain, lymphadenopathy, non-specific cough, headache and diaphoresis (excessive sweating) may also occur. Bone pain, typically vaguely localized to the pelvis or back, is also a frequent symptom. Rarely, patients present with symptoms due to extramedullary leukemia (id est outside the blood and marrow), such as myeloid sarcoma (a tumor mass consisting of myeloid blasts); AML may infiltrate skin, lymph nodes, the gastrointestinal tract, soft tissue or testes. Patients who present with isolated myeloid sarcoma typically develop blood and/or marrow involvement quickly thereafter. Central nervous system (CNS) involvement is uncommon in AML (approximately 5% of cases, in contrast to acute lymphoblastic leukemia where CNS involvement is common). Physical findings. Fever, infection and hemorrhage are often present at the time of diagnosis. Splenomegaly, hepatomegaly and lymphadenopathy may also be present but are relatively uncommon. Hemorrhagic complications are most commonly and classically found in acute promyelocytic leukemia (APL). Affected patients often present with disseminated intravascular coagulation (DIC)-associated hemorrhage and may have intracranial hemorrhage or bleeding at other sites. Thrombosis is a less frequent but well-recognized feature of APL. Bleeding associated with coagulopathy may also occur in monocytic AML, or with extreme degrees of leukocytosis or thrombocytopenia in other morphologic subtypes. Retinal hemorrhage due to thrombocytopenia and extreme leukocytosis is detected in approximately 15% of patients. Infiltration of the gingiva, skin, soft tissues or meninges (CNS) with leukemic blasts at diagnosis is infrequent but characteristic of the monocytic subtypes (see Tables 2.2 and 2.3) and those with 11q23 chromosomal abnormalities.
Hematologic findings. Red blood cells. Anemia is usually present at diagnosis but is not severe and is usually normocytic and normochromic (the size of red blood cells and level of hemoglobin are within normal limits). Decreased erythropoiesis often results in a reduced reticulocyte (immature red blood cell) count, and red blood cell survival may be shortened through accelerated destruction. Hemorrhage may rarely contribute to the anemia. White blood cells (WBC). The median presenting WBC count is about 15 000/L (compared with a normal range of 4500-10 000/L) but lower counts are commonly seen in older patients and in those with an antecedent hematologic disorder such as myelodysplastic syndromes or myeloproliferative neoplasms (see pages 18-19). The predominant WBC is the myeloid blast. Occasionally, patients may present with no detectable leukemic cells in the blood. Extreme leukocytosis (> 100 000/L, 'hyperleukocytosis') occurs in about 20% of cases. On peripheral blood smears, the cytoplasm of the myeloid blast often contains primary (non-specific) granules, and the nucleus shows fine, lacy chromatin, with one or more nucleoli characteristic of immature cells. The presence of abnormal rod-shaped granules called Auer rods (Figure 2.1) on light microscopy of a blood smear or bone aspirate indicates a diagnosis of AML. Additionally, the morphology in some AML subsets is distinctly different and strongly suggests diagnosis of a particular subtype; however, this will need to be confirmed by a combination of additional diagnostic tests, such as cytochemistry, immunophenotyping, karyotype and molecular methods. For example, the myeloblasts in APL are classically hypergranular with clusters of Auer rods (called faggot cells) with a bi-lobed or reniform nucleus. While these features strongly suggest a diagnosis of APL, the final diagnosis will require demonstration of the appropriate molecular abnormality by either karyotyping or other molecular methods. Most patients present with at least mild thrombocytopenia (below the normal lower limit of 150 000/L); about 25% have severe thrombocytopenia, with platelet counts below 25 000/L.
Differential diagnosis. Many bone marrow disorders present with pancytopenia and its attendant signs and symptoms. Highest in the differential diagnosis of pancytopenia due to acute leukemia (myeloid or lymphoblastic) are other hematologic cancers with marrow infiltration, and bone marrow failure syndromes such as aplastic anemia. Myeloproliferative disorders with bone marrow fibrosis can also present similarly. Rarely, pancytopenia caused by the displacement of hemopoietic bone marrow tissue by fibrosis, tumor or granuloma (called myelophthisis) may mimic acute leukemia. Examples include solid organ cancers with extensive marrow involvement, and granulomatous disease such as tuberculosis. However, acute leukemia is the appropriate diagnosis in patients with circulating blasts visualized on light microscopy. The circulating myeloid blasts contain Auer rods. Auer rods are not seen in lymphoblastic leukemia; their presence confirms a diagnosis of myeloid leukemia. Pretreatment evaluation. Once a diagnosis of AML is suspected, cytogenetic and genetic tests are required to aid rapid diagnosis, assess prognosis and to inform the best approach to treatment. Initial assessments should evaluate the functional integrity of the cardiovascular, pulmonary, hepatic and renal systems, and patients should also be evaluated for infection and DIC. Preparation for transfusion of blood or platelets requires blood type and crossmatch to be determined, and human leukocyte antigen (HLA) testing is required early in the treatment course (before chemotherapy) in consideration of future allogeneic hematopoietic cell transplantation. Cytogenetics has been a part of AML diagnosis for several decades, and typically requires bone marrow for successful testing (the increased number of proliferating cells in marrow yields better results than blood for karyotype analysis). Genetic testing (from blood or marrow) continues to evolve in both complexity and number of tests, and should include at least FLT3 (with allelic ratio), NPM1, CEBPA (bi-allelic), IDH1, IDH2 and TP53. Note that the European LeukemiaNet (ELN) recommendations do not include IDH1 and IDH2 (although this might be expected to change given the advent of treatments targeted at the encoded proteins); the World Health Organization (WHO) classification also recognizes RUNX1 - and ASXL1 -mutated AML as distinct entities. The diagnosis of AML is based on a finding of 20% or more myeloid blasts by histology, cytochemistry or (more commonly) flow cytometry. However, this criterion is not required for diagnosis if any of the following recurrent cytogenetic abnormalities is present: t(15;17), t(8;21), inv(16) or t(16;16). Historically, the diagnosis and classification of AML was based on the French-American-British (FAB) criteria, which assigned patients to one of eight groups, designated M0-M7, based on morphologic and cytochemical features (Table 2.2). Essentially, M0-M5 describe stages of myeloid maturation; M6 and M7 describe erythroid leukemia and megakaryocytic leukemia, respectively. The FAB classification has been superseded by the WHO classification (Table 2.3) and the ELN risk stratification (Table 2.4), which include highly relevant cytogenetic and genetic aberrations. However, morphologic description using the FAB terminology is still a common feature of clinical discussion among clinicians and with patients. Cytogenetic analysis of leukemic cells provides important independent prognostic information and is a feature of both the WHO and ELN systems.
The prognostic categories using cytogenetics are 'favorable', 'intermediate' and 'adverse' risk, based on the presence of structural and/or numeric chromosomal abnormalities. Patients with t(15;17), for example, have an excellent prognosis ( around 85% cured), and those with t(8;21) or inv(16) (inversion of chromosome 16) also have a favorable prognosis ( around 55% cured), at least when treated appropriately (see 'Treatment' chapter). Patients with no cytogenetic abnormalities (cytogenetically normal AML; CN-AML) have an intermediate risk ( around 40% cured). Patients with a complex karyotype such as t(6;9), inv(3) or -7 (absence of chromosome 7) have an adverse prognosis, with few cures, particularly for older patients. Cytogenetic analysis should be performed on the bone marrow aspirate taken at diagnosis in any patient suspected to have AML. The prognostic value of cytogenetic analysis is discussed in further detail in Chapter 5. Clinical genetic testing for several mutated genes, particularly in patients with cytogenetically normal AML, can help with further risk stratification. Favorable genetic risk is observed in those with NPM1 mutations without a high allelic ratio of FLT3 internal tandem duplications (FLT3 -ITD) and also in those with bi-allelic CEBPA mutations. Conversely, TP53 mutations (typically, but not exclusively, observed in the setting of complex karyotypes) are associated with adverse risk. IDH2 and IDH1 mutations may contribute to prognostication but, more importantly, identify patients likely to respond to novel therapies that target these aberrant pathways, at least in the relapsed/refractory setting (see Chapter 3). The number of genes mutated in AML that are known to have (or suspected of having) prognostic or therapeutic relevance is growing. Mutations beyond those used in the WHO and ELN systems are beyond the scope of this book; WHO is expected to add further genetic mutations in its 2018 update. Novel drugs that inhibit/modulate aberrant pathways activated by some of these genes (exempli gratia IDH1, IDH2, KMT2A) are currently being evaluated in clinical trials (see Chapter 6). FLT3 is expressed on nearly all AML cells; the FLT3 gene is mutated and constitutively activated in about 30% of patients with de novo AML. The mutation is most commonly an ITD. FLT3 -ITD is an adverse prognostic factor, associated with an increased risk for relapse and poor survival outcomes (see Chapter 5). Furthermore, the ratio of mutated to wild-type alleles is an important consideration because a 'high' FLT3 -ITD ratio (defined variably) confers greater risk; this allelic ratio is part of the updated ELN classification (see Table 2.4). Mutations in the tyrosine kinase domain are less common (8% of patients) and have uncertain prognostic significance. Identification of FLT3 -ITD at diagnosis is important not only as a prognostic factor but also to identify patients who may respond to targeted treatments, such as a tyrosine kinase inhibitor (TKI); several TKIs in addition to midostaurin are being investigated in AML, including quizartinib, gilteritnib, crenolanib and sorafenib.
Recurrent ovarian cancer. Despite initial treatment, ovarian cancer will recur in nearly two-thirds of women who originally presented with advanced disease. In this situation, the aims of treatment change from achieving cure to. extending survival. preserving quality of life by mitigating treatment-related adverse effects and ameliorating symptoms of the recurrent disease. Recurrent ovarian cancer may present in a number of ways, and may be asymptomatic or symptomatic. With asymptomatic (chemical) recurrence, disease may be minimal and the value of treatment in this situation has been challenged. The use of routine CA125 monitoring after front-line therapy varies globally, and is still common in the USA despite data questioning the value of such monitoring. Imaging studies may be ordered if CA125 is not used to monitor for recurrence, but this approach is costly and exposes the patient to significant radiation over time. Furthermore, the sensitivity and specificity of imaging is suboptimal in ovarian cancer, because patients often exhibit numerous postsurgical changes; moreover, recurrences can occur with small-volume disease. Many clinicians reserve imaging for patients who present with symptoms or physical examination findings that are suggestive of recurrence. Some patients with recurrent ovarian cancer may present with abdominal symptoms such as bloating, early satiety, pain or a change in urinary or bowel habits, or with more systemic symptoms such as fatigue. Elevated CA125, or other tumor markers such as human epididymis secretory protein 4 (HE4), may be confirmatory, along with imaging. The choice of treatment in patients with recurrent ovarian cancer will depend on a number of patient-, drug- and tumor-related factors (Figure 7.1). Traditionally, the choice of treatment has been determined by the time that has elapsed since the last platinum-based chemotherapy: in practice, however, the treatment-free interval, irrespective of last platinum dose, may be even more valuable. In the near future, these somewhat restrictive determinants of treatment will be supplemented with more predictive factors such as BRCA status or tumor histology, and by gene- or pathway-based assessments that facilitate individualized therapy. Nevertheless, the traditional definitions of platinum resistance or sensitivity remain relevant in guiding treatment (Figure 7.2). Platinum-sensitive patients are generally treated with platinum-containing regimens. Recent trials with poly (ADP-ribose) polymerase (PARP) inhibitors have introduced the concept of switch maintenance, whereby a PARP inhibitor is used for those who are responding to subsequent platinum-based therapies (see pages 74 -). Patients in whom recurrence occurs more than 6 months after primary platinum treatment are often subdivided into two groups. highly platinum-sensitive patients, in whom recurrence occurs more than 12 months after primary platinum treatment. intermediately sensitive patients, in whom recurrence occurs 6-12 months after primary treatment. Platinum-resistant patients, in whom recurrence occurs within 6 months of primary platinum treatment, are generally treated with non-platinum options; these usually include single agent chemotherapies with or without bevacizumab. Some clinicians define patients who actually progress on front-line treatment as platinum-refractory. They are often treated similarly to platinum-resistant patients, although some clinicians recommend continuing the platinum backbone while substituting the taxane with an alternative chemotherapeutic. The prognosis and probability of response to subsequent treatment correlate directly with the time from the previous platinum or other treatment (Figure 7.3). It should be emphasized that any patient who can potentially participate in a clinical trial should be offered this option as the preferred treatment. Chemotherapy for platinum-sensitive disease. The initial treatment decision for a patient with platinum-sensitive disease is whether or not to perform surgery (see page 54). Chemotherapy options for platinum-sensitive disease have continued to expand. Such patients are actively recruited for clinical trials, because this population is widely believed to show a good balance between performance status, time to recurrence and potential for response to therapy. Many platinum-sensitive patients will respond to numerous regimens; as described above, some clinicians treat patients with recurrence at 6-12 months differently to those with recurrence more than 12 months after previous platinum treatment.
This is especially true when the recurrence is close to the 6-month cut-off, because many such patients have a response and prognosis similar to those who are platinum-resistant. Typically, platinum is combined with either a taxane, pegylated liposomal doxorubicin (PLD) or gemcitabine. The choice of combination depends on several factors. For example, cumulative toxicities such as neuropathy can preclude retreatment with taxanes, and an alternative with a different mechanism of action, such as PLD or gemcitabine, may be preferred if the time from the previous platinum and taxane has been short. Key Phase III clinical trials with chemotherapy in patients with platinum-sensitive recurrent disease are summarized in Table 7.1. Biological therapy for platinum-sensitive disease. Bevacizumab represents another option for platinum-sensitive patients. The OCEANS trial showed a significant improvement in progression-free survival (PFS) among those treated with bevacizumab, compared with placebo, when combined with carboplatin/gemcitabine. Other studies have shown that bevacizumab can be combined with carboplatin/PLD or carboplatin/paclitaxel. Recently, the GOG213 study reported a significant improvement in PFS in patients treated with carboplatin/paclitaxel plus bevacizumab, compared with chemotherapy alone (median PFS 13.8 months versus 10.4 months, respectively, HR: 0.61, p < 0.0001); there was also a trend toward a significant improvement in overall survival (OS) (median 5.3 months increase, p = 0.056), which became statistically significant upon sensitivity analysis (p = 0.0447). Bevacizumab increased the rate of hypertension requiring medical intervention from 1% to 12%, but the rate of gastrointestinal wall disruption did not change significantly (control 1% versus bevacizumab 2%). As discussed in Chapter 8, PARP inhibitors confer an excellent response in platinum-sensitive patients, and their role appears to be expanding beyond patients with biomarkers such as BRCA mutations. The challenge for clinicians will be to decide upon the best use of these agents as either treatment or maintenance options in platinum-sensitive patients. Chemotherapy for platinum-resistant disease. Patients with disease that recurs within 6 months of primary platinum treatment are generally treated with single agents, with or without bevacizumab. The most commonly used agents for retreatment include (usually weekly) paclitaxel, docetaxel, PLD, gemcitabine, topotecan and pemetrexed. Key clinical trials with these agents are summarized in Table 7.2. Unfortunately, the survival data and the objective response rates (less than 15%) are not impressive for any of the available single agents in the platinum-resistant setting; novel compounds and approaches are needed. An important decision for clinicians is how best to use bevacizumab for the treatment of ovarian cancer. In some regions, bevacizumab is used in the front-line setting, while in others it is restricted to platinum-sensitive patients. Both the AURELIA trial in platinum-resistant patients, and the OCEANS trial in platinum-sensitive patients, yielded identical hazard ratios of 0.48 for PFS. Importantly, factors such as the presence of ascites or effusions, potential bowel toxicity with later lines, response or toxicity in earlier lines and affordability need to be considered in optimizing the use of bevacizumab. Although the value of tumor debulking surgery is well established for the treatment of primary tumors, the value of cytoreduction for recurrent epithelial ovarian cancer is not well defined. Appropriate patient selection is crucial (see Chapter 5).
Management: physical activity. For our ancestors, physical activity was a vital element of life, for hunting and to avoid being hunted. Nowadays, the modern workforce is much more sedentary, as manual jobs have been mechanized and transport systems have flourished. Leisure time is also more high-tech and sedentary. An estimated 75% of adults get less physical activity than they should. One study showed that 56% of men and 52% of women believed they were sufficiently active to benefit health, whereas only 36% and 24%, respectively, achieved even moderate activity. Patients may believe they are fit because they manage routine chores without undue tiredness or breathlessness, but their daily tasks may not expose their lack of fitness. Even when metabolically normal, obese subjects tend to be at increased risk for cardiometabolic diseases. Physical activity must be reintroduced to our lives - by scheduled exercise for those who enjoy and can maintain it, or by increasing the amount and quality of activity during the daily routine, or ideally both. Risks of inactivity. Sedentary behavior and obesity are closely linked but are not necessarily concurrent in the same individual and should be considered independent risk factors for disease. Inactivity, for example prolonged sitting, carries a risk irrespective of bodyweight. Lack of physical activity is a risk factor for stroke, coronary artery disease and type 2 diabetes mellitus and causes a twofold increase in risk for all-cause mortality, hard on the heels of smoking and hypertension in terms of damage done (Figure 6.1). Benefits of physical activity. Physical activity is an integral and essential part of any long-term weight management program. improves fitness decreases cardiometabolic risk. helps sustain weight loss maintains weight after initial 5-10% loss. A study carried out among Boston policemen showed minimal difference in weight loss with diet alone compared with diet plus exercise, but the group that exercised sustained weight loss in the long term. Other studies have confirmed these findings. It is difficult to lose weight by physical activity alone but exercise reduces the adverse effects of overweight and obesity even if weight is not lost (Table 6.1). Increasing physical activity is as beneficial to general health as giving up smoking. Maximum benefits come from a combination of resistance and aerobic activity. An increase in muscle bulk at the expense of fat has multiple benefits: apart from reduction in cardiovascular disease and diabetes, it reduces the risk of cancer and loss of cognitive function, improves mood and sense of well-being, facilitates better sleep, and reduces the risk of frailty, falls and dependence in older age. The waist circumference of someone who becomes fitter by increasing levels of activity will decrease, although their weight may change little. Encouraging patients who exercise. Individuals who attempt weight loss by physical activity alone should be congratulated on increased fitness, and should be encouraged to monitor their waist circumference; otherwise they may become frustrated and demoralized by lack of change in weight or body mass index (BMI). Recommended levels. While many country-specific recommendations exist, the World Health Organization recommends 45-60 minutes of moderate-intensity exercise on most days to prevent unhealthy weight gain, and 60-90 minutes per day to lose weight or prevent weight regain after substantial loss. At least 10000 steps per day (as monitored on a pedometer) are recommended for health, and 15000 to lose weight or maintain weight loss. Extra activity can be undertaken at home, in the gym, park, sports field or shopping center, as long as it is additional activity, preferably enjoyable, and likely to be maintained in the long term to avoid rebound weight gain. In addition to walking for 45-60 minutes each day, advise the use of light weights and maximum repetition across a range of muscle groups three or four times a week. This can be easily done at home sitting upright in a chair that provides back and neck support and using handheld dumbbell weights and strap-on ankle weights, starting at no more than 0.5-1 kg.
In this position an exercise session might include upper body biceps curls, shoulder presses, straight arm raises and lateral arm raises, and for lower body, knee extensions; and, lying on the floor or bed, straight leg raises. Aim for 20 repetitions and two sets (Figure 6.2). Because of the dose-response relation between physical activity and health, people who wish to improve their personal fitness further, reduce their risk for chronic diseases and disabilities or prevent unhealthy weight gain may benefit by doing more than the minimum recommended amounts of physical activity. Reducing sedentary behavior and increasing incidental activity. Figure 6.3 shows patterns of energy expenditure over 1 day for a sedentary person (blue line), a person who engages in planned vigorous exercise during leisure time but is otherwise sedentary (red line), and a person with a sedentary job with short bouts of physical activity throughout the day (green line). Individuals with sedentary occupations should ensure that periods of sitting are broken at least every 30 minutes by at least standing for an equivalent or even greater period of time and where possible some form of activity for a brief period. Desks that allow people to stand and work, or even walk and work are available and such behaviors confer health benefits. Strategies to increase incidental activity are shown in Table 6.2. If possible, at least some brisk exertion should be undertaken. The optimum degree of 'brisk' exertion is that which quickens the pulse, induces perspiring and quickens the breathing rate but still allows a person to talk. Individual tailoring. It is important to tailor physical activity recommendations to the individual, setting realistic goals. Any increase in activity level, however small, is important, and induces an almost immediate improvement in insulin sensitivity. Long-term medical conditions may affect a person's ability to achieve meaningful levels of activity, and appropriate advice must take this into account. For an obese person with chronic bronchitis, a few steps in the garden every hour may represent a significant increase in activity; a patient with asthma may need to be advised to use their salbutamol inhaler before exercising; a slow stroll on even ground may be appropriate for someone with severe vascular disease; advice to stand up while talking on the phone may be helpful for someone with severe physical limitations. The US Surgeon General's report recommends wheeling oneself in a wheelchair for 40 minutes as being a reasonable level of activity for health benefits, whereas a patient with lower-limb orthopedic problems may be advised to perform low-impact activities, or even arm exercises alone. Elderly patients often suffer from low muscle mass (sarcopenia), so increased physical activity of any degree and particularly with a resistance component should be vigorously promoted for anyone with severe physical limitations. Armstrong and colleagues demonstrated that almost 50% of girls and 38% of boys did not achieve one 10-minute period of exercise equivalent to brisk walking in 3 school days. The home is safer and more comfortable than ever; children watch an average of almost 3 hours of television per day, often whilst snacking on high-calorie foods, and play computer games with ever increasing complexity, requiring more and more practice to reach the final goal. Gortmaker calculated that the risk of overweight is increased 4.6 times in children who watch more than 5 hours of television per day. The UK National Audit Office has produced some recommendations to prevent childhood obesity. at least 2 hours' physical activity a week for all pupils. a safer and more integrated network of appropriate routes, footpaths and cycle lanes. adoption of cycling and walking in preference to traveling by car. broadening the range of activities schools offer to encourage young people to participate in different forms of physical recreation.
Specific populations. Women of childbearing age. All doctors treating women with epilepsy should consider the following areas. Carbamazepine (CBZ), eslicarbazepine acetate (ESL), felbamate (FBM), oxcarbazepine (OXC), phenobarbital (PB), phenytoin (PHT), primidone (PRM), rufinamide (RFN), and topiramate (TPM) at doses over 200 mg daily all induce the metabolism of female sex hormones. This metabolism can alter the menstrual cycle and increase turnover of the components of oral contraceptive pills and depot formulations of steroid hormones (Table 8.1). The risk of breakthrough pregnancy is not insignificant. An oral contraceptive formulation containing 50 g of estrogen, with subsequent adjustment depending on the presence or absence of breakthrough bleeding, can provide secure contraception, as can barrier methods. Other birth control measures must be taken until the pattern of menstruation has been stable for at least 3 months. Lamotrigine (LTG) reduces levonorgestrel levels by about 20%, which is a potentially significant decrease. Levonorgestrel implants are contraindicated in women taking enzyme-inducing antiepileptic drugs (AEDs) as they have an unacceptably high failure rate. This is also likely to be the case with the progesterone-only pill. Medroxyprogesterone injections appear to be effective, though they need to be given more frequently than is usually recommended. The morning-after contraceptive pill can be used after unprotected intercourse. The effectiveness of the hormonal method of emergency contraception is reduced by enzyme-inducing drugs; a copper intrauterine device may be offered, or the dose of levonorgestrel should be increased. Up to 20% of women with epilepsy have abnormal ovarian function, including anovulatory menstrual cycles and polycystic ovaries. These problems may be more common in patients treated with sodium valproate (VPA). Some women find that their seizures worsen mid-cycle or around menstruation, a phenomenon known as catamenial epilepsy. This exacerbation is thought to be a consequence of an imbalance between the proconvulsant estrogen and anticonvulsant progestogen concentrations. Manipulating the cycle with hormonal preparations is often unsuccessful, however, and may cause unwanted effects such as weight gain and depression. Another option is intermittent clobazam (CLB) for the few days just before and shortly after the onset of menstruation. The fertility of women with treated epilepsy is one-quarter to one-third lower than that of the general population. However, when women do conceive, most can expect to undergo uneventful pregnancies and deliver healthy babies. During pregnancy, metabolic processes change and close attention needs to be given to AED concentrations. Total serum concentrations of some drugs will fall, particularly those of PHT (Figure 8.1) and LTG. Women whose epilepsy is well controlled usually remain seizure free during pregnancy and delivery. Conversely, those who continue to report seizures before conception may have increased seizures during pregnancy. Before conception. Although it would be ideal to withdraw AED treatment in women contemplating pregnancy, for many this would result in recurrence or exacerbation of seizures that could be dangerous for both mother and fetus. If the criteria for discontinuation are met (see Therapy withdrawal), the AED should be stopped over a suitable interval before conception. If AED therapy cannot be withdrawn completely, it should be tapered to a minimally effective dose of, if possible, a single drug. In addition, supplemental folic acid, 4-5 mg daily, should be started before conception in an attempt to prevent neural tube defects. Folate treatment should be continued for the first 5 weeks of gestation, and current advice is to continue taking it at least until the end of week 12. These and other guidelines for managing epilepsy in women who are contemplating pregnancy are set out in Table 8.2.
Fetal health. The incidence of minor and major fetal malformations increases in women with epilepsy, even if they are untreated. Commonly quoted figures are 3-6% for women with epilepsy compared with 2-3% in the general population. The risk increases disproportionately with the number of AEDs taken, being approximately 3% for one drug (similar to background risk), 5% for two, 10% for three and over 20% in women taking more than three AEDs (Figure 8.2). A syndrome initially ascribed to hydantoins including PHT (fetal hydantoin syndrome), but now known to occur with other AEDs including CBZ and VPA, consists of facial dimorphism, cleft lip and palate, cardiac defects, digital hypoplasia and nail dysplasia. There are no clear data indicating differences in safety among PHT, CBZ, PB and PRM. Current evidence suggests that the risk of major congenital malformations is two to four times higher with the use of VPA than with other AEDs such as CBZ and LTG. Absolute rates have ranged from 6% to 11%, although the risk may be minimized by keeping daily doses at or below 1000 mg. High-dose exposure to VPA in utero may impair later cognitive function. The teratogenic risk associated with LTG monotherapy is low and is similar to that associated with CBZ, although preliminary data suggest the possibility of greater risk of major malformations at higher dosage. There are still insufficient data regarding the safety of other modern AEDs. After birth. The older enzyme-inducing AEDs (CBZ, PHT, PB and PRM) can cause transient and reversible deficiency in vitamin K1-dependent clotting factors in the neonate. The risk of intracerebral hemorrhage increases if the birth is traumatic. Accordingly, some clinicians believe that babies at risk should receive intramuscular vitamin K1 immediately after birth, and mothers should take oral vitamin K1, 10 mg daily, for the last few weeks of pregnancy. After delivery, all mothers should be encouraged to breastfeed their babies. The concentrations of PHT, CBZ and VPA in breast milk are low and not usually harmful. PB, PRM and LTG can accumulate in the breastfed baby because of slow elimination. Gabapentin (GBP) and vigabatrin (VGB) are unlikely to accumulate in infants as these AEDs are excreted mainly unchanged in the urine. There are few data relating to the other newer AEDs. As a general rule, if the baby is noted to be drowsy or sedated, breastfeeding should be alternated with bottle feeding or stopped altogether. Pregnancy registries. The Antiepileptic Drug Pregnancy Registry was established in the USA in 1996 to determine prospectively the risk of major malformations from AEDs. Women with epilepsy who become pregnant should call the toll-free number (1 888 233 2334) to enroll. Physicians cannot enroll patients; the woman herself must call as part of the informed consent process. There are three brief interviews: an initial 15 minutes, 5 minutes at 7 months' gestation and 5 minutes 2-4 weeks after birth. In Europe, a similar project is under way. This registry requires input from the attending clinician and not the patient. The European Registry of Antiepileptic Drugs and Pregnancy (EURAP) is a consortium of independent research groups that have agreed on a common protocol for a prospective assessment of pregnancy outcome. The registry has now expanded beyond Europe to include Australia, Japan and other Asian countries. All physicians who care for women taking AEDs during pregnancy are invited to contribute. They can contact their individual national coordinators or the central project commission via dbattino@istituto-besta. There is also an active prospective pregnancy register in the UK.
Elderly patients. Old age is now the most common time in life to develop epilepsy. Approximately 1.5% of the population over the age of 70 years is diagnosed with active epilepsy. The number of elderly people diagnosed with epilepsy is set to rise further with the aging of the population. Nearly all de-novo seizures in elderly people are partial-onset with or without secondary generalization. Underlying factors can be identified in a greater proportion of elderly patients than younger patients, and include cerebrovascular disease, dementia and tumor. New-onset idiopathic syndromes are rare. Diagnosis of epilepsy can be challenging and may depend on a witnessed event. Complex partial seizures presenting as confusion may be misdiagnosed as psychiatric symptoms. Postictal confusion can be prolonged in the elderly and may contribute to physical injury sustained during a seizure. AEDs are the mainstay of treatment, and are effective in most patients. Complete seizure control can be expected in more than 70% of elderly patients. A subgroup, often with progressive neurodegenerative disease, will continue to have seizures despite all attempts at pharmacological prevention. Elderly patients are particularly sensitive to the adverse effects of AEDs, possibly because of age-related pharmacokinetic changes caused by the delay in gastric emptying, reduction in body fat, and decreased hepatic metabolism and renal elimination. Low doses are generally recommended in the elderly in order to minimize adverse effects. The patient, and often the spouse and children, must be convinced of the need for lifelong treatment. Sympathetic explanation and assured support will help an elderly person regain their self-confidence after epilepsy has been diagnosed and AED treatment established. Choice of drug depends on the side-effect and interaction profiles. Drugs with a high propensity for neurotoxicity should be avoided (see Table 4.5). In patients with multiple concomitant medications, AEDs that do not produce pharmacokinetic interactions are the preferred choice (see Table 4.6). Few clinical trials of AEDs have been performed specifically in the elderly. Double-blind trials support the newer agents LTG and GBP over CBZ for the treatment of partial seizures and generalized tonic-clonic seizures (GTCS), primarily because they produce fewer neurotoxic side effects. Levetiracetam (LEV) is a suitable alternative as it is also well tolerated in this population and is implicated in fewer drug interactions than is CBZ or PHT. Some types of epilepsy, such as the idiopathic syndromes juvenile myoclonic epilepsy (JME) and GTCS on awakening, are most likely to manifest during the teenage years.
Sleep deprivation, photosensitivity and major stresses such as school examinations are common triggers. Partial seizures can also present during the teenage years, either de novo or as a recurrence of a dormant childhood condition such as mesial temporal sclerosis. Children who develop epilepsy should be re-evaluated during their teenage years, and AED levels should be monitored. At puberty, hepatic metabolism slows to a rate similar to that in adults, which may lead to a rise in circulating AED concentrations. AED doses may, therefore, need to be reduced as a child grows older. However, such a rise is often offset by a teenage growth spurt. Falling AED levels may indicate imperfect compliance, a common occurrence in this age group. The teenage years are an appropriate time for counseling on contraception, clarifying the possible side effects of AEDs, and predicting prognosis and eventual drug withdrawal. Driving, social interactions and career advice are other issues that doctors caring for teenagers with epilepsy must address (see Chapter 9). Patients with learning disabilities. Epilepsy has the highest prevalence in people with learning disabilities, ranging from 5% in mildly affected individuals to 75% in those with coexisting severe cerebral palsy or postnatal brain injury. Diagnosis relies heavily on an accurate description of events, as routine investigations are rarely helpful. Tonic-clonic seizures are common, but many patients also have partial and other generalized seizure types. The clinical picture is often complicated by stereotypies and behavioral disorders. Co-prescription of antipsychotic drugs may further reduce the seizure threshold. Before the doctor's first visit, a great deal of useful information can be obtained from a home assessment by a specialist epilepsy nurse following an agreed protocol. The home assessment should include. description of the episodes. evaluation of IQ. details of concomitant medication. previous and current AED treatment. circulating AED levels if appropriate. details of the carer's concerns and so on. Home video recordings can help to confirm or refute the diagnosis of epilepsy. At the outset, a management plan, including outcome aims, should be formulated with the full involvement of the carer(s) and family. Numbers and doses of AEDs should be minimized as much as possible. Attention should be paid not only to seizure frequency and severity, but also to behavior, mood, appetite, communication, cooperation, alertness and sleep pattern. Broad-spectrum AEDs, such as VPA, LTG, TPM, zonisamide and LEV, should be the preferred choice, and barbiturates and benzodiazepines should be avoided. The endpoint need not always be freedom from seizures, but perhaps better control accompanied by improved alertness, mood and cooperation.
Urologic symptoms are clearly a key aspect in the diagnosis of urinary disorders, but may not be reliable when used alone; as many have stated, the bladder may be an unreliable witness. The onset of urinary symptoms, their duration and severity, and the time(s) when they occur, should be recorded. Symptoms can be divided into filling/storage, voiding/emptying and postmicturition. Filling symptoms. Frequency is the complaint of voiding too often by day and is usually defined as more than eight voids per 24 hours. Increased daytime frequency can occur with a normal bladder capacity where there is excessive fluid intake, or where the bladder capacity is affected by detrusor overactivity, impaired bladder compliance (compliance refers to the tonic change in bladder pressure during filling - usually, bladder compliance is normal if there is little or no change in detrusor pressure during filling) or increased bladder sensation (hypersensitivity). The causes of daytime frequency are shown in Table 2.1. Nocturia is the complaint of waking to void one or more times during the night. It may occur for the same reasons as daytime frequency, but may also occur in association with congestive heart failure (causing nocturnal polyuria) or because the normal circadian rhythm of antidiuretic hormone (ADH; desmopressin) secretion becomes reversed. Urgency is a sudden compelling desire to void that is difficult to defer. Urgency implies detrusor overactivity, but can also occur if there is an underlying bladder inflammatory disorder. Storage symptoms. Stress (or effort-related) incontinence is the involuntary loss of urine during physical exertion. This occurs without any contraction of the detrusor and may be associated with a number of physical activities, including laughing, coughing, sneezing, running, jumping, aerobics and sexual activity. Urine loss is usually in small amounts (drops, squirts) and may not be a daily event. It is caused by failure of the bladder outlet to remain closed and thereby maintain continence when intra-abdominal pressure is raised. Urgency (previously called urge) incontinence is the involuntary leakage of urine accompanied, or immediately preceded, by urgency. Urgency incontinence can take the form of frequent small losses between voids or large urine losses from sudden complete bladder emptying. It can occur several times a day or week. It is caused by involuntary detrusor contractions during bladder filling/urine storage. Nocturnal enuresis is the loss of urine during sleep. When taking a history, it is important to inquire about childhood nocturnal enuresis (bed-wetting), as delayed bladder control in childhood is often associated with detrusor overactivity in adulthood. Continuous incontinence tends to be associated with urinary tract fistulas or with chronic urinary retention with so-called overflow incontinence. Urinary tract fistulas are usually iatrogenic in developed countries, but are commonly associated with unsupervised childbirth in developing countries. Chronic retention is most common in men with obstruction secondary to prostate enlargement or in women with severe pelvic organ prolapse (POP), such as vaginal vault prolapse. Mixed incontinence is the coexistence of stress incontinence with urgency incontinence or urge symptoms. Pressure (bladder, suprapubic) is the sensation that the bladder is full and the urge to void will occur shortly. Its causes include incomplete bladder emptying. Discomfort that is relieved by voiding may indicate interstitial cystitis or bladder pain syndrome. 
Voiding/emptying symptoms. With the exception of postmicturition dribble, all of the symptoms described below can be associated with bladder outlet obstruction, a poorly contracting detrusor or loss of coordination between detrusor contractility and relaxation of the external urethral sphincter, termed detrusor-sphincter dyssynergia (DSD). These symptoms are more often seen in men than in women. Hesitancy is described as difficulty in initiating micturition, resulting in a delay in the onset of voiding when the individual is ready to pass urine. Intermittent stream or intermittency describes urine flow that stops and starts once or more during micturition. Slow stream is the perception of a reduced urine flow, usually compared with previous performance or with the flow of others. Terminal dribble describes a prolonged final part of micturition, when the flow slows to a trickle. Incomplete emptying is a self-explanatory term for a feeling experienced by an individual after passing urine. Postmicturition dribble refers to the leakage of urine after micturition. Approximately 80% of men experience this symptom at some time, and it can present at any age. The symptom arises from the leakage of a few drops of urine that are pooled in the bulbar urethra after micturition has been completed and that drain under gravity a few moments later. Postmicturition dribble is seldom associated with clinical abnormalities, and can be avoided by waiting until the remaining urine has been passed or by milking the urethra at the end of micturition (Figure 2.1). Urethral stricture or diverticulum may be rare causes of this condition. If voiding dysfunction is present, uroflowmetry should be the first investigation (see 2. Physical examination. All patients presenting with bladder symptoms should undergo a full physical examination, including neurological examination. Neurological examination. Neurological conditions that are associated with bladder problems (such as multiple sclerosis, stroke, Parkinson's disease or spinal injury) are usually obvious when the patient first presents. If a neurological cause is suspected, it is important to pay particular attention to sacral neuronal pathways. The gait, abduction and dorsiflexion of the toes (S3) should be assessed, as should sensory innervation of the perineum (L1-L2), sole and lateral aspect of the foot (S1) and posterior aspect of the thigh (S2). Perineal (S3) and cutaneous reflexes (bulbocavernosus and anal reflexes) should also be tested. Abdominal examination. Scars from previous surgery should be noted. Increased abdominal striae may be found in association with other markers of abnormal collagen metabolism, and are more common in women with POP and stress incontinence.
An attempt to palpate the kidneys should be made. Abdominal examination or suprapubic percussion may identify a distended bladder (if more than 300 mL in bladder) or a pelvic mass that is compressing the bladder. Genital examination is essential in both women and men. The urethral meatus, skin of the vulva and perineum should be examined, and any atrophic vaginitis identified. Excoriation and maceration of the vulva may occur with constant wetness and may cause secondary infections. Speculum examination will enable assessment of the vaginal walls to identify atrophy, urethral caruncle or POP (cystocele, rectocele, uterine or vaginal vault prolapse). Pelvic masses may also be identified. The woman should be asked to cough and strain in an attempt to demonstrate stress incontinence. Ideally, this should be done with a full bladder. It may be necessary to examine the woman whilst she stands with one foot on a stool, in order to detect POP or stress incontinence. The strength of the pelvic floor muscles (PFM) should be assessed and can be quantified using a validated grading system such as the Oxford 1-5 scale. Factors to be assessed include strength, duration and repeatability of contractions, and displacement of the pelvic floor. Low-tone pelvic floor dysfunction refers to the examination findings of an impaired ability to isolate and contract the pelvic floor musculature in the presence of weak and atrophic musculature and is seen in women with POP, urinary or fecal incontinence, and vaginal weakness. High-tone pelvic floor dysfunction refers to the clinical condition of hypertonic spastic PFM with resultant impairment of muscle isolation, contraction and relaxation and is seen in women with interstitial cystitis/bladder pain syndrome, voiding dysfunction, overactive bladder, pelvic pain and sexual dysfunction with dyspareunia. The appearance of the external urethral meatus and prepuce may be a guide to distal urethral causes of voiding difficulties or postmicturition dribble that the patient may describe as urinary incontinence (id est strictures). Digital rectal examination (see Figure 5.1) should include palpation of the prostate to assess size, symmetry and consistency of the gland, its position in relation to the rectum and pelvic side wall and the presence of nodularity or induration. Symptoms of bladder overactivity can be caused by locally advanced prostate cancer or an enlarged prostate. Rectal masses are obviously abnormal and require prompt referral. The status of the anal sphincter musculature should be noted by determining strength and tone.
Further investigations. Further investigation is usually necessary to confirm the cause of bladder symptoms. The investigations undertaken will depend to some extent on the facilities available. Psychological and cognitive assessment. The patient should be assessed for clinical depression, which can compromise the success of behavioral and surgical treatments. A mini mental-state examination or clock-drawing test can be used if cognitive impairment is suspected, as incontinence can be related to memory changes. Functional and environment assessment. In elderly patients, an assessment of functional abilities should focus on self-care tasks or activities of daily living (ADL) (ability to ambulate, disrobe). Mobility problems (exempli gratia inability to access the toilet, history of falls) are stronger predictors for developing urinary incontinence than cognitive impairment. Environmental assessment should include identifying the location of the toilet. Quality of life. There are a number of ways to assess the impact of incontinence symptoms on a patient's quality of life. However, the only valid way to measure the patient's perception of their symptoms is through the use of psychometrically robust self-completion questionnaires, such as the International Continence Society's International Consultation on Incontinence modular questionnaire (ICIQ; Figure 2.2) A wide variety of questionnaires were assessed by the Continence Society the questionnaires recommended for the assessment of quality of life for patients with urinary incontinence alone or in the presence of lower urinary tract symptoms are listed in Table 2.2. Frequency/volume bladder record. Use of a bladder record or diary is a simple and practical method to obtain information on a patient's normal voiding pattern, including frequency and amount of micturition and episodes of leakage, in addition to the time and volume of fluid ingested. The patient records the times and volumes of all voids over a specific time period, which should be at least 24 hours so that both day and night are included. Episodes of urinary incontinence are recorded and whether they are associated with, for example, urgency, straining or coughing. Eliciting an estimate of the volume of leakage during incontinence episodes is helpful. The following descriptions of urine leakage can be used. Small volume (< 30 mL) - enough to make underwear wet if no protective pad is worn. Moderate volume (31-100 mL) - enough to wet or soak underwear and leak down the legs if no protective pad is worn. Large volume (101+ mL) - soaks through clothing and onto floor or furniture and usually is the entire bladder volume. Objective information is obtained not only on daytime frequency and nocturia, but also on the normal functional bladder capacity, mean voided volume, total voided volumes and diurnal distribution of micturition. Abnormalities that may be demonstrated on a frequency/volume bladder record include. regular voiding of small quantities of urine, which is associated with filling abnormalities or may be a result of defensive voiding (voiding to ensure an empty bladder so as to decrease the chance of urine leakage). nocturia and nocturnal polyuria (passing more than one-third of the 24-hour output during normal sleeping hours). fluid restriction. excessive fluid intake (through habit or on medical advice). polyuria (an excessive volume of urination, which in an adult would be more than 2500 mL/day). urinary incontinence and the associated circumstances (id est urgency, effort, unaware). type and number of absorbent pads used. The frequency/volume bladder record is also useful for assessing and monitoring treatment, and to demonstrate the benefits of treatment to a patient. An example of a completed record is shown in Figure 2.3. Urinalysis and culture. Dipstick urinalysis is used to detect hematuria, glycosuria, pyuria and bacteriuria. It should be carried out for all patients presenting with urinary incontinence to exclude the possibility of infection, inflammation, urinary tract malignancy and diabetes. A positive dipstick test should be followed up by formal urine microscopy and culture to detect a urinary tract infection (UTI) before treatment and to allow antibiotic sensitivity to be evaluated. The presence of hematuria or red blood cells on microscopy should be investigated further with urine cytology, an imaging study of the upper tracts (kidneys and ureters) and endoscopic examination of the bladder and urethra to rule out malignancy, especially in a patient over 50 years of age with symptoms of bladder irritation. The investigation and management of hematuria are described in detail in Chapter 6. 
A preliminary plain abdominal radiograph can be performed in patients with suspected renal tract calculi or soft tissue masses. Ultrasonography has become the first-line method for detecting abnormalities of the kidneys, including scarring, calculi, dilatation and tumors. Ultrasound can also be used to detect increased bladder-wall thickness (which suggests outlet obstruction and/or detrusor overactivity) and to look for bladder calculi; portable ultrasound devices or 'bladder scanners' can be used to measure postvoid urine volumes. Intravenous urography has largely been superseded by ultrasound for initial investigation of microscopic hematuria but may be appropriate if ultrasonography suggests obstruction or leakage from a fistula. CT urography has become the investigation of choice if ultrasonography is not diagnostic or gross hematuria is present. It has a higher sensitivity for small calculi and early neoplasms. Urodynamic studies. These include tests that generate quantitative data relevant to events in the bladder and bladder outlet during the filling/storage and emptying/voiding phases of micturition. Many clinicians request urodynamic investigation for any patient, especially a woman, who complains of lower urinary tract symptoms; however, the clinician may have little appreciation of the clinical indications, what the test involves and its limitations. Urodynamic investigation is safe, but men in particular may experience some discomfort related to catheterization, which can last for up to 24 hours after the test. The incidence of culture-proven UTI following urodynamic testing is approximately 1%; prophylactic antibiotics should therefore be considered for patients who may be at risk (exempli gratia immunosuppression or prosthesis implanted within last 2 years). Urodynamic investigation may be indicated in the following cases. treatment failure. complex mixed lower urinary tract symptoms. before incontinence surgery. symptoms suggesting detrusor overactivity. voiding symptoms. women with POP. urinary retention. neurogenic disease. Complex mixed lower urinary tract symptoms or treatment failure. Some patients present with such a complicated history that it is impossible to make any judgment as to the cause of their symptoms. Empirical treatment is therefore not possible. The patient should undergo urodynamic investigation so that appropriate treatment can be offered. Before incontinence surgery. In our opinion, urodynamic information is essential if surgery to treat incontinence, especially stress incontinence, is contemplated. There is, however, some debate as to the need for urodynamic investigation before incontinence surgery. The UK's National Institute for Health and Clinical Excellence (NICE) has issued guidelines stating that preoperative urodynamic studies are not necessary for patients undergoing a primary procedure and whose symptoms and examination findings suggest simple stress incontinence. We disagree with this recommendation, as it has been claimed that surgery carried out on the basis of symptoms alone is inappropriate in up to 25% of cases. Furthermore, surgery for stress incontinence can lead to voiding dysfunction and de novo detrusor overactivity (the urodynamic term describing involuntary bladder contraction), or may exacerbate pre-existing symptoms, so it is important to perform a preoperative assessment. Symptoms suggesting detrusor overactivity. Most patients with symptoms suggestive of detrusor overactivity can be treated empirically. This assumes that other obvious causes for the symptoms (exempli gratia urinary infection) have been excluded and that neither hematuria (blood in the urine) nor significant bladder, urethral or pelvic pain is present. Urodynamic investigation is appropriate if symptoms of detrusor overactivity are unresponsive to drug and/or behavioral therapy and if the diagnosis is essential, either to avoid unnecessary continuation of drugs or to exclude other pathology. It is important to understand that a urodynamic study provides only an artificial 10-20-minute 'snapshot' and does not always replicate real-life events.
Thus, a patient with symptoms of detrusor overactivity may have a normal urodynamic study. Some of these patients will have detrusor overactivity that has been missed, and a trial of drug and/or behavioral therapy is entirely appropriate. Voiding symptoms and urinary retention are more common in men than women, reflecting a higher incidence of bladder outlet obstruction; however, voiding symptoms do occur in women, especially in association with POP or poor detrusor contractility. The initial investigation should be measurement of free urinary flow and postvoid residual volume. If abnormal emptying is confirmed, full urodynamic assessment is required to differentiate between detrusor hypocontractility and outlet obstruction or both. Pelvic Organ Prolapse. Urodynamic investigations can be useful prior to undertaking surgery for POP as it is common for this to coexist with urinary incontinence. Treatment of POP can also unmask urinary incontinence that was being concealed by urethral kinking. In this situation it is common to carry out urodynamic investigations with a temporary ring pessary in place. Neuropathic bladder. Patients with neurological disease and lower urinary tract symptoms are at risk of neurogenic detrusor overactivity, low compliance and DSD. The main concern is the potential for upper urinary tract damage resulting from increased detrusor/intravesical pressure, which can lead to dilatation of the ureters and renal impairment. A simultaneous investigation of the renal tract anatomy and function may be valuable in these patients (videocystourethrography with concurrent urodynamic investigation). Uroflowmetry is a simple test in which the patient voids in privacy into a commode that incorporates a urinary flowmeter that measures urine flow over time (Figure 2.4). A voided volume of at least 150 mL is desirable for the flow rate to be interpreted accurately. Normal findings are. total voided volume greater than 200 mL. volume passed over a period of 15-20 seconds. maximum flow rate above 20 mL/second. smooth crescendo parabolic curve. It should be recognized that most 'normal' data relate to uroflowmetry in patients younger than 55 years of age; thus, flow rate should be interpreted with consideration for the minimum acceptable flow for given sex and age groups. Subsequent measurement of the postvoid residual volume by bladder scanning or catheterization gives further information about bladder emptying. Cystometry records bladder pressure during filling and voiding, with the aim of explaining a clinical problem in pathophysiological terms. The bladder is filled with a saline-like solution at room temperature via a small-bore urethral catheter, alongside which is passed a fine pressure transducer. The pressure in the rectum is recorded simultaneously to differentiate rises in intravesical pressure secondary to increases in abdominal pressure (exempli gratia coughing, straining, talking and changing position) from those due to detrusor contractions. Table 2.3 lists the measurements required during filling cystometry. Figure 2.5 shows some typical traces. Filling cystometry is unphysiological in that the bladder is normally filled at a very slow rate without any change in pressure, and higher filling rates may produce significant artifacts, particularly in patients with neurological problems. Filling rates are classified as follows. fast: above 100 mL/minute. medium: 10-100 mL/min. slow: less than 10 mL/min. A filling rate of 50-60 mL/min during conventional filling cystometry appears to be a good compromise, allowing practical filling times with a low incidence of artifacts. The following can be evaluated during filling cystometry. Capacity - the volume infused during filling is measured, and hence the bladder volume is calculated. Sensation - the patient is asked to comment on bladder sensation during filling. First sensation of bladder filling - this is the feeling that the patient has when he or she first becomes aware of bladder filling during filling cystometry, usually at about half capacity (150-200 mL).
First desire to void - this is the feeling that leads the patient to pass urine at the next convenient moment, although voiding can be delayed if necessary; commonly at 75% of capacity. Strong desire to void - this is a persistent desire to void but without fear of leakage, felt at capacity. Urgency - this is a sudden compelling desire to void, described particularly by patients with detrusor overactivity or inflammatory bladder conditions. Maximum cystometric capacity - this is the volume at which the patient feels unable to delay micturition (though the test is usually stopped before this). Detrusor function - abnormal detrusor contractions during bladder filling are noted; detrusor contractility during voiding can be assessed by recording the detrusor pressure in the voiding phase. Compliance - this indicates the change in volume for a change in pressure and is expressed in mL/cmH O. Little or no pressure change occurs during normal filling in a patient with normal compliance. Compliance may be reduced if bladder pathology is present or in patients with neurological disease or injury. Urethral function - an incompetent urethral closure mechanism is defined as one that allows leakage of urine in the absence of a detrusor contraction. Stress incontinence can be demonstrated by asking the patient to cough during the filling phase. If urine loss is noted, the subtracted detrusor pressure trace should be checked to ensure that there is no associated detrusor contraction. If stress incontinence has not been demonstrated once the functional bladder capacity is reached, the filling catheter can be removed, leaving a pressure-recording catheter or transducer in place, and the patient is asked to undertake a variety of provocative maneuvers, such as coughing, squatting, heel bouncing and jumping. Voiding cystometry (pressure-flow study). When filling cystometry has been completed, the filling line is removed as described above, and the patient is asked to void with the bladder and abdominal pressure transducers still in place. The urinary flow rate and detrusor pressure are recorded. A normal man voids with a detrusor pressure of 20-40 cmH O and a normal woman with a considerably lower pressure of 0-25 cmH O. Measurement of detrusor pressure gives an indication of the contractility of the bladder and, when combined with the urinary flow rate, the outflow resistance: high pressures and low flow rates indicate outlet obstruction. Videourodynamic investigation combines cystometry with simultaneous radiological screening of the bladder and urethra. A radiopaque contrast medium is used instead of saline. Videourodynamic investigation provides information about the appearance of the bladder, urethra and sphincters, and will identify reflux into the ureters. It may be more useful than cystometry alone in the assessment of complex cases. It is especially useful to identify the site of obstruction in an individual with high pressure-low flow and to differentiate low pressure-low flow from outlet obstruction. Diagnostic cystourethroscopy can be carried out using either a rigid or a flexible cystoscope, with or without anesthesia. Water is the preferred distension medium used during cystoscopy. An angled (flexible) cystoscope (30 or 70 degrees) is normally required to visualize the whole of the bladder. Comment should be made on the appearance of the urethra, trigone, bladder mucosa and ureteric orifices. If bladder filling symptoms are present, the volume of fluid infused should be noted. Diagnostic cystourethroscopy is indicated in cases of recurrent UTI, hematuria, bladder pain and suspected bladder injury. It is also used intraoperatively when carrying out continence procedures and when inserting suprapubic catheters.
6 Research directions - the next wave. Clinical gene therapy is a very active field, with a large number of clinical trials, approaches and disease targets. It is not feasible to review all of these here. Newer technologies such as gene editing will undoubtedly shape the design of future gene therapies. In the meantime, there are already a number of promising candidates in early-phase clinical trials that may add to the success of those more advanced gene therapies summarized in chapter 4 (Table 6.1). Several examples are described in the following sections. Neuromuscular and neurological disorders. X-linked myotubular myopathy (XLMTM) is a rare genetic neuromuscular disorder that has more recently been treated by systemic adeno-associated virus (AAV) vector delivery. Although results from more patients and longer-term follow-up data are still emerging, initial clinical trial results are quite remarkable. XLMTM is typically associated with severe muscle weakness. Symptoms are often already present at birth but may also develop later in infancy or early childhood. The defect is in a gene called MTM1 that encodes the enzyme myotubularin, which is required for the development and function of skeletal muscle. In addition to not being able to move on their own, boys born with the disease typically require assisted breathing and feeding. Approximately half of these children die within the first 1.5 years after birth. In a first clinical trial, nine patients aged from 8 months to 6 years were treated by systemic administration of a high dose of AAV8 vector, with the hope of widely transferring the MTM1 gene to muscle cells. Substantial restoration of myotubularin levels and improvements in muscle fiber development were observed on muscle biopsy. Impressively, at least four boys were able to sit up without help; three started to take steps with assistance. Some patients were able to vocalize sounds for the first time and eat food. These advances were presented at the 2019 annual meeting of the American Society of Gene and Cell Therapy. Cerebral adrenoleukodystrophy. Mutations in the ABCD1 gene cause an enzyme deficiency called adrenoleukodystrophy (ALD). As a consequence of the missing enzyme function, very-long-chain fatty acids accumulate, causing demyelination in the central nervous system (CNS) and the adrenal cortex. To initially test whether hematopoietic stem cell (HSC) gene transfer could treat this debilitating neurological disorder, two individuals with adrenoleukodystrophy were enrolled into a study. Lentiviral gene transfer was performed with autologous HSCs, and cells were transplanted following complete myeloablative conditioning. Impressively, these patients showed persistent therapeutic levels of aldolase protein for at least 3 years.
Demyelination of the CNS was halted, resulting in stabilization of disease without any major safety complications. Cerebral ALD affects boys between ages 4 and 10 years, causing permanent disability; death occurs within 4-8 years of disease onset. The lentivirus/HSC approach is now being evaluated in such children in a Phase II/III trial. The absence of major functional disabilities at 24 months after transplantation serves as the primary efficacy endpoint for the study. Major functional disabilities include loss of ability to communicate, cortical blindness, need for tube feeding, total incontinence, wheelchair dependence and complete loss of voluntary movement. Among an initial group of 17 patients with median follow-up of 29.4 months, 15 were alive in 2017 with minimal clinical symptoms. As of early 2018, 29 individuals had received the gene therapy. Huntington's disease is a progressive brain disorder that causes loss of cognitive functions, involuntary movements and emotional instabilities. The underlying genetic problem is the extension of a CAG trinucleotide repeat located in the HTT gene, encoding the huntingtin protein. While the CAG segment is normally repeated 10 to 35 times within the HTT gene, people with 40 or more repeats nearly always develop disease. Huntington's disease usually appears in people in their thirties or forties. The increased size of the CAG segment leads to the synthesis of an abnormally elongated huntingtin protein, which is broken down into smaller toxic fragments that aggregate and accumulate in neurons, thereby disrupting normal cell function and eventually leading to death of neurons. Hope for treatment of this disease comes from preclinical studies using AAV vectors that express not a therapeutic protein but a micro (mi)RNA that targets the HTT messenger (m)RNA and induces its degradation (the miRNA machinery of post-transcriptional regulation has evolved to specifically eliminate unwanted gene expression in a given cell type) (Figure 6.1). For example, an AAV5-miHTT vector was able to decrease production of the huntingtin protein by more than 50% in areas of gene transfer in the brains of minipigs that had been injected with the vector. Intracranial injections led to dose-dependent vector distribution to the striatum, putamen and spreading to the cerebral cortex. These areas are affected by neuropathological changes at different stages of the disease. Using this approach, sustained lowering of huntingtin protein levels and functional improvements have been demonstrated in a mouse model of the disease. For clinical trial design, high-resolution MRI scans of people with Huntington's disease serve to identify injection sites for safe and hopefully efficient delivery of the vector to the desired target regions of the brain. A Phase I/II trial is under way.
Lysosomal storage disorders. Lysosomal storage disorders (LSDs) are rare inherited metabolic diseases that result from deficiencies in enzymes that normally aid in the degradation of molecules such as lipids or glycogen in lysosomal organelles, thereby preventing accumulation of toxic materials. Among the nearly 50 known LSDs, glycogen storage disease (GSD)Ia is one of the most common. Glycogen storage disease type Ia. Mutations in the gene for the enzyme glucose 6-phosphatase-alpha (G6Pase-alpha) may result in the inability to degrade the polysaccharide glycogen. The accumulation of glycogen affects functions of multiple tissues. Children with GSDIa may have thin arms and legs, short stature and suffer from enlarged livers and kidneys. The hypoglycemia that results from the disease can be life-threatening. If chronically untreated, severe lactic acidosis, progression to renal failure and even death may occur in infancy or childhood. While there are no pharmacological therapies, consumption of sugar-free foods and drinks and regular intake of uncooked cornstarch, combined with other dietary adjustments, is helpful, particularly in milder forms of the disease. An investigational gene therapy is currently being evaluated in a Phase I/II trial, based on systemic delivery of an AAV8 vector with hopes of achieving stable expression and activity of G6Pase-alpha under control of its native gene promoter. In preclinical studies, this gene therapy was particularly effective in reducing hepatic glycogen levels. Clinical efficacy measures include time to hypoglycemia during a controlled in-hospital fasting challenge, glycogen storage in liver (as determined by MRI) and changes in biomarkers such as lipids and uric acid. It has been reported that 'two of three patients in the first dose cohort experienced a sufficient increase in time to hypoglycemia such that they may be able to sleep through the night without taking supplemental cornstarch'. Improved glucose control throughout the day and the ability to decrease daily cornstarch intake are further encouraging signs that the therapy is effective. Muscle disorders. Duchenne muscular dystrophy (DMD) is the X-linked lethal muscle-wasting disease caused by mutations in the gene encoding dystrophin. Approximately 1 in 5000 boys is born with DMD. The rod-shaped cytoplasmic protein dystrophin is a critical component of a protein complex that connects the cytoskeleton of muscle fibers to the surrounding extracellular matrix through the cell membrane, thereby supporting muscle fiber strength. Individuals with deleterious mutations in dystrophin typically show signs of muscle weakness at 2 to 3 years of age, followed by substantial deterioration of muscle function by 7 years of age.
Most people die in the second to third decade of their life because of diaphragm and/or cardiac failure. Correction of the disease by gene transfer is substantially complicated by the large size of the dystrophin gene (covering 2.3 mb, making it one of the largest known genes) and the need to correct a large number of muscles throughout the body. The dystrophin protein is made up of 3685 amino acid residues. Seeking a way out of this dilemma, several laboratories have taken advantage of the fact that the dystrophin protein comprises structural repeats. Recombinant shortened forms of dystrophin with fewer repeat structures may not provide quite the same strength, but in animal studies these were nonetheless found to compensate substantially for the lack of wild-type dystrophin. Therefore, one approach is to use antisense oligonucleotides or phosphorodiamidate morpholino oligomers that affect mRNA splicing and promote skipping of exons, thereby producing a truncated dystrophin. A more universally applicable and, moreover, lasting correction can be achieved by expressing 'mini'- or 'micro'-dystrophins from an AAV vector. These can be engineered in such a way that the packaging capacity of the vector is not exceeded. Using these constructs, dystrophin transgene expression in muscle fibers of patients has already been demonstrated following local intramuscular administration of vector. Furthermore, AAV capsids (such as AAV9 or AAVrh10) have been identified that are capable of infecting muscles throughout the body following high-dose intravascular administration. Intravascular delivery of AAV vectors expressing microdystrophins significantly ameliorated muscle pathology, increased muscle force and attenuated dystrophic cardiomyopathy in various small and large animal models such as dystrophic mice and dogs. Multiple competing Phase I/II clinical trials have now been initiated to test this approach in patients with DMD (Figure 6.2). These utilize AAV9 or AAVrh74 and muscle-specific promoters. Given the very large vector doses that are required (> 10 vector genomes/kg), there are some concerns for toxicities and immune responses. For instance, dystrophic muscles may be inflamed, and T-cell responses against therapeutic dystrophin have been observed in some patients in prior clinical studies. While steroids are given to limit inflammation, incidents of complement activation have also been reported. Still, the field is cautiously optimistic that these problems can be solved and that a lasting therapeutic effect can be accomplished in this challenging disease.
Mutations in globin genes can cause various forms of anemia. Deficiency in either alpha-globin or beta-globin causes an imbalance in the adult globin chains, resulting in alpha- or beta-thalassemia. Historically, the severe homozygous condition has been referred to as thalassemia major, in contrast to the less severe heterozygous condition termed thalassemia minor. The thalassemias are the most common monogenic diseases in the human population, with the highest gene frequencies occurring throughout the Mediterranean, the Middle East, the Indian subcontinent, parts of Southeast Asia and the Pacific Islands. No ideal treatments are available, making these diseases major contributors to morbidity and mortality worldwide. Current management for thalassemia involves frequent blood transfusions and chelation therapy to prevent the accumulation of iron. Bone marrow transplant is theoretically curative but requires a matched donor. Gene therapy based on lentiviral transduction of HSCs should be a route to a cure for these diseases. However, there is no selective advantage for gene-corrected cells, and obtaining sufficient levels of the globin transgene product has not been easy to accomplish. The first success in correcting beta-thalassemia in humans was based on reinfusion of autologous CD34+ HSCs that had been transduced with a self-inactivating lentiviral vector encoding a functional copy of the beta-globin. Myeloablative conditioning was performed to create space for the gene-corrected cells. The first patient treated in this manner gained independence from blood transfusions by 1 year after gene therapy. Vector-derived hemoglobin accounted for around 30% of total hemoglobin and levels remained stable for more than 8 years. As explained earlier (see page 46), a dominant clone with an insertion inside the HMGA2 gene emerged that reached a plateau between years 1 and 3, but subsequently declined. More recent clinical trials have utilized lentiviral vectors with increased yield and modified treatment and mobilization regimens to improve HSC harvest and have applied more rigorous myeloablative conditioning to ensure engraftment. For example, stem cells were mobilized with granulocyte colony-stimulating factor (G-CSF) and plerixafor. A myeloablative conditioning regimen with reduced toxicity has also been introduced (using treosulfan and thiotepa rather than busulfan). Results from more than 20 patients indicate that individuals with betaE/beta0 thalassemia, who have residual endogenous expression of functional beta-globin chain (in the form of the betaE variant) benefited more strongly from gene therapy and mostly achieved sustained transfusion independence, reaching nearly normal hemoglobin levels in a few cases. In contrast, few individuals with b0/b0 thalassemia, wholly lacking beta-globin function, achieved transfusion independence, though some had a reduction. Sickle cell disease. In individuals with sickle cell disease (SCD), the substitution of valine for glutamate at position 6 of the beta-globin protein generates an abnormal form of hemoglobin, called hemoglobin S or sickle hemoglobin. As a result of deoxygenation-induced hemoglobin S polymerization, red blood cells appear 'sickle-shaped', lack flexibility and adhere to vessel walls. This cellular sickling increases blood viscosity and hemolysis, causes pain and can block blood vessels, with risk of stroke and multiorgan damage due to lack of oxygen supply to tissues. Gene transfer of a beta-globin sequence with a mutation that confers 'anti-sickling' properties (betaA-T87Q) has been developed. The approach is again based on lentiviral gene transfer to CD34+ HSCs. A therapeutic beta-globin level of around 50% was achieved in a 13 year old with SCD who had not responded to treatment with hydroxyuria (used to increase fetal globin gene expression, which can somewhat counter the effects of the sickle beta-globin). This individual no longer required red blood cell transfusions and had a substantial clinical improvement that has been stable for at least 4 years. Several current clinical trials are now attempting to reproduce this positive outcome. Alternative strategies include the erythrocyte-specific expression of a small interfering (si)RNA to silence a repressor for fetal globin (BCL11A), as well as gene-editing approaches. 
Pharmacological management. Starting treatment. Several questions need to be addressed when deciding whether to prescribe an antiepileptic drug (AED) to a patient presenting after one or more seizures. What is the chance of recurrence?. What are the potential negative consequences on the patient's life if seizures recur?. What are the potential adverse effects of treatment?. After a single seizure. Whether treatment should be started after a single episode remains controversial. Depending on study methods and inclusion criteria, the probability of recurrence over the next 5 years after a single unprovoked seizure ranges from 31% to 71%. As a substantial proportion of such patients will not have further episodes, most specialists do not routinely recommend treatment after a single seizure. Prospective randomized studies have shown that, compared with delaying treatment until a further episode, immediate treatment after a first generalized tonic-clonic seizure (GTCS) does not improve the long-term remission rate. However, treatment should be considered after the first seizure when the chance of recurrence is high - for instance, in the presence of an underlying cerebral lesion, an abnormal EEG or a strong family history of epilepsy, or if the patient has an epilepsy syndrome such as juvenile myoclonic epilepsy (JME) that is characterized by a high likelihood of seizure recurrence. In some instances, the patient may wish to start treatment after a single event because they are concerned about the potentially significant impact that recurrent seizures could have on their lifestyle, such as their ability to legally drive a car. After more than one seizure. Generally speaking, patients reporting more than one well-documented or witnessed seizure require treatment. Exceptions can include widely separated seizures, provoked seizures for which specific treatments or avoidance activity may be sufficient prophylaxis (exempli gratia concomitant illness such as infection or metabolic disturbance, photosensitive epilepsy, alcohol withdrawal) and certain benign childhood epilepsy syndromes such as benign rolandic epilepsy. In addition, treatment is unlikely to succeed in patients unlikely or unwilling to take medication as prescribed (exempli gratia some alcohol abusers or drug addicts, or people who refuse to take medication on principle). An informed choice. The decision whether or not to start treatment should be made after full discussion with the patient and their family of the risks and benefits of both courses of action. The information should be presented to the patient in the context of what is known and what is conjecture about the risk of recurrent seizures, the chance of a successful outcome with treatment and the likelihood of remission. Pushing the issue if there is doubt about the diagnosis, particularly if the patient resists the introduction of AED therapy, may be counterproductive. Ideally, the patient and their immediate family should be encouraged to make an informed commitment to the treatment plan. Reasons for taking prophylactic therapy should be discussed at the outset. When prescribing an AED, the clinician must also discuss all common side effects, as well as uncommon but serious drug-related problems such as the risk of teratogenesis in women of childbearing potential. That this particular risk has been touched upon should be documented in the patient's case notes. Similarly, the regulations regarding driving should be raised and documented. Time should be taken to deal with the patient's fears, misconceptions and prejudices, as well as those of the family. The importance of total compliance with medication should also be stressed. These issues often require further emphasis at subsequent visits. The possibility of sudden unexpected death should be touched upon, especially if compliance is an issue or if seizures remain uncontrolled (see Mortality). The provision of written material can be a useful way to ensure that nothing important has been overlooked. 
Principles of drug selection. The goal of treatment should be to enable the patient to lead as normal a lifestyle as possible, which generally requires complete seizure control without, or with minimal, side effects. Choosing the most suitable AED for an individual patient requires knowledge of the characteristics of the epilepsy, the patient and the available AEDs. The issues discussed below should be included in the decision-making process. In comparison with combination therapy, monotherapy is associated with better compliance and fewer side effects. It is therefore also likely to be more cost-effective. For these reasons, in general, serial monotherapy trials of two AEDs that are appropriate first-line treatment for the patient's seizure type(s) should be undertaken before combinations are tried (Figure 4.1). The chance of remission is highest with the first drug - 60% of patients with newly diagnosed epilepsy achieve seizure control with the first or second AED. Substantial attention should therefore be given to choosing the most appropriate initial AED, taking into account a range of factors, including the seizure type(s) and/or epilepsy syndrome. Other relevant issues include age, sex, weight, psychiatric and other comorbidities, childbearing potential and concomitant medication. Efficacy and tolerability. AED effectiveness is a function of efficacy and tolerability. Given that lifelong treatment is often required, even in patients with mild epilepsy, safety and lack of long-term sequelae are also important considerations in addition to effectiveness when selecting treatment. Titration and monitoring. Approximately 50% of newly diagnosed patients will be able to tolerate and become seizure free with the first AED, often in low or moderate doses. In general, the AED should be started at a low dose, with increments over a number of weeks to establish an effective and tolerable regimen, although some agents, such as sodium valproate (VPA) and levetiracetam (LEV), can be commenced at effective doses with, or even without, a rapid titration phase. Slow titration will help avoid concentration-dependent side effects as with carbamazepine (CBZ) or topiramate (TPM), in particular central nervous system toxicity, the presence of which is likely to discourage the patient from persevering with therapy in the long term. An additional benefit of a cautious approach is that it allows the development of tolerance to sedation or cognitive impairment. Such a policy will also ensure early detection of potentially serious idiosyncratic reactions, such as rash, hepatotoxicity and blood dyscrasias (see Side effects). Slow titration with lamotrigine (LTG) has been shown to reduce the risk of skin rash. Measuring serum AED concentrations can help to determine the extent of compliance, assess side effects and establish the most effective dose for a seizure-free patient. Serum AED concentrations associated with optimal control or with neurotoxicity vary from patient to patient and may occur below, within or above the so-called 'therapeutic' or 'target' ranges for the drugs, particularly in children and the elderly. These ranges should be regarded, therefore, purely as a guide to prescribing. Routine measurement of serum levels of the newer AEDs is not otherwise recommended, as they do not correlate well on a population basis with efficacy or side effects. Measurement of free serum phenytoin (PHT) concentrations can occasionally be useful when patients have low serum albumin levels or take other medications that bind tightly to protein. Women who experience an exacerbation of seizures just before their menses should have serum AED concentrations checked in the premenstrual period and compared with mid-cycle concentrations, as levels can drop markedly just before and during menstruation. This can be a particular problem with LTG. If the first drug is well tolerated but the seizures persist, the dose should be increased towards the limit of tolerability, aiming for complete seizure freedom.
If the first AED produces an idiosyncratic reaction or side effects at low or moderate doses, or fails to improve seizure control an alternative drug should be substituted. Matching treatment to seizure type. The profile of activity against different seizure types differs among the AEDs (Tables 4.1 and 4.2). Certain epilepsy syndromes have been found to be particularly responsive to specific therapeutic agents. For instance, JME responds well to VPA, while many pediatricians regard vigabatrin (VGB) as the drug of choice for infantile spasms associated with tuberous sclerosis. On the other hand, myoclonic and absence seizures can be exacerbated by PHT, CBZ, gabapentin (GBP), pregabalin (PGB), oxcarbazepine (OXC), tiagabine (TGB), eslicarbazepine acetate (ESL) and VGB. It is therefore of paramount importance to classify the patient's seizure type(s) and epilepsy syndrome accurately (see Chapter 2). Recommended drug choices for adults and children according to seizure types are shown in Tables 4.3 and 4.4, respectively. The recommendations are based on the current literature and UK and US treatment guidelines. Because of the paucity of head-to-head comparative studies, particularly for the newer AEDs, the evidence base is supplemented by the authors' personal experience. For partial seizures and GTCS (the most common seizure types), the established AEDs, with the exception of ethosuximide (ESM), appear to have similar efficacy. There is a possible small benefit of CBZ over VPA for partial seizures. Phenobarbital (PB) and primidone (PRM) have demonstrated higher withdrawal rates because of their sedative effects at higher dosages. None of the newer AEDs has shown superior efficacy when tested against the established agents for the treatment of partial seizures and GTCS, but some have demonstrated better tolerability, in particular fewer neurotoxic side effects (Table 4.5). Thus, LTG and OXC have shown better overall effectiveness than CBZ and PHT, respectively, for partial epilepsy whereas VPA may be more efficacious than LTG for some generalized epilepsies. Side effects. Safety concerns include idiosyncratic reactions, long-term complications and teratogenicity. The most common idiosyncratic reaction to AEDs is skin rash, which can range from a trivial evanescent exanthema (up to 5-10%) to rare but life-threatening severe cutaneous reactions (Stevens-Johnson syndrome or toxic epidermal necrolysis), usually within 8 weeks of initiating treatment. Overwhelming evidence from recent studies found that carriers of the HLA-B*15:02 allele have a greatly increased risk of developing severe cutaneous reactions after taking CBZ. This allele is particularly prevalent (10-15%) in many Asian populations (including southern China, Taiwan, Thailand and Malaysia) but is rare (< 2%) in white populations. As a result of these findings, the US Food and Drug Administration (FDA), UK Medicines and Healthcare products Regulatory Agency (MHRA) and other national health regulators recommend testing for HLA-B*15:02 in individuals with ancestry from these areas before CBZ is initiated. The drug should be avoided if the patient is a carrier of this allele. Drug labeling has been amended by manufacturers accordingly. Until definitive evidence becomes available, clinicians should also consider avoiding the use of other AEDs associated with a high risk of severe cutaneous reactions (PHT, PB, LTG and OXC) in individuals known to have the allele. A similar association with HLA-A*31:01 has recently been reported in European and Japanese patients.
Felbamate (FBM) is associated with other idiosyncratic reactions including aplastic anemia and hepatotoxicity; as a consequence, it is relegated to the drug of last choice. Rare cases of acute glaucoma and clinically important hypohydrosis can complicate TPM administration; the latter has also been reported for zonisamide (ZNS). Long-term use of some AEDs can lead to dysmorphic changes, such as gum hypertrophy with PHT, and weight gain with VPA, GBP and PGB. TPM and ZNS, on the other hand, often produce weight loss. VPA can also be associated with polycystic ovaries and hyperinsulinemia in susceptible women. The high incidence of concentric visual field defects in patients receiving VGB has substantially reduced the clinical use of this otherwise effective agent. Certain AEDs, such as PB, TPM, ZNS and VGB, are more commonly associated with neuropsychiatric complications and should be used cautiously in patients with a history of mental illness. LEV can particularly produce emotional agitation and aggression. The established AEDs have all been shown to increase the likelihood of fetal malformation. Analysis of several large-scale prospective registries suggests that the risk of major congenital malformation may be particularly high with VPA and perhaps also with PB compared with other established agents. Dose-dependent risk of major malformations has been associated with VPA, CBZ, PB and LTG administration. An association with facial clefts has been reported with TPM. Data regarding the newer AEDs are accumulating, and the teratogenic risk associated with LEV appears to be lower than with CBZ. Cognitive impairment may occur in children who were exposed to VPA in utero. Pharmacokinetics and drug-drug interactions. An ideal AED should demonstrate complete absorption, linear kinetics and a long elimination half-life, allowing once- or twice-daily dosing. Low protein binding, lack of active metabolites and clearance by the renal route can also be regarded as advantageous, as a drug with these characteristics is likely to be easy to use and less likely to be implicated in pharmacokinetic interactions. However, the dose for drugs that are excreted unchanged by the kidney, such as GBP and PGB, will need to be adjusted in patients with renal impairment in relation to creatinine clearance. Older AEDs are notorious for their ability to produce pharmacokinetic interactions among themselves as well as with other medications via their effect on the hepatic cytochrome P450 (CYP) enzyme system (Table 4.6). PB, PRM, PHT and CBZ induce CYP enzymes that accelerate the breakdown of many commonly prescribed lipid-soluble drugs metabolized by the same system, including oral contraceptives, cytotoxics, antiretrovirals, cardiac antiarrhythmics, immunosuppressants and warfarin. VPA is a weak CYP enzyme inhibitor, and as such can slow the clearance of other AEDs such as PHT and LTG. AEDs can also be targets for drugs other than AEDs that induce or inhibit hepatic metabolism. The newer AEDs are less likely to interfere with hepatic metabolism, although OXC, FBM, rufinamide (RFN) and ESL, and TPM at daily doses above 200 mg, selectively induce the breakdown of the estrogenic component of the oral contraceptive pill. As well as controlling seizures, some AEDs have demonstrated efficacy for the treatment of other conditions that may coexist with epilepsy (Table 4.7). For instance, VPA has traditionally been used in bipolar affective disorder. It is also effective prophylaxis for migraine, an indication for which TPM has also been approved.
GBP is effective for the treatment of certain neuropathic pain syndromes, while PGB has demonstrated efficacy for neuropathic pain and generalized anxiety disorder. LTG has been licensed for bipolar disorder. With a widening spectrum of indications, AED selection may be tailored according to the patient's neurological and psychiatric comorbidities (see Chapter 9, Quality of life). Bone health. Long-term AED therapy can lead to hypocalcemia and decrease biologically active vitamin D levels, resulting in reduced bone mineral density and higher risk of fractures. Most of the available data pertain to the older drugs, but information regarding newer agents is emerging. Both enzyme-inducing and non-enzyme-inducing agents are implicated, and the effects may be additive. A variety of mechanisms for AED-induced osteoporosis have been suggested, the most important of which appears to be an increased rate of bone turnover. Bone loss can be detected by dual-energy X-ray absorptiometry (DEXA) or quantitative ultrasound. To reduce the risk of osteoporosis, patients taking long-term AED therapy are advised to maintain the optimal level of physical activity, a balanced diet and exposure to sunshine. They should be advised against smoking and excessive intake of alcohol or caffeine, all of which can exacerbate bone loss. Risk factors for osteoporosis include. prolonged AED therapy. exposure to multiple drugs. a non-ambulatory lifestyle. concomitant corticosteroid therapy. People at risk are advised to take calcium and vitamin D supplements and undergo regular DEXA scans. Once osteopenia or osteoporosis has developed, the patient should be referred to an endocrinologist for appropriate therapy (see Fast Facts: Osteoporosis). Late in 2008, the FDA announced it would require warnings concerning suicidal thoughts or behavior in the prescribing information for all AEDs based on its meta-analysis of a large number of controlled clinical trials involving the newer AEDs. The FDA's announcement put the increased risk in perspective, saying that it amounted to one additional case of suicidal thoughts or behavior for every 500 patients treated with AEDs compared with placebo. As more data become available, comorbid depression is increasingly being implicated as a causative factor rather than the AEDs themselves. Nonetheless, clinicians should monitor patients closely for suicidal ideation when starting or increasing doses of AEDs. Combination therapy can be considered if two successive monotherapy attempts with first-line AEDs are ineffective, as the chance of successful seizure control with a third choice of monotherapy is slim. If the first AED produces a good response but complete seizure freedom remains elusive, adding a drug with a different mechanism of action may be a more pragmatic strategy than substitution for some patients. Combination therapy reduces seizure frequency or severity in some patients. The number of possible two-drug regimens is growing rapidly. Some evidence supports the suggestion that combinations involving a sodium channel blocker and a drug that facilitates the inhibition of gamma-aminobutyric acid (GABA) or a drug with multiple mechanisms of action are particularly beneficial. The only combination, however, for which there is hard evidence of synergism is that of VPA and LTG. Some useful combinations are listed in Table 4.8. The practical difficulty with combination therapy is that troublesome or disabling side effects are common at high doses, and complex pharmacokinetic interactions can occur. Consequently, it is advisable to combine drugs with different side-effect profiles and those that do not have the potential for deleterious drug interactions. Practical guidelines for prescribing AEDs are summarized in Table 4.9.
Drug-resistant epilepsy. The International League Against Epilepsy (ILAE) defines drug-resistant epilepsy as 'a failure of adequate trials of two tolerated and appropriately chosen and used AED schedules (whether as monotherapies or in combination) to achieve sustained seizure freedom'. An online classifier, intended to aid the application of the ILAE definition of drug-resistant epilepsy, is available at www. Fulfillment of the definition in a patient should prompt a comprehensive review of the diagnosis and management, preferably by an epilepsy specialist. Work-up for epilepsy surgery can be considered at this point, particularly if a potentially operable structural abnormality, such as mesial temporal sclerosis, has been identified. This is also a good time to evaluate whether there may be any factor responsible for a state of 'pseudoresistance' (Table 4.10), by reviewing. security of the diagnosis. accuracy of the seizure and/or syndrome classification. results of brain imaging. compliance with medication. the presence of negative lifestyle factors, such as erratic sleeping habits or covert alcohol or drug abuse. If the first AED combination is ineffective and epilepsy surgery is not an option, a sequence of drug combinations with potential complementary modes of action could be tried. Data to guide further pharmacological management are lacking. A small proportion of patients will become seizure free with three AEDs, but treatment with four or more is highly unlikely to be tolerated and therefore successful. Drug burden is a function of dose as well as number of drugs, and so further introductions may be made possible by reducing the dose of one or more AEDs. Successful treatment outcome can be regarded as freedom from seizures without side effects. Individuals in whom treatment is successful are more likely to lead rewarding lives - with optimal intellectual and emotional development, and positive educational and vocational achievements - than patients with uncontrolled, particularly daytime, seizures. In short, they will have a better chance of fulfilling their potential. Eventually, many patients can have their medication withdrawn and remain in remission. Patients who are 'doing well' may want to stop treatment for a variety of reasons, including the awareness of side effects or the subjective perception of subtle deterioration in cognitive function. Some patients do not equate taking medication with normal health. Finally, the patient may want to start a family and may be concerned about the possible negative effects of AEDs on reproductive function, along with the specter of teratogenesis. Several studies have shown that after a long period of perfect seizure control, medication can be stopped without seizure recurrence (for several years at least) in around 60% of patients. There are no data to guide the length of the seizure-free period. In children, 2 years of seizure freedom is reasonable before considering AED withdrawal. Although there is no defined timescale, a flexible 5-year seizure-free period would be more prudent in adults. Seizure type or epilepsy syndrome is not absolutely predictive of recurrence. However, a few specific childhood syndromes, such as benign rolandic epilepsy and benign familial neonatal convulsions, tend to do well after drug withdrawal, whereas JME has a high probability of relapse.
Some forms of idiopathic generalized seizures, either absence or tonic-clonic, are less likely to recur once they are under control. Even complex partial seizures can disappear after a long period of freedom from seizures. The highest probability of remaining seizure free can be seen in patients with the following characteristics. relatively few seizures before and after starting AED therapy. treatment with a single AED. seizure free for many years. normal neurological examination. no structural lesion on brain imaging. The EEG is not a huge help in predicting seizure recurrence, but a normal investigation is reassuring. There are no standard protocols defining optimal regimens for tapering medication. Most specialists advise slow reduction by increments over at least 6 months. If the patient is taking two AEDs, one drug should be slowly withdrawn before the second is tapered. More than 90% of recurrences will occur during the year following withdrawal, and many will present during or shortly after the tapering period. Referral to a specialist. The primary goal of epilepsy management is to restore the patient's functional capacity to its maximal potential. Attaining this goal is often a team effort involving medical and social service professionals and the patient's family, friends and coworkers. The role of the primary care provider varies according to the clinical setting, their experience and the patient's needs. Primary care physicians must be familiar with all the diagnostic and therapeutic options, because they will usually perform the initial evaluation of the first seizure, primarily to exclude non-epileptic causes such as syncope and hypoglycemia. Even experienced doctors may not feel qualified, however, to assume full responsibility for the diagnosis, planning and follow-up of patients with epilepsy. Often, they do not have direct access to the necessary investigational techniques. If a non-epileptic cause of the symptoms is ruled out, the patient should see a neurologist or other appropriate specialist for further diagnostic studies to determine the likelihood of further seizures and to consider the need for, and choice of, AED therapy. Dose adjustments can be undertaken later by the primary care physician. The patient should be referred back to the epilepsy specialist if. seizures are unresponsive to the first two AED schedules. treatment causes significant side effects. a pregnancy is being planned. he or she is considering stopping therapy. Attending to the patient's psychosocial, cognitive, educational and vocational needs is an important part of caring for people with epilepsy. Both the primary care physician and the epilepsy specialist should work closely with other medical and social service professionals, and extend their roles beyond that of clinician to educator and advocate. Subsequent referral to a comprehensive epilepsy center for EEG monitoring, investigational drugs or devices, or consideration for epilepsy surgery is indicated for compliant patients whose seizures prove resistant to two or three reasonable attempts at pharmacological manipulation using new and established AEDs singly and in combination.
Hypertension and diabetic nephropathy. Hypertension is the most common chronic disease in the Western world; by the age of 60 years, over 50% of the population will have developed high blood pressure (see. Fast Facts: Hypertension). Hypertension is even more common among patients with chronic kidney disease (CKD); by the time patients develop end-stage kidney disease (ESKD), over 80% have elevated blood pressure, and hypertension accounts for approximately 20% of all cases of kidney failure. After diabetes mellitus, hypertension is the second leading cause of kidney failure in the USA, though it is less common as a formal cause of ESKD in Europe. Hypertension is arbitrarily defined as a pressure of 140/90 mmHg or more (Table 5.1); a new category of risk termed prehypertension was created in the USA in guidelines from the Seventh Report of the Joint National Committee on Prevention, Detection, Evaluation and Treatment of High Blood Pressure (JNC 7). After the age of 60 years, most of the population either has hypertension or is at risk of developing it. However, only 59% of these individuals receive treatment, and of these, only 34% are well controlled. The situation is much worse for individuals with CKD; only 14% achieve a blood pressure level of 140/90 mmHg or lower. Essential hypertension represents an elevation in systemic arterial blood pressure in the absence of a known etiology and accounts for 90% of cases, with most of the remainder being caused by renal disease. Endocrine or metabolic causes are rare. Hypertension associated with parenchymal kidney disease represents a potent vicious cycle; it is both a consequence of CKD and a cause of progressive kidney damage. Evidence from many studies shows that treatment of hypertension is crucial to slowing progression of renal disease, particularly among those with significant albuminuria (> 1 g/24 hours). The major complications associated with elevated blood pressure are an increased risk of cardiovascular and cerebrovascular disease, kidney failure (Figure 5.1), retinopathy and peripheral vascular disease. Since hypertension is a common chronic condition, the evaluation and treatment of comorbidities associated with hypertension should be an integral part of management. In particular, evidence from the Anglo-Scandinavian Cardiac Outcome Trial (ASCOT) attests to the importance of treating hypertensive patients with a statin. In this study, among hypertensive patients with normal or only moderately raised levels of cholesterol, patients receiving atorvastatin were 30% less likely to suffer coronary events and 27% less likely to have a stroke. The widespread availability of ambulatory electronic and manual devices and the proliferation of automated devices have made the standardization of blood pressure measurement critical to diagnosis and subsequent follow-up. Blood pressure should be measured using the correct size of cuff, with the patient at rest in the correct position, either seated or supine (see Fast Facts: Hypertension). Two measurements should be taken at least 2 minutes apart in order to reduce variability. In addition, blood pressure should be measured two or three times, separated by an interval of several weeks before a definitive diagnosis is made, unless the initial measurement is sufficiently high (> 180/100 mmHg) to justify immediate treatment. The UK National Institute for Health and Clinical Excellence (NICE) guidelines indeed suggest that 24-hour ambulatory measurement of blood pressure in primary care is the most clinically and cost-effective method of confirming hypertension by avoiding false positive results and repeat office visits. Investigations should be undertaken to identify the underlying cause, assess the extent of target organ damage, and recognize the presence of comorbid conditions that should influence the aggressiveness of therapy.
Routine laboratory evaluation should include urinalysis, blood glucose, complete blood count, urea and electrolytes, lipid profile and an ECG. In most patients, urine protein measurement is a useful indicator of kidney involvement. Screening for microalbuminuria is recommended in type 2 diabetics at the time of diagnosis and in type 1 diabetics at 5 years after the initial diagnosis. Antihypertensive therapy is beneficial in reducing both cardiovascular and renal events, as well as lowering mortality. In patients with CKD, treating hypertension is important in slowing disease progression and in reducing cardiovascular risk. In patients with ESKD, the focus should be directed towards reducing cardiovascular morbidity. Patients with albuminuric CKD (> 500 mg albumin/24 hours) require aggressive management of blood pressure, with a target of less than 130/80 mmHg. A more relaxed target of less than 140/90 mmHg is recommended for patients who are normoalbuminuric. First-line therapy should comprise lifestyle changes, such as reducing salt intake, moderating alcohol intake, stopping smoking and taking regular exercise, which together can lower blood pressure by 10/5 mmHg. An angiotensin-converting enzyme (ACE) inhibitor or an angiotensin- receptor blocker (ARB) is recommended as first-line therapy in patients with CKD (Figure 5.2). However, usually more than one drug is required, and a beta-blocker, a calcium-channel blocker or a diuretic agent can be added. The key features of several commonly used antihypertensive drug classes are shown in Table 5.2. The choice of drugs can be influenced by comorbid conditions, as shown in Table 5.3. Studies such as the Antihypertensive and Lipid-Lowering Treatment to Prevent Heart Attack Trial (ALLHAT) have shown that achieving optimal blood pressure goals requires multiple drugs, that control of blood pressure using a stepped approach can take up to 2 years and that, in simple essential hypertension, the newer, more expensive drugs do not offer great benefits over thiazide diuretics. Renovascular disease is a remediable form of hypertension and should be excluded in high-risk patients, such as elderly hypertensives with evidence of diffuse atherosclerosis, in refractory or malignant hypertension, in those with 'flash' pulmonary edema and in individuals with an abdominal bruit. In young women with hypertension of recent onset, fibromuscular renal artery disease should be excluded. The preferred diagnostic tests include magnetic resonance angiography and ACE-inhibitor renography. Duplex ultrasonography with Doppler flow measurements can be a useful screening test but is rather operator dependent, and in many patients the renal arteries cannot be visualized. The definitive diagnostic tests in almost all patients are still digital subtraction angiography or arteriography (Figure 5.3), but both carry a risk of contrast nephropathy and cholesterol embolization. Magnetic resonance angiography with gadolinium is not recommended in patients with an estimated glomerular filtration rate (GFR) of less than 60 mL/min/1.73m because of the risk of gadolinium-associated nephrogenic skin fibrosis, although the risk is very small and possibly dependent on the type of gadolinium used. Angioplasty (with or without renal artery stenting) does not benefit all patients, as many have diffuse vascular disease including involvement of intrarenal blood vessels, and intervention carries risks (see above).
In fact, controlled trials have been unable to demonstrate an overall benefit of intervention compared with aggressive medical therapy. Patients unlikely to have a positive response (and perhaps all patients) should receive aggressive medical therapy to control blood pressure, together with statins and acetylsalicylic acid (ASA; aspirin). Surgery to bypass the stenotic lesion is now reserved for lesions that cannot be treated by angioplasty. Hypertensive emergencies are situations in which an immediate reduction in blood pressure is required in order to prevent or treat acute, progressive, target-organ damage. In such cases, a thorough history and physical examination should be undertaken (Table 5.4), including fundoscopy and urine tests for hematuria, proteinuria and red-cell casts. Other investigations should include tests for microangiopathic hemolytic anemia with red-cell fragmentation, kidney dysfunction, evidence of left ventricular hypertrophy and strain, and ischemia on electrocardiography and echocardiography. The management of a hypertensive emergency requires immediate hospitalization, usually in an intensive care unit, with arterial blood pressure monitoring, central vein catheterization and, occasionally, the use of parenteral drugs such as nitroprusside, glyceryl trinitrate, hydralazine (apresoline), labetalol or fenoldopam (this last not licensed in the UK). Parenteral drugs are rarely needed, since blood pressure can usually be controlled with oral agents such as captopril (fast acting), labetalol (both alpha- and beta-blockade) and clonidine. Hypertension during pregnancy is defined as any rise in systolic blood pressure of more than 30 mmHg or a rise in diastolic blood pressure of more than 15 mmHg above baseline, or the use of antihypertensive agents. It is classified according to its presentation (Table 5.5). Chronic hypertension is more common in multiparous women, and is present at the first antenatal visit. On the other hand, pre-eclampsia is more common in primigravidas (in 10% of first pregnancies), and represents an important cause of maternal and perinatal mortality. It usually presents only after 20 weeks of gestation, with or without proteinuria and a raised serum urate. Elevated levels of soluble fms-like tyrosine kinase-1 (sFlt-1 or sVEGFR-1) and endoglin have been reported in pre-eclampsia and have been implicated in disease pathogenesis. Pre-eclampsia may progress to full-blown eclampsia, which is characterized by seizures and also associated with acute kidney injury (AKI). Management of eclampsia comprises immediate delivery, and magnesium sulfate, anticonvulsant and antihypertensive therapy. Antihypertensive agents must be reviewed in women with renal disease who wish to get pregnant. Methyldopa, labetalol, hydralazine and nifedipine are all safe drugs for treating hypertension in pregnancy, but diuretics should be avoided. ACE inhibitors are contraindicated from the second trimester, but may be important in controlling the progression of CKD. Women may therefore continue to take these as they plan a pregnancy but must stop when pregnant.
Diabetic nephropathy. Diabetic nephropathy is the most common cause of ESKD in Europe and the USA. Its prevalence is increasing as patients with diabetes are living longer and the complications of diabetes are better controlled. In addition, those who require renal replacement therapy are now accepted onto dialysis programs from which they had previously been excluded. There is considerable racial/ethnic variability in the incidence of diabetes (see Fast Facts: Diabetes Mellitus). In the USA, diabetes, and thus ESKD, is more common among native Americans, Hispanics (especially Mexican-Americans), African-Americans, Asian Indians and South Asians (Figure 5.4). ESKD secondary to diabetes mellitus is seen in 45% of patients in the USA, compared with 15% in the UK. Diabetic nephropathy is defined by the presence of albuminuria. Overt nephropathy is characterized by albuminuria of more than 300 mg/24 hours or 200 g/minute; in addition, hypertension and kidney dysfunction, with a progressive decline in kidney function over time, may be present. The pathology of nephropathy in type 1 and type 2 diabetes is identical and comprises glomerular hypertrophy, glomerular basement membrane thickening, mesangial matrix accumulation and nodular glomerulosclerosis (Figure 5.5). Natural history (Table 5.6). The earliest clinical evidence of nephropathy is the appearance of microalbuminuria, which is a low but abnormal amount of albumin in the urine (> 30 mg/24 hours or 20 g/minute). Microalbuminuria can also be detected by an increased albumin:creatinine ratio in a spot urine sample (> 2.5 mg/mmol in men or 3.5 mg/mmol in women), which avoids the need for timed urine collections. Patients with early disease have incipient nephropathy and without specific interventions may progress to overt or clinical albuminuria over a period of 2-10 years. Most patients will also develop hypertension, which initially manifests as the absence of a nocturnal dip in blood pressure, but later becomes sustained hypertension. The degree of albuminuria varies quite significantly, but not uncommonly reaches nephrotic levels (> 3.5 g/24 hours). Once clinical albuminuria has developed, the GFR often declines relentlessly by 5-15 mL/minute/year. The mortality of patients with diabetic nephropathy is higher than in other patients because of a four- to eightfold increase in the rate of cardiovascular complications. In patients with both type 1 and type 2 diabetes, the onset of microalbuminuria is a major risk factor for both cardiovascular disease and mortality, and is also associated with retinopathy, left ventricular cardiac dysfunction and dyslipidemia, as well as hypertension. In all patients with microalbuminuria, other cardiovascular risk factors should be managed by, for example, lowering low-density lipoprotein (LDL) cholesterol, the use of antihypertensive therapy, cessation of smoking and taking exercise. Diabetic nephropathy in pregnancy can lead to worse hypertension, an increased risk of pre-eclampsia and an accelerated decline in kidney function. Four methods of screening for microalbuminuria are available (Table 5.7). A definitive diagnosis of microalbuminuria requires a positive result from at least two of three samples within a 3-6-month period, because of high day-to-day variability in urine albumin excretion. Screening dipsticks for microalbumin have acceptable sensitivity (95%) and specificity (93%) when used by trained personnel. All positive tests using reagent strips should be confirmed by more specific methods. Type 2 diabetes. All patients with type 2 diabetes should be screened for incipient or established diabetic nephropathy, because microalbuminuria is present at diagnosis in approximately 25% of patients. If urinalysis is positive for protein (in the absence of infection), it is very likely that the patient has clinical albuminuria (> 300 mg/24 hours), which has important implications in terms of the progression of renal disease and overall cardiovascular risks. Positive urinalysis should be confirmed quantitatively (exempli gratia spot urine albumin:creatinine ratio), and patients in whom urinalysis is negative for protein require quantitative assessment for microalbuminuria. Type 1 diabetes. In contrast, microalbuminuria is rarely present at the time of diagnosis of type 1 diabetes or before puberty, and therefore screening should begin with the onset of puberty or 5 years after the diagnosis and be performed annually.
Risk factors. The major factor influencing the development of diabetic nephropathy is hyperglycemia. Persistent hyperglycemia leads to the development of glycosylated macromolecules and their conversion to advanced glycosylation end products. This process results in thickening of the basement membranes (including the glomerular basement membrane) and accumulation of matrix proteins within the glomeruli. Although genetic factors predispose individuals to develop diabetes, whether they also influence the rate of progression of the nephropathy remains controversial. The racial and familial distribution of diabetes also suggests a role for as yet undetermined polygenic influences. Other risk factors include male sex and smoking. Apart from glycemic control, the other major factor determining progression through diabetic nephropathy is hypertension. Prevention and treatment. Aggressive intervention in patients with either incipient or established nephropathy has been shown unequivocally to prevent progression of renal disease. Strategies include aggressive glycemic control, angiotensin blockade, blood pressure control, protein restriction and smoking cessation (Figure 5.6 and Table 5.8). The US Diabetes Control and Complications Trial (DCCT) and the UK Prospective Diabetes Study (UKPDS) have clearly shown that intensive therapy can significantly reduce the risk of the development of microalbuminuria and overt nephropathy in individuals with diabetes. In the UKPDS, there was a 75% reduction in the relative risk of doubling serum creatinine over 12 years. Overwhelming evidence implicates hypertension as a major risk factor in the progression of diabetic nephropathy. Hypertension usually appears during the microalbuminuric phase of disease, although in type 2 diabetes it may result from other causes, such as renovascular disease. Both systolic and diastolic hypertension accelerate the progression of diabetic nephropathy. Aggressive management of hypertension reduces the progression of diabetic renal disease. The VIIth report of the Joint National Committee on the Prevention, Detection, Evaluation and Treatment of High Blood Pressure recommends a target blood pressure in diabetics with renal disease (who are albuminuric) of 130/80 mmHg or lower. Angiotensin blockade should be the first-line therapy, but most patients will require multiple drugs. The important role of angiotensin blockade in retarding the progression of both type 1 and 2 diabetic nephropathy is now well established. The use of either ACE inhibitors or ARBs should be integrated into the overall antihypertensive management strategy. In addition to reducing systemic blood pressure, angiotensin blockade reduces intraglomerular pressure, thereby protecting the nephron from ongoing damage, and has effects independent of lowering blood pressure. Many studies have shown that in diabetic (and non-diabetic) patients with kidney disease, angiotensin blockade can reduce the level of albuminuria and the rate of progression of renal disease to a greater degree than other antihypertensive agents. The use of angiotensin blockers may, however, exacerbate hyperkalemia in patients with advanced renal insufficiency and/or hyporeninemic hypoaldosteronism (type IV renal tubular acidosis). Patients with bilateral renal artery stenosis and those with advanced renal disease being treated concurrently with non-steroidal anti-inflammatory drugs (NSAIDs) or who are volume depleted, may experience a rapid decline in kidney function when started on an ACE inhibitor; it is likely that ARBs have a similar effect. This is usually reversible when the drug is discontinued. As a high proportion of patients progress from microalbuminuria to overt nephropathy and subsequently to ESKD, ACE inhibitors or ARBs are recommended for all patients with microalbuminuria. The effect of an ACE inhibitor appears to be a class effect, so the choice of agent may depend on cost and compliance issues; the same is probably true for ARBs. Protein restriction has been shown to be effective in retarding progression of renal disease in several animal models by reducing glomerular hyperfiltration and intraglomerular pressures. While small studies in humans with diabetic nephropathy have shown that protein restriction (0.6 g/kg/day) confers a moderate benefit in retarding progression, restricting protein in diabetics is hard to achieve and may lead to protein malnutrition. Nevertheless, the general consensus is to prescribe a protein intake of approximately the adult recommended dietary allowance of 0.8 g/kg/day (10% of daily calories) in patients with overt nephropathy.
Social, psychological and economic influences on smoking. Social norms. Cigarette smoking is markedly influenced by social norms and other environmental influences. In cultures in which smoking is taboo for women, few women smoke. Smoking prevalence has declined as smoking has become more marginalized in sections of western society. The influence of social norms is also manifest in regional variations in smoking prevalence within countries. In the UK, for example, smoking prevalence increases with 'northerliness', even controlling for social class. Psychiatric disorders. There is a strong link between smoking and many psychiatric disorders, including mood disorders, schizophrenia and substance abuse. Smoking is also prevalent among homeless people, many of whom suffer from mental health disorders. Not only are persons with psychiatric disorders more likely to smoke, but they are also likely to smoke more heavily than others. It has yet to be established why these links exist: whether smoking causes or exacerbates these conditions, whether a disorder makes it more likely that a patient will smoke and be unable to stop, or whether there is a common underlying cause. It is widely thought that smoking is particularly closely linked to schizophrenia, but in fact the dominant factors are the severity of the psychiatric disorder and whether the patient is institutionalized (Figure 3.1). Thus, discussion about whether specific links exist between smoking behavior and the mechanisms that underlie schizophrenia or its treatment are somewhat premature. It is, however, clear that there is no evidence for deterioration overall in mental health when people with psychiatric disorders stop smoking and in some cases there may be an improvement. Alcohol intake and other drug-related disorders. Smokers are particularly likely to experience problems with illicit psychoactive drugs and alcohol. The link with alcohol abuse and alcoholism is particularly strong, leading some to suggest that heavy smokers should be screened for alcoholism. Similarly, although drug abuse is rare in an absolute sense, smokers, especially heavy smokers, are far more likely than non-smokers to engage in drug abuse. Smokers with alcohol use disorder find it harder to stop smoking; if they do stop, it does not adversely affect their prospects of recovery from that disorder, and may improve it. Criminality and antisocial behavior. Adolescents who engage in antisocial behavior, truant from school and express antisocial attitudes are more likely to smoke than those who do not engage in antisocial behavior. In adulthood, there is a strong correlation between having a criminal conviction and being a smoker. Government actions. Probably the greatest impact on smoking prevalence can be achieved through large-scale social and governmental policies. Table 3.1 summarizes the options and their likely effects. Taxation is the most clearly established of the many ways in which governments can influence smoking. A 10% increase in the price of cigarettes relative to earnings is linked, on average, to a 4% reduction in consumption by adults.
In teenagers, the effect is greater, with a 10% increase in cost linked to a 10% decrease in consumption. In countries such as the UK, smokers support these kinds of policies, presumably because they recognize that they can provide an added incentive to stop - which is what they mostly want to do. Taxation has to be linked to effective countermeasures against illicit supply. It is important to structure it to prevent tobacco companies using pricing policies to undermine its deterrent effect. Not all of the decrease in consumption involves smokers giving up altogether; in some cases smokers merely reduce the number of cigarettes they smoke. The health benefits of this action may be undermined, however, because smokers tend to smoke each cigarette more intensively. It is also not fully known how reductions in consumption translate into health benefits. Nevertheless, it is clear that increasing the cost of smoking is, potentially, an important public health measure. Mass media campaigns can be effective in promoting smoking cessation and reducing smoking prevalence. The size of the effect is related to the intensity of the campaign (as indexed by viewing numbers) and whether it is sustained. Specific quitting events, for example the UK's 'No Smoking Day' and 'Stoptober' (a mass quitting event that challenges smokers to be smoke-free for a month as a stepping stone to permanent cessation), and the US 'Great American Smokeout' (which encourages quitting for a day), have been evaluated as a highly cost-effective way of promoting smoking cessation. Bans on tobacco advertising. Tobacco companies spend many billions of dollars annually promoting their brands. In many countries, including the USA, tobacco advertising is banned from television but is permitted in print media. Where there have been more comprehensive bans on promotion, there has been some evidence of an effect in reducing smoking. For this reason, and because of moral concerns over permitting promotion of a product that is addictive and often lethal when used as intended, several countries, including those in the European Union, have implemented a total ban on tobacco promotion. An international treaty, the Framework Convention on Tobacco Control (see Chapter 8) mandates this and other tobacco control measures. Plain packaging. Several countries have implemented legislation requiring cigarette and tobacco to be sold in standard plain packages carrying graphic health warnings and the brand name in a standard font with no brand imagery. The effect is expected primarily to be on take-up of smoking - with cigarettes becoming less attractive to adolescents. Indoor smoking bans have been introduced in public indoor areas in many countries, including the UK, Australia and New Zealand, and in most jurisdictions in the USA. Although the bans were introduced to protect the health of non-smokers, there is some evidence that they might have at least a short-term effect in reducing smoking prevalence.
Etiology and pathogenesis. The origins of eating disorders are widely seen as a complex combination of biological and environmental factors. Various genetic, developmental, psychological, familial and social variables have been suggested as risk factors. Biological factors. Genetic factors. The current state of the genetics of eating disorders is that while there is familial clustering of these syndromes, it is not at all clear that they are in their entirety genetically transmitted disorders. Whatever the genetic component to each one of them, it seems to be stronger for anorexia than bulimia. And granting even that, the current literature supports a polygenetic transmission of risk which needs substantial accumulation of environmental risk to have disorders come to the fore. The most potent risk factor for eating disorders is female gender (see Chapter 2). This association is likely to reflect biological as well as psychosocial factors. Family studies indicate that first-degree relatives of patients with anorexia have an increased prevalence of eating disorders (Table 3.1), and that first-degree relatives of patients with bulimia or binge-eating disorder appear to be at increased risk of eating disorders, mood disorders and substance abuse. Twin studies of anorexia and bulimia suggest there is approximately a 50-80% genetic contribution to liability accounted for by additive genetic factors. Candidate gene studies have initially focused on the serotonergic and other neurotransmitter system genes involved in bodyweight regulation, with hardly any unequivocally confirmed findings. Systematic genome-wide linkage scans based on families with at least two individuals with an eating disorder revealed initial linkage regions on chromosomes 1, 3, 4 (anorexia) and 10p (bulimia). Ongoing international collaborations are working at pooling cases and using newer forms of analysis such as genome-wide associations. Similar to other complex disorders, the eating disorders are likely to have a polygenic etiology, each gene having a relatively small effect. Physiological factors. Both high and low premorbid body mass index (BMI) may predispose to anorexia. Being overweight or obese (with a high BMI) is a risk factor because of the likelihood of dieting, while low BMI may characterize individuals who are already prodromally ill with anorexia but who have not yet reached the weight criteria for definitive diagnosis. Childhood obesity is a risk factor for bulimia and binge-eating disorder. A pear-shaped body and relatively high amounts of body fat may also predispose to eating disorders. People with one or both of these characteristics may have fat deposits that are hard to move by normal dieting; this may lead to the adoption of extreme weight-control measures.
Hormonal and neurohormonal influences. The study of the neurobiology of eating disorders has demonstrated large hormonal and neurohormonal differences in adult and late-adolescent patients who are acutely ill. Levels of histidyl-proline diketopiperazine, a hormone that is involved in the induction of satiety, are shown to increase as individuals with anorexia gain weight. This hormonal change may be responsible for premature feelings of satiation. Around 20% of individuals with diabetes mellitus develop an eating disorder. Patients with type 1 diabetes are more likely to develop anorexia or bulimia, while patients with type 2 diabetes are more likely to develop binge-eating disorder. This suggests that insulin-glucagon systems may be involved in the predisposition for eating disorders. In addition, because energy requirements for weight rehabilitation and maintenance have been shown to differ in anorexic patients, bulimic patients and control populations, it is possible that premorbid metabolic abnormalities act as risk factors for eating disorders. Neurobiological factors. Although many of the biological findings in eating disorders can be best understood as results of starvation and disturbed eating behaviors, some are causally linked as risk or maintaining factors. Considerable evidence suggests that altered brain serotonin function contributes to dysregulation of appetite, mood, and impulse control in eating disorders. Dysregulation of the serotonergic system is usually secondary to weight changes. Studies have suggested that diminished serotonin activity may trigger some of the cognitive and mood disturbances associated with bulimia. The findings also indicated that chronic depletion of plasma tryptophan may be one of the mechanisms whereby persistent dieting can lead to the development of eating disorders in vulnerable individuals. Brain monoamine function in eating disorders has been studied in both the acute state and after recovery, using specific ligands and positron emission tomography. There is a reduction in 5-HT 2A receptors and an increase in 5-HT 1A receptors in both the acute and recovered state, and dopamine receptors (DA) within the striatum are increased after recovery. Anomalies in the dopamine system could heighten food reward in bulimia and binge-eating disorder. Abnormalities in both illness-related (food and body shape) and non-illness-related information processing have been detected with functional brain imaging studies. These functional abnormalities could maintain some of the eating disorder behaviors.
Environmental factors. Family functioning. Historically, the role of dysfunctional styles of family interaction was put forward in theories of the development of eating disorders. Unfortunately and unfairly, this has led to some parenting styles being blamed for causing eating disorders in children. Still, the type of relationships that children and adolescents develop with food and body image, both healthy and unhealthy, is strongly influenced by. parental attitudes toward eating, weight and body shape. parental eating and weight-related behavior modeled in the home. excessive parental control over a child's nutritional intake such that the child is unable to make independent food choices. Perceived pressure to be thin, family criticism regarding weight, and maternal investment in slenderness predict eating disturbances in adolescents. Parents also have a strong influence over their children's internalization of the aesthetic ideal. When compared with control families, those with an anorexic child tend to show more rigid organization, less clear interpersonal boundaries, and avoidance of open discussion of disagreements between parents and children. Patients with an eating disorder seem to be more likely than controls to experience attachment disturbances, such as insecure and anxious attachment styles and deactivating defensive strategies. Furthermore, families of individuals with anorexia tend to have a higher rate of parental eating disorders, family dieting and adverse comments from family members about eating, weight or appearance. The families of individuals with bulimia are often more chaotic and conflicted. These families tend to have higher rates of alcoholism, substance misuse and affective disorders, as well as higher levels of perfectionism and a sense of ineffectiveness. Societal influences. The preoccupation with body image and the drive to attain thinness that are characteristic of eating disorders relate to the idealized representation of the human figure within a given culture, and to the pressure to conform from peers and the media. In Western countries, the body dimensions of female cultural icons, such as fashion models and actresses, have become progressively thinner over the past decades, with a concomitant rise in disordered eating among women. Mounting evidence implicates the mass media in the promotion of body-image and eating disturbances, with the emphasis on dieting and other weight-control behaviors often targeted at women rather than men, thus paralleling the gender distribution of eating pathology. A longitudinal prospective study of ethnic Fijian adolescent girls demonstrated an increase in key indicators of disordered eating following novel prolonged television exposure to the aesthetic ideal. The internalization of societal pressures has been shown to have a clear effect on body dissatisfaction and eating dysregulation in population samples. The first diet in adolescent girls is most frequently triggered by comparison with others' appearance and their own self-ideal. Societal influences and internalization of the thin ideal may lead directly to body dissatisfaction and unhealthy eating, or may be mediated by more general psychological processes such as intrapersonal (self-esteem, mood, personality) or interpersonal functioning and/or emotional regulation and coping. Stressors and life events precipitate the onset of eating disorders in 70% of cases, and include parental neglect, abuse, indifference, loss and separation. In some studies, a high incidence of sexual abuse during childhood is reported by women with diagnosed eating disorders; rates of abuse are seemingly higher in bulimia than anorexia. In a US statewide representative sample, both sexual and physical abuse were strong independent risk factors for disordered eating in adolescent girls and boys. The nature of this relationship is difficult to assess because of differences in diagnostic criteria for abuse, a high base rate of sexual abuse in the general female population and a high rate of abuse associated with other psychiatric diagnoses. The issue is insufficiently explored in young people. Male university students who reported physical and sexual abuse in childhood were also at a greater risk for eating disorders. A wide range of other childhood adversities and stressful life events has been associated with eating problems during adolescence. In one large referred population of patients with eating disorders, 55% reported recent stressful life events, most often separation (exempli gratia death of an important other, parental divorce, separation from close ones) or other changes in family or living conditions. The effects of stress on eating are not uniform. Stress-induced hyperphagia (an abnormally increased appetite for, and consumption of, food) occurs more often in children and adults who are dietary-restrained eaters than in those who are not. Thus, for the vast majority of children and adults, the effects of current stress on food intake seem to be moderated by intermittent dietary restraint. 
Developmental factors. Life-stage risk factors have been identified at the major developmental phases of childhood and adolescence. It is still debatable whether there is a correlation between eating problems in early childhood and those in adolescence. The demographics of early feeding problems suggest no association, because boys are more likely to exhibit disordered eating in early childhood, whereas girls are at greater risk in adolescence. However, early childhood feeding and eating problems, such as pica, picky eating, digestive problems, as well as eating conflicts, struggles around meals, and unpleasant meals, have been found to predict eating disorders in adolescence or young adulthood. Mothers of patients with eating disorders have reported reliance on scheduled feeding and prematurely introduced solids more frequently than controls. However, these practices were also used with siblings who did not become ill, suggesting that other factors are likely to be involved in the pathogenesis of eating disorders. In one study, six eating behaviors were evaluated at three time points over 2.5 years by maternal interview in two different overlapping cohorts (aged 0-10 and 9-18 years). Early maladaptive eating patterns were associated with a greater likelihood of problems later in life. Picky eating and digestive problems predicted preanorexic behavior. Early eating problems were also a predictor of the future onset of bulimia. School-age children. In elementary school (7-12 years), significant numbers of children want to be thinner than they are. Studies indicate that 37% try some form of weight loss, and 6.9% score in the pathological range on an adapted version of the Eating Attitudes Test (see Table 4.8). There are few significant differences between boys and girls. Body-image distortions in this age group are associated with dieting and weight concerns. Food refusal, ritualistic behavior during meals, phobic behavior and elevated internalizing behaviors such as depression and anxiety have all been described in school-age children who eventually suffer from an eating disorder. Prepuberty and adolescence. Studies have also confirmed that eating problems emerge in response to pubertal change, especially fat accumulation. Girls who feel most negatively about their bodies at puberty are at a higher risk of developing eating difficulties. Several studies have identified associations between disturbed eating in adolescents and. anxious attachment. weight concerns. deficient self-regulation. affective lability. concerns about current body shape. Contextual risk factors during this developmental phase include. teasing by peers. discomfort about discussing problems with parents. maternal preoccupation with diets. acculturation to Western values in immigrants. First-generation immigrants, for example, are less likely to develop anorexia than the second and later generations. Interestingly, homosexual boys (12-20 years) have a far greater frequency of disordered eating and weight concerns than heterosexual boys, for complex reasons that are currently not well understood. Anorexia seems to occur at a precise timepoint in adolescent development, possibly reflecting the young person's inability to manage the developmental demands of adolescence. Adolescence is also a time marked by an increase in fat deposition for girls, and an unease and unhappiness with bodily appearance. In a sample of 808 girls (12-14 years) with no clinical symptoms, 34% stated a strong desire to lose weight; of these, 34% had a body mass index (BMI) of less than 20 (id est in the normal or thin range). Furthermore, 24% restricted their food intake to influence their weight and shape, 5% engaged in 8-hour fasting for more than half of each week and 38% undertook vigorous exercise to control their weight. The biological changes of puberty certainly contribute to the onset of eating disorders. In girls, the peripubertal rise in estrogens affects mood and appetite regulation, because estrogens modulate serotonergic function via a variety of mechanisms, including changes in serotonin receptor number, serotonin synthesis and metabolism. Interestingly, dieting, a common factor that precipitates binge-eating behavior, alters brain serotonin function more markedly in women than in men. Estrogens also enhance stress responsiveness through downregulation of the hypothalamo-pituitary-adrenocortical axis. In girls, an association between early pubertal timing and eating-disorder symptoms as well as diagnosis has been observed in a number of studies. 
Individual variables. Intrapersonal factors. Affective disturbances have been proposed as underlying disorders for bulimia, and a similar relationship has been proposed between anorexia and obsessive-compulsive disorders. Depression is one of the most salient psychiatric comorbidities in both men and women with eating disorders. Dysregulation in serotonin, a neurotransmitter involved in the regulation of both mood and satiety, has been implicated as a causal factor in depression and eating disorders. Antidepressants that selectively block serotonin reuptake are effective in the treatment of both depression and bulimia or binge-eating disorder. Anxiety and obsessive-compulsive behavior. Similarly, increased levels of anxiety and obsessiveness have been described in young people and adults with eating disorders. Premorbid overanxious disorder and obsessive-compulsive tendencies are seen significantly more often in patients with anorexia than in controls. Similarly, the risk for bulimia is increased by the presence of overanxious disorder or social phobia relative to controls. However, longitudinal studies in other fields indicate that premorbid anxiety disorders and negative affectivity are also risk factors for the development of other psychiatric conditions such as affective disorders and substance abuse. Clinicians have described internalized anger and rage as primary emotions in patients with eating disorders. Researchers have found higher levels of perceived external control, lower assertiveness, lower self-esteem and more self-directed hostility in individuals with anorexia and bulimia than in controls. Body-image disturbance is a well-known risk factor for clinical and subclinical levels of eating disorders. It has been related to indices of global psychological functioning, such as self-esteem, anxiety, depression and negative affectivity. However, body-image disturbance seems to be more closely related to restricted eating, and global psychological functioning more closely related to bulimia. Body dissatisfaction is not only a defining characteristic of eating disorders but also a contributing factor in their development and maintenance. The unhealthy weight-control behaviors that characterize eating disorders, such as strict dieting, excessive exercise or laxative/diuretic abuse, are used to compensate for negative perceptions of body size, shape or weight. In a Canadian study of middle-school adolescents (early teens), negative perception of physical appearance and high importance of social acceptance correlated significantly with high levels of disordered eating. Personality traits. Girls and women with anorexia tend to be anxious, inhibited and controlled, while those with bulimia tend to be more affectively labile and impulsive. Negative self-evaluation, perfectionism and obsessive-compulsive behavior are all psychological traits that promote dieting behavior and are more commonly found in those with anorexia than in those with other psychiatric disturbances. A desire to avoid conflict predisposes the anorexic to focus on the more easily controllable domain of the body and weight.
Being a perfectionist and a self-disciplinarian allows a person to maintain the strict diet and exercise regimen necessary for the sustainment of the disorder. Psychological risk factors associated with bulimia include negative self-evaluation, shyness, a lack of close friends, missing school, perfectionism and mood lability. Interpersonal functioning. Girls have been described as different from boys in their self-perceptions, emotions and behaviors pertaining to interpersonal relationships. Numerous studies have been conducted on the influence of peers on eating behavior and body image. Pressure to be thin from friends or weight-based teasing from peers has been associated with subclinical eating disturbances in high-school girls and with full-blown eating disorders in college-age women. Negative social feedback, in the form of teasing a person about their appearance, has been indicated as a possible etiologic factor in the development of disordered eating. Impulse behaviors and substance abuse. Bulimic patients have a history of impulsive behaviors, including alcohol and drug misuse, suicidal behaviors and shoplifting. These problematic behaviors are often overlooked on initial contact. The preoccupation with food often impairs patients' functioning in social-, school- and work-related activities. The pattern of impulsive behaviors associated with bulimia is not common in those with anorexia. A review of the literature on the comorbidity of eating disorders and substance abuse reveals an association between the two, especially among bulimics. 17% of bulimics report a current or past history of drug, substance or alcohol abuse and/or dependence, or treatment for any of the above. 20% of drug abusers report a current or past history of bulimia or bulimic behaviors. Among bulimics, the family history studies show alcoholism rates of 39%, and drug or substance use rates of 19%. When treating a patient with either an eating disorder or a substance-abuse problem, the clinician should always consider the possibility of a comorbid substance-abuse or eating problem. Several psychological and biological mechanisms have been proposed to explain the comorbidity of eating disorders and substance abuse. Psychological factors include the hypothesis of an addictive personality that predisposes individuals to becoming addicted to substances, viewing food and drugs as functional equivalents. Another explanation, which has received empirical support, is that the initial development of an addiction fosters psychological and behavioral patterns that enable individuals to develop other addictions. Another mechanism is the self-medication hypothesis, which states that individuals with an eating disorder turn to alcohol and drugs in order to cope with, or treat, their eating problems. The same has been said about depression - afflicted individuals turn to food or drugs to treat their depression. Biological factors have been investigated in terms of the genetic contributions to alcoholism, drug abuse and eating disorders. Similarities in the process of these disorders suggest biological mechanisms underlying the addiction.
Epidemiology and pathophysiology. Prostate cancer is the most common malignancy to affect men of middle age and beyond in most developed countries and, increasingly in developing countries, it is second only to lung cancer as a cause of cancer deaths in men. The lifetime risk of developing clinical prostate cancer in western countries is about 1 in 8. Approximately 80% of men aged 80 years have prostate cancer at autopsy. However, many of these cancers grow slowly and the risk of developing clinically detectable cancer is about 13%; the lifetime risk of actually dying from prostate cancer is approximately 3%. Worldwide, there has been a steady increase in the incidence of clinically significant prostate cancer, although in the USA the number of incidental diagnoses of prostate cancer has fallen by 28% since the US Preventive Task Force issued a draft guideline in 2011 (which became a final recommendation 2012) discouraging prostate-specific antigen (PSA)-based screening in all men (Figure 1.1). However, prostate cancer primarily affects men over the age of 50 years, so the number of men diagnosed with prostate cancer is predicted to increase substantially over the next two decades as a result of the worldwide trend towards an aging population. The debate around measurement of PSA is discussed in detail in Chapter 3 (pages 36 -). Mortality from prostate cancer in Europe rose to a peak in 1993, reached a plateau, and has now started to decrease. Mortality in the USA has also shown similar trends (see Figure 1.1); the rate of decline has increased significantly in recent years and is now four times faster than in the UK. Some have attributed this drop to the efforts made in North America to detect and treat prostate cancer early through PSA testing a decade or two earlier, although several other factors may also have contributed, such as changes in lifestyle and better treatment outcomes. Risk factors. Despite the high incidence of prostate cancer, relatively little is known about the underlying causes. However, several risk factors have been identified (Table 1.1). Age is the greatest factor that influences the development of prostate cancer. Clinical disease is rare in men under the age of 50 years but the incidence increases markedly over 60 years of age. Marked geographic and racial variations are seen in the incidence of clinical prostate cancer (Table 1.2). The risk is highest in North America and northern European countries, and lowest in the Far East. In the USA, the risk is higher in black men than in white men, and black men also appear to develop more aggressive disease earlier. The incidence of prostate cancer is lowest in Chinese and Japanese races, although the prevalence is now increasing in both. The incidence of latent (clinically insignificant) disease is similar in all populations studied. The incidence of prostate cancer in men who emigrate from a low- to a high-risk area increases to that of the local population within two generations. This suggests that environmental influences such as diet and lifestyle factors may have a profound effect on the development of prostate cancer and on the progression of latent to clinically detectable cancer. Family history/genetic risk. Epidemiology studies show that heritable factors account for a small proportion of prostate cancer risk but a higher proportion of early-onset disease. However, a host of studies have suggested the existence of prostate cancer susceptibility genes. Family history is a strong risk factor for prostate cancer: the risk of a man developing prostate cancer is increased approximately 2.5-fold if he has a first-degree relative who is affected. The relative risks for developing prostate cancer based on family history are shown in Table 1.3. The high incidence of familial prostate cancer prompted a search for germline mutations. Early linkage analyses suggested the human prostate cancer 1 gene (HPC1) located at 1q24-25, and studies in men with familial prostate cancer but without male to male transmission led to the identification of the X-linked human prostate cancer X gene (HPCX) at Xq27-28. Further loci have also been identified on chromosomes 2, 3, 5, 6, 8, 10, 11, 13, 15, 17, 19, 20 and 22.
Familial prostate cancer is a far more heterogenous condition than familial breast cancer, with contributions from many more gene loci. The predictive value of any one allele is low; hence, a clinically useful genetic test has not yet been identified. The strongest evidence for direct causality comes from families who develop cancer syndromes such as Lynch syndrome - the risk of developing prostate cancer is elevated twofold for those carrying the abnormal gene. Mutations in the BRCA2 breast cancer susceptibility gene are rare in men with prostate cancer but appear to be associated with earlier diagnosis and more aggressive disease, such as higher Gleason score (see pages 14 -) and higher PSA level and tumor stage and/or grade at diagnosis. Furthermore, carriers of BRCA2 mutations may have lower overall survival (OS) and prostate cancer-specific survival compared with non-carriers. Knowledge of a man's BRCA2 status therefore has prognostic value. The US National Cancer Institute web pages on the genetics of prostate cancer provide a thorough review of the current status of this fast-changing field and are updated regularly (www. gov/types/prostate/hp/prostate-genetics-pdq). Testosterone and its more potent metabolite dihydrotestosterone (DHT) are essential for normal prostate growth and also play a role in the development of prostate cancer (Figure 1.2). Prostate cancer almost never develops in the rare case of men who are for some reason castrated before puberty, or in men deficient in 5alpha-reductase - the enzyme with type I and II isoforms that converts testosterone to DHT. Trials have shown that the type II 5alpha-reductase inhibitors finasteride and dutasteride reduce the development of prostate cancer by about 25%, suggesting a key role for DHT. However, the incidence of prostate cancer increases with age, while serum testosterone levels decrease. In addition, men diagnosed with advanced prostate cancer often have a lower average testosterone level than men of a similar age who do not have prostate cancer. Early studies showed an increased risk of prostate cancer in obese men whereas more recent studies indicate that levels of detected prostate cancer are in fact lower in obese men. This may be because they have lower levels of PSA and androgens, such that fewer obese men underwent biopsy and were diagnosed with prostate cancer in the PSA era. A recently published study on risk factors for prostate cancer reported that being obese was associated with a 44% increase in the risk of prostate cancer diagnosis. Prostate cancer mortality is significantly higher in men who are obese. The mechanism by which obesity increases the likelihood of death from prostate cancer is not known but may be through the activation of pro-carcinogenic pathways such as the insulin-like growth factor (IGF) axis. Western diets tend to be high in animal fat, protein, meat and processed carbohydrates, and low in plant foods. A number of studies support links between the intake of saturated fat and red and processed meats in particular and the development of prostate cancer. There is also some evidence that alpha-linoleic acid, an omega-3 polyunsaturated fatty acid, increases the risk of prostate cancer and of developing advanced disease, which may be the result of oxidative stress and subsequent DNA damage or the development of obesity. Omega-3 fatty acids from marine sources may, however, decrease the risk of developing prostate cancer. Sun exposure and vitamin D. The risk of dying from prostate cancer is related geographically to ultraviolet (UV) light exposure, and men with prostate cancer have lower levels of vitamin D. Vitamin D levels are determined by dietary intake and conversion in the skin by UV light; however, the mechanism by which vitamin D levels protect against prostate cancer is not known, and vitamin D supplementation doe not decrease the risk. Calcitriol (vitamin D) has been used in the treatment of advanced prostate cancer but evidence of efficacy is lacking.
Histological features. Most prostate cancers are adenocarcinomas. The majority (> 70%) appear to arise in the peripheral zone of the gland (Figure 1.3); 5-15% arise in the central zone and the remainder from the transition zone, which is where benign prostatic hyperplasia (BPH) also develops. Microscopic foci of 'latent' prostate cancer are a common autopsy finding and may appear very early in life: approximately 30% of men over 50 years of age have evidence of latent disease. However, these microscopic tumors often grow very slowly, and many never progress to clinical disease. Beyond a certain size, however, these lesions progressively de-differentiate, probably as a result of clonal selection, and become increasingly invasive. A tumor with a volume greater than 0.5 cm or that is anything other than well differentiated is generally regarded as clinically significant. The Gleason grading system is widely used for grading prostate cancer (Figure 1.4). In this system, the tumor is first graded 1-5 according to aggressiveness; the numbers of the two most widely represented grades are then summed to produce the Gleason score. More recently, the grouping of Gleason scores has been introduced, more accurately conveying the risk/aggressiveness of a cancer and allowing better counseling of patients. The system is described in more detail below. Because prostate cancers are often heterogeneous, the numbers of the two most widely represented grades are added to produce the Gleason score (exempli gratia 3 + 4), which provides useful prognostic information. Occasionally, more than two grades are observed in prostatectomy or biopsy specimens, the least common being known as the tertiary grade. If the tertiary grade has a high score (4 or 5), the patient has increased risk of disease progression even if the primary and secondary grades are lower, and the tertiary rather than the secondary grade informs the score. In 2015, a consensus conference of the International Society of Uropathology proposed a new Gleason grade grouping, which ranges from 1 to 5, 1 being the most indolent and 5 the most aggressive. Table 1.4 shows the allocation of the scores to the grade groups. The higher the Gleason grade group, the greater the likelihood that primary treatment with radical prostatectomy will fail, validating this method of reporting prostate cancer grade (Figure 1.5). Patterns of disease spread. Prostate cancer is also classified according to the spread of the disease, using the American Joint Committee on Cancer tumor-nodes-metastasis (TNM) system (Table 1.5). The tumor stage (T1-T4) describes the pathological development of the tumor. T1 represents 'incidental' status, in which the tumor is discovered after transurethral resection of the prostate (TURP) or, more commonly, by PSA testing, and is not detectable by palpation or ultrasonography. T2 represents a cancer that is palpable but still confined to the prostate gland. T3 represents a cancer that has extended through the prostate capsule into the surrounding fat or seminal vesicles. T4 represents advanced disease, where the tumor has invaded neighboring organs (Figure 1.6). The nodal stages (N0-N1) and metastatic stages (M0-M1c) reflect the clinical progression of the disease. Metastases are most common in the lymph nodes (N1) and bones (M1b); the lungs and other soft tissues are less commonly involved. It is not currently possible to distinguish unambiguously between tumors that will remain latent throughout the patient's life and those that will definitely progress to clinical disease. Studies of incidental carcinomas diagnosed after TURP suggest that the median time to progression for T1b tumors (high-volume: moderately or poorly differentiated) is 4.75 years, compared with 13.5 years for T1a tumors (low-volume; well-differentiated) (Figure 1.7). Thus, elderly men with T1a tumors are most appropriately managed by active surveillance alone (see page 60), whereas for younger men with T1b disease, options for more aggressive, potentially curative, therapy can be discussed. 
The first symptoms. The typical history. The characteristic history of amyotrophic lateral sclerosis (ALS) is one of often insidious, yet always progressive, asymmetric weakness without prominent sensory symptoms. The most common complaints are of a weakness in one arm or leg, or a change in the voice. Individuals are often able to date the onset of symptoms to a specific month. While individuals often try to associate the onset of symptoms with other events (exempli gratia a viral infection or surgery), there is no firm epidemiological evidence to support a consistent trigger in this regard. Occasionally, an individual with ALS will perceive that the initial symptom came on overnight. Despite this assertion, they rarely seek acute services, unlike those who have a stroke, even though such individuals are frequently, and erroneously, referred for assessment as such. The history of progressive weakening is critical. In the early stages, symptoms may fluctuate from day to day or week to week. More rarely, an individual with ALS will report transient remission of weakness. In these cases, it is important to consider the broader overall trajectory of the condition to establish the clear presence of progression of weakness. Individual symptoms depend on the region affected and the relative burden of upper and lower motor neuron disease. Upper motor neuron (UMN) damage manifests as slowness, stiffness, clumsiness or loss of fluidity of movement. Lower motor neuron (LMN) disease causes more overt weakness, loss of muscle bulk (atrophy), muscle twitching (fasciculation) and cramps. More unusual presentations of ALS include respiratory-onset disease, with weight loss and orthopnea, and cognitive or behavioral change. Limb symptoms. Most people with ALS first develop symptoms in the limbs. This is called spinal-onset ALS. Approximately one-third first develop symptoms in the lower limbs and approximately one-third in the upper limbs. Individuals with ALS may report a weak limb, hand or foot as 'dead' despite normal sensation, so it is important to clarify such language. Lower limb symptoms. LMN disease in the legs presents as weakness, most commonly starting in the ankle dorsiflexors. The individual finds it difficult to lift their foot and presents with frequent tripping or stumbling due to foot drop. The gait may sound different to family members, as the foot slaps on the ground. Onset in more proximal muscles can make it difficult to climb stairs and get out of low chairs, or the individual may feel that the leg does not support them. UMN-predominant involvement of the legs will result in loss of fluidity or awkwardness when walking, poor balance, falls and 'spasms' (or clonus). Upper limb symptoms. In the arms, LMN symptoms most commonly start with hand weakness, with reduced grip strength and, for example, difficulty undoing bottle tops or pinching zippers (Case report 3.1). Less frequently, the onset of weakness is in the shoulder and proximal arm, with difficulty holding the arm above the head, which affects grooming and dressing, or lifting items that could previously be carried with ease. UMN-predominant upper-limb disease usually presents as stiffness or clumsiness with fine motor tasks, such as typing, writing or (un)fastening buttons. Painful muscle cramps are common in the general population and may be physiological. They are often worse after prolonged exercise. Painful muscle cramps may also be a sign of muscle denervation; they can occur in nerve lesions of any kind. Those with distal peripheral neuropathies often complain of cramps in their feet and calves, less commonly in their hands. Those with ALS also have frequent, disabling cramps. The cramps may occur anywhere in the body, reflecting the asymmetric, non-length-dependent pattern of motor neuron loss in ALS. Fasciculation is painless, involuntary, rapid twitching that is seen or felt in the muscle. Like cramps, fasciculation is non-specific. Both are frequently described in the general population and not pathological in isolation. However, in the presence of progressive muscular weakness it is a common and important sign in ALS (see Case report 3.1). Distinguishing benign fasciculation, with or without cramps, from fasciculation caused by ALS is obviously an important diagnostic distinction (Table 3.1). Fasciculation without weakness, especially in someone under the age of 40, is rarely due to ALS, but muscle weakness is always pathological. 
Bulbar symptoms. In at least 25% of those with ALS, the onset of symptoms is in the bulbar region, a system comprising the central nuclei and motor neurons of the IXth, Xth, XIth and XIIth cranial nerves (LMN disease) or the corticobulbar tracts (UMN). Bulbar symptoms initially comprise changes in speech and, later, problems with swallowing. In lower motor neuron bulbar disease, the dysarthria usually involves slurring; individuals may report that they 'sound as if [they] have been drinking'. If vocal cord movement is impaired, the voice may sound hoarse. Dysphagia may occur because of tongue weakness and difficulty manipulating food in the mouth. Individuals may describe food 'getting stuck' between the gums and the cheeks, especially with food that has a crumbly or leafy texture. LMN dysfunction of the pharyngeal phase of swallowing often results in coughing or choking, especially with thin liquids; swallowing may feel effortful or individuals may report 'food sticking'. Weakness of lip closure together with reduced swallowing frequency may result in sialorrhea (drooling). In upper motor neuron (cortico)bulbar disease, speech becomes strained and slow (Case report 3.2). Swallowing symptoms may be similar to those reported in LMN-predominant bulbar disease, with impaired coordination of pharyngeal muscle contraction. As a result, the individual may lose the automatic ability to coordinate swallowing or coughing in response to laryngeal stimulation, with explosive, though normally transient, choking as a result. More specific UMN symptoms include frequent gagging due to hypersensitivity of the gag response, and laryngospasm. Laryngospasms are brief, often painful, episodes of abrupt 'tightening' or 'squeezing' felt inside the throat, which impair inhalation and speech. They may be triggered by cold air, food particles or saliva. A less obvious but frequently encountered UMN bulbar symptom is pseudobulbar affect (PBA). This is an exaggerated emotional response, also known as emotional lability (see Case report 3.2). The individual may report inappropriate or incongruent episodes of crying or laughing that are easily triggered, difficult to control and disproportionate to how they are feeling. The episodes are often embarrassing and may be mistaken by others for depression or cognitive impairment. Excessive yawning is a frequently concurrent symptom. Depression occurs surprisingly infrequently in ALS but is distinct from PBA, though both may respond to selective serotonin-reuptake inhibitors. The hallmark signs on examination are asymmetric weakness, with mixed UMN and LMN signs (Table 3.2). Weakness may be an LMN sign or a UMN sign. LMN weakness often has associated atrophy. Atrophy can be best observed by comparing muscles side by side and looking for asymmetry, which is the more common pattern in ALS. The lateral intrinsic hand muscles, especially the first dorsal interossei, are typically more affected than the medial ones. This so-called 'split hand' in ALS is not easily explained in terms of peripheral or proximal limb innervation and may instead reflect the cortical organization associated with the uniquely opposable thumb in humans, or selective involvement of motor neuronal pools in the anterior horns of the cervical spinal cord (Figure 3.1). The tongue, quadriceps, tibialis anterior and calf muscles are also frequently affected by visible atrophy. Tongue atrophy in ALS is invariably symmetric (unlike atrophy in the limbs) (Figure 3.2). Fasciculation may be missed unless the individual is undressed to expose the upper chest, back and proximal arms and legs (see Case report 3.1). Individuals with ALS usually do not report fasciculation (unlike those with benign fasciculation). Fasciculation in the tongue can be difficult to distinguish from brief semi-voluntary movements, exaggerated by protrusion. It is best appreciated with the tongue relaxed, resting in the floor of the mouth, with the mouth gently opened. Muscle tone is reduced in LMN disease.
This is not commonly a useful sign, as it only becomes apparent when weakness is advanced. Conversely, the increased tone seen in UMN disorders may be detectable before any weakness becomes apparent. 'Clasp-knife' spasticity may occur and there may be clonus, commonly at the ankles and, more rarely, at the jaw and occasionally at the patella or forearm. Repetitive movements are slowed in UMN disease. This is a crucial part of the examination in suspected ALS, and is best appreciated by asking the individual to. rapidly move their tongue from side to side. attempt to tap their fingers and feet rapidly. Lower motor neuron weakness in a limb characteristically affects muscles supplied by more than one peripheral nerve or nerve root. Any muscles may be affected, but a common combination is to see weakness in the hand involving the median innervated abductor pollicis brevis (ask the individual to put their palms face up and point their thumbs to the ceiling) and weakness of the ulnar interossei (ask the individual to spread their fingers against resistance). Early 'finger drop' (distal upper-limb extensor weakness) is unusual for ALS and should prompt consideration of multifocal motor neuropathy with conduction block (see page 62). Proximal weakness in the arms may result in weakness of shoulder abduction or of flexion and extension at the elbow. Proximal arm weakness may become bilateral: the brachial amyotrophic diplegic phenotype ('flail arm' or 'man-in-a-barrel'; see page 18). When onset is in the leg, the ankle dorsiflexors are often involved early. Listening and watching for a foot drop and steppage gait can be a sensitive way to detect this, as can asking the individual to walk on their heels. Cramping on muscle strength testing, and occasionally when the patient bends to remove their socks or shoes, is a non-specific sign but often seen in ALS. The facial muscles must also be examined. Eye closure is usually normal, but a degree of mild facial muscle weakness may be found. The eyes should have a full range of movement in all but very advanced cases of ALS. Tongue strength can be checked by asking the individual to put their tongue in their cheek, opposing while the examiner presses on the outside of the cheek. Neck flexion and extension is an important test; a degree of weakness in one or both directions is a common finding in ALS. Upper motor neuron weakness typically affects a limb in a pyramidal pattern. The flexors are stronger than the extensors in the arms. The extensors are stronger than the flexors in the leg. UMN weakness of the tongue is difficult to detect; it is the slowing of repetitive tongue movements and characteristically spastic speech that alert the physician to UMN bulbar disease. Reflexes are reduced or absent in LMN disorders. In a weak limb, preserved or easily elicitable reflexes may be considered a 'presumed UMN' sign. This is a qualitative distinction but is not seen in other neuropathic processes, so it may help in the early diagnosis of ALS. In UMN disorders, the reflexes are brisk. Trapezius and pectoral reflexes are commonly found, as well as positive finger jerks (Hoffmann's sign). Above the neck, preservation of the blink response on repeated glabellar tap, often exaggerated, is a useful UMN sign. Orbicularis oris reflexes may emerge, including the pout and an exaggerated jaw jerk, occasionally with clonus, which portends an aggressive form of ALS. Reflexes may spread to adjacent regions or cross to the other leg (a 'crossed adductor' reflex on checking the patella reflexes). A positive Babinski sign (extensor plantar response) is surprisingly absent in many cases of advanced ALS involving the lower limbs, and so has limited value in the examination. Sensation is normal in ALS. Mild deficits in distal sensation in the feet, or reduced vibration sensation, is a common, non-specific sign, especially in the elderly, and should not derail a diagnosis of ALS that is clearly supported by typical symptoms and signs.
Less common presentations. Respiratory-onset ALS. Respiratory failure is the commonest cause of death in ALS, and forced vital capacity is strongly predictive of prognosis. Signs of respiratory insufficiency (Table 3.3) should be sought whenever a diagnosis of ALS is suspected. Rarely, they are the presenting symptoms (approximately 3% of ALS cases are 'respiratory onset'). As well as dyspnea at rest, on talking or with exertion, check for signs of nocturnal hypoventilation (fragmented or unrefreshing sleep, excessive day time sleepiness or morning headache due to hypercapnia). Other signs of hypercapnia include headaches on waking, mental fogging or confusion. Weight loss is a key symptom and may be prominent (Case report 3.3). In respiratory-onset ALS, it is predominantly due to the increased caloric demand from the sheer work of breathing. Both dyspnea and weight loss in ALS are frequently incorrectly associated with either cardiovascular or pulmonary disease or to occult malignancy, respectively. This results in inappropriate referrals, investigations and diagnostic delay. It is therefore important to be alert to ALS as a possible underlying cause and to examine the individual carefully for signs of motor neuron involvement (see Case report 3.3). Individuals with respiratory-onset ALS often have associated weakness of their axial muscles. They may have a stooped posture, or even camptocormia, significant weakness of neck extension ('head drop', which may be a presenting feature) or a protuberant abdomen due to abdominal wall weakness. Chest wall fasciculation is often pronounced but asymptomatic and only recognized if the individual is undressed. Other evidence of respiratory compromise may include raised respiratory rate, use of accessory muscles of respiration, weak cough and poor chest expansion on inhalation. ALS-FTD presentation. It was previously thought that cognition was unaffected in ALS. However, it is now apparent that executive dysfunction and deficits in language and fluency, psychomotor processing and visual and immediate verbal memory are frequently present. In most people with ALS, these deficits are relatively subtle, do not manifest in the primary care setting and are insufficient to culminate in a diagnosis of dementia. However, in some clinical series, concomitant frontotemporal dementia (FTD) has been found in up to 15% of cases. When FTD is the presenting feature of ALS, motor features are often recognized soon after the cognitive or behavioral changes. The hallmarks of what is typically the behavioral variant of FTD (rather than semantic or non-fluent aphasic subtypes) are changes in personality, with progressive social withdrawal and apathy, and socially inappropriate behavior, or a progressive decline in judgment, impulse control and empathy. Lack of insight is intrinsic to this diagnosis. The individual usually does not recognize the change in their behavior, may deny that it is inappropriate and is often unconcerned by the discussion others are having about them. Conversely, memory is usually relatively spared. If a change in behavior or personality is raised, most usually by a family member, it is important to actively seek symptoms and signs of motor neuron involvement so that these do not go unrecognized. Primary lateral sclerosis presentation. Approximately 3% of those diagnosed within the spectrum of ALS have a slowly progressive, pure UMN syndrome called primary lateral sclerosis (PLS) (see page 15). Weakness is far less prominent a symptom than in typical ALS. Instead, PLS tends to involve stiffness and poor balance, sometimes with falls. The symptoms most commonly start in the legs. In a minority with an initial bulbar presentation, common symptoms include a strained or forced voice and marked emotional lability. Distinguishing PLS from UMN-predominant ALS is challenging (see Chapter 4). 
Type 2 diabetes mellitus. Type 2 diabetes occurs when insulin secretion is insufficient to meet insulin demand, resulting in hyperglycemia. The precedent pathophysiological events are characterized by varying degrees of metabolic derangement, which often differ in degrees between individuals. The keys are insulin resistance in muscle (thereby increasing insulin demand), insulin resistance in liver (thereby increasing hepatic gluconeogenesis) and eventual beta cell failure with relative insulin deficiency (that is, unable to meet insulin demand). While there is relative insulin deficiency in type 2 diabetes, it is rarely absolute deficiency as in type 1 diabetes, thus ketoacidosis does not occur with hyperglycemia. The symptoms of type 2 diabetes are outlined in Table 4.1. Type 2 diabetes is often considered a lifestyle disease. This is a misunderstanding, as type 2 diabetes has a strong genetic background, stronger in fact than that for type 1 diabetes. In monozygotic twins, there is a 95% concordance for type 2 diabetes; in contrast, the concordance for type 1 diabetes is 30-50%. The genetic predisposition for type 2 diabetes is, however, brought out very strongly by the environmental factors of a sedentary lifestyle, consumption of energy-dense foods and overweight or obesity. Type 2 diabetes mellitus represents hyperglycemia, the end stage of beta cell dysfunction, with numerous pathways to reach this endpoint. This has become evident as different genes have been found in different populations. Further, there are many different clinical phenotypes of the condition, varying from slim antibody-negative individuals who demonstrate the need for early insulin replacement, through to obese individuals whose hyperglycemia resolves with modest weight loss and minimal medication. There are often varying degrees of dyslipidemia and associated endocrine diseases such as polycystic ovary syndrome. Interestingly, most of the genes identified for type 2 diabetes concern the beta cell. This indicates that the genetic predisposition for type 2 diabetes lies in a defect affecting insulin secretory capacity or the long-term ability to continue increased insulin secretion in the face of high demand or, perhaps, programming for long-term failure. Mutational forms of type 2 diabetes explain only a small proportion of cases, fewer than 2%. These include the various forms of monogenic diabetes, also referred to as maturity onset diabetes of youth (MODY) (see Chapter 5). The mutations in certain genes causing monogenic forms of diabetes are listed in Table 4.2. In contrast, a number of different genetic polymorphisms have been described in different populations that explain susceptibility to the commoner forms of type 2 diabetes. These are also listed in Table 4.2. Environmental factors. Environmental factors play a major role in increasing diabetes risk. These include a sedentary lifestyle, consumption of energy-dense foods and drinks (high carbohydrate and/or high fat) and obesity. Modifying these environmental factors is the focus of international public health efforts to curb the increasing global prevalence of type 2 diabetes and is the cornerstone of treatment and/or prevention of type 2 diabetes. The effectiveness of this approach is best demonstrated by the Diabetes Prevention Program, in which increased physical activity and modest weight reduction reduced the conversion of impaired glucose tolerance to type 2 diabetes by over 50% at 3 years. Obesity has the strongest impact on promoting all forms of diabetes, including those without a clear genetic susceptibility. Ethnic origin. In certain ethnic groups, loss of traditional lifestyles and adoption of a western diet have had catastrophic effects on prevalence rates, with the majority of adults now affected by type 2 diabetes. Ethnic groups including indigenous Australians, Nauruans (Pacific) and Pima Indians (USA) appear particularly susceptible.
Adoption of a western lifestyle has also increased the susceptibility to type 2 diabetes among Hispanic Americans and populations in Southern and South East Asia, the Middle East and the Pacific Islands. Insulin resistance is considered a central metabolic feature of many cases of type 2 diabetes. While it is often present in obesity-associated diabetes, it is not universal. In fact, insulin resistance is not present in many people with type 2 diabetes, particularly leaner individuals. Insulin resistance accompanies abdominal obesity and is present in atherothrombotic cardiovascular disease. It accompanies dyslipidemia characterized by hypertriglyceridemia and low-HDL cholesterol, which is often found in type 2 diabetes, abdominal obesity and metabolic syndrome. Insulin resistance is considered the link that explains the clustering of abdominal obesity, type 2 diabetes, heart disease, hypertension and dyslipidemia. Insulin resistance refers to the reduced ability of circulating insulin to result in cellular glucose uptake. Glucose serves as the major energy substrate for many tissues including muscle and the brain. Insulin acts by stimulating its receptor on the cell surface (Figure 4.1). This sets up a cascade of phosphorylation steps of subcellular enzymes that are controlled by key regulators. The eventual result is the movement of a glucose transporter, glucose transporter 4 (GLUT4), from the cell cytosol to the cell surface, and this permits glucose uptake into the cell (Figure 4.1). Within this complex pathway, there are many points at which signaling may be perturbed. Overall, such perturbation results in a reduced glucose-uptake response to insulin (insulin resistance). The body attempts to override this by producing more insulin (hyperinsulinemia). When the pancreatic beta cells can no longer produce sufficient insulin to overcome insulin resistance, blood glucose levels start to rise. Insulin resistance may occur at many points in the insulin signaling cascade. Rarely, genetic mutations within the insulin receptor or its substrate can result in insulin resistance. More frequently, increased circulating fatty acids (from nutrient excess or obesity) can interfere with insulin signaling through a mechanism termed lipotoxicity. In addition, low-grade systemic inflammation is often found in obesity; disturbed adipose tissue secretion of molecules called adipokines (such as adiponectin, tumor necrosis factor alpha [TNFalpha], interleukin-6) can contribute to this. Adipokines can interfere with insulin action through a number of specific pathways involving intracellular inflammation signal transduction pathways that link in to insulin signaling pathways. These disturbances in inflammation add to the metabolic disturbances in the regulation of insulin signaling and contribute to insulin resistance. Insulin resistance is also found in the liver. With failing insulin secretion, there is insufficient insulin to suppress hepatic gluconeogenesis, particularly overnight. This results in a rising fasting glucose: as the insulin secretory capacity diminishes, fasting glucose rises further (Figure 4.2). Insulin secretion is further hindered by rising glucose levels which directly damage beta cells through a mechanism called glucotoxicity. Lipotoxicity in type 2 diabetes is found in the muscle cells, where glucose is the major substrate. However, it has also been reported in the pancreas, where the effects of excess lipid can contribute to damage to insulin-secreting beta cells. Thus, in the situation of nutrient excess and obesity, where there are excess amounts of circulating fatty acids, lipotoxicity can increase insulin resistance (and the demand for insulin) and diminish the ability to respond by worsening beta cell impairment (see Figure 4.2).
How cancer immunotherapy works. In the context of cancer, the term immunotherapy encompasses a variety of approaches, targeting diverse immunologic targets. The history of immuno-oncology. The concept of immuno-oncology dates back more than 100 years, to 1893 (Figure 3.1). In that year, William Coley, an American surgeon and cancer researcher, observed remission of cancer in patients with postoperative bacterial infections, and suggested that activation of the immune system must play a role in combating cancer. Subsequently, in 1909, Paul Ehrlich suggested that the immune system must play an important role in preventing the development of cancer. However, it was not until the mid-20th century when Lewis Thomas and Frank MacFarlane Burnet hypothesized that the immune system is capable of eliminating cancerous cells through a process known as immune surveillance, and that this process depends on recognition of tumor-associated antigens by the immune system. Subsequently, through the laboratory work of Lloyd Old and Robert Schreiber, the concept of immune surveillance has evolved into 'immunoediting', reflecting the ability of tumor cells to evade the immune system. Increasing understanding of the underlying mechanisms of immunoediting has identified numerous potential therapeutic targets, some of which - notably immune checkpoint inhibition as first demonstrated by James Allison in the 1990s - are already yielding promising results in clinical practice. Indeed, in 2013, cancer immunotherapy was cited as the 'breakthrough of the year' by the journal Science. What types of tumor are potentially susceptible to immuno-oncology?. Clearly, the potential sensitivity of a given cancer to immuno-oncology therapies will depend on the ability of the tumor to trigger an immune response (immunogenicity). Cancer is characterized by an accumulation of genetic mutations, many of which result in the expression of cancer-specific antigens that can bind to major histocompatibility complex (MHC) class I molecules on the cancer cell surface. These antigen-MHC complexes can be recognized by cytotoxic CD8+ lymphocytes, that, if activated, could potentially mount an immune response against the tumor. As a result, tumors with high somatic mutation rates may be more susceptible to immuno-oncology therapies than those with lower mutation rates. Somatic mutation rates differ markedly, both between tumor types and within an individual tumor type: the rate may vary more than 1000-fold between tumors with the highest and lowest rates (Figure 3.2). The highest rates are seen in cancers of the skin, lung, bladder and stomach, while the lowest are seen in hematologic and pediatric cancers. It is noteworthy that the highest rates occur in tumors that are induced by carcinogens such as tobacco smoke or ultraviolet light. Potential targets for cancer immunotherapy. A variety of cancer immunotherapy strategies are currently being investigated, or have already entered clinical practice. These are conventionally classified as passive or active immunotherapies, according to their ability to activate an immune response against tumor cells (Table 3.1), although this classification does not adequately reflect the complexity of drug-host-tumor interactions. As a result, it has been suggested that immunotherapies should be classified according to their antigen specificity; however, even therapies initially directed against a single antigen may eventually become responsive to multiple antigens, a phenomenon known as epitope spreading. Passive immunotherapies. Tumor-targeting monoclonal antibodies. Monoclonal antibodies (mAbs) that specifically target malignant cells are among the best characterized forms of cancer immunotherapy.
These may act in a number of ways, including. inhibition of signaling pathways in tumor cells. delivery of conjugated cytotoxins or radionuclides to tumor sites. opsonization of tumor cells and activation of antibody-dependent cell-mediated cytotoxicity (ADCC), antibody-dependent phagocytosis and complement-mediated cytotoxicity. bispecific T-cell engagers (BiTEs ; Amgen) consisting of single-chain variable fragments from two antibodies targeting a tumor-associated antigen (TAA) and a T-cell surface antigen. Examples of mAbs that act via these processes are shown in Table 3.2. Adoptive cell transfer (ACT) is a form of cell-based cancer immunotherapy in which circulating or tumor-infiltrating lymphocytes are collected from the patient, modified ex vivo as necessary to attack specific neoantigens, and reinfused into the patient following lymphodepletion and conditioning (Figure 3.3). Response rates of 80-90% have been achieved with ACT in hematologic cancers, but at present this approach is only available in a few specialized centers. CAR-T therapy. A form of ACT that is currently attracting considerable attention is the use of chimeric antigen receptor-expressing T (CAR-T) cells. Such T cells are genetically modified to express a transmembrane protein consisting of a synthetic T-cell receptor that targets a predetermined antigen expressed by the tumor. Following infusion of these cells, the patient's immune system actively surveys and engages specific cancer cells that express the antigen. Promising clinical trial results have been achieved with this approach in patients with CD19-positive B-cell hematologic malignancies. Later generations of CAR-T cells included additional co-stimulatory domains to optimize cell activation, and further modifications produced 'armored CAR-T cells', which have been optimized to secrete cytokines or express ligands that offer enhanced efficacy in a hostile tumor microenvironment. Clinical trials are ongoing in patients with various hematologic malignancies and solid tumors, and an important landmark has now been reached with the first approval of a CAR-T therapy. The US Food and Drug Administration (FDA) has approved Kymriah (tisagenlecleucel, formerly CTL019) intravenous infusion for the treatment of patients up to 25 years of age with B-cell precursor acute lymphoblastic leukemia (ALL) that is refractory or in second or later relapse, based on an 82% remission rate in a multicenter Phase II registration trial. Oncolytic viruses are non-pathogenic viruses that specifically infect cancer cells. Such viruses may kill cancer cells in two ways. an innate cytopathic effect resulting from overloaded cell metabolism in response to viral infection. expression of potentially lethal gene products. To date, Imlygic (T-VEC/talimogene laherparepvec) - a genetically engineered oncolytic herpes simplex viral strain that can be injected directly into locally advanced unresectable melanoma tumors - is the only oncolytic virus therapy approved by the US FDA and European Medicines Agency, based on the results of a positive Phase III randomized controlled trial. Clinical trials of T-VEC are ongoing for a variety of cancers, either alone or in combination with immune checkpoint inhibitors.
Active immunotherapies. Preventative antiviral vaccines. Viruses are involved in the development of a range of malignancies (Table 3.3), and such associations are likely to increase as new links between viral infection and cancer are established. These relationships also create the potential for antiviral vaccines to be used to prevent cancer. The best developed of these are the hepatitis B vaccine and vaccines against subtypes of human papillomavirus (HPV) that cause approximately 70% of cervical cancer cases. Data from Taiwan, where there has been mandated universal vaccination of infants against hepatitis B since 1984, have demonstrated reductions in the incidence of both hepatitis B and hepatocellular cancer. Data for the preventive effects of HPV vaccination are preliminary, although modeling has suggested that this will be a highly successful and cost-effective means of preventing HPV-associated malignancies, especially in low-income populations with high incidence of cervical and head and neck cancers. Therapeutic vaccines. Dendritic cell-based immunotherapies. Typically, dendritic cell-based immunotherapies involve the isolation of monocytes from the patient or donor, and amplification and differentiation ex vivo in the presence of agents such as granulocyte macrophage colony-stimulating factor (GM-CSF) to induce dendritic cell maturation. The activated dendritic cells are then exposed to a source of TAAs, or mRNA coding for TAAs, and reinfused into the patient. Alternatively, dendritic cells may be allowed to fuse with inactivated cancer cells ex vivo, to create a hybrid known as a dendritome. In both cases, the dendritic cells become loaded with TAAs or TAA mRNA, thereby priming the immune system to mount a response against the relevant antigens. There is currently no consensus on the optimal approach to dendritic cell-based immunotherapy. To date, only one cell-based product containing dendritic cells has been approved: Sipuleucel-T was approved by the FDA in 2010 for the treatment of asymptomatic or minimally symptomatic metastatic castration-resistant prostate cancer, based on a positive Phase III randomized controlled trial. The cost-benefit ratio for this vaccine has been questioned and limits its widespread use. Peptide- and DNA-based vaccines. Anticancer vaccines could potentially be either peptide-based or DNA-based. With peptide-based vaccines, the patient is exposed to TAA peptides, together with adjuvants such as Bacillus Calmette-Gurin (BCG - see below) or lipopolysaccharide (LPS) to stimulate an immune response. With DNA-based vaccines, TAAs are encoded into a bacterial plasmid which is injected into the patient and taken up by native cells, including antigen-presenting cells (APCs); the APCs can then produce the antigen themselves to trigger an immune response. At the time of publication, one of the most widely investigated peptide-based vaccines is the gp (glycoprotein) 100 vaccine for the treatment of metastatic melanoma. This product has been investigated in several clinical trials but the results have been conflicting: in trials that have shown clinical benefit, the vaccine was typically administered with another immuno-oncology therapy such as interleukin (IL)-2 or an immune checkpoint inhibitor. Similarly, no DNA-based vaccine has yet been shown to be beneficial in clinical trials. Whole-cell tumor vaccines are another potential anticancer vaccination. In this approach, cells are removed from the tumor and inactivated by exposure to ultraviolet radiation, freeze-thawing or heat shock, leading to the release of antigens that will subsequently be recognized by APCs. The attenuated tumor cells are then combined with an appropriate adjuvant and injected back into the patient, triggering an immune response. Whole-cell vaccines have the advantage that the patient is exposed to the full range of TAAs expressed in the tumor, whereas protein-based or DNA-based vaccines involve only a limited number of antigens.
Uncertainties with therapeutic vaccines include their safety and efficacy, although clinical trial safety data look reassuring. Many other questions on optimizing efficacy remain. Where do they fit in the current treatment paradigm since the advent of effective therapeutic antibodies?. What is their optimal clinical use: the treatment of advanced disease or (more likely) as a postoperative adjuvant therapy?. What is the most effective immune-stimulatory adjuvant? Some of the more effective adjuvants, including BCG and LPS, produce activation of toll-like receptors (TLR) resulting in activation of innate immunity. In addition, endogenous 'alarmins' and chaperone proteins, including heat shock proteins (HSP), may activate adaptive and innate immunity and enhance the activity of vaccine therapies; their use is also being evaluated. Oncophage is an autologous HSP vaccine that reached Phase III trials in melanoma and renal cell cancer but the data were inconsistent. What is the optimal time to start vaccine therapies, how frequently should they be administered and at what intervals?. Immunostimulatory cytokines. Typically, immunostimulatory cytokines are used as adjuvants to augment the response to other immunotherapies, although some have been approved in Europe and the USA as standalone therapies (Table 3.4). IL-12 activates both innate (natural killer [NK] cells) and adaptive (cytotoxic T lymphocytes) immunities and has also been evaluated preclinically and in early phase clinical trials; however, results to date have been disappointing. Cytokine antibodies and targeted agents. Pro-inflammatory cytokines have a role in the development of malignancy and regulation of the immune response to malignancy. IL-6 in particular plays a critical role in the differentiation of dendritic cells and of B cells into plasma cells, leading to production of antibodies. It is also important in regulating T helper cell function. In cancer, IL-6 is also important in. differentiation of myeloid-derived suppressor cells (MDSCs). regulation of self-renewal of cancer stem cells. inhibition of apoptosis, thereby promoting tumor growth and progression. enhancing angiogenesis. the development of cancer cachexia syndrome. IL-6 signaling occurs through binding to the IL-6 receptor in conjunction with gp130 protein and activation of the JAK/STAT signaling pathway (janus kinase/signal transducer and activator of transcription). Antibodies to IL-6 have been successfully used in inflammatory conditions such as rheumatoid arthritis (RA), and tocilizumab is registered with the FDA for the treatment of adult and juvenile forms of RA. Therapeutic antibodies to IL-6 have also been trialed in cancer patients, including studies in the management of cancer cachexia syndrome. Other cancer therapies target the JAK/STAT signaling pathway. However, despite encouraging results from a Phase II trial of the JAK 1 and 2 inhibitor ruxolitinib in patients with pretreated pancreatic cancer and elevated C-reactive protein concentrations, two Phase III trials failed to improve outcomes. There are also preliminary studies under way of antibodies targeting colony-stimulating factor receptors in order to influence dendritic cell and myeloid cell function in malignancy. Immunomodulatory monoclonal antibodies. In contrast to the mAb therapies described on pages 38 -, immunomodulatory mAbs act by altering the function of various components of the immune system, thereby eliciting a new immune response or restoring an existing response. Such mAbs act in several ways. immune checkpoint blockade, including agents acting via the PD-1 receptor (exempli gratia pembrolizumab, nivolumab, atezolizumab) or CTLA- 4 (exempli gratia ipilimumab) pathways. activation of co-stimulatory receptors on the surface of immune effector cells, such as tumor necrosis factor (TNF) receptor superfamily member 4 (OX40).
neutralization of immunosuppressive factors produced in the tumor microenvironment, such as transforming growth factor (TGF)-beta. Of these, immune checkpoint inhibitors are currently the only agents approved for clinical use in the EU and USA, largely based on clinical benefit shown in randomized Phase II or III clinical trials. Their clinical development and applications are detailed in the next chapter. Currently licensed products are listed in Table 3.5. Inhibitors of immunosuppressant metabolism. Indoleamine 2,3-dioxygenase (IDO) catalyzes the first, rate-limiting, step in the metabolic pathway converting the essential amino acid tryptophan into kynurenine. IDO has a strong immunosuppressant effect, probably because of depletion of tryptophan in T cells, and has been implicated in the development of immune tolerance in cancer. A number of small-molecule inhibitors of IDO have been investigated in clinical trials, and promising results have been obtained from a Phase II trial of a combination of an IDO inhibitor with the checkpoint inhibitor pembrolizumab in patients with advanced melanoma. Further development in clinical trials is warranted. Pattern recognition receptor (PRR) agonists are a class of proteins that recognize a variety of danger signals, including microbe-associated molecular patterns (MAMPs), such as bacterial LPS, and damage-associated molecular patterns (DAMPs), such as mitochondrial DNA. Examples of PRRs include toll-like receptors (TLRs) and nucleotide-binding oligomerization domain-containing (NOD)-like receptors (NLRs). PRRs play essential roles in the immune response to pathogens and in the reactivation of anticancer immune responses following chemotherapy, radiotherapy or immunotherapy. A number of PRR agonists have been approved for use in cancer patients, including. monophosphoryl lipid A, an LPS derivative used in the Cervarix  vaccine against HPV. imiquimod, a trigger of TLR7 signaling that is used in the treatment of superficial basal cell carcinoma. BCG is a live attenuated form of mycobacterium bovis that was introduced in 1921 as a vaccine against tuberculosis, and is still used for this purpose; it is the most widely used vaccine in the world. Several different strains of BCG are used worldwide and it is unclear which is the most effective. Use of BCG in the first 6 months of life halves mortality, presumably by increasing resistance to sepsis. The anticancer effects of BCG result from the recruitment and activation of immune cells, including CD4+ T cells, which eliminate cancer cells that have internalized BCG. Administration of BCG also increases the number of monocytes and increases both pro- and anti-inflammatory cytokine production, as well as increasing production of interferon (IFN)-gamma in unstimulated cells. It is used in cancer treatment as an intravesical adjuvant therapy to prevent recurrence of localized (non-muscle-invasive) bladder cancer. Serious adverse events are uncommon (fewer than 8% of patients require treatment cessation for toxicity), although bladder irritation, malaise and fevers are common. Dose reduction and anti-inflammatory medications are usually effective in the management of significant side effects. Inducers of immunogenic cell death. Some forms of chemo- or radiotherapy can stimulate malignant cells to express DAMPs that bind to APCs, triggering a cancer-specific immune response via a process known as immunogenic cell death (ICD). Chemotherapy agents that have been shown to induce ICD include doxorubicin and related anthracyclines, bleomycin, oxaliplatin, cyclophosphamide and bortezomib.
Assessing the benefits and risks of immunotherapy in cancer. Assessment of the benefits and risks of cancer immunotherapies can be challenging, as the criteria used for conventional therapies cannot generally be extrapolated to immunotherapies. Assessing efficacy. A key issue in immuno-oncology is that patients may show a survival benefit in the absence of an objective response as defined by conventional RECIST (Response Evaluation Criteria in Solid Tumors) criteria. Response to immunotherapies may vary markedly between patients: while some patients may show an initial response or stable disease, in others the response may be delayed because of the need to restore T-cell responses - a process that requires the interaction of numerous other immune cells. Indeed, in some patients, there is an initial phase of pseudoprogression during which the tumor appears to enlarge due to infiltration of newly reactivated T cells and subsequent inflammation. For these reasons, a set of immune-related response criteria (irRC) have been proposed (Table 3.6), relating to four patterns of response. shrinkage of baseline lesions similar to that observed with conventional chemotherapy or targeted agents, without development of new lesions. durable stable disease, which may be followed by a slow, steady decline in tumor burden in some patients. response after an initial increase in tumor burden. response in the presence of new lesions. Assessing safety and tolerability. Some immunotherapies, notably checkpoint inhibitors, are associated with immune-related adverse events such as fatigue, diarrhea, nausea and altered liver or kidney function (Figure 3.4). Many of these resemble the adverse events often seen with conventional chemotherapy but have different etiologies: while adverse events with conventional chemotherapy usually reflect cytotoxic effects on healthy tissue, adverse events with immunotherapies typically reflect actions on the immune system. For example, diarrhea associated with immunotherapy may be due to a reaction to gut-associated or self-antigens. Such adverse events require careful management, because although most are mild or moderate in severity, failure to recognize them as immune related could lead to suboptimal management, with potentially serious or life-threatening consequences. For example, if untreated, diarrhea from immune-related colitis may become self-perpetuating, potentially leading to gut perforation. Patient education about such adverse events is essential: patients should be warned not to ignore 'slight' or 'mild' symptoms, but to seek medical advice as soon as possible. A multidisciplinary approach, involving oncologists, organ specialists and intensive care physicians, is essential for patients with severe immune-related adverse events. In rare cases, cytokine release syndrome (CRS) may occur as a result of inflammatory cytokine release following administration of mAbs or BiTEs, or in patients undergoing CAR-T therapy. This syndrome is characterized by diverse systemic and organ-related symptoms that occur during or immediately after infusion of the antibody (Table 3.7). CRS can usually be managed symptomatically but in severe cases it may be necessary to use a mAb directed against IL-6, such as tocilizumab, to reverse the inflammatory process. Combining immuno-oncology drugs and other treatments. As experience with immunotherapies in oncology accumulates, attention is turning to the possibility of combining immunotherapy with other treatment modalities.
Combining different immunotherapies. Combinations of immunotherapies acting on different immune pathways offers the potential for additive or synergistic antitumor activity. Studies in patients with melanoma receiving a combination of the CTLA-4 inhibitor ipilimumab and the PD-1 inhibitor nivolumab suggest that the combination is significantly more effective than ipilimumab alone; on the basis of these findings, the FDA has approved this combination for the treatment of BRAF wild-type advanced melanoma. Combinations of CTLA-4 inhibitors and PD-1/PD-L1 inhibitors are being investigated in other cancers, including non-small-cell lung cancer (NSCLC) and renal cell carcinoma. Combining immunotherapy with targeted therapies. Combinations of checkpoint inhibitors and targeted inhibitors of the BRAF and MEK oncoproteins are being investigated in patients with metastatic melanoma and other solid tumors. A Phase I trial with ipilimumab and the BRAF inhibitor vemurafenib was halted prematurely because of severe hepatotoxicity, but this has not been seen with other BRAF inhibitors. Preclinical data suggest that a combination of PD-1 blockade with anti-BRAF therapy may be beneficial in advanced melanoma; this approach is being investigated. Combining immunotherapy with chemotherapy. As described above, certain forms of chemotherapy can sensitize tumors to immunotherapy by promoting ICD. Similarly, some other chemotherapies, such as cisplatin, can enhance the efficacy of T cell-based immunotherapies by sensitizing the malignant cells to T cell-induced death rather than by ICD. Clinical trials in patients with advanced melanoma have found that the addition of dacarbazine to ipilimumab increased response rates and produced a slight increase in overall survival, compared with dacarbazine alone, but these benefits were achieved at the cost of an increased rate of severe (grade 3/4) immune-related adverse events. A randomized Phase II trial of carboplatin and pemetrexed with and without pembrolizumab showed improved response rates (55% versus 29%, p = 0.0032) and progression-free survival (13 versus 8.9 months, p = 0.0205) with the combination in patients with untreated advanced NSCLC. This provided the basis for accelerated FDA approval of the chemo-immunotherapy combination for patients with untreated advanced NSCLC. The confirmatory Phase III randomized trial is ongoing. Combining immunotherapy with radiotherapy. Radiotherapy can modulate local and systemic immune responses, possibly through ICD, increased uptake of tumor antigens by dendritic cells and by enhancing CD8+ T-cell responses. Although this effect is not sufficient to overcome immune tolerance in cancer cells, the combination of radiotherapy with immunotherapy may be beneficial. To date, however, clinical trials of this strategy have shown that such combinations are less toxic than combinations of immune checkpoint inhibitors with targeted therapies, but local and distant (abscopal) anti-tumor responses are limited.
6 Early management. Patients with mild acute pancreatitis (AP) (65% of cases) have an uneventful disease course, with initial severe pain that improves quickly. In general terms, these patients are easily managed. In contrast, patients with moderate-to-severe AP may be challenging: they may require aggressive fluid resuscitation and they have prolonged pain and sometimes intolerance to oral refeeding. These patients may develop organ failure (OF) and be prone to late complications. This chapter addresses the early management of AP, focusing on the evidence underpinning each intervention. Monitoring organ failure and metabolic complications. Virtually all patients without OF will survive, but OF (especially when persistent or multiple) is associated with an increased risk of mortality. For this reason, monitoring patients to enable early detection of OF is particularly important. In general terms, likely causes of OF in AP include the following. A systemic response to initial sterile local inflammation of the pancreas and surrounding tissues. A systemic response to sepsis (pancreatic sepsis, acute cholangitis, acute cholecystitis, other nosocomial infections). Primary disease of the organ (contrast-associated renal failure, fluid overload, exacerbation of pre-existing disease, pulmonary thromboembolism, etc). Abdominal compartment syndrome, which may be a cofactor associated with the other causes of OF. Respiratory failure is the most frequent form of OF associated with AP. Cardiovascular failure is associated with higher mortality than renal or respiratory failure. Patients with predicted severe disease (see page 37) should be monitored carefully, including frequent evaluation of blood pressure, heart rate, respiratory rate, O saturation, temperature, urine output, blood leukocyte count, serum creatinine and blood urea nitrogen (BUN). In patients with biliary AP and OF, acute cholecystitis and cholangitis should be ruled out; physical examination, blood tests (alanine aminotransferase, bilirubin and alkaline phosphatase serum levels) as well as abdominal ultrasonography are usually the first steps in identifying patients with biliary complications. Pulmonary auscultation can be helpful to detect signs of pleural effusion, fluid overload and alveolar damage. In patients with dyspnea, tachypnea, low O saturation or altered pulmonary auscultation, measurements of arterial blood gases are indicated. Urine output is especially important for the early detection of renal failure and patients in need of aggressive fluid resuscitation. New-onset diabetes or poor control of glycemia in former diabetics can complicate an episode of AP, so serum glucose levels should be monitored. Hypocalcemia is an infrequent complication of moderate-to-severe AP so calcium serum levels (corrected by albumin or proteins) should be monitored in the early stages of disease. There is no specific treatment for OF in AP. The indications and general principles for the use of vasoactive drugs, mechanical ventilation and hemodialysis are beyond the scope of this book. Supportive therapy. With no effective pharmacological therapies for treating AP, early management currently relies on supportive care. Fluid resuscitation is considered a cornerstone in the management of AP because patients have several factors that tend to decrease intravascular volume. Abdominal pain, nausea and vomiting in the early phase affect a patient's ability to tolerate oral nutrition. Vomiting, increased sweating and tachypnea due to systemic inflammatory response syndrome (SIRS) are associated with increased fluid losses. Paralytic ileus and local complications are associated with fluid sequestration. AP is associated with vascular leak syndrome (increased systemic vascular permeability with extravasation of fluids and protein into tissues), which further increases fluid sequestration. In patients with severe disease, arteriolar tone may decrease due to an uncontrolled proinflammatory cytokine storm, resulting in refractory hypotension (distributive shock). Each patient therefore has different fluid requirements. Hypovolemia and fluid sequestration are associated with the severity of AP, so patients with mild disease may have little or no fluid sequestration, but local and systemic complications (which define moderate-to-severe AP) are closely linked to it.
Marked hypovolemia may result in decreased urine output, renal failure and hypotension. Fluid rate and volume. In 1998, a case-control study compared hematocrit among patients with necrotizing versus mild AP. Hemoconcentration or the failure of the admission hematocrit to decrease at 24 hours were strongly associated with pancreatic necrosis. The authors suggested that hemoconcentration was the cause of pancreatic necrosis through impaired pancreatic blood flow and recommended aggressive fluid resuscitation to improve outcomes in all patients with AP. In the following decade, guidelines and reviews addressing AP recommended aggressive fluid resuscitation as the standard of care. However, local complications such as pancreatic necrosis are associated with fluid sequestration, as intravascular liquid leaks to the retroperitoneum. This results in hemoconcentration and it has been suggested that an increased hematocrit is in fact a marker for local complications, not their cause (reverse causation bias). In 2009 and 2010, two studies were published by the same research group. These were open-label randomized controlled trials (RCTs) comparing a more aggressive versus a moderate fluid volume administration (10-15 vs 5-10 mL/kg/hour) and a more rapid versus a slower hemodilution (hematocrit < 35% vs >= 35% at 48 hours) in patients with severe AP according to the classic 1993 Atlanta criteria (presence of local complications and/or organ failure). In both trials, the more aggressive approach was associated with worse outcomes, including decreased survival. These studies suggested that vigorous resuscitation is dangerous in patients with severe AP. The studies had some flaws, including being open-label single-center RCTs, with suboptimal randomization, an unexpectedly high rate of sepsis in patients with aggressive resuscitation and large differences in mortality between treatment groups despite a very small sample of patients, and they require validation. A recent open-label RCT conducted in the USA included patients with predicted mild AP (without SIRS criteria or OF at presentation). Patients received aggressive (20 mL/kg bolus followed by 3 mL/kg/hour) or standard (10 mL/kg bolus followed by 1.5 mL/kg/hour) hydration with lactated Ringer's solution. The main outcome was clinical improvement within 36 hours, a composite variable which required all of the following to be fulfilled: decrease in hematocrit, BUN and creatinine from baseline; decrease in abdominal pain; and tolerance of oral nutrition. A higher proportion of patients treated with aggressive resuscitation showed clinical improvement at 36 hours (70% vs 42%; p = 0.03) and the frequency of patients with persistent SIRS was lower. Unfortunately, the outcome variable was too dependent on hemodilution, which is not necessarily a marker of a better course of disease; patients receiving more aggressive resuscitation simply become hemodiluted more quickly. A study examining goal-directed fluid resuscitation (mainly based on serum BUN) did not find an association with improved outcomes, but it was probably underpowered because to a lower than expected incidence of SIRS. Well-designed RCTs are still needed to ascertain the role of aggressive and goal-directed fluid resuscitation. Currently, it is clear that fluid volumes and rates need to be tailored to individual patient requirements. Urine output is a particularly sensitive measure for detecting early hypovolemia due to fluid sequestration and increased losses. Decreased urine output and blood pressure due to true hypovolemia tend to respond quickly to aggressive fluid resuscitation; however, distributive shock results in refractory cardiovascular and renal failure regardless of fluid volume, and will require vasoactive drugs in an intensive care unit setting. Fluid type. Both an open-label and a triple-blind RCT have demonstrated that fluid resuscitation using lactated Ringer's solution (a balanced salt solution) is associated with a decreased inflammatory response when compared with fluid resuscitation with normal saline (with a high chloride content). According to in vitro experiments, it seems that this anti-inflammatory effect depends on lactate. However, both studies included a small number of patients, and the influence of lactated Ringer's solution on important outcomes such as OF or mortality is unknown.
In studies addressing other clinical scenarios, balanced fluids such as lactated Ringer's solution seem to be associated with a decreased need for blood products and a lower incidence of renal replacement therapy, hyperkalemia and postoperative infections when compared with normal saline. For the time being, it seems that lactated Ringer's solution is a good choice for fluid resuscitation in AP. Opioids are widely used for the management of pain in AP. According to a systematic review, when compared with other analgesic options, opioids may decrease the need for supplementary analgesia. There is currently no evidence for any difference in the risk of complications of pancreatitis or serious adverse events resulting from opioid use versus other drugs. Metamizole, an atypical non-steroidal anti-inflammatory drug (NSAID), is effective against pain in AP but it has been associated with a high rate of neutropenia, although current meta-analysis suggests that it is a safe drug. Traditional NSAIDs are also effective against pain in AP, but there are concerns about their adverse effects, particularly gastrointestinal bleeding due to gastric and duodenal ulcers and renal failure. As there are only a few studies, involving small samples of patients, comparing NSAIDs with other drugs or placebo, a higher incidence of adverse events has not been demonstrated in AP. A recent review of the use of NSAIDs in managing perioperative pain concluded that they do not increase the incidence of gastrointestinal bleeding or renal failure. Therefore, at present, NSAIDs seem safe for use in AP, at least in stable patients without renal impairment and no history of peptic ulcers. However, specific studies are lacking. Epidural analgesia is an option for managing refractory pain and in patients with severe disease. Some observational studies suggest that it may improve outcomes and RCTs are currently being performed comparing epidural versus conventional analgesia in AP. Oral refeeding and nutritional support. Almost all patients with AP initially suffer abdominal pain, nausea and vomiting, and in most cases of mild AP these symptoms are the only impediments to oral refeeding. In moderate-to-severe disease, several factors influence the reintroduction of oral feeding and the need for nutritional support. Paralytic ileus may be present in some patients, but it usually resolves in 24-48 hours. Local complications may be associated with early satiety and/or gastric outlet obstruction in a small proportion of patients. Moderate-to-severe AP seems associated with hypermetabolism and negative nitrogen balance with negative energy balance. AP may induce new-onset or worsening of pre-existing diabetes mellitus. Patients with persistent OF are usually sedated, so oral refeeding is not possible. Nutrition in mild AP. In the past, relapse of pain due to early or too rapid reintroduction of oral feeding was of great concern; however, it is not a frequent problem in mild disease. An RCT in patients with predicted mild AP showed that oral refeeding after abdominal pain has resolved is safe and shortens the duration of hospitalization compared with a more gradual reintroduction of oral feeding. Other RCTs have confirmed that early reintroduction of solid oral feeding in mild AP is safe and does not result in a higher incidence of complications or prolonged hospitalization. Therefore, oral refeeding in patients with mild AP should be initiated after pain has resolved. The type of diet appears irrelevant in terms of outcomes, so a fully solid diet can be given. Nutrition in moderate-to-severe disease. As intrapancreatic activation of zymogens seems to be the cornerstone of AP, and oral feeding is associated with increased secretion of enzymes, before the 1990s it was believed that 'pancreatic rest' improved outcomes in cases of moderate-to-severe AP, based mostly on basic science studies. As a consequence, parenteral nutrition was considered a standard of care in patients with predicted severe disease. In the 1990s, several RCTs compared nasojejunal tube-based enteral nutrition and parenteral nutrition in patients with predicted severe AP. Parenteral nutrition was associated with higher mortality, more prolonged hospital stays, a higher risk for pancreatic infections, OF and a need for surgery, so nasojejunal tube-based enteral feeding was considered as a standard of care in predicted severe disease.
Because pancreatic secretion is stimulated by the presence of acid and nutrients in the duodenum, a nasojejunal tube was viewed as a perfect way to deliver enteral feeding in AP, as it would not activate pancreatic secretion, but would result in a series of theoretical advantages, such as strengthening the barrier function of the gut, resulting in a lower permeability to toxins and bacteria. However, placement of a nasojejunal tube is not easy, it is time-consuming and results in great patient discomfort. Furthermore, it has been shown that standard nasojejunal feeding does in fact stimulate pancreatic secretion. Three studies comparing a nasogastric versus a nasojejunal route found no difference in outcomes, so both feeding routes can be recommended, depending on the presence or absence of gastric outlet obstruction. In 2014, the Dutch Pancreatitis Study Group published an RCT in patients with predicted severe AP which compared early (within 24 hours) nasojejunal enteral nutrition versus on-demand nasojejunal enteral nutrition (the patients tried oral refeeding on the third day and enteral nutrition was given on the fourth day only to those who could not tolerate oral feeding). The study showed that on-demand oral feeding was not associated with worse outcomes, and only 31% of patients needed a nasojejunal tube. Overall, therefore, in patients with moderate-to-severe AP, oral refeeding on day 3 to 4 should be encouraged and nasogastric or nasojejunal tube feeding from day 4 should be reserved for patients who cannot tolerate oral refeeding or who are sedated. The nasojejunal route is indicated in patients with gastric outlet obstruction. Carbapenems, quinolones, metronidazole and high-dose cephalosporins are antibiotics known to penetrate the pancreas, and are effective against gut-derived bacteria. The prophylactic use of antibiotics in necrotizing pancreatitis has not been linked to improved outcomes in well-designed double-blind RCTs. The treatment of infected pancreatic necrosis is discussed in Chapter 7. Early endoscopic retrograde cholangiopancreatography. Gallstone AP is associated with choledocholithiasis, but in most cases the stones are cleared to the duodenum spontaneously. The first single-center RCTs suggested a benefit for early endoscopic retrograde cholangiopancreatography (ERCP) in AP, but later, better designed RCTs which excluded patients with acute cholangitis, showed that regardless of severity and the presence of choledocholithiasis, patients undergoing ERCP within the first 72 hours do not have better outcomes. Patients with AP and acute cholangitis may benefit from early ERCP, as the first studies (those showing better outcomes for early ERCP) included patients with this complication. Preliminary data from a large Dutch multicenter study, the APEC trial, have confirmed the lack of benefits from early ERCP in AP. Other supportive measures. Deep vein thrombosis and peripancreatic vein (portal vein, splenic vein, superior mesenteric vein) thrombosis are frequent in patients with moderate-to-severe AP, leading to a high risk of pulmonary embolism and left-sided portal hypertension, respectively. As there are no specific recommendations for preventing deep vein thrombosis in AP, the clinician should apply existing general guidelines addressing acutely ill patients. Also, following general principles for the prevention of gastrointestinal bleeding in acutely ill patients (particularly in those in an intensive care unit setting), medical prophylaxis should be initiated when risk factors for stress ulcers are present: proton pump inhibitors, anti-H2 medications, sucralfate or antacids are all appropriate according to a recent Cochrane review, although many questions remain regarding the effect of these measures on outcomes in critically ill patients.
Status epilepticus and seizure clusters. Status epilepticus (SE) is a life-threatening medical emergency characterized by frequent and/or prolonged epileptic seizures. Community-based studies in the USA suggest the incidence may be as high as 50 per 100000 people per year, peaking in children under 1 year of age and in adults over 60 years of age. With the aging of the population, it is likely that SE will become an increasingly important public health problem. Traditionally, SE is diagnosed when the patient has continuous or repeated seizure activity without regaining consciousness for more than 30 minutes. This time frame is defined on the basis of decompensatory cerebral damage after 30 minutes of seizure activity when physiological changes fail to compensate for the increase in cerebral metabolism. In practice, however, most authorities would recommend emergency antiepileptic drug (AED) treatment when a seizure has lasted more than 5-10 minutes, excluding simple febrile seizures. The most readily recognized type of SE is tonic-clonic SE, but it has been estimated that 25% of SE cases are 'non-convulsive' in nature. Diagnosis of the latter can only be established by concurrent EEG recording. Depending on the electrographic changes, non-convulsive SE is subdivided into complex partial and absence SE. SE is a neurological emergency that requires immediate treatment. SE may result from a variety of causes (Table 7.1), the commonest of which include non-compliance with antiepileptic medication, consumption of alcohol, metabolic problems, acute stroke and hypoxia. Mortality/morbidity. Mortality and morbidity reflect the underlying cause and the physiological effects of prolonged convulsions, including hypertension, tachycardia, cardiac arrhythmias and hyperthermia. Mortality is as high as 10%, rising to 50% in elderly patients. Mortality is higher when SE is secondary to an acute insult (exempli gratia acute stroke, anoxia, trauma, infections, metabolic disturbance). Conversely, SE resulting from a previous stroke, alcohol or AED withdrawal has a more favorable prognosis. A long duration of SE is associated with poor outcome. An effective management protocol should therefore be initiated immediately (Table 7.2). Any delay in treatment worsens the prognosis and reduces the likelihood of stopping seizures without having to resort to general anesthesia. The importance of a coordinated effort in the treatment of convulsive SE - involving ambulance technicians, emergency medicine specialists, medical intensivists and neurological specialists - cannot be overemphasized. Most centers would initiate treatment with a benzodiazepine intravenously (most commonly lorazepam or diazepam), followed by phenytoin or fosphenytoin, or phenobarbital. If the seizure persists, the patient might be considered as having refractory SE and general anesthesia would be warranted. Although not tested in randomized control trials, AEDs with intravenous formulations (exempli gratia sodium valproate, levetiracetam, lacosamide) are often used when first-line therapies fail.
Table 7.3 lists the schedules for treating resistant SE, as discussed at the first London Colloquium on Status Epilepticus on behalf of the Taskforce on Status Epilepticus of the International League Against Epilepsy in 2007. In persistent SE, it is important to watch for potential complications including hypothermia, acidosis, hypotension, rhabdomyolysis, renal failure, infection and cerebral edema. An underlying cause should continue to be investigated. Treatment response should be monitored clinically and with EEG. Seizure clusters. Some patients experience clusters of seizures (also called acute repetitive seizures) lasting from minutes to hours. Patients with frontal lobe epilepsy are particularly prone to clustering of seizures at night. Seizure clustering may occur around menstruation in women, or when patients do not take their usual AED therapy. In most cases, however, precipitating factors cannot be readily identified. These seizure clusters may not be defined as SE but nonetheless require therapeutic intervention. Acute treatment with a benzodiazepine such as clobazam after the first seizure can be given in an attempt to prevent further attacks. If the seizure cluster has occurred as a result of AED omission or dose reduction, reintroduction of the drug may be sufficient to abort it. During a seizure cluster, oral therapy in a child may be problematic and intravenous access is usually unavailable or difficult. Rectal diazepam administered by parents or other caregivers may be effective in this situation. Rectal diazepam is absorbed more rapidly than rectal lorazepam or oral diazepam because of its high lipid solubility. A gel-containing prefilled unit-dose rectal delivery system is commercially available. The doses used in clinical studies (0.5 mg/kg for children aged 2-5 years, 0.3 mg/kg for children aged 6-11 years, 0.2 mg/kg for those over 12 years) were effective and well tolerated, and did not produce respiratory depression. The most common side effect was somnolence. Buccal midazolam, available in Europe, is being increasingly used instead of rectal diazepam. In adults and children over 10 years of age, 10 mg can be given and repeated once if necessary. Lower amounts can be used in younger children. Nasal formulations of benzodiazepines are under development. Parents and caregivers must be adequately trained by knowledgeable healthcare professionals to be able to recognize seizure clusters, administer rectal diazepam or buccal midazolam, monitor the patient for potentially dangerous respiratory depression and summon emergency medical help when necessary. Excessive use of rectal diazepam can result in rebound seizures.
Management of essential thrombocythemia. The myeloproliferative neoplasms (MPNs) each present their own therapeutic challenges, and treatment should be tailored to the individual. However, the following principles apply to the management of all MPNs. low-dose aspirin should be considered unless contraindicated. cardiovascular risk factors such as hypertension or dyslipidemia should be treated aggressively, and smoking cessation actively promoted. Aims of treatment. Essential thrombocythemia (ET) is characterized by vascular features, including both thrombosis and paradoxical bleeding. In addition to the risk of major vessel thrombosis, thrombosis in the microcirculation (most likely transient) can give rise to symptoms such as red and painful extremities (erythromelalgia; Figure 4.1), headache, paresthesia, loss of vision or hearing and transient ischemic attack. Given this risk of thrombosis and hemorrhage, a key aim in the treatment of ET is to prevent blood clotting and bleeding; however, it is also important to alleviate any problematic symptoms. Stratification of treatment according to risk. As described in Chapter 3, the risk of thrombosis in ET is assessed on the basis of age and previous history of thrombosis. Thus, a patient below the age of 60 years with no previous history of thrombosis would be considered at low risk of vascular complications, whereas an older patient or one with previous thrombosis is considered to be at high risk. Similarly, Janus kinase 2 (JAK2) mutations are associated with a higher risk of thrombosis than are calreticulin (CALR) mutations, and may therefore be considered an additional risk factor for thrombosis in patients with ET (Figure 4.2), although this has yet to be fully incorporated into clinical practice. The choice of treatment depends on the patient's risk level. In general, patients at the lowest level of risk defined by the International Prognostic Score for Essential Thrombocythemia (IPSET) scoring system (Table 3.6, page 38) - younger patients without JAK2 mutations, no cardiovascular risk factors and no history of thrombosis - can often be managed by observation only, and they may not require aspirin (yet to be validated in clinical practice). Low-risk patients with JAK2 mutations, or cardiovascular risk factors in the absence of JAK2 mutations, should perhaps be treated with aspirin. As a general principle, all high-risk patients should be treated with aspirin (unless contraindicated) and cytoreductive therapy to reduce the platelet count. The National Comprehensive Cancer Network guidelines for the management of ET are summarized in Figure 4.3. Management of low-risk patients. Treatment to reduce cardiovascular risk factors, and promotion of a generally healthy lifestyle, may be sufficient intervention for very-low-risk patients. Smoking cessation should be actively promoted because cigarette smoking has been shown to be an independent risk factor for thrombosis in patients with ET. Low-dose aspirin has been shown to reduce both microvascular symptoms (exempli gratia erythromelalgia) and transient neurological and visual disturbances such as hemiparesis, scintillating scotomas, amaurosis fugax (transcient monocular blindness) and seizures. There is also evidence that antiplatelet therapy reduces the risk of venous thrombosis in patients with JAK2 V617F mutations and cardiovascular risk factors (arterial thrombosis). Higher doses of aspirin (up to 500 mg daily) may be required in patients with acute erythromelalgia. Cytoreductive therapy does not reduce the risk of thrombosis compared with observation alone in low-risk patients with ET, and recent data from the PT-1 study have shown that early cytoreductive therapy does not reduce mortality from thrombotic events. Management of high-risk patients. The agents most commonly used for cytoreductive therapy in high-risk patients with ET are hydroxyurea (also known as hydroxycarbamide), interferon-(IFN)-alpha, which suppresses hematopoietic cells in the bone marrow, anagrelide and busulfan (Table 4.1). 
First-line treatment. The aim of cytoreductive treatment in ET is to maintain a normal platelet count, although a direct correlation between platelet count and thrombotic events has not been reported in either ET or PV. Cytoreductive treatment reduces the rate of vascular complications in high-risk patients but it is not possible to identify a specific threshold platelet count that confers optimal protection against such complications. Hydroxyurea should be considered as first-line therapy, aiming to achieve a normal platelet count. The rationale for this comes from a landmark randomized trial in which the addition of hydroxyurea to aspirin significantly reduced the incidence of thrombotic events, from 24% to approximately 4% (p = 0.05). Although some studies have suggested that hydroxyurea is associated with an increased risk of transformation to acute myeloid leukemia (AML) or myelodysplastic syndrome (MDS), this has not been confirmed in large cohort studies. For example, in a large population-based case-control study in Sweden, the relative risk of AML or MDS in patients receiving hydroxyurea was 1.2 (95% confidence interval 0.6-2.4). Adverse events associated with hydroxyurea include neutropenia, macrocytic anemia, fever and mucocutaneous events such as skin lesions (including skin cancer) and leg ulcers. IFN may be considered as first-line therapy in younger patients (< 65 years) and in certain situations, such as pregnancy (see Chapter 8), but toxicity is a potential concern. For example, in a recent Phase II study of pegylated IFN-alpha-2a, 22% of patients with ET or PV discontinued treatment because of adverse events. The most common adverse events are fevers and flu-like symptoms. In addition, approximately one-third of patients have to discontinue treatment because of chronic adverse events such as weakness, myalgia, weight loss or gastrointestinal, autoimmune, psychological or cardiovascular symptoms. Anagrelide (see below) is also approved as first-line therapy in some countries. Second-line treatment. Anagrelide, IFN-alpha or busulfan may be considered for second-line treatment if the platelet count is uncontrolled with hydroxyurea. In the PT-1 study, which compared anagrelide and hydroxyurea in 809 high-risk patients with ET (defined according to the Polycythemia Vera Study Group criteria), anagrelide reduced venous thrombosis rates but was associated with higher rates of arterial thrombosis, major bleeding and transformation to myelofibrosis. Furthermore, compared with hydroxyurea, anagrelide was associated with significantly higher rates of cardiovascular, gastrointestinal, neurological and constitutional adverse events. A further study (ANHYDRET) showed non-inferiority of anagrelide compared with hydroxyurea in 259 high-risk patients with ET (World Health Organization criteria). Anagrelide is frequently used in combination with hydroxyurea and is generally well tolerated in this setting. These studies highlight a risk of bleeding when anagrelide is combined with aspirin. Busulfan remains a useful treatment but is often restricted to patients with limited life expectancy because of the increased risk of acute leukemia when used after hydroxyurea.
Busulfan can cause prolonged pancytopenia and must therefore be used with caution; it is often given as either a single bolus or a short course of 1-2 weeks, or for several weeks until the blood count begins to fall. The efficacy of the JAK1/2 inhibitor ruxolitinib in ET has recently been investigated in the MAJIC trial, which involved 110 patients with refractory disease or who were intolerant of hydroxyurea. After 1 year, there was no significant difference in complete response rates with ruxolitinib and best available therapy (46.6% versus 44.2%, respectively, p = 0.40). Similarly, rates of thrombosis, bleeding and transformation at 2 years did not differ between the groups. Moreover, ruxolitinib was associated with higher rates of grade 3/4 anemia and thrombocytopenia. Management of extreme thrombocytosis. Extreme thrombocytosis (platelet count > 1000 x 10 /L) may be associated with acquired von Willebrand syndrome, signs of which are present in most patients with ET or PV. Aspirin should be used with caution because of the increased risk of bleeding. It is recommended that such patients are screened for ristocetin cofactor activity (a measure of von Willebrand factor function), and withdrawal of aspirin considered if this activity is less than 30%. Note that extreme thrombocytosis has not, in itself, been shown to constitute a high risk in patients with ET, and the use of cytoreductive therapy does not affect the risk of bleeding or microvascular complications in young (< 40 years) asymptomatic patients with extreme thrombocytosis. A platelet count above 15 000 x 10 /L is regarded as an indication for treatment. Recognizing treatment failure. Typically, evaluation of treatment responses involves monitoring of blood cell counts and assessment of constitutional symptoms and their impact on the patient's quality of life and general wellbeing. Since the treatment of ET aims primarily to reduce the risk of thrombosis and bleeding, response criteria have traditionally focused on platelet count. Response criteria cannot be defined in low-risk patients treated with aspirin, whereas the aim of treatment in higher-risk patients receiving cytoreductive therapy is to reduce platelet counts to below 400-450 x 10 /L. However, the likelihood of thrombosis during cytoreductive treatment appears to be influenced more by the degree of leukocytosis (> 10 x 10 /L) than by the platelet count, which suggests that this is a more relevant response criterion. In clinical practice, an inadequate response to treatment in patients with ET (or indeed PV) may be identified by various criteria, including elevated platelet or white blood cell counts, burdensome symptoms and clinical events such as thrombosis or bleeding (Table 4.2). It is important to recognize developing resistance to standard therapy - hydroxyurea in particular - because of an increased risk of disease transformation.
Diet, lifestyle and chemoprevention. Effect on development of prostate cancer. Diet and lifestyle are clearly linked to the development of prostate cancer. The effects of obesity and a western-style diet as risk factors for the development of prostate cancer were mentioned in Chapter 1. A large number of studies have evaluated the effects of dietary manipulation/supplementation in reducing the incidence of prostate cancer; the current evidence is summarized in Table 2.1. Although randomized clinical trials have provided some indication that selenium and vitamin E have a protective effect, a large chemoprevention study (the Selenium and Vitamin E Cancer Prevention Trial [SELECT]), designed to determine whether they reduced the likelihood of prostate cancer when used singly or in combination, was ended prematurely because of disappointingly negative results. Cohort studies show that lycopene (in tomatoes) and isoflavonoids (found in soy products) may be associated with a decrease in the incidence of prostate cancer. Evidence for other dietary supplements is weak. A recently published but slightly dubious cohort study on risk factors for prostate cancer reported that having more than seven sexual partners increased the risk by 100%, and having more than five orgasms per month increased the risk by 59%. However, there is no evidence that reducing these factors reverses the risk!. Chemoprevention refers to the use of drugs to reduce the risk of cancer. The 5alpha-reductase inhibitor finasteride has been shown to reduce the incidence of prostate cancer by 24.8% compared with placebo over a 7-year period, although at the cost of a small but significant increase in sexual side effects. However, this is counterbalanced by the finding that a small proportion of cancers in the finasteride group tended to be more aggressive than those in the placebo group, although this may have been an artifact of taking biopsies from the smaller prostates in the active treatment arm resulting from the shrinkage effect of finasteride. A 2013 study reported that there was no difference in the overall survival (OS) rate, or survival after a diagnosis of prostate cancer, between the placebo-treated and finasteride-treated patients after 18 years of follow-up. Another 5alpha-reductase inhibitor, dutasteride, has been evaluated for its effect on the occurrence of prostate cancer in the REDUCE study (Reduction by Dutasteride of Prostate Cancer Events). Dutasteride resulted in a 23% reduction in the development of prostate cancer, mainly by suppressing the well-differentiated cancers, with only a slight (statistically insignificant) increase in Gleason pattern 7 or 8-10 poorly differentiated tumors. It also effectively treated the symptoms arising from benign prostatic hyperplasia (BPH). Neither of these compounds were approved by the regulatory authorities for chemoprevention. Statins have also been reported to have some chemopreventative properties although the evidence is weak and conflicting: a recent review found that only eight of 43 studies reported a positive association between statin use and a reduction in the development or progression of prostate cancer. Effect on progression. Very few trials have investigated the effect of diet and lifestyle change on prostate cancer progression. Table 2.2 outlines the current body of evidence. In addition, a large number of compounds - many of them herbal - have been tested in the laboratory and show potential; these include green tea and other polyphenols, resveratrol from red wine, vitamin D, epilobium and Serenoa repens (saw palmetto). It must be remembered that cardiovascular disease is still the primary cause of death in men, with or without prostate cancer, and heart-healthy lifestyle choices will reduce mortality in men with prostate cancer. These include improving lipid profiles, decreasing obesity and increasing physical fitness. A healthy diet and regular vigorous exercise may help protect the individual against various forms of cancer, in addition to decreasing the risk of death from cardiovascular causes. Evidence for a beneficial effect of exercise on prostate cancer-specific mortality is also increasing, and post-diagnosis recreational activity has been shown to significantly lower prostate cancer-specific mortality. The mechanism many not be merely related to decreased sedentary activity, however, as there was no association between sedentary activity and increased mortality. 
Anatomy, physiology and epidemiology. The pancreas. The pancreas is the shape of a small flat fish, 6-8 inches long and salmon pink in color. It lies behind the stomach, stretching between the duodenum on the right, to the center of the spleen (hilum) on the left (Figure 1.1). It is conventionally divided into the head, uncinate process, neck, body and tail. The pancreas is important for the production of. digestive enzymes - from the acinar cells. bicarbonate - from the duct cells (to neutralize gastric acid). insulin - from the cells of the islets of Langerhans (essential for glucose control). Epidemiology of pancreatic disease. In the year 2000 there were 1.15 million patients with non-malignant pancreatic disease in the USA. Each year 125 000 North Americans present with acute pancreatitis, 100 000 present with chronic pancreatitis and at least 45 000 die from diseases of the pancreas. Pancreatic cancer is a highly lethal cancer and ranks fourth among cancer-related deaths in the USA. It is estimated that about 48 960 people will be diagnosed with pancreatic cancer and about 40 560 people will die of pancreatic cancer in 2015 in the USA. In a study of a well-defined German population, the incidence rates for acute pancreatitis, chronic pancreatitis and pancreatic cancer per 100 000 inhabitants/year were found to be 19.7, 6.4 and 7.8 respectively. For acute pancreatitis, the highest incidence rates are in the USA and Finland. The incidence rate in Finland is 73.4 cases per 100 000 people. Similar incidence rates have been reported for Australia. The biliary and pancreatic ducts. The main pancreatic duct joins the bile duct to form the common channel or ampulla of Vater (also known as the major papilla). In 90% of people, the embryonic dorsal and ventral pancreatic ducts are fused to form this pancreatic duct, meeting in the head of the pancreas. In the other 10%, the ducts drain separately into the duodenum (pancreas divisum) and the dorsal duct (known as the accessory duct) drains through the minor papilla. Small sphincters around the ends of the main bile and pancreatic ducts control the flow of bile and pancreatic juice, respectively; the sphincter of Oddi controls the outflow from the ampulla of Vater. The gallbladder is tucked under the right liver lobe in the gallbladder fossa and is connected via the cystic duct to the common hepatic duct to form the common bile duct. Bile acids, essential for the absorption of fats and fat-soluble vitamins, are made in the liver and travel in canaliculi to reach the bile ducts. The intrahepatic bile ducts drain into the right and left hepatic ducts, which fuse to form the common hepatic duct. Epidemiology of biliary tract disease. Gallstones are prevalent worldwide and are a considerable cause of morbidity and mortality. They may cause acute biliary colic, acute cholecystitis or chronic cholecystitis, acute pancreatitis or cholangitis. Gallbladder carcinoma is the fifth most common gastrointestinal (GI) cancer in the USA and the most common GI cancer in Native Americans. Incidence and mortality are very high in certain Latin American countries, especially Chile. Of the most commonly seen GI cancers in the USA, Europe and Australia, gallbladder cancer is the least common compared with other parts of the world. In most EU countries (with similar trends in the USA and Australia), mortality rates for gallbladder cancer have declined by approximately 30% among women and 10% among men, but mortality is still high in central and eastern Europe. Bile duct cancer, or cholangiocarcinoma, may arise in the intra- or the extrahepatic biliary system, usually in people between 50 and 70 years of age. Sclerosing cholangitis affecting the biliary system may occur in association with diseases such as ulcerative colitis and in secondary form due to conditions such as AIDS. The gallbladder and the biliary system may also be affected by dyskinetic conditions such as sphincter of Oddi dysfunction and gallbladder dyskinesia. 
Brain metastases. Improvements in therapy for systemic cancer have increased the number of patients living long enough to develop symptomatic brain metastases. Indeed, brain metastases are the most common intracranial tumor. In particular, human epidermal growth factor receptor-2-positive (Her2+) breast cancer patients with systemic disease controlled with trastuzumab appear to have an increased risk of developing brain metastases. Thus, the development of new strategies to prevent and treat brain metastases is increasingly important. Approximately 1 in 4 patients with cancer will develop a brain metastasis; patients with melanoma, lung or breast cancer are at the greatest risk (Table 4.1). Most patients present with headaches or focal neurological deficits; 20% or more present with or develop seizures. Radiographically, metastases are ring-enhancing lesions, most often located at the gray-white matter junction. There is often significant surrounding edema. About half are single lesions (Figure 4.1), with the remainder being multiple lesions (Figures 4.2 and 4.3). Skull and dural metastases are most commonly seen in association with prostate and breast cancer. Patients with a new diagnosis of brain metastasis should be systemically restaged as appropriate for their primary tumor. The Radiation Therapy Oncology Group (RTOG) has delineated specific prognostic categories for patients with newly diagnosed brain metastases (Table 4.2). These may be helpful in determining appropriate treatment options and long-term plans (exempli gratia hospice care) for individual patients. A treatment algorithm for newly diagnosed brain metastasis is shown in Figure 4.4. Symptomatic management. Symptomatic management can result in a significant improvement in quality of life for patients with brain metastases. Vigilance for less common complications of brain metastases, such as the syndrome of inappropriate antidiuretic hormone secretion (SIADH), is critical. Vasogenic edema secondary to metastases typically responds to treatment with corticosteroids within a matter of hours. Dexamethasone is the corticosteroid most often used. For initial symptom control, it can be administered in two, three or four daily doses, with each dose between 4 mg and 20 mg. Unfortunately, there are significant side effects associated with corticosteroids including, but not limited to, myopathy, hyperglycemia, edema, weight gain, avascular necrosis and psychosis. All patients on prolonged corticosteroids should receive prophylactic therapy for Pneumocystis carinii pneumonia. Steroids should be tapered as rapidly as possible (decreasing the dose every 3 days, as tolerated) to minimize side effects. Eliminating doses given late in the day and minimizing multiple daily doses may help to limit the side effects. Anticonvulsants are indicated for any patient presenting with a seizure. Prophylactic anticonvulsants have not been shown to be effective and therefore only complicate patient management, increasing the risk of medication-related side effects. Non-convulsive status epilepticus should be considered in any patient with an altered level of consciousness (see Fast Facts: Epilepsy). Radiotherapy and surgery. Standard therapy for a patient with newly diagnosed brain metastasis remains whole-brain radiotherapy. A total dose of 3000 cGy is administered in ten daily doses of 300 cGy. Prognosis following standard therapy is poor, with an average survival of 3-4 months.
About 50% of patients die from progressive neurological disease; the remainder die as a result of systemic tumor progression. Aggressive focused management of brain metastases may improve the prognosis for some patients. Those with a single brain metastasis located in a surgically accessible region clearly benefit from a combination of surgical resection and whole-brain radiotherapy. Stereotactic radiosurgery may also be helpful. In this patient group, the median survival with either of these approaches is 8-9 months. These results, while encouraging, have been obtained in highly selected patient populations, most of whom had a high Karnofsky performance status, limited systemic tumor burden and small brain metastases. Such patients have an inherently better prognosis, and consequently these results may not be easily extrapolated to all patients. (The Karnofsky performance status is scored from 0 to 100, with higher scores meaning a patient is better able to carry out daily activities. Controversy still surrounds several specific issues. First, there is no consensus on the number and size of lesions that are appropriate to treat with stereotactic radiosurgery. In general, it is reasonable to treat up to three individual metastases that are smaller than 3 cm at the largest diameter. Second, the optimal timing of whole-brain radiotherapy following surgical resection or stereotactic radiosurgery is unclear. While early whole-brain radiotherapy improves local control, there is no clear survival benefit. As a result, many practitioners are inclined to defer whole-brain radiotherapy until there is progressive disease in the brain. This strategy may minimize treatment-related neurocognitive deficits and permit much needed palliation at a future time. Use of chemotherapy in the treatment of brain metastases is generally disappointing. Unfortunately, by the time most patients develop brain metastases, they have already been exposed to the most effective chemotherapeutic agents and the metastatic clones are relatively chemoresistant. Although the blood-brain barrier is focally disrupted, water-soluble chemotherapy may not penetrate sufficiently to reach a therapeutic concentration. In spite of this, chemotherapy may be useful in treating some individuals with brain metastases. Newly diagnosed patients who are chemotherapy nave and neurologically asymptomatic may respond favorably to systemic chemotherapy. They should be followed closely with serial MRI and neurological examinations to monitor their response to treatment. In particular, patients with brain metastases from small-cell lung cancer, breast cancer, testicular cancer or choriocarcinoma may have a better than average response to chemotherapy. Chemotherapy or radiosensitizing agents may be combined with whole-brain radiotherapy in an attempt to improve the outcome compared with either modality alone. A European Organization for Research on Treatment of Cancer (EORTC) study found a significant survival advantage for patients with small-cell lung cancer treated with teniposide in combination with whole-brain radiotherapy, when compared with teniposide alone. Similarly, a significant improvement in radiographic response was seen in patients with newly diagnosed brain metastases who were treated with temozolomide in combination with whole-brain radiotherapy, when compared with whole-brain radiotherapy alone. Finally, chemotherapy may be useful in the setting of recurrent brain metastases. At recurrence, most patients have multiple lesions that are not amenable to radiosurgery or focal therapy. Furthermore, the majority have active and often symptomatic systemic tumors. This clinical scenario makes palliative chemotherapy an appropriate intervention.
Definitions and diagnosis. Problems defining depression. 'Depression', or 'clinical depression', refers to a mood disorder that ranges in severity from mild to severe, in duration from brief to enduring, and in pattern of illness from single to recurrent episodes. Diagnosis is based on the symptom profile and the severity, duration and course of the disorder. It is characterized by low mood and the absence of positive affect (id est loss of interest and enjoyment in everyday activities), together with a range of emotional, cognitive, physical and behavioral features. Clinical depression is a heterogeneous disorder, with some forms having clear biological underpinnings whereas others arise from psychosocial adversity. While classification systems have specified diagnostic criteria, uncertainties remain about the validity of the threshold that differentiates clinical depression from 'normal' unhappiness. Some investigators and commentators have questioned the validity of depression on the basis of variations that are purported to exist in its presentation, prevalence, prognosis and meaning within different cultures. Some commentators identify psychiatric diagnoses such as depression as 'Western' categories that can be imposed on non-Western peoples - a form of medical imperialism. There are then fundamental problems in the notion of depression as a psychiatric disorder. But where does this leave us? Some important aspects of validity adopted from psychometric theory are relevant to considering the robustness of the concept of depression as a disorder: there is good evidence of the concurrent and predictive validity of this diagnosis; the diagnosis is associated consistently with disability, reduced quality of life and particular service needs; and there is extensive literature indicating the suffering that is associated with the condition and the natural history of the illness, its response to various treatments and its relationship to other conditions. Defining the boundaries of depression. The validity of the concept of depression is questioned on a number of counts. Much of the debate hinges on the basis for defining depression as a disorder and the associated difficulty in identifying a natural boundary between this construct and normal unhappiness following adversity on the one hand and grief responses on the other. Depression, like most psychiatric disorders, is defined by its symptom profile. In this respect, psychiatry is in the position occupied by most of medicine 200 years ago of delineating conditions by their symptoms and lacking clear evidence of etiopathology or biomarkers. Alongside the lack of clear biological characteristics for depression (as well as for a large number of other psychiatric conditions), there is a problem of where to draw the boundary between normal and pathological depression. The distinction between depression and the distress and angst that are part of normal human experience requires a dichotomization of this continuum of symptoms. Rather than being based on 'hard' criteria related to a clear causal mechanism, with associated biological markers, this differentiation is based on clinical characteristics. Although this is problematic, it is useful to bear in mind that this also applies to other common medical conditions such as diabetes, hypertension and irritable bowel syndrome, where the variation between extensive and disabling symptoms and few and insignificant symptoms are on a continuum. Delineating depression from other mental disorders. In the community, as well as in primary care settings, the most prevalent mood disorder is a combination of depression and anxiety. The overlap between symptoms such as low mood, lack of energy, insomnia, worry and irritability is considerable; for example, in 2014, nearly 8% of the UK household population demonstrated this combination of symptoms to a clinically significant extent, although smaller proportions have sufficient symptoms to be diagnosed with depression or anxiety disorders. Among people who meet the diagnostic thresholds for depression or anxiety disorders there is also a high degree of concurrence of these conditions, raising questions about the specificity of these diagnostic categories. While there is no distinct etiology for depression - it is best conceptualized as having a mix of biopsychosocial and lifestyle factors that contribute to its onset - a common genetic factor for anxiety and depressive conditions is apparent, and similarities in the types of environmental adversities that seem to provoke depression and anxiety, such as childhood adversity, have been reported. Moreover, the pharmacological treatments principally classified as antidepressants are also effective in anxiety disorders, and similar types of individual, group and internet-based psychological treatments appear to be effective for both conditions. The benefit of defining depression. Despite some very real questions about the central validity of the concept of depression, there is little doubt among researchers and clinicians that this classification (like most psychiatric diagnoses) is useful, if not invaluable. The diagnostic entity of depression allows clear communication between researchers, clinicians and service developers and providers, as well as service users and carers; it also enables us to develop greater understanding through international research that will provide richer and more precise detail of etiology and risk, illness course, and management and treatment. It is inconceivable that these activities could persist in the absence of clear criteria for this condition based on the presence of particular symptoms. A consensus exists that the many benefits of applying the classification system for depression, as for other psychiatric conditions, outweigh the disadvantages. 
The cluster of symptoms experienced in depression is central to its classification as a mental health disorder. The number, intensity and effects of these symptoms are central to differentiating depression from normal experiences and from other disorders. There is a consensus concerning the symptoms of depression. The two most widely used diagnostic systems are the World Health Organization's International Classification of Diseases and Related Health Problems, currently in its tenth revision (ICD-10), and the American Psychiatric Association's Diagnostic and Statistical Manual of Mental Disorders, currently in its fifth edition (DSM-5). The criteria used by these classification systems are broadly similar, although the symptom thresholds differ. The DSM-5 criteria are used as the basis for clinical descriptions of depression and the related disorders considered in this book, as the majority of research and the most recent clinical guidelines that inform knowledge about depression and its management use the DSM criteria. In DSM-5 the depressive disorders are separated from the bipolar disorders, even though episodes of depression are common to both. Depression arising in individuals with bipolar disorder is referred to as 'bipolar depression' and its treatment is different from the treatment for 'unipolar' major depression. Distinguishing bipolar depression (characterized by a prior history of mania or hypomania) is important in primary care, as inappropriate treatment could trigger an episode of mania (see Fast Facts: Bipolar Disorder). Major depressive disorder. In the DSM-5 classification, the central condition of 'depression' is 'major depressive disorder', which is experienced as a single or recurrent episode. This classification also has a number of specifiers relating to severity, remission status, pattern of illness (single or recurrent episodes, seasonality), onset (specifying a perinatal onset) and clinical features, as well as describing subtypes of depression (Figure 2.1). Within ICD-10, it is simply called 'depressive episode' or 'recurrent depressive episode'. The terms 'clinical depression' and 'unipolar depression' are also often used. A diagnosis of (major) depression is based on a person experiencing a particular group of unpleasant persistent symptoms that affect mood, thinking, motivation and physical functions and have clear effects on the person's ability to conduct normal activities. The cardinal features of low mood and diminished interest are central to the condition; at least one of these has to be present for a diagnosis to be made. A minimum number of additional depressive symptoms must also be present, such that at least five of the group of nine symptoms are present in total (Table 2.1). Major depression needs to be distinguished from normal bereavement where similar symptoms may occur; however, a major depressive episode in addition to the normal response to a significant loss may be considered. A diagnosis of depression is excluded if the symptoms are judged to be due to the direct physiological effects of a medical illness or prescribed or illicit medications. Single episode or recurrent. A generation ago, standard psychiatry texts typically considered depression as an acute illness best managed by specialist treatment. The findings of longitudinal observational studies have increasingly revealed the variability of illness course and show that, for many people, depression has a lifelong episodic course, characterized by relapses. There is consistent evidence from population-based studies (as well as primary and specialist care samples) that around 50% of people who have an initial depressive episode will have further episodes. Each episode of depression increases the risks for additional episodes, with a 70% risk of relapse following two episodes and a 90% risk after three. People with a history of depression have been found to have, on average, between five and nine further episodes during their lifetime. As well as specifying whether depressive disorder is a single episode, recurrent or persistent, the severity of the current episode is an important clinical descriptor.
Severity is clarified from the number and intensity of symptoms, with categorization as shown in Table 2.2. Psychotic features. The experience of abnormal perceptions (hallucinations) or beliefs (delusions) may be part of the presentation in a severe episode of depressive disorder. This is part of the symptom picture for around 15% of people who fulfill the criteria for a major depressive episode, providing a 0.4% lifetime risk for major depression with psychotic features. These features are particularly associated with experiencing feelings of worthlessness or guilt and may often share the themes of self-deprecation and blame; they are a serious aspect of presentation and may interfere with the individual's ability to make sound judgments in a way that puts them at increased risk of harming themselves. The presence of psychotic symptoms is a strong indication that specialist multiprofessional management is required, possibly including inpatient assessment and treatment. Melancholic features. Melancholia used to be known as 'endogenous' depression. It is an important subtype of depression as it has a more neurobiological basis and requires pharmacological treatment. It refers to a generally severe presentation in which there is pervasive anhedonia - the loss of interest or pleasure is extreme, with a complete or near-complete lack of reactivity of mood to positive events. There is a pattern of the depressed mood that is substantially worse in the morning (diurnal variation). Sleep is disturbed, with the person waking early in the morning and being unable to get back to sleep. Psychomotor retardation or agitation is likely to be pronounced, and significant appetite and weight loss and excessive or inappropriate guilt are characteristic of this subtype. In DSM-5, the diagnostic features include anhedonia and non-reactivity along with at least three of the following. distinct quality of depressed mood. depression worse in the morning. early morning awakening. marked psychomotor change. significant anorexia or weight loss. excessive or inappropriate guilt. Seasonal affective disorder (SAD) occurs when depression recurrences follow a seasonal pattern. Rather than a separate diagnosis, this is a specific category of major depression (and so the person must meet depression diagnosis criteria). DSM-5 criteria for this particular pattern of recurrent major depressive disorder specify that. depressive episodes occur repeatedly at a particular time of the year. remissions between episodes occur at a particular time of the year. two major depressive episodes must have occurred exhibiting this seasonal pattern during the past 2 years, with no non-seasonal episodes in this period. seasonal depressive episodes outnumber other depressive episodes during the individual's lifetime. Typically, episodes occur during the winter months and are characterized by decreased activity and reduced energy, and withdrawal from social activity, together with atypical depressive symptoms - most usually increased sleep and cravings for carbohydrates, leading to weight gain. Although the geographic distribution of SAD has not been rigorously studied, evidence suggests that SAD is more prevalent in northern latitudes. The validity of SAD is less well accepted in Europe than in the USA, and the ICD-10 has only provisional criteria for research. Perinatal or peripartum depression is a term that encompasses depression during pregnancy as well as following delivery. It is not considered a separate mood disorder from major depression: its symptoms are no different from depression symptoms arising at other times in life. In DSM-5, peripartum refers to onset of depression during pregnancy or within 4 weeks following childbirth (see Chapter 5). Around 10-15% of women may be affected.
Persistent depressive disorder. Persistent depressive disorder has replaced the terms 'dysthymia' and 'chronic major depression' in DSM-5. It is a chronic and unremitting form experienced by 10-20% of people with depression. Mixed anxiety and depressive disorder. Depression and anxiety disorders frequently coexist, with community studies consistently identifying a substantial proportion of people either currently meeting the diagnostic criteria for, or having a lifetime history of, both conditions. The strong association between lifetime risks for anxiety disorders and major depression means that an individual with one of these disorders has a 25-50% chance of developing the other disorder. Adjustment disorder. Depression is differentiated from adjustment disorder, which is a rather broad and ill-defined category comprising the emotional or behavioral response to a stressful event, characterized by marked distress or clearly reduced social functioning. Although features vary, they include one or a combination of depressed mood, anxiety or worry. The effects do not persist beyond a 6-month period, and the precipitating stressful event does not include bereavement. Bipolar disorder. An important distinction is made between 'unipolar' forms of depression, such as major depression and persistent depressive disorder, and bipolar depression, which is the form of depression found among individuals who have had an episode of hypomania or mania. 'Manic depression', a term first used in the late 19th century, was initially applied to this condition but the term bipolar disorder is now used. The defining characteristic of this disorder is having an episode of elevated mood and increased energy, differentiated by level of severity as either mania (bipolar I disorder) or hypomania (bipolar II disorder), as well as episodes of depression that are referred to as bipolar depression. In DSM-5, bipolar disorder is considered in a separate chapter from the depressive disorders. The symptoms of bipolar depression are the same as those of major depression, and differentiating bipolar depression from major depression requires a longitudinal perspective. Frequently, an episode of depression is the first 'mood' episode for individuals with bipolar disorder and so the diagnosis of bipolar disorder is not made and will only become manifest when an episode of (hypo)mania arises. The time between the first mood episode and the diagnosis of bipolar disorder can be up to 7 years. The individual may therefore not get the correct treatment for bipolar disorder (mood stabilizing medication) over this time. The treatment of bipolar depression is different from that of major depression. The response to antidepressants is poor and there is the risk of 'switching' to an episode of mania or hypomania.
For this reason, it is generally recommended that any antidepressant medication is accompanied by a mood stabilizer. While unipolar forms of depression are more common in women than in men, bipolar disorder affects men and women equally, and is far less common than depressive disorder, affecting about 5 people in 1000, with a lifetime risk of around 1% (although around 5% may experience subthreshold symptoms or bipolar spectrum disorder). The diagnosis of bipolar disorder is based on the person experiencing an episode of abnormally and persistently elevated or irritable mood (Table 2.4). Key features of this mood disturbance center upon inflated self-esteem or grandiose ideas, a decreased need for sleep, and increased mental and physical activity, involving over-talkativeness and racing thoughts. The person's attention span is often reduced and there is a tendency to engage in reckless and uncharacteristic behavior - such as spending sprees, extravagant or impractical schemes, sexual indiscretions, gambling or substance misuse. Behavior may become intolerant or aggressive, and in extreme forms of a manic state the person may experience psychotic features - delusions and hallucinations - that are usually congruent with the person's disturbed mood. The commonest psychotic symptom in bipolar disorder is grandiose delusions, but paranoid delusions or exaggerated mistrust and suspicion of others are often present; other psychotic symptoms may occur, including thought disorder, hallucinations and mood-incongruent psychotic symptoms. In the ICD-10 classification, bipolar affective disorder is diagnosed on the basis of more than a single episode of elevated mood, which is categorized as mania or the less extreme form, hypomania. Depressive episodes are a common part of this condition but are not regarded as essential for diagnosis within this classification. In the DSM-5 classification system, there are two disorder types: bipolar I disorder and bipolar II disorder. Bipolar I disorder involves the occurrence of one or more manic episodes or mixed episodes - involving a combination of the features of mania and depressive disorder - nearly every day, lasting for at least 1 week. Although individuals will often also have had one or more major depressive episodes, these are not necessary for a diagnosis. In bipolar II disorder, the person experiences an episode of hypomania rather than mania. Additionally, at least one major depressive episode must have been experienced. Although the diagnostic criteria specify a short minimum duration of elevated mood, this usually lasts for between 2 weeks and 5 months, with a median duration of around 4 months. See also Fast Facts: Bipolar Disorder.
National data have demonstrated an alarming increase in the prevalence of diabetes mellitus in the developed world. According to the US 2011 National Diabetes Fact Sheet, the prevalence of diabetes in the US population is 8.3%, with estimates ranging from 5.8% in Vermont to 11.3% in Mississippi. Prevalence varies from 7.1% in non-Hispanic whites to 12.6% in non-Hispanic blacks (with even higher rates in Mexican Americans). This equates to 18.8 million people with diabetes (and 7.0 million with undiagnosed diabetes). Data from Diabetes UK reveals that 4.45% of the UK population had diabetes in 2011, equating to 2.9 million people. This figure is expected to grow to 5 million people by 2025. With increasing urbanization in India there has been an explosive increase in the prevalence of diabetes, which has now reached 8.0%, with 50 million people with type 2 diabetes. In China, diabetes has become a major public health problem, with an estimated prevalence of 9.7%. It is clear that, of the 219 countries and territories monitored by the International Diabetes Federation (IDF), some nations have substantially higher rates of diabetes. Data from the IDF's 2013 Diabetes Atlas show the highest national prevalence of diabetes in adults in selected Pacific Islands, including Tokelau, Micronesia, the Marshall islands, Kiribati, the Cook islands and Vanuatu, followed by Saudi Arabia (Table 1.1). Worldwide comparative prevalence of diabetes is shown in Figure 1.1. According to the IDF, diabetes caused 5.1 million deaths globally in 2013 - approximately 6% of total world mortality. Even more people have died from cardiovascular disease, the risk of which is increased by diabetes-related comorbidities such as hyperlipidemia, hypertension and renal disease. Data for deaths attributable to diabetes according to world regions are shown in Figure 1.2. Indirect burden. Globally, most people with diabetes are in the age range 40-59 years, a time in which productivity at work and contribution to family life is anticipated. Illness, disability and premature death in this age group profoundly affect personal and family life, communities and national productivity. IDF data show that preventable complications of diabetes account for an additional 23 million years of life lost through disability and reduced quality of life. Predicted picture. Data extrapolations suggest the prevalence will continue to rise steeply and that by 2035, 592 million people will be affected. The IDF has identified several reasons for the steep increase anticipated in diabetes prevalence: overweight and obesity, unhealthy eating, sedentary lifestyles, urbanization and an aging population. In terms of the population-based burden of diabetes, China has the highest number of people affected, based on 2013 data collected by the IDF, followed by India and the USA (Table 1.2). Rapid changes in lifestyle associated with westernization have led to large increases in the prevalence of diabetes throughout Asia. Of concern, recent data show no sign that the rate is slowing. People of Asian descent develop diabetes at lower degrees of obesity and at younger ages. There are also data to suggest Asian people suffer longer from diabetes complications and die earlier than people in other regions. An accelerating factor appears to be childhood obesity, which is increasing at alarming rates in Asia. Access to drugs. There are clear disparities between nations in access to drugs. About 80% of people with diabetes live in the world's poorest nations, but 80% of medical expenditure for diabetes occurs in the world's economically richest nations. Insulin is not available in many such areas because of its cost. Inevitably, people with type 1 diabetes who cannot access insulin die. Recent trends in children. Type 1 diabetes develops in about 79 000 children under the age of 14 every year. Recent data indicate the rate of new cases of type 1 diabetes is increasing by 3% every year, promoted by, among other factors, escalating rates of childhood obesity. About 25% of all cases of type 1 diabetes are in South East Asia, with about 20% in Europe. Finland, Sweden and Norway have the highest incidence of type 1 diabetes. Rates of type 1 diabetes also appear to be increasing in eastern European countries. The rate of type 2 diabetes is also increasing in children, though more data are required. 
Treatment of relapsed and refractory PTCL. Relapsed and refractory PTCL are associated with dismal outcomes, with median overall survival (OS) of 5.3 months reported in a population-based retrospective series of 163 patients from British Columbia. A recent prospective international registry study (the T Cell Project) reported survival outcomes from 633 patients with relapsed or refractory disease following frontline therapy: median OS was 11 months for those with relapsed disease and 5 months for those with refractory disease. Only 16% of this large cohort proceeded to stem cell transplantation. Given these outcomes, treatment within a clinical trial is recommended for patients with relapsed or refractory PTCL. Several novel agents have recently been approved for the treatment of relapsed and refractory PTCL, and studies are defining how these agents are best used (id est as monotherapy or in combination) to improve survival outcomes. Treatment options. Decisions on the treatment of relapsed or refractory PTCL should first consider whether a patient is a candidate for potentially curative allogeneic stem cell transplantation (see Chapter 7). For these patients the National Comprehensive Cancer Network guidelines recommend either multi- or single-agent chemotherapy for salvage. Patients who are not candidates for transplant should receive single-agent therapy to provide palliative benefit with minimal toxicity. Multi-agent chemotherapy regimens for PTCL include conventional lymphoma salvage regimens such as ICE, DHAP and ESHAP (Table 6.1). Gemcitabine-based regimens have also shown activity, including GDP, GVD and GemOx. Single agents. Single agents approved for relapsed or refractory PTCL include pralatrexate, romidepsin and belinostat. Chidamide is a histone deacetylase (HDAC) inhibitor only approved in China and mogamulizumab is approved for human T-cell lymphotropic virus 1 (HTLV-1)-associated ATLL in Japan. Response rates seen with these treatments are presented in Table 6.2 and their administration and adverse effects in Table 6.3. Pralatrexate was the first drug for PTCL to be approved by the US Food and Drug Administration (FDA), in 2009. It is a folate analog that binds with high affinity to reduced folate carrier-1 and inhibits dihydrofolate reductase, leading to depletion of thymidine monophosphate and other nucleotides, and ultimately inducing apoptosis. PROPEL was a Phase II single-arm trial of pralatrexate in 115 patients with relapsed or refractory PTCL. Of 111 evaluable patients, 53% had PTCL-NOS, 15% had ALCL and 12% had AITL. Pralatrexate was administered intravenously over 3-5 minutes at 30 mg/m once weekly for 6 weeks, followed by 1 week of rest, until progressive disease or unacceptable toxicity. All patients also received vitamin B12 injections every 2 months and folic acid daily. The overall response rate (ORR) was 29%, including 11% of patients with complete response (CR) and 18% with partial response (PR). Response rates were lower in patients with AITL than in those with PTCL-NOS or ALCL: 8%, 32% and 35%, respectively. The median progression-free survival (PFS) was 3.5 months, and median OS was 14.5 months. Adverse events. The most frequent adverse events were mucositis, which required dose reductions in 23% of patients and caused withdrawal from treatment in 6%, and cytopenias. Subsequent experience has shown that addition of leucovorin, 25-50 mg, on the day after pralatrexate reduced the incidence of mucositis, and a prospective study of pralatrexate with leucovorin is ongoing. Histone deacetylase inhibitors were the second class of drugs to be approved for relapsed and refractory PTCL: romidepsin was approved by the FDA in 2011 and belinostat in 2014.
Romidepsin is also approved in the USA for the treatment of cutaneous T-cell lymphoma (CTCL). Neither HDAC inhibitor is currently approved in Europe for the treatment of PTCL. Inhibition of HDAC prevents the acetylation of histone lysine residues, thereby modulating genes responsible for multiple cellular processes, including growth inhibition, cell cycle regulation, apoptosis and immune modulatory pathways. HDAC inhibitors may also alter the post-translational acetylation of proteins in the cytosol, although the exact mechanism of action in T-cell lymphoma is still to be fully elucidated. Romidepsin is a cyclic tetrapeptide or depsipeptide that inhibits class 1 HDAC. It is approved at a dose of 14 mg/m infused over 4 hours on days 1, 8 and 15 every 28 days, on the basis of two clinical trials. An early trial involving 47 patients with relapsed PTCL reported an ORR of 38% (18% CR) and a median duration of response (DOR) of 8.9 months. A subsequent multicenter single-arm registration study of 130 patients with relapsed or refractory PTCL (including 69 with PTCL-NOS, 27 with AITL and 21 with ALK  ALCL) reported an ORR of 25% (19% CR); the ORR was 29% in PTCL-NOS, 30% in AITL and 24% in ALK - ALCL. Notably, 29% of patients in this study responded to romidepsin despite being refractory to their most recent systemic therapy. Histology, type of prior therapy and prior transplant did not predict response. While the median PFS for the whole cohort was 4 months, it was 18 months in patients who achieved a CR, and CR was sustained for longer than 3 years in some patients. The most common adverse events included nausea, asthenia and fatigue, thrombocytopenia, infections and diarrhea. Grade 3 or worse events included thrombocytopenia in 24%, neutropenia in 20% and infections in 19%. Electrocardiographic changes (mainly QTc interval prolongation) have been described with romidepsin and other HDAC inhibitors, so care should be taken to exclude other medications that have this effect. Belinostat is a hydroxamic acid-derived pan-HDAC inhibitor. It is administered at 1000 mg/m, by a short intravenous infusion on days 1-5 of every 21-day cycle. A Phase II study in 24 patients with relapsed or refractory PTCL or CTCL demonstrated an ORR of 25%, with 8% CR. The pivotal single-arm Phase II trial of belinostat in relapsed and refractory PTCL (BELIEF) enrolled 129 patients and reported a similar ORR of 26%, with 11% CR. Notably, the response rate was higher in patients with AITL than in those with ALK  ALCL or PTCL-NOS (ORR 46%, 5% and 23%, respectively). No responses were seen in patients with EATL or HSTCL. For the whole cohort, median PFS and OS were 1.6 months and 7.9 months, respectively. However, responding patients experienced a median DOR of 13.6 months. Twelve patients went on to receive stem cell transplantation. Adverse events occurring in over 25% of patients were nausea, fatigue, anorexia, anemia and vomiting. Only 12-14% of patients had grade 3 or worse hematologic toxicity, and grade 3 or worse constitutional toxicities (fatigue, asthenia, nausea) occurred in fewer than 3% of patients.
Monoclonal antibodies and immunoconjugates. Brentuximab vedotin is an immunoconjugate comprising a humanized CD30-specific antibody and the microtubule disrupting agent methylauristatin E (MMAE). It binds to the CD30 receptor and is internalized, where it liberates the MMAE fragment into the cytosol to disrupt the microtubule network and induce apoptosis. CD30 is expressed to a varying degree in PTCL, including in about 50% of PTCL-NOS cases, 20% of AITL, and most cases of EATL and ALCL (ALK  and ALK +). It is also thought that small amounts of MMAE are released by tumor cells and affect the tumor microenvironment, potentially explaining the activity of brentuximab vedotin in malignancies with low levels of CD30 expression. Efficacy and dosage. Brentuximab vedotin is approved in the USA for use in combination with chemotherapy (CHP + A) for previously untreated CD30-expressing subtypes of PTCL, irrespective of the degree of expression of CD30. In Europe, it is approved only for relapsed and refractory systemic ALCL. The standard dose is 1.8 mg/kg every 3 weeks. A Phase II registration study of brentuximab vedotin as monotherapy in 58 patients with relapsed/refractory ALCL reported an ORR of 86%, with CR in 57% of patients. Median PFS was 13.3 months, and median OS was not reached but was estimated at 64% at 4 years. By design, 70% of participants in the pivotal trial (ECHELON-2) had ALCL so the study was underpowered to identify any significant differences in outcomes in other PTCL subtypes. In a Phase II study of 35 patients with mature T-cell lymphomas of other subtypes with variable CD30 expression, ORR was 33% for PTCL-NOS and 54% for AITL, with CR rates of 14% and 38%, respectively. There was no correlation between the degree of CD30 expression and response in this trial. Adverse events. In the Phase II registration study, grade 3 or greater neutropenia was the most common adverse event, occurring in 55% of patients; peripheral neuropathy occurred in 53% and anemia in 52%. Grading and management of peripheral neuropathy is an integral part of therapeutic planning with brentuximab vedotin, and informs dose adjustments, treatment delays and discontinuations. The Total Neuropathy Score, which includes pinprick and vibration sensations, is a validated tool to measure sensory, autonomic and motor symptoms and may be helpful in the management of patients receiving brentuximab vedotin.
Mogamulizumab (KW-0761) is a humanized antibody targeted against CCR4 (also designated CD194) with enhanced antibody-dependent cellular cytotoxicity. CCR4 is expressed in up to 65% of patients with PTCL, as well as in a subset of normal T cells, including type 2 helper T cells and regulatory T cells. Efficacy and dosage. The registration trial that supported approval in Japan was a multicenter Phase II study in patients with relapsed or refractory ATLL, who received mogamulizumab intravenously at a dose of 1.0 mg/kg weekly for 4 weeks and then every 2 weeks. Twenty-eight patients were enrolled, 52% with the acute form of the disease, 22% with the lymphoma presentation and 26% with chronic ATLL. The ORR was 50%, with a CR rate of 31%, and the drug was active at all disease sites. A retrospective study in Japan reported outcomes from 77 patients with relapsed (23) or refractory (54) ATLL. The ORR was 42%, with 18 CR and 15 PR. The median survival time from administration of mogamulizumab was 7.7 months. A prospective study that compared mogamulizumab versus the investigator's choice of treatment of ATLL, conducted in the USA and Europe, reported best response rates of 28% and 8%, respectively. Blinded independent review determined an ORR of 11%. The activity of mogamulizumab in other PTCL subtypes has been explored in two studies. A European study enrolled 35 patients with relapsed PTCL, 11% of whom responded, with a PFS of 2 months. The response rate was 13% in those with PTCL-NOS and 15% in those with AITL; no responses were seen in patients with ALCL. A Japanese study involving 38 patients reported a response rate of 35% (13 of 37), including five CRs. Median PFS was 3 months. Responses were seen in patients with PTCL-NOS (19%) and AITL (50%), and in one patient with ALK  ALCL. Adverse events in the registration trial included infusion reactions in 89% of patients and skin rash in 63%. Of 14 patients with grade 2 or higher skin rash, 8 had a CR and 5 had a PR. Lymphopenia occurred in 96% of patients. The most common side effects in the Japanese study were cytopenia, rash, pyrexia and infusion reactions. Almost half (22 of 51 patients) who received more than four doses of mogamulizumab had skin reactions. Interestingly, disease outcomes were superior in those patients who experienced a rash, as compared to patients with no skin reaction.
PI3K inhibitors. The phosphatidyl-inositol 3-kinase (PI3K) inhibitors have shown activity in both B- and T-cell malignancies. Duvelisib (IPI-145) inhibits the gamma and delta subunits of PI3K. In a Phase I study of duvelisib, 25-100 mg daily, in patients with CTCL and PTCL (19 and 16 patients, respectively), the ORR was 50% in patients with relapsed and refractory PTCL, with 3 CR, and a median PFS of 8.3 months. Adverse effects included elevated liver enzymes, cytopenia, skin rash and immune-mediated pneumonitis. Studies of duvelisib in combination with romidepsin and bortezomib are in progress. Copanlisib (BAY-80-6946) is a PI3K alpha and delta inhibitor which showed activity in B-cell lymphomas and is approved by the FDA for the treatment of follicular B cell lymphoma. It is currently being evaluated in T-cell lymphomas. Checkpoint inhibitors. Checkpoint inhibitors have shown therapeutic benefit in many types of cancer. They act at a crucial intersection between malignant cells and immune effector cells in the tumor microenvironment. PD-1 recognizes ligands such as PD-L1 on tumor cells, leading to evasion of the host immune response. Inhibitors of PD-1 and other immune-blocking epitopes such as CTLA-4 have shown promise and are approved for the treatment of a range of refractory malignancies. Checkpoint inhibitors have yet to be investigated extensively in PTCL. Some partial responses were seen in five patients with PTCL treated with nivolumab in a Phase IB study. ENKTCL appear to be particularly sensitive to checkpoint blockade. A study of seven patients with relapsed ENKTCL treated with pembrolizumab, 100 mg every 2 weeks, reported 2 CR and 2 PR, and response correlated with reduction in Epstein-Barr virus titers. Results in patients with ATLL associated with the HTLV-1 virus have been mixed, with some patients demonstrating disease progression. Further studies are exploring the role of checkpoint inhibitors, alone and in combination, in patients with aggressive T-cell lymphomas. Other novel agents. Alisertib is an inhibitor of Aurora A kinase, which is overexpressed in PTCL. A Phase II study of alisertib in 37 patients with relapsed and refractory PTCL reported an ORR of 30%. A randomized Phase III trial compared alisertib, 50 mg daily for 7 days of a 21-day cycle, versus the investigator's choice of treatment (romidepsin, pralatrexate or gemcitabine) in 271 patients with relapsed or refractory PTCL. The ORR for alisertib was 33%, with PFS of 115 days, versus 104 days for the comparator arm. The ORRs in the comparator arm were 35% for gemcitabine (n = 8 of 23), 43% for pralatrexate (22 of 51 patients), and 61% for romidepsin (11 of 18 patients). Adverse events with alisertib included bone marrow suppression and diarrhea. Bendamustine is a bifunctional alkylating agent with chloroethylamine attached to a benzimidazole moiety. The Phase II BENTLY trial evaluated the activity of bendamustine, 120 mg/m, in 60 patients with relapsed or refractory PTCL. The ORR was high but the DOR was short (3.5 months) and only 7% of patients had responses lasting a year. Lenalidomide has anti-proliferative, anti-antiogenic and immunomodulatory effects across a range of hematologic malignancies, including demonstrable activity in relapsed PTCL. Several single-arm trials exploring lenalidomide, 25 mg per day for 21 consecutive days in 28-day cycles, reported response rates of 22-30% in heavily pretreated patients with AITL, ALCL or PTCL-NOS. The DOR was 3-5 months, and there was no evidence of sustained benefit. Lenalidomide in combination with romidepsin was explored in a Phase II trial in which romidepsin was administered at 8-14 mg/m on days 1, 8 and 15, and lenalidomide at 15-25 mg/day for 21 consecutive days of a 28-day cycle. The response rate in patients with PTCL was 67%. 
Pathophysiology and clinical stages. Advances in the management of heart failure (HF) over the past 20 years have been informed by a better understanding of its pathophysiology. There are few situations in cardiology where treatment has been as closely linked to an appreciation of the underlying science. Vicious cycle of heart failure. HF is a disease of inappropriate adaptation to injury. The body has a limited range of compensatory responses to circulatory impairment, mainly vasoconstriction and sodium and water retention (see below). In general, however, these adjustments to hypovolemia are poorly suited to pump failure, and increases in the preload and afterload of the failing heart lead to worsening HF (Figure 2.1). Neurohormonal pathways activated in HF include the sympathetic nervous system (SNS), the renin-angiotensin-aldosterone system (RAAS) and the natriuretic peptide (NP) system. They play a significant role in the pathophysiology of HF, and pharmacological interventions have been developed accordingly (see Chapter 7). Sympathetic nervous system. Sympathetic activation of the adrenergic system leads to vasoconstriction, which increases the resistance to blood flow and helps to maintain arterial pressure in the early stages of HF when cardiac output is reduced. However, vasoconstriction also increases the afterload on the heart, leading to a worsening of HF (see Figure 2.1). Renin-angiotensin-aldosterone system. Enhanced sympathetic outflow also activates the RAAS (Figure 2.2). Renin release from the kidneys causes increased formation of angiotensin I from angiotensinogen and, via the action of angiotensin-converting enzyme (ACE), angiotensin II. Angiotensin II causes systemic vasoconstriction and acts on the adrenal cortex to produce aldosterone, leading to sodium and water retention. In addition, aldosterone (which may be released even in the setting of ACE inhibition) contributes to myocardial and vascular fibrosis. Sympathetic stimulation also releases antidiuretic hormone, which leads to retention of free water and contributes to dilutional hyponatremia. Natriuretic peptide system. The natriuretic peptide family consists of A (atrial) and B (brain) type natriuretic peptides (ANP and BNP), which are produced by cardiomyocytes in response to atrial and ventricular stretch, and C type natriuretic peptide, which is secreted by endothelial and renal cells. NPs, mainly BNP, lead to increased sodium excretion and vasodilation, especially in the early phases of HF. BNP also has anti-remodeling properties. The biological action of BNP is mediated through membrane-bound natriuretic peptide receptors (NPRs) and the peptide is degraded by neutral endopeptidase (including neprilysin). It is postulated that HF is a state of relative BNP deficiency caused by both lack of biologically active peptide and resistance at a receptor level. In end-stage HF, the peptides may not be released because of myocyte loss. Other pathways also reflect an inappropriate response to injury. Cytokine release is increased in HF, leading to a variety of consequences including apoptosis. The role of these as contributors to the progression of HF, rather than a correlate, is debated. Certainly, the failure of tumor necrosis factor (TNF) inhibitors to improve outcome argues against a causative role. Remodeling of the myocardium. Global and local responses to maladaptive stimuli lead to myocardial remodeling, namely increased myocardial volume and mass and a net loss of myocytes.
The heart has the ability to change the force of contraction, and therefore stroke volume, in response to changes in venous return (the Frank-Starling mechanism). A reduction in stroke volume due to myocardial injury can be overcome by left ventricular (LV) enlargement. This is not a response that can keep occurring indefinitely - eventually a loss of LV function will occur due to reduced interaction between contractile elements, caused by their separation. LV hypertrophy (LVH) maintains wall stress as the LV enlarges. However, it is also eventually maladaptive as the hypertrophied myocardium exceeds the growth of its blood supply. Autonomic reflexes. The increase of sympathetic tone associated with HF leads to disturbance of the autonomic reflexes. Persistent elevation of the heart rate is maladaptive in the ventricle: disturbances of LV relaxation are common, so the shortening of diastole that occurs with tachycardia (the duration of systole remains stable) is not well tolerated. Insulin resistance is an important metabolic sequel to HF. It contributes to the disturbance of myocyte energy metabolism, leading to the description of the failing heart as 'an engine out of fuel'. Causes of insulin resistance include the underlying etiologies of HF (central obesity, diabetes mellitus) and loss of skeletal muscle (see below). Peripheral vasoconstriction, as described above, symptomatically may contribute to cold sensitivity. Loss of skeletal muscle is an important manifestation of HF, reflecting inactivity, consequences of circulating substances such as tumor growth factor (TGF)-beta and reduced cardiac output. In its most advanced manifestation, loss of skeletal muscle may lead to cachexia. The consequences of this process include contributions to insulin resistance as well as loss of the skeletal muscle circulatory bed. The loss of this vasculature represents an additional decrement in the amount of vasculature that can undergo vasodilation (and therefore unload the LV). Cardiorenal interactions. Reduced renal perfusion in HF (due to reduced stroke volume and vasoconstriction) is an important contributor to sodium and fluid overload. The exact links between cardiac and renal function have yet to be resolved. More marked disturbances of renal function, leading to coexisting renal failure, may also occur and pose problems for volume control. Clinical stages and functional classes. The clinical syndrome of HF represents the final manifestation of advanced disease. Although progress has been made in the management of this entity, the greatest hope of avoiding the adverse outcome of HF is to intervene at an earlier subclinical stage, when there is more likelihood of reversing the process. The American Cardiology Association (ACC)/American Heart Association (AHA) guidelines divide progression of the disease into four preclinical and clinical stages (Table 2.1). It is important to distinguish these from functional classes, as described in the New York Heart Association (NYHA) classification system, which is based on severity of symptoms and exercise capacity and can be used to assess response to treatment (see Table 2.1). ACC/AHA stages A and B are preclinical; these patients fall into NYHA functional class I. ACC/AHA stage C reflects patients with symptoms or signs of HF, so these patients may be classified functionally in any of the NYHA classes I to III. The functional status of patients in ACC/AHA class D (with marked symptoms and signs of HF) is usually limited (NYHA class II to IV). The association between functional class, cardiac function (LVEF) and prognosis is discussed in more detail in Chapter 9.
Acne care pathway. Initial assessment should include a thorough history (see Table 3.1, page 19) and take into account clinical presentation, extent and overall severity of the patient's acne (graded using a system such as The Leeds Acne Grading Scale; see Chapter 3). The type of treatment may be dictated by the predominant type of lesion, so good lighting is required for the physical examination in order to detect non-inflamed lesions, which can easily be missed. Clinicians must also take into consideration the psychosocial effects of the condition, using questionnaires such as Assessment of the Psychosocial Effects of Acne (APSEA), the Cardiff Acne Diability Index (CADI) or the Dermatology Life Quality Index (DLQI) (see Chapter 4), along with the presence of and potential for scarring, and failure to respond to previous therapy. Discussion with patient. A summary of the questions frequently asked by patients are shown in Table 8.1. Provide reassurance. It is important to reassure patients that acne is common, will not last forever, and that effective treatment is available. Dispel myths. Dispelling popular myths about acne can have a positive effect on a person's motivation to cope with the condition, improve adherence to treatment and stop the patient from adopting unnecessary or potentially harmful behavior (see page 17). Myths about the disease should be dispelled in a frank and thorough discussion with the patient. For example, certain foods do not cause acne, acne is not infectious and excessive washing does not help manage the disease (see Table 8.1). Inform about treatment duration and response. It should be stressed that acne is a chronic disease and that no response will be seen before a minimum of 4 weeks of therapy. This is especially important for teenage patients, who often become discouraged if results are not seen quickly despite the fact that they have been following their prescribed regimen. Patients must be informed of the potential duration of treatment. For mild disease, topical treatment may be required for up to 4 years. For moderate-to-severe disease, oral therapies (see Chapter 6) will be required for variable lengths of time; for example, a course of oral antibiotics may be given for 3 months. Oral contraceptives may be prescribed for at least 6 months, up to 12 months or more if necessary. Treatment with oral isotretinoin may last for 6-12 months according to the severity of the disease and the clinical response of the patient. In some cases, maintenance therapy, based on a topical retinoid, may be required for up to 1-2 years. Give skin care advice. Patients should be advised to undertake two gentle washings a day, and to dry without rubbing. The active pharmacological agent, such as a topical retinoid, can be applied 5-10 minutes later. After another 15-20 minutes, a moisturizer can be applied if required. Advice on oil-free cosmetics and sun protection should also be given. Provide clear advice on administration and side effects. It is important that patients receive clear instructions on how to use their treatment, and are made aware of the potential side effects. The prescriber should be satisfied that this has been understood. Provide useful resources. Pamphlets and online resources that summarize the cause of the disease and the principles of its treatment are available from the American Academy of Dermatology, the British Association of Dermatology, acne support groups and from some pharmaceutical companies (see Useful resources, page 100). Such information is helpful, but is not a substitute for an adequate discussion with the patient. 
Choosing the right treatment. In all cases, the choice of a specific agent or agents should depend on the type and severity of acne lesions as well as the presence of scarring and/or psychological disability. Clinicians should seek to achieve maximum efficacy and tolerability with minimum risk of adverse effects. With the exception of early comedonal acne in prepubertal children, it is rare for a patient to present with a single type of acne lesion. For this reason, combination therapy is the mainstay of acne treatment. For example, different topical therapies may be alternated morning and evening, or formulations containing a stable combination of products may be prescribed. Therapeutic agents should be chosen to match the type and severity of lesions observed in each individual patient. Rational use of combination therapy requires consideration of the pathogenic factors involved (see Chapter 2) and an understanding of how the various antiacne agents target one or more of these factors (see Chapters 5 and). Regimens should be designed to take advantage of the synergistic effects of agents with different mechanisms of action that can target a combination of these pathogenic factors. Management principles. A treatment algorithm based on the severity of acne is shown in Figure 8.1, and the care pathway depicted is discussed in more detail below. Early treatment of acne is likely, although not proven, to prevent or modify development of the disease; thus, early use of appropriate topical therapy in mild acne (see below) may well prevent development of moderate acne. Recognizing predictive factors for severity (see page 38) is important. A family history of acne and scarring should be taken into account when implementing therapy to prevent significant disease. Scarring has been shown to develop with more prolonged disease and the duration of inflammation appears to be relevant. Hence, early targeted therapy is associated with better prognosis. Mild acne. Comedonal acne with a mild inflammatory component can be managed with topical retinoids as first-line therapy (see pages 51-3). If this fails and the number of inflammatory lesions increases, the addition of topical antibiotics and/or benzoyl peroxidase (BPO) should be considered (see pages 54-5). In general, 40-80% of patients with non-inflammatory or mild-to-moderate inflammatory acne respond to topical therapy; this minimizes potential adverse effects associated with the use of systemic agents. When designing a topical therapeutic regimen, attention should be given to the specific formulations available. Existing preparations of some topical agents can cause significant local irritation (see page 55), which decreases tolerability and may, therefore, decrease adherence, particularly in teenage boys who are not accustomed to applying creams or lotions to their faces. Management of this and other side effects is discussed in Chapter 5. Moderate acne.
As the inflammatory component of acne increases and/or the acne fails to repond to topical combination therapy, oral antibiotics (systemic therapy) can be used in place of topical antibiotics (see pages 59-65), still in conjunction with a retinoid/BPO combination. However, given the more significant and more diverse side-effect profiles of systemic therapies - and the increasing problem of antimicrobial resistance to oral antibiotics - this requires careful patient selection and management. In general, the addition of a topical antibiotic to a regimen containing oral antibiotics offers no extra benefit; furthermore, if based on a different chemical group, it may lead to the development of multiple antibiotic-resistant strains of bacteria. For these reasons, combining topical and oral antibiotics is not recommended. Oral antibiotics. Second-generation tetracycline antibiotics (see pages 59-64) should be the systemic treatment of choice for those patients who can tolerate them. Second-generation tetracyclines are absorbed more rapidly, are more rapidly efficacious and achieve better adherence than first-generation tetracyclines. They are usually administered as capsules. If minocycline is used (not as first-line treatment, see pages 61-2), it is available as aqueous film-coated tablets. Patients who crush their tablets because they are unable to swallow entire pills may not absorb an adequate amount of antibiotic. Minocycline is thought to be associated with the lowest incidence of P. acnes resistance but development of resistance to oral tetracyclines is a potential problem in patients with acne. Resistance should be suspected if a patient's acne fails to respond, or improves then worsens after 3 months of treatment. In such cases, an alternative agent should be chosen. Erythromycin is now used only as a substitute for tetracyclines in women who 'may become' pregnant. The third-line treatment is oral trimethoprim (see page 65). Maintenance therapy. Global overutilization of antibiotics and the resulting emergence of antibacterial resistance (see pages 91-3) have led to calls to limit the use of oral antibiotics in acne treatments. Three well-designed trials indicate that the benefit achieved with a 12-week course of antibiotics plus a retinoid or retinoid/BPO fixed combination can be maintained with continued use of the retinoid or retinoid/BPO combination after the course of antibiotics. This approach limits the long-term use of antibiotics, thus minimizing the ongoing selective pressure for development of antibiotic-resistant strains of bacteria. Since microcomedones are thought to be the precursor lesions of both inflammatory and non-inflammatory acne, a topical retinoid regimen aimed at reducing microcomedones may be useful as maintenance therapy following treatment with oral antibiotics or isotretinoin (see below).
Moderate-to-severe acne. Hormonal therapy. As well as assessing specific cutaneous lesions, all patients with acne, particularly women, should be carefully examined for signs and symptoms of hyperandrogenism such as hirsutism, androgenetic alopecia, irregular menses and polycystic ovaries. This is important in order to decide whether or not to conduct an endocrine work-up (see Hyperandrogenism, pages 28-9). Details of the most commonly administered hormonal treatments (combined oral contraceptives, cyproterone acetate, spironolactone) are given in Chapter 6, pages 66-71). Hormonal therapy is indicated in moderate papulopustular, nodular and conglobate acne in female patients. It can also be considered as an alternative to first-line treatment with oral isotretinoin (see below) in more severe forms of acne. It is more frequently prescribed as adjuvant rather than standalone therapy. Hormonal therapy can be combined with topical and systemic antiacne drugs such as antibiotics, BPO, retinoids and azelaic acid, and will act in a complementary fashion. In Europe, the combined oral contraceptive containing cyproterone acetate (CPA), 2 mg, and ethinyl estradiol (EE), 35 g, is the treatment of choice in patients with moderate acne who need oral therapy and who are sexually active and need a contraceptive pill. The popularity of the oral contraceptive containing EE and drospirenone has also greatly increased because of its efficacy and good tolerability. As with regimens containing oral antibiotics, if the patient has comedones in addition to inflammatory lesions, agents such as topical retinoids should be added to the antiacne regimen. Expected response. Patients can expect to experience some clinical response after 3 months and the full effect of contraceptives after 6-9 months of treatment. Acne improves in 50-90% of cases, with a 30-70% reduction of inflammatory and non-inflammatory lesions. Isotretinoin (13- cis -retinoic acid). Indications for the use of oral isotretinoin are listed in Table 8.2. Dosage, response and side effects are discussed in detail in Chapter 6 (pages 71-4). Young men with highly inflammatory or cystic acne are at particular risk of developing a severe flare, and in some cases exuberant lesions similar to pyogenic granulomas may develop. It is strongly recommended that patients with severe inflammatory cystic acne who are at risk of complications such as these be referred to a dermatologist for treatment with isotretinoin and possibly a concomitant course of oral corticosteroids. Successful use of isotretinoin requires careful patient selection and education, personalization of the dosage (according to factors such as age, sex and bodyweight) and appropriate monitoring and follow-up on the part of an experienced dermatologist. Patients should be given immediate access to a dermatologist if any problem with therapy arises or if they fail to respond adequately to the drug. Expected response. Since its inception, the use of oral isotretinoin treatment has been responsible for a dramatic improvement in the appearance and psychological wellbeing of numerous individuals affected by moderate-to-severe acne. By the end of an adequate course of treatment, acne will have cleared in the majority of patients. Antibiotic resistance. Antibiotics have often been given for up to 6-12 months, with repeat courses over months or years. As a result, antibiotic-resistant P. acnes has been detected on an international scale (Table 8.3). Reasons to suspect P. acnes resistance are shown in Table 8.4. There are concerns that overuse of antibiotics will drive resistance in other commensal bacteria. Preventing resistance. It is difficult to culture P. acnes and, therefore, some overall guidelines are necessary to help the physician correctly suspect, prevent and treat P. acnes resistance.
The emergence of resistance has raised the need for antibiotic prescribing policies and the need to consider the use of non-antibiotic preparations wherever possible. Principles for the optimal use of oral antibiotics are summarized in Table 8.5. Current thinking is that antibiotics should be administered for the shortest duration possible, and that combining them initially with topical retinoids will enhance efficacy. The advised length of treatment is 12 weeks. If longer than 12 weeks is required, anti-resistance agents such as BPO must be used concurrently. Combining non-antibiotic and antibiotic therapies results in more rapid efficacy, and this in turn may reduce the antibiotic exposure time. The same antibiotic should be used again if it previously produced a good clinical response. Antibiotics should not be used in the maintenance phase of therapy. acnes resistance should be suspected in patients who are not responding, patients who were responding but have relapsed, and those who have had 2-3 courses of different topical and oral antibiotics. Suspected resistant P. acnes should be targeted with antibiotics for which there is less evidence of resistance; higher rather than lower doses of a topical antibiotic should also be used. Likewise, higher oral doses of therapy may be indicated. Some preparations, such as non-antibiotic antimicrobials, id est BPO and topical retinoids, will never be associated with P. acnes resistance, nor will oral contraceptives or oral isotretinoin. Poor response to therapy. The major reasons for poor response to conventional antibiotic therapy are outlined in Table 8.6. Adherence issues. Poor adherence to treatment is the main reason for poor response, usually as a result of inadequate instructions or the failure to emphasize to the patient the need for long-term and continued therapy, provided there are no side effects and progress is adequate. Side effects will interfere with the rate of progress, but many alternative therapies are usually possible. Initial follow-up at 6-8 weeks is important to assess and provide support. Patients can be motivated to adhere to prescribed regimens with some simple explanations and encouragement. For example. Treatment may be associated with short-term side effects; these may improve with continued use. At the start of treatment, acne can appear to worsen. It is a reaction to therapy; it will get better soon. It is important to explain why a specific treatment was selected, and to suggest ways in which the treatment can be incorporated into the patient's lifestyle and routines. Antibiotic resistance. acnes resistance is becoming an increasing reason for poor response. However, the relationship between the presence of high levels of resistance and clinical failure is not clear. The microbiome for every follicle is different: while most of the P. acnes strains may be sensitive to antibiotic in one follicle, requiring a low concentration of antibiotic to destroy all the bacteria, most of the P. acnes strains in another follicle may be resistant to antibiotic, therefore requiring higher than the minimal inhibitory concentration of antibiotic. Furthermore, some antibiotics such as the tetracyclines have anti-inflammatory action independent of their effect on P. However, it is likely that the microbiological evidence of resistant P. acnes in about 25% of patients is clinically relevant. Referral or consultation. In the UK, up to 14% of people consult their general practitioner (GP) about acne, and about 0.3% will require referral to a dermatologist. Urgent referral or consultation is appropriate for. patients who may require treatment with isotretinoin for. severe inflammatory, nodulocystic or scarring acne. acne fulminans. gram-negative folliculitis. patients whose acne may be associated with an endocrine disorder. patients with social or psychological problems. Routine referral is appropriate for. patients with, or who are likely to develop, acne scarring. pregnant patients with acne. patients with moderate acne that has failed to respond to treatment. The UK's National Institute for Health and Care Excellence (NICE) referral guidelines are shown by color coding in Figure 8.1.
Management of advanced and metastatic disease. A core principle in the management of solid tumors is to consider control of both the primary and metastatic disease. Several clinical scenarios occur in bladder cancer. Cancers that are clearly confined to the bladder can potentially be cured if the primary tumor can be removed or ablated, through surgery, radiation therapy or multimodality approaches, and there is no metastatic disease; 5-year survival rates are about 70%. Management of these cancers is discussed in chapter on 'Management of non-muscle-invasive disease'. Locally advanced cancers and tumors that have spread to loco-regional lymph nodes are associated with an increased risk of local relapse and a higher risk of micrometastatic disease at diagnosis. Five-year survival is about 35%. Lymph node involvement is identified through lymph node dissection or accurate imaging. The primary cancer may still be amenable to local therapy, and is potentially curable in the absence of distant metastases. Optimum systemic therapy with cisplatin-based chemotherapy is standard of care for patients with adequate renal function who are suitable for chemotherapy. Cancers predominantly involving the bladder but where there is small-volume overt metastatic disease are unlikely to be cured with currently available modalities; treatment is therefore palliative in intent, which must be clearly understood from the outset. Five-year survival for patients with metastatic disease is approximately 5%, although this varies depending on the bulk of disease, response to therapy and patient factors, including comorbidities. The mainstay of therapy is systemic treatment to control the progression of metastatic disease although management of the primary cancer may be necessary to alleviate symptoms and to prevent future complications such as pain, intractable hematuria and upper urinary tract obstruction. Surgery or radiation therapy may therefore be necessary for management of the primary tumor, and sometimes for management of metastatic disease. Cancers where the bulk of disease is metastatic are currently incurable; the intention of treatment is therefore palliative. Ablative local therapies such as surgery or radiation therapy should only be used to ameliorate current and potential issues that compromise quality of life. All these strategies are predicated on whether the patient is able to tolerate the proposed treatment, and other factors such as comorbidities may require modification of the treatment plan. Treatment decisions usually require multidisciplinary input. A key component of such discussions should be whether participation in a clinical trial is possible and an appropriate option for the patient to consider. This chapter summarizes current clinical recommendations for the systemic therapy of loco-regional advanced or metastatic disease. The treatment landscape and resulting recommendations are likely to change over the next few years as clinical trials are completed, particularly those of immunotherapies (see page 71). Systemic therapy for metastatic bladder cancer. The activity of various cytotoxic drugs such as doxorubicin (Adriamycin), methotrexate, vinblastine and cisplatin against urothelial bladder cancer has been recognized since the 1970s. Response rates with cisplatin monotherapy were initially reported to be as high as 40%, although subsequent studies showed that response rates of 10-15% were more realistic. Other drugs such as paclitaxel and gemcitabine were also subsequently shown to have activity as monotherapies. The first combination chemotherapy regimen to be widely used in clinical practice was cisplatin, methotrexate and vinblastine (CMV), which showed an overall response rate of 56% in a small single-arm study, half of which were complete remissions (notably in the era before computed tomography).
MVAC (methotrexate, vinblastine, doxorubicin and cisplatin) was shown to be superior to single-agent cisplatin for relevant endpoints of tumor response, duration of remission and overall survival (OS). CMV and MVAC have been the mainstay of chemotherapy for metastatic urothelial bladder cancer for many years. Unfortunately, however, both regimens are difficult to deliver safely and are associated with substantial toxicity, including bone marrow suppression, neutropenic sepsis, mucositis, neuropathy, ototoxicity, nausea and vomiting, requiring frequent hospitalization, and sometimes leading to treatment-related death. These cytotoxic regimens can now be used in a broader range of patients since the development of supportive care measures such as antiemetics and colony-stimulating factors; previously, only the fittest patients were able to tolerate these cytotoxic regimens. The treatment of metastatic bladder cancer was transformed following publication of a randomized Phase III trial comparing MVAC versus gemcitabine and cisplatin (GC). The study was somewhat ambitiously designed to demonstrate a 33% improvement in OS with GC, rather than as an equivalence or non-inferiority study. The survival curves for the two regimens were similar, and the trial did not meet its primary endpoint. However, GC was found to be better tolerated than MVAC and was adopted as standard of care for the treatment of metastatic disease. GC has become the de facto standard for trials in other settings, even though this is not supported by high-level evidence. Modifications to the regimen are frequently made (often with little supporting evidence), such as splitting the dose of cisplatin, dropping treatment weeks or shortening treatment cycles, modifying the gemcitabine dosage, or substitution with drugs such as carboplatin. It is important to recognize when and how far we should go beyond high-level evidence when making treatment decisions with patients. Improvements in supportive care have led to renewed interest in older chemotherapy combinations. One new approach is the intensification of treatment, such as accelerated or high-dose MVAC, with granulocyte colony-stimulating factor support. These regimens are generally much better tolerated than the original MVAC regimen and outcomes are at least comparable to those with GC, and perhaps numerically superior, although this has been difficult to prove statistically and remains controversial. Further modifications such as the addition of a taxane to GC increased the toxicity without substantially improving outcomes and therefore are not recommended. Chemotherapy regimens in common use for the first-line treatment of metastatic bladder cancer are summarized in Table 7.1. The dosing and scheduling details of cytotoxic chemotherapy regimens for advanced or metastatic bladder cancer are summarized in Table 7.2. Useful reviews of chemotherapy have been published by Yafi and colleagues and Oing and colleagues. Recent clinical trials have targeted a so-called 'cisplatin-ineligible' population. The criteria for this designation are rather soft and can include. Eastern Cooperative Oncology Group performance status (PS) 2 or worse. glomerular filtration rate (GFR) less than 60 mL/min. pre-existing neuropathy or ototoxicity. increased risk for neuropathy or ototoxicity (exempli gratia diabetes, alcohol misuse, occupational noise exposure). congestive cardiac failure or other inability to manage a fluid load. However, not all these factors indicate unsuitability for cisplatin. Many 'cisplatin-ineligible' patients who enter trials have only one of these criteria, most commonly poor PS or reduced GFR, which means that many of these trials are not generalizable to real-world practice.
While poor PS and reduced GFR are both relative contraindications to cisplatin, PS may be compromised by other conditions that will be unaffected by cisplatin, or may improve if symptoms are due to a tumor that subsequently responds to cisplatin. Patients with GFR of 40-60 mL/min can often be managed safely by splitting the cisplatin dose over 2 weeks (although there is little evidence to support this approach). In addition, it may be possible to improve renal function through the use of percutaneous nephrostomy tubes or ureteric stents if hydronephrosis or hydroureter are contributory factors. These points need to be considered when deciding whether cisplatin is appropriate for a specific patient, and when designing clinical trials. Second and subsequent lines of chemotherapy. Tumors that have progressed after first-line chemotherapy cannot be cured with chemotherapy, and response rates are typically lower than with first-line setting. It is important to reiterate at this stage that the intent of treatment is palliative, which can often be an uncomfortable conversation. Chemotherapy may still be a reasonable palliative option as long as all other palliative objectives are addressed and quality of life is not impaired by treatment. The literature contains many examples of small studies in the post-first-line setting, for both cisplatin-eligible and cisplatin-ineligible patients. Response rates with active agents range from 20% to 80%. Use of complex regimens such as doublets or triplets does not seem to improve outcomes, and toxicity is substantially worse with combinations than with monotherapy. Patients who have had good responses previously may sometimes respond well to re-challenge with the same agents. Example regimens are shown in Table 7.2 (noting that not all drugs will be available in all regions). General principles of treatment. The general principles of chemotherapy for advanced or metastatic disease are summarized in Table 7.3. Palliation and best supportive care. Patients with incurable disease must be managed with palliative intent. Any treatment must provide some possibility of benefit, and this benefit should outweigh the predicted risks. Early involvement of a broad multidisciplinary management team is important and should include palliative care, nursing and allied health professionals. Active treatment may still be appropriate, particularly if symptoms are amenable to such therapies and the patient is well enough to tolerate them. It is therefore reasonable to consider surgery, radiation therapy, cytotoxic chemotherapy or other active approaches within the context of supportive care and palliative intent. Practitioners delivering these therapies must be experienced and fully aware of the benefits and limitations of what they offer, bearing in mind the wellbeing of the patient and their broader social situation, including effects on family and carers. The financial implications of treatment may also require consideration. Many symptoms can be treated with simple and non-invasive measures, for example. optimal pain management, possibly involving a specialist pain management team. detection and management of urinary tract infections. modification of other treatments (such as cessation of antiplatelet drugs in a patient with intractable hematuria). blood transfusion (with defined stopping points). radiation therapy for symptomatic metastatic disease. management of other symptoms of metastatic disease.
Measurement in clinical trials. The goal of any trial is to determine both the safety and efficacy of a new medical product. Measures within the trial process must demonstrate the product's safety and efficacy to regulators before it is allowed to be labeled and marketed for use by patients. However, measurement in clinical trials does not only inform regulatory decision-making. Early in the development of a new drug or novel medical device, a company will make business decisions about whether to advance its new product for further testing. Early intelligence is extremely valuable to biopharma companies, which face a US$2 million revenue opportunity per day the drug is on the market (or not). Digital measures allow for the collection of data outside of the clinic, providing a more continuous stream of data points on whether the drug or device is working or not. Additionally, measurement data from clinical trials inform reimbursement decisions, which impacts the value of the market. In countries like the US that depend on third-party (exempli gratia non-government) payers, insurance companies need evidence to decide whether to reimburse the manufacturer for their approved product and at what price. In countries with a single-payer system, often the decision about pricing coincides with the regulatory approvals process. Trial success is not correlated with number of measures. It is much more important to select the right measures - those that are the most informative regarding the product's safety and efficacy - rather than the most measures. In fact, medical product manufacturers, regulators, patients and ethics review boards all worry about burdening participants with excessive tasks, activities and technologies. The ability of a measure to effectively and accurately operate in the wild (exempli gratia out of a patient's home and across many geographies and languages) is also a concern. Medical product manufacturers are often reluctant to assume even more risk - not just in their new product but also in a novel measure - without a substantial body of evidence. Decisions regarding the inclusion of digital measurement tools in clinical trials are complex, affect many stakeholders and require extensive consideration of factors related to the clinical implications of the measure itself. Additionally, the operational aspects of the measure and the potential effects on the trial design and participants need to be considered (Figure 8.1). Changing clinical trials. Traditional clinical trials collect snippets of data when a participant visits the study site and represent a tiny snapshot of patients' lived experience with a disease or condition. Yet researchers, industry sponsors and regulators rely on this limited information to make life-or-death decisions and multibillion-dollar investments. Digital measurements will convert that snapshot into a movie, with the ability to collect near continuous data outside the physical confines of the clinical environment, such as in a person's home, using connected products, including smartphones, wearables, implantables and ingestible devices and sensors. Decentralized clinical trials. Digital tools enable new forms of research such as decentralized clinical trials (DCTs), which are conducted outside of the clinic to capture data about a study participant in their day-to-day life (Figure 8.2). DCTs have a number of potential benefits, including faster participant recruitment, improved participant retention in the trial, greater control and convenience for participants, increased diversity (exempli gratia because it is easier to enroll in the first place) and trial results that are more generalizable. DCTs offer a way to make better-informed decisions about the efficacy of new therapies.
More sensitive, objective measures from digital technologies coupled with a greater density of information - continuously sampling multiple times a day, not just once a quarter - will help the industry fail faster and win more efficiently. Two features of data collection determine how 'decentralized' a clinical trial is: where and how the data are collected. Where are data collected? In traditional clinical trials, drugs, devices and therapies are administered in a clinic or research hospital. In newer direct-to-patient or remote trials, participant data are collected in the home or in the study participant's natural environment. How are the data collected? In the past, most data were collected via an intermediary - someone from the team would record information in a custom software system and/or case report form. As digital tools advance, we can collect more endpoint-supporting data at home via digital surveys and sensors, and study teams can 'visit' patients at home via telemedicine conference calls. This means that more of the data are participant-generated and collected 'virtually', without an intermediary. In context. A doctor who remote teleconferences with a patient would be conducting a 'remote trial', but they might collect the data manually through a survey, so the study would not be considered a 'virtual trial'. In contrast, a study team might collect all the data passively from a smartwatch in a clinic, and this study would be a 'virtual trial' from a data collection perspective, but not a 'remote trial' because the patient is in a centralized location. Notably, the industry has not yet settled on language around these types of trials, which is not unexpected. As a new field emerges, so does a new vocabulary. Historically, some researchers, primarily behavioral scientists, have referred to this type of study as a community-based clinical trial (CBCT) as a clinic may not be involved in the intervention being tested. The US Food and Drug Administration (FDA) has been using the term DCT more often in the past few years to describe trials taking place at home or in the community. What are real-world data?. Real world is a term that is important to define as it is often misused. The regulatory definition of real-world data is the data collected outside of a traditional clinical study, such as a randomized controlled trial (RCT). These data sources include electronic health records (EHRs), claims and billing activities, product and disease registries, patient-generated data including in home-use settings, and data gathered from other sources that can inform on health status, such as mobile technologies. Real-world evidence is the evidence derived from real-world data. In the context of a traditional RCT, if study participants contribute to some measurements at home, such as pain measurement via an electronic patient-reported outcome (ePRO) or step count from a wearable sensor, many often mistakenly believe this would constitute real-world data. However, these measures would not constitute real-world data because the participants have been preselected for study entry by the inclusion and exclusion criteria of a given trial. They do not represent the overall population in a certain indication. Therefore, when working with clinical research, it is important to focus on the benefits of health-related data collected in natural settings - which may not be classified as 'real world' by a strict regulatory definition.
5 Aggressive B-cell lymphomas. The aggressive B-cell lymphomas (also known as high-grade B-cell lymphomas) include diffuse large B-cell lymphoma (DLBCL), mediastinal large B-cell lymphoma, Burkitt lymphoma, and lymphoblastic lymphoma or acute lymphoblastic leukemia (see page 80). It can sometimes be difficult for pathologists to distinguish DLBCL from Burkitt lymphoma. Another subtype of lymphoma has therefore been identified, known for now as 'aggressive B-cell lymphoma with features intermediate between DLBCL and Burkitt lymphoma'. Other terms sometimes used for particularly aggressive types of DLBCL include ' c-Myc -positive DLBCL' and 'double hit' and 'triple hit' lymphomas, depending on the number of BCL2 and MYC translocations. Left untreated, these tumors can rapidly be fatal. Treatment aims to provide a cure. Relapse is associated with a poor prognosis. Diffuse large B-cell lymphoma. The most common aggressive lymphoma is DLBCL, which accounts for approximately 30% of all cases of non-Hodgkin lymphoma (NHL). Typically, the cells are large (Figure 5.1) and express B-cell markers. While there are a few recurring cytogenetic and molecular abnormalities, they are not particularly helpful diagnostically (Table 5.1). Importantly for treatment, most DLBCL express CD20, which is the target for rituximab. The condition may present at any age, but is increasingly common in later life. The most common finding is sheets of large cells, which stain with the B-cell markers CD19, CD79a and CD20. Molecular profiling using expression microarrays (see page 37) has identified two main types of DLBCL, namely those derived from germinal-center B cells (GCB subtype) and those derived from a more differentiated stage in the B-cell life cycle - activated B-cell (ABC) type. In accordance with their germinal-center derivation, BCL-6 and CD10 are expressed in about 50% of GCB subtype DLBCL. There is substantial evidence that the molecular subtype has prognostic significance. The ABC subtype has been found to confer a worse prognosis. Clinical presentation. Clinically, there are some important considerations. About 50% of all cases involve extranodal sites. Approximately 30% of patients have underlying indolent B-cell lymphomas. DLBCLs are aggressive, but approximately 50% are curable with combination chemotherapy. 5% will relapse in the central nervous system (CNS). Management of DLBCL, as with all lymphomas, starts with staging (see Chapter 4). Although outcome is partly dependent on the extent of disease at presentation, treatment is usually with combination chemotherapy, such as R-CHOP, regardless of clinical stage. Staging is, however, crucial, because it is important to identify all the sites involved so that response during therapy can be properly assessed. Staging investigations include. PET/CT scans of chest, abdomen and pelvis; extranodal sites may need to be reimaged if not adequately visualized. bone-marrow biopsy. complete blood count, renal and liver profile, and immunoglobulins. lactate dehydrogenase (LDH). virology, which should include hepatitis C, hepatitis B and HIV. Prognosis can be usefully predicted using the International Prognostic Index (IPI). Patients are divided into four outcome groups depending on their score (Table 5.2). low risk. low-intermediate risk. high-intermediate risk. high risk. Clinical trials are under way to determine whether more intensive initial chemotherapy will improve the outcome in the poor prognostic groups; however, retrospective data already exist to describe the impact of rituximab on outcome, and a revised IPI has been proposed - the R-IPI. This defines three prognostic groups (Table 5.3). Prospective confirmation of these data is awaited. The standard treatment for DLBCL is six courses of R-CHOP given every 21 days. GELA (Groupe d'Etude des Lymphomes de l'Adulte) demonstrated an overall survival advantage by adding rituximab to CHOP in the elderly. Stratified analysis showed the significant superiority of R-CHOP over CHOP in terms of overall survival in low-risk patients (80% vs 62% at 5 years; P=0.023).
However, the difference in high-risk patients was only of borderline significance (48% vs 39%; P=0.062). Subsequent trials extended this observation to younger patients. Early-stage DLBCL is best managed with abbreviated R-CHOP (id est a total of four courses). Some centers will use adjunctive radiotherapy. Refractory disease. Those patients who do not achieve complete remission by the completion of their fourth course of R-CHOP may be considered to have refractory disease in this setting, and the possibility of using salvage regimens should be discussed. Treatment according to subtype. Emerging data suggest that different treatment approaches should be considered depending on the molecular subtype of DLBCL. Although still the subject of clinical trials, the evidence to date indicates that some drugs, for example those that target the B-cell receptor pathway (including fostamatinib, a spleen tyrosine kinase [SYK] inhibitor, and ibrutinib, a Bruton's tyrosine kinase [BTK] inhibitor), are more effective for ABC subtypes of DLBCL than GCB subtypes. Patients with aggressive subtypes of DLBCL such as c-Myc -positive DLBCL, which has a high cell proliferation rate, tend to fare poorly with standard treatment compared with those who have c-Myc -negative disease. Regimens administered as continuous infusions, such as dose-adjusted EPOCH-R (EPOCH plus rituximab), are being evaluated as potentially more active treatments in these cases. Follow-up. Patients who are in complete remission can be followed up in an outpatient clinic. Most centers see their patients every few months during the first 2 years when the chance of relapse is highest. Patients can probably be seen less often after 2 years provided they remain free of disease. Many centers now discharge patients after 5 years. Relapse or progressive disease. The prognosis for patients who relapse after treatment is generally poor, with only 20-30% of patients rescued with salvage/relapse protocols. Most patients destined to relapse will do so in the first 2 years after initial treatment. Salvage protocols vary, but are usually based on regimens containing platinum. The key consideration is response, because patients achieving a satisfactory response may then benefit from high-dose therapy and peripheral stem-cell rescue (Figure 5.2). The following salvage/relapse regimens are used. R-DHAP and R-ESHAP are similar regimens given by infusion over 5 days. R-ESHAP is an excellent stem-cell mobilizer, but has a long-term survival rate (over 5 years) of only 20% when used as the sole therapeutic agent (id est without subsequent high-dose therapy). R-ICE is given as an infusion over 3-4 days; like R-ESHAP, it is a good stem-cell mobilizer. As suggested above, patients who have relapsed do better if they are able to receive high-dose therapy (see page 115), as long as they achieved a satisfactory response to the initial salvage chemotherapy. Patients who do not respond to initial salvage therapy have chemorefractory disease and are best either entered into clinical trials of novel agents or given palliative care. Occasionally, radiotherapy provides excellent symptomatic relief for localized disease. Central nervous system prophylaxis. CNS involvement is a rare but devastating complication of DLBCL. Patients with involvement of two or more extranodal sites together with a raised LDH are at particular risk of relapse in the CNS. Furthermore, certain anatomic sites may confer risk, such as involvement of the bone marrow, testis, breast, tonsil and paraspinal territory. or disease that involves the facial bones and surrounding structures. This area of lymphoma management is controversial, but most centers invoke some form of prophylaxis for patients thought to be at risk of CNS involvement. Chemotherapy (exempli gratia methotrexate and cytosine arabinoside) delivered directly into the cerebrospinal fluid by lumbar puncture is one such strategy. An alternative is administration of a sufficiently high dose of systemic drugs to pass the blood-brain barrier (exempli gratia high-dose methotrexate and high-dose cytosine arabinoside).
Burkitt lymphoma. Burkitt lymphoma is a rare and important disease with a distinctive immunophenotype and cytogenetics. It can often be cured with aggressive sequential chemotherapy. Burkitt lymphoma is probably the most aggressive tumor affecting humans. Every cell is undergoing cell division, leading to potentially devastating presentations with huge tumor burdens. There are three main types. endemic Burkitt lymphoma, which is almost always associated with Epstein-Barr virus (EBV) infection. sporadic Burkitt lymphoma, which occurs more often in Western Europe and the USA, is not as frequently associated with EBV and requires aggressive chemotherapy (Table 5.4). Burkitt lymphoma associated with HIV. Biopsy of an affected lymph node or other involved tissue will show sheets of small- to medium-sized lymphoid cells with a so-called 'starry-sky' appearance caused by the presence of phagocytic macrophages dotted about within the tumor (Figure 5.3). A characteristic feature of Burkitt lymphoma is a very high proliferation rate, usually close to a 100%. Occasional cases of DLBLC will exhibit a similar high rate so this finding is not in itself diagnostic. The immunophenotype is one of a mature GCB phenotype expressing CD19, CD20 and CD10. The extreme mitotic rate is, in part, due to overexpression of the MYC oncogene, which forces the cell to proliferate. The gene is located on chromosome 8 and can translocate to one of three immunoglobulin genes located on chromosomes 14, 22 and 2 (t[8;14], t[8;22] or t[2;8]). This places MYC under the control of the immunoglobulin gene enhancer, which drives expression. Clinical presentation. Many patients present with rapidly enlarging lymph nodes, but the disease is almost always widespread and should be treated as such. Extranodal involvement is very common; extensive bone-marrow infiltration, and CNS, ovarian, gastrointestinal and breast involvement are all well recognized. Occasionally, patients will present with florid leukemia. The large tumor bulk often seen in these patients is reflected in very high levels of LDH in some cases. Full staging is mandatory and should include analysis of the cerebrospinal fluid. Burkitt lymphoma requires treatment with high-dose sequential chemotherapy. Effective treatment regimens expose the malignant and rapidly dividing cells to longer periods of chemotherapy than would be achieved with simple CHOP. Another important component of therapy is CNS-directed treatment, usually in the form of high-dose methotrexate or high-dose cytosine arabinoside. Cure rates of 65-80% can be achieved when sequential chemotherapy is delivered such as CODOX-M or, for higher-risk patients (high LDH and advanced stage) CODOX-M/IVAC. These regimens contain high-dose methotrexate and/or high-dose cytosine arabinoside to protect or treat the CNS. Because of the large bulk at presentation and the rapid response to chemotherapy, steps must be taken to avoid the clinical consequences of killing tumor cells too quickly (tumor lysis syndrome): adequate hydration of the patient with careful recording of fluid balance is essential, along with regular monitoring of renal and cardiac function and electrolytes, and administration of an agent to prevent deposition of uric acid, such as rasburicase (recombinant uric acid oxidase), in the kidneys.
Primary mediastinal (thymic) large B-cell lymphoma. Primary mediastinal (thymic) large B-cell lymphoma is a rare aggressive locally invasive lymphoma that presents in young adults and more commonly in women. It was once considered a variant of DLBCL, but it is now listed as a separate entity in the WHO classification (see page 131). Mediastinal large B-cell lymphoma is thought to arise from thymic B cells and is characterized by the presence of medium or large lymphoma cells with clear cytoplasm enmeshed in a variable amount of reactive fibrosis (Figure 5.4, Table 5.5). Mediastinal large B-cell lymphoma presents a fairly distinct immunophenotype. The cells express the B-cell antigens CD20, CD19 and CD79a, and may also express CD30, which is better known as a marker of Hodgkin lymphoma. Characteristically, the cells do not express surface immunoglobulin, CD10 or human leukocyte antigen class I molecules. Furthermore, unlike DLBCL, primary B-cell mediastinal lymphomas do not show rearrangements of either the BCL2 or BCL6 genes. Interestingly, gene-expression microarray studies show that the signature of mediastinal large B-cell lymphoma is more like that of classic Hodgkin lymphoma than that of DLBCL. Cytogenetics of mediastinal large B-cell lymphoma show a characteristic pattern of genomic aberrations with gains on the short arm of chromosomes 9 and 2 (involving the JAK2 and REL [also known as C-REL ] genes, respectively). Clinical presentation reflects the fact that this tumor tends to remain localized to the upper anterior mediastinum (Figure 5.5), is aggressive and will invade local structures. Obstruction of the superior vena cava causing cough, dyspnea and stridor is a relatively common clinical presentation. Phrenic nerve palsy or recurrent laryngeal nerve palsy may occur as a direct result of local tissue invasion. Studies have found that primary mediastinal large B-cell lymphoma has a favorable outcome compared with DLBCL. Most centers treat mediastinal large B-cell lymphoma using a DLBCL protocol (R-CHOP), though others may use infusional regimens based on the excellent outcome data with dose-adjusted EPOCH-R. Whether the addition of radiotherapy to the mediastinum at the completion of chemotherapy confers survival advantage is controversial. Some centers routinely administer radiotherapy to residual masses, while others reserve radiotherapy for treatment at relapse or progression after salvage and high-dose therapy.
Intra-articular therapies. Intra-articular corticosteroids. Synthetic corticosteroids are excellent anti-inflammatory agents. Intra-articular corticosteroids have been used in OA since the 1950s, although relatively few randomized controlled trials using these agents have been carried out. It is assumed that the analgesic efficacy of corticosteroids is related to their anti-inflammatory actions, although this has not been clearly demonstrated. As most readers will be aware, there are a number of different corticosteroid preparations but experts have not reached consensus on what dose or which particular agent should be used for a given OA joint. Methylprednisolone and triamcinolone are commonly used, the latter being a more potent steroid, although few data exist on whether this provides any clinically significant differences in outcome. In general, a greater dose of drug is given into a large joint (exempli gratia methylprednisolone, 80 mg for the knee versus 10 mg in the base of the thumb). On balance, steroid injection studies, including a recent individual patient data meta-analysis, have shown a moderate benefit in reducing symptoms for 1-3 weeks after injection. Most studies have focused on knee OA, but a 2016 systematic review supports similar benefit in patients with hip OA. It should be noted that intra-articular injection may miss the joint, even when given by experts. Hip injections should always be given under imaging guidance. Such guidance may be of benefit for some knee and hand injections as well. The increasing use of ultrasonography may be a valuable addition to the assessment of OA joints and, more particularly, in correct placement of intra-articular therapies. In OA, intra-articular steroids may be useful in patients. who need sufficient pain relief to perform appropriate strengthening exercises. with a large knee effusion that interferes with function. with a ruptured Baker's cyst (see pages 60 -). To facilitate exercise. Most commonly, steroid injections are administered as a pain 'circuit-breaker' to allow appropriate exercises to be performed. Patients are usually able to start exercise of the joint a few days after the injection. This can be particularly useful in patients with moderately to severely painful trochanteric bursitis, secondary to limping and poor quadriceps strength, where the bursitis is hampering the patient's muscle-strengthening efforts. Large knee effusion. Although injection can be useful in patients with a large knee effusion that interferes with function, the presence of an effusion does not automatically warrant intra-articular therapy, although aspiration may be helpful. Ruptured Baker's cyst. The patient may have been aware of a swelling or lump at the back of the knee for some time, but then will be aware of a burning sensation and swelling of the calf, perhaps in association with a flare up of knee swelling. The major differential diagnosis is that of a deep venous thrombosis. As most Baker's cysts communicate with the knee joint cavity, intra-articular steroid into the knee joint often reduces fluid production and leakage from the cyst. Direct aspiration and injection of the cyst under imaging guidance can also be considered. Number of injections. One of the most commonly asked questions in OA is: how many times can an individual joint be injected? Much of the information on this comes from animal model studies and there are no good data in humans on which to make such recommendations. A general rule of no more than three or four injections in a given year is often quoted. Certainly injection therapy should never be given as a sole therapy but combined with appropriate exercises. Injection technique. Intra-articular injections are usually performed using an aseptic technique. They should only be undertaken under the supervision of, and after training from, a physician with expertise in the appropriate techniques. Ultrasound guidance improves injection accuracy and, potentially, patient comfort and clinical response. Side effects. The major risk from intra-articular injection is that of septic arthritis, which is thankfully extremely uncommon when proper preparation of the site and technique are used. It is wise to warn patients that a flare up of joint pain may occur after injection, albeit in a small percentage of patients. This may be due to a reaction to the steroid preparation and usually settles within 24 hours. If steroid is deposited subcutaneously it can cause fat atrophy, which may cause unsightly dimples in visible areas.
One of the potential risks of intra-articular steroid injections is that of worsening OA ('steroid arthropathy'). Again, much of this concept is based on animal studies and no good clinical evidence is available to support it. Most patients have structural damage before injection, and subsequent monitoring of structural damage in the clinic is very difficult given current radiographic imaging (this may change with MRI monitoring). The few retrospective studies are confounded by the degree of patient symptoms and structural damage. Certainly, prolonged repeated use of intra-articular steroid alone is not optimal management of OA and other options should be discussed with appropriate experts. Novel corticosteroid formulations. New formulations of existing therapies are being developed. Recently, the US Food and Drug Administration approved the use of triamcinolone acetonide in an extended-release biocompatible microsphere that prolongs intra-articular residency. This novel formulation demonstrated sustained analgesic benefits following a single injection through 12 weeks in phase II and III trials. Proof of concept of intra-articular dwell time was further demonstrated in a study showing no short-term blood glucose disruption after injection in diabetics with knee OA. The safety profile has been shown to be similar to saline-placebo. Such extended-release formulations may change how we view the use of intra-articular steroid injections in clinical practice. Intra-articular hyaluronans. Hyaluronan (hyaluronic acid; HA) is a major non-structural component of both the synovial and cartilage extracellular matrices (see Chapter 1). It is also found in the synovial fluid. Levels in the synovial fluid of individuals with OA are reduced, which has led to a theoretical concept known as viscosupplementation - the injection of exogenous or synthesized HA into a joint to improve synovial fluid viscosity and subsequently symptoms. Although a number of mechanisms have been proposed for the action of exogenous HA, such as beneficial effects on chondrocyte function and improving HA production by synovial fibroblasts in vitro, there are no convincing data on how HA works in humans. In particular, HA is generally cleared from the joint within 24 hours and any mechanism of action must therefore take into account the prolonged clinical benefits reported. A variety of HA preparations of differing molecular weight and origin are available, and they are recommended with differing treatment protocols, some being used as single injections and others as 3-5-week courses of weekly injections. Many clinical trials of HA therapy, the vast majority in OA of the knee, have been conducted. On balance, these clinical trials have demonstrated, at best, a modest benefit in reducing symptoms, starting with improvements immediately after the course of injections, peaking at around 2 months, and persisting for up to 6 months. Studies suggest that this small effect may only occur in mildly arthritic knees. Practitioners must also take into account the cost-benefits of these therapies. In the UK, the National Institute for Health and Care Excellence (NICE) has advised against HA therapy following a cost-effectiveness analysis; disagreement in guidelines from other societies and countries reflects the ongoing controversy surrounding this treatment option. Side effects. The most common side effect of these therapies is a post-injection flare up of knee pain (sometimes with florid synovitis), which is quite uncommon. Occasionally, there can be local irritation around the injection site. As with all intra-articular therapies, there is a very small risk of infection. Other intra-articular therapies. Platelet-rich plasma (PRP) injections for hip and knee OA have increased in popularity over the last few years. PRP is an autologous blood product with concentrated platelets that contains various growth factors and cytokines. A recent review including 15 randomized controlled trials found that these injections appear to be safe, and may provide some short-term symptomatic benefit. However, the trials were all of low to moderate quality, used a variety of different protocols and preparations of PRP, and did not assess structural benefit or harms or longer-term outcomes. To date, given the lack of high-quality evidence to support its use, PRP is not recommended in any clinical guideline.
Non-small cell lung cancer. Lung cancer is the principal cause of death from cancer worldwide. The identification of biomarkers that can inform treatment selection is therefore of vital importance. 'Must test' genes. For advanced stage NSCLC, international guidelines by the College of American Pathologists (CAP), the International Association for the Study of Lung Cancer (IASLC) and the Association for Molecular Pathology (AMP), the National Comprehensive Cancer Network (NCCN) and ASCO define a minimum panel of genes - the 'must test' genes - that should be tested to inform treatment decisions. The use of TKIs is guided by the presence of. alterations in EGFR. gene fusions involving ALK and ROS1. alterations in BRAF. Mok et al. demonstrated, for the first time, that gefitinib, a first-generation epidermal growth factor receptor (EGFR) TKI, was more effective than carboplatin-paclitaxel doublet chemotherapy in patients with EGFR mutations (12-month PFS 24.9% versus 6.7%, respectively). A similar result was obtained by Rosell et al. for another EGFR first-generation TKI, erlotinib. In this clinical trial, the 1-year PFS with the TKI was also higher than that with chemotherapy (40% versus 10%, respectively). The study authors focused attention on the better response of patients harboring EGFR exon 19 deletions rather than point mutations in EGFR exon 21. The second-generation EGFR TKI afatinib has also been shown to be more effective than chemotherapy in patients with EGFR mutations. More recently, patients receiving the third-generation EGFR TKI osimertinib have been shown to have a higher PFS (18.9 months versus 10.2 months) and a lower number of high-grade adverse events compared with those taking first-generation TKIs. ALK rearrangements occur in a limited number of patients with advanced stage NSCLC (3-5%). Despite the low number, patients with an ALK fusion have been shown to respond well to the first-generation anaplastic lymphoma kinase (ALK)- fusion TKI crizotinib when compared with chemotherapy (PFS 10.9 months versus 7.0 months and objective response rate [ORR] 74% versus 45%), with a significant improvement in quality of life. The second-generation ALK TKI alectinib was investigated to overcome the limitations of crizotinib (in particular, the inability to act on metastasis in the central nervous system); it has been shown to be more effective than crizotinib in ALK -rearranged patients.
ROS1 rearrangements. Patients with ROS1 -positive rearrangements showed responsiveness when treated with crizotinib (median PFS 19.2 months and 3 of 50 participants having a complete response). More recently, in the STARTRK-1 Phase I clinical trial, the tyrosine kinase multi-inhibitor entrectinib has shown promising results in patients with ROS1 rearrangements. Identification of the BRAF p. V600E mutation is increasingly important as it supports treatment with dabrafenib plus trametinib in advanced stage NSCLC. Programmed death-ligand 1. In addition to the must test genes, the guidelines (see above) strongly recommend evaluating PD-L1 expression to inform immunotherapy decisions. Two clinical trials (KEYNOTE-024 and KEYNOTE-042) have shown pembrolizumab to be an effective first-line treatment when at least 50% of cancer cells express PD-L1, or a second-line therapeutic choice when at least 1% of cancer cells express PD-L1, respectively (Table 5.1). 'Should test' genes are other clinically relevant genes that are being investigated in this therapeutic area. In addition to a negative prognostic role, acquired KRAS mutations have a predictive value in advanced stage NSCLC. A Phase I study showed promising results in terms of efficacy and safety for a novel small molecule, AMG510, that is able to target, irreversibly, the KRAS p. G12C point mutation. Although the mutations are rare, cancers with RET and NTRK rearrangements show high sensitivity to cabozantinib and larotrectinib, respectively. Entrectinib is another drug that targets the products of NTRK rearrangements. Patients harboring NRG1 gene fusions respond to afatinib treatment. A high response rate was found when crizotinib and cabozantinib were administered to patients with a MET mutation causing exon 14 skipping. Capmatinib is another MET inhibitor being evaluated as a treatment for NSCLC with MET exon 14 skipping. Tumor mutational burden. As far as immunotherapy decisions are concerned, TMB is another predictive biomarker. A high number of non-synonymous mutations (at least ten) has been shown to be predictive of response to nivolumab plus ipilimumab, regardless of PD-L1 expression.
Breast cancer. Breast cancer remains the most common cancer type among women in the USA and Europe. Despite advances in detection and treatment, breast cancer remains the second leading cause of death for women in the western world, with most deaths attributed to metastatic disease. With increasing treatment options for metastatic breast cancer (mBC), it is of fundamental importance to know the genomic landscape of the disease and how to incorporate tumor genomic findings into clinical practice. Primary breast cancer. Besides the classic biomarkers used by pathologists in every case of primary breast cancer, such as estrogen receptor (ER), progesterone receptor (PR) and HER2, which guide prognostication and treatment selection, gene expression profile assays have recently been incorporated into the biomarker assessment of early breast cancer. Commercially available genomic assays that provide these profiles include Oncotype DX, MammaPrint, Predictor Analysis of Microarrays 50 (PAM50), EndoPredict and Breast Cancer Index. Data from two large randomized clinical trials examining Oncotype DX and MammaPrint have yielded important evidence for use in discussions about potential benefit from chemotherapy in specific patient populations. When using the Oncotype DX assay, chemotherapy is not recommended for patients older than 50 years whose tumors have a recurrence score of less than 26. For those patients younger than 50 years whose tumors have a recurrence score of less than 16, there is little to no benefit from chemotherapy; however, clinicians may offer chemoendocrine therapy to those with a recurrence score in the range 16-25. In addition, oncologists may offer chemoendocrine therapy to any patient with recurrence score of 26-30. There are many guidelines published concerning the use of these assays. Metastatic breast cancer. In breast cancer, CGP is more applicable in the metastatic setting. The recent successes with a PI3K inhibitor for the treatment of PIK3CA -mutated hormone receptor (HR)-positive mBC and of poly(ADP-ribose) polymerase (PARP) inhibitors in deleterious germline BRCA1 / -mutated mBC have solidified the role of genomic testing to guide therapy for patients with mBC. The routine implementation of NGS in many laboratories has allowed the readout of large amounts of DNA, making it possible to detect multiple genetic alterations at the same time, using the same assay, leading to the concept of 'multigene sequencing'. This can be applied to tumor tissue, CTCs, ctDNA and normal tissue (with germline DNA). The genomic landscape of mBC is broad; alterations in multiple genes have been found, many with potentially actionable changes. Here, we will discuss only those classified as tier I-A (prospective randomized clinical trials show the alteration-drug match in a specific tumor type results in a clinically meaningful improvement in a survival endpoint) according to the ESMO Scale for Clinical Actionability of molecular Targets (ESCAT) or level 1 (FDA-recognized biomarker predictive of response to an FDA-approved drug in this indication - see Chapter 4) by the precision oncology knowledge base OncoKB. These are PIK3CA mutations, germline BRCA1 / mutations, HER2 amplification, MSI and NTRK translocations. PIK3CA codes for the catalytic subunit of PI3K. A gain-of-function mutation can cause the activation of multiple downstream signaling cascades, including the PI3K/AKT/mTOR pathway that promotes cell survival and proliferation (see Figure 1.6). The Phase III randomized SOLAR-1 trial compared the combination of the PI3K inhibitor alpelisib and fulvestrant with fulvestrant alone in patients with HR-positive HER2 -negative mBC who had progressed on prior endocrine therapy. Participants receiving alpelisib-fulvestrant had superior PFS compared with those receiving fulvestrant alone, leading the FDA to approve alpelisib for HR-positive mBC.
These findings stress the importance of clinical testing for the PIK3CA mutation in patients with HR-positive mBC who experience progression on first-line endocrine therapy. Alpelisib is now also authorized for use in the EU. BRCA1/2 germline mutations result in homologous recombination deficiency (HRD). PARP enzymes are essential for DNA single-strand break repair. Tumors with germline HRD rely more heavily on PARP enzymes for DNA repair; therefore, inhibition of PARP enzymes leads to persistence of DNA single-strand breaks and eventual cell death through synthetic lethality. The randomized OlympiAD trial compared olaparib, a PARP inhibitor, with single-agent chemotherapy in patients with HER2 -negative mBC who harbored a germline BRCA1 / mutation. The results showed significant improvement in PFS in the olaparib group compared with the chemotherapy group (7.0 months versus 4.2 months). The ORR in the olaparib group was 59.9% compared with 28.8% in the chemotherapy group. On the basis of this study, olaparib received FDA approval for the treatment of HER2 -negative mBC with germline BRCA1 / mutations in patients previously treated with chemotherapy. Olaparib is also approved for use in the EU. Similarly, the Phase III EMBRACA trial compared the PARP inhibitor talazoparib with single-agent chemotherapy in patients with mBC harboring germline BRCA1 / mutations. The median PFS was significantly longer in the talazoparib arm than in the chemotherapy arm (8.6 months versus 5.6 months). On the basis of this study, talazoparib received FDA approval for the treatment of HER2 -negative mBC with germline BRCA1 / mutations, regardless of prior chemotherapy use. Talazoparib is also approved for use in the EU. The role of PARP inhibitors for the treatment of mBC with somatic BRCA1 / mutations is under investigation. Although NGS is able to detect HER amplification, it is more commonly determined in the clinical setting using IHC or in-situ hybridization (ISH). Multiple clinical trials have confirmed the role of different HER2-directed therapies in the treatment of HER2 -amplified breast cancer across multiple settings. Evaluation of MSI that leads to defects in DNA mismatch repair has become a standard of care in metastatic solid tumors. Patients with tumors that harbor MSI are candidates for treatment with the immune checkpoint inhibitor pembrolizumab. MSI is rare in breast cancer, with rates between 0.9% and 1.5%, though the frequency is higher in triple-negative breast cancer (TNBC). NTRK1, NTRK2 and NTRK3 genes encode the three transmembrane tropomyosin receptor kinase (Trk) proteins, TrkA, TrkB and TrkC. Fusion of an NTRK gene induces constitutively active protein function, resulting in an oncogenic driver. Two Trk inhibitors, larotrectinib and entrectinib, have gained FDA approval for the treatment of solid tumors that harbor an NTRK gene fusion. Larotrectinib has also been approved by the EMA. In breast cancer, NTRK fusions are found most commonly in secretory breast carcinomas and mammary analog secretory carcinomas. The frequency of NTRK fusions in mBC is low; one study that examined 12 214 consecutive patients with mBC found that 0.13% of tumors harbored NTRK gene fusions. Other mutations. Several other alterations described in multigene sequencing, such as mutations in ESR1 (related to increased resistance to endocrine therapy), HER2, PTEN and AKT1, are being investigated in clinical trials. The value of assessing TMB in mBC also remains investigational.
Colorectal cancer. Globally, CRC is the third most commonly diagnosed cancer and the second most common cause of cancer-related deaths in men and women. More than 90% of cases are sporadic, the other 10% resulting from hereditary cancer syndromes. Prognostic and predictive biomarkers have been well established in CRC, and NGS is adding to this well of information. Three major pathways of carcinogenesis have been elucidated. chromosomal instability (CIN), which accounts for around 85% of all CRCs. MSI, which accounts for around 15% of CRCs. CpG island methylator (CIMP), which is found in 17% of CRCs and shows overlap with the MSI pathway. Chromosomal instability is characterized by alterations in chromosomes 17p and 18q. In addition, the tumors acquire mutations in oncogenes and tumor suppressors including APC, TP53, KRAS and BRAF. According to the Vogelstein model, there is initial inactivation of APC, followed by mutations of RAS with inactivation of the TP53 suppressor gene. The most clinically relevant pathways affected are the Wnt and MAPK pathways. Alterations in the Wnt signaling pathway, which occur in 93% of all CRC tumors, lead to cell proliferation. The MAPK pathway is activated by RTKs, such as EGFR, though it can be activated by other downstream signaling molecules, such as KRAS proto-oncogene, GTPase (KRAS), NRAS proto-oncogene, GTPase (NRAS) and BRAF, as well as ERK. Microsatellite instability is due to generalized instability of short tandem repeats of DNA sequences known as microsatellites, resulting from mutations of the MMR genes MLH1, MSH2, MSH6, PMS2, or silencing of MLH1 by hypermethylation of the CpG-rich promoter sequence (see Chapter 3). Mutations resulting in the inability to repair replication errors result in Lynch syndrome. Hypermethylation of the MLH1 promoter can cause sporadic cancer. MSI can be determined by IHC: loss of staining of one or more of the MMR proteins indicates MMR deficiency (that is, MSI). PCR can also be used, with commercial kits available to test for the five microsatellite loci, referred to as the Bethesda panel, BAT-25, BAT-26, D2S123, D5S346 and D17S250, as proposed by the NCI. NGS can also be utilized to determine MSI status and allows for analysis of over 100 loci. NGS has 98% sensitivity and 100% specificity as compared with PCR. MSI-H tumors have been shown to have a better prognosis. Importantly, fluorouracil-based chemotherapy has no benefit in MSI-H CRC. CpG island methylator phenotype. The CIMP pathway is characterized by hypermethylation of CpG island loci and inactivation of suppressor genes. Sporadic MSI CRCs are associated with CIMP-associated methylation of the MLH1 promoter which, in turn, is associated with the presence of BRAF mutation. NGS testing for predictive biomarkers. EGFR activation of the RAS/RAF/MEK/ERK pathway (see Chapter 1) plays an important role in oncogenesis in CRC.
Up to 50% of CRCs show activating mutations of KRAS. Anti-EGFR antibodies, such as cetuximab and panitumumab, can be beneficial in patients with metastatic CRC provided that there is no downstream mutation activating the RAS/RAF/MEK/ERK pathway. Initially, patients with codon 12/13 KRAS mutations did not benefit in clinical trials, and subsequently mutations in other codons of KRAS and in NRAS were also shown to confer resistance to antibody-based therapies. Thus, guidelines require an 'extended' RAS analysis be performed. At present, the minimum testing required to determine whether anti-EGFR therapy may be of benefit is this extended RAS testing of KRAS and NRAS. codons 12 and 13 of exon 2. codons 59 and 61 of exon 3. codons 117 and 146 of exon 4. Wild-type phenotype for KRAS and NRAS is thus an indication for anti-EGFR therapy. Theoretically, activating mutations in the genes for any of the downstream molecules in the RAS/RAF/MEK/ERK pathway should result in a pathological scenario similar to that with RAS mutation. At present, though, there are insufficient data to support differentiating the treatment of wild-type RAS and BRAF -mutated cancers and nor is there evidence to support the use of anti-BRAF agents (as there is in melanoma). Nonetheless, a BRAF mutation is a negative prognostic indicator. US and European guidelines mandate extended RAS and BRAF testing and MSI analysis using the most appropriate methods (Figure 5.1). NGS provides the widest genetic coverage and most cost-effective solution. Where gene chips for multiple genes are utilized, extra information may be gained. In particular, mutations in other genes (PIK3CA, PTEN) and overexpression of HER2 (ERBB2) may be detected, as may other novel biomarkers. HER2 overexpression has been detected in around 3.5% of CRCs. Promising responses to anti-HER2 therapies have been reported in clinical trials (HERACLES-A, HERACLES-B and HERACLES-RESCUE). Patients with MSI-H CRC tumors respond to therapy with pembrolizumab. The group of potentially benefiting patients has now been expanded to include patients with high TMB, which is emerging as an important predictive biomarker for response to immune checkpoint inhibitor drugs in the subset of MMR-deficient (MSI-H) CRC. TMB can be detected by NGS technology (see page 45). The use of validated biomarkers and those currently being investigated is summarized in Table 5.2. Liquid biopsy. Undoubtedly, liquid biopsies - being minimally invasive - will become more commonly used to determine biomarkers. Already, the presence of ctDNA is a predictor of relapse. Although ctDNA may be utilized predictively in the future to guide patient management, at present it has relatively high false-positive and -negative rates. Current ESMO and ASCO guidelines do not recommend its use for the initial diagnosis of, or as a predictive biomarker for, CRC. As NGS and newer technologies overcome the technical shortcomings, it is highly likely that liquid biopsy will find a more significant role in diagnostics, which will necessitate new guidelines.
Melanoma is a highly mutated malignancy, with mutations documented in all subtypes (Table 5.3). The KIT mutation is associated with chronic sun damage. The BRAF mutation is present in 11% of patients with mucosal melanoma, NRAS mutation in 5% and KIT mutation in 15-20%. From a molecular point of view, uveal melanoma is a distinct condition characterized, in 50% of patients, by the presence of a GNAQ mutation. The presence of BRAF and NRAS mutations has not been described in uveal melanoma. The serine/threonine kinase BRAF is involved in the downstream signaling of the RTK and RAS proteins. Approximately half of melanomas show BRAF point mutations (Figure 5.2). In the majority of cases, a valine at position 600 is mutated to glutamic acid or lysine (BRAF p. V600E and BRAF p. V600K); both mutations are associated with kinase activation that results from relieving an intramolecular autoinhibitory interaction between the activation segment and P-loop of the protein. The events result in abnormal activation of the MAPK pathway, including the serine/threonine kinases MEK1/2, ERK1/2, and many downstream targets, resulting in many hallmarks of cancer, such as proliferation, migration, defense from apoptosis, and cellular metabolism. BRAF mutations are most common in the nodular and superficial spreading melanoma types, and they are rare in acral lentiginous (5-10% of cases) and non-cutaneous melanomas. BRAF mutation correlates with distinct histopathological features, such as intraepidermal melanoma nest formation and a larger rounder border of the tumor with the surrounding skin, suggesting surrogate markers can be used to select patients for molecular testing. BRAF mutations also arise more commonly in patients who are younger at presentation and those with lymph node metastasis (rather than satellite tumors or visceral metastasis). BRAF inhibitors can lead to remarkable early tumor responses in melanoma, though these may be of short duration in some patients. Approximately 20% of patients with mutant BRAF melanoma show no response, and most patients treated with monotherapy relapse, with a median PFS of 8-9 months. Vemurafenib is an orally available small molecule kinase inhibitor with activity against BRAF with the p. V600E mutation; its indication is restricted to melanoma patients with a demonstrated BRAF p. V600E mutation by an FDA-approved test.
This agent was approved by the FDA in 2011 and by the EMA in 2012. Vemurafenib has shown an improvement in PFS and overall survival in patients with unresectable or advanced melanoma. Dabrafenib, an orally available, small molecule, selective BRAF inhibitor was approved by the FDA and EMA in 2013. This agent demonstrated an improvement in PFS compared with dacarbazine (DTIC) in the international multicenter BREAK-3 trial. MEK inhibitors. Trametinib is an oral, small molecule, selective inhibitor of MEK1 and MEK2 that was approved by the FDA in 2013 for melanoma patients with unresectable or metastatic melanoma with BRAF p. Trametinib is associated with improved PFS versus dacarbazine. Cobimetinib is an orally available, small molecule, selective MEK inhibitor approved by the FDA and EMA in 2015 for use in combination with the BRAF inhibitor vemurafenib for the same indication. c-KIT inhibitors. Early data suggest that mucosal or acral melanomas with activating mutations or amplifications in KIT may be sensitive to a variety of c-KIT inhibitors. BRAF and MEK inhibitors in combination. In 2014, the combination of dabrafenib and trametinib received accelerated approval from the FDA for patients with unresectable or metastatic melanomas that carry the BRAF p. The combination demonstrated improved durable response rates over single-agent dabrafenib. The treatment combination is also approved for use in the EU. In 2015, the combination of vemurafenib and cobimetinib was approved by the FDA and the EMA for metastatic melanomas that carry the BRAF p. Mechanisms of resistance to BRAF/MEK inhibitors. The majority of patients with metastatic melanoma who present with the activating BRAF mutation (p. V600K) respond to treatment. However, 20% of these patients are primarily refractory to selective BRAF inhibitors and do not respond. The mechanisms of intrinsic resistance can include RAC1 mutations, loss of PTEN, dysregulation of cell cycle proteins, and changes to the tumor microenvironment. These abnormalities are summarized in Table 5.4. Tumor mutational burden. A high TMB augments tumor immunogenicity and increased numbers of tumor neoantigens; it may stimulate an immune response. DNA damage from exogenous factors is responsible for the high TMB seen in melanoma. The finding of high TMB is associated with higher response rates to immune checkpoint inhibitors.
Sarcomas are a highly heterogeneous group of malignant tumors showing differentiation toward adult mesenchymal tissue types. They are a divergent group of tumors morphologically, genetically and behaviorally. Although much is known about the individual molecular pathology of many soft-tissue tumors, this knowledge has been applied mainly in the diagnosis of sarcomas, with gene panels used mostly for diagnostic purposes. However, some tumors, such as GISTs, have well-known molecular abnormalities that provide not only prognostic information but also serve as predictive biomarkers for targeted therapy. Examples are aberrations affecting c-KIT and platelet-derived growth factor receptor (PDGFR), which can support imatinib therapy. Gastrointestinal stromal tumors. Around 80% of GISTs show mutations in KIT that result in constitutive activation of the RTK, c-KIT. Most commonly, there is mutation (SNV or indel) of exon 11, then exon 9, followed by exons 13 and 17 (Table 5.5). Up to 50% of KIT -mutation-negative GISTs have activating mutations of the PDGFRA gene, usually in exons 18, 12 and 14. A small subset of GISTS display mutations in BRAF, KRAS, NF1, NRAS or SDH. GISTs with a deletion in exon 11 (codon 557-8) are more biologically aggressive than those with substitutions in the same codons. Standard commercially available gene profiling chips (NGS) are highly appropriate to determine the genes associated with GISTs. The presence, type and location of the abnormality predict response to TKIs. Initially, imatinib was the drug of choice, but the newer TKIs sunitinib and regorafenib have also shown efficacy, particularly for GISTs resistant to imatinib. Avapritinib was approved by the FDA in early 2020 for GIST harboring a PDGFRA exon 18 mutation, including the D842V mutation, while the kinase inhibitor ripretinib was approved later in the same year as fourth-line therapy for adults with an advanced GIST. Crenolanib, which selectively inhibits PDGFRA mutant protein, particularly that arising from PDGFRA D842 mutation, is in clinical trials for use in patients with GIST. The second- and third-generation TKIs dasatinib, nilotinib and ponatinib, which target BCR - ABL products, have shown limited results. Infantile fibrosarcoma is caused by the fusion of NTRK3 with ETV6, which results in the ETS transcription factor contributing its helix-loop-helix domain to the kinase domain of NTRK3 in the resulting product. Clinical trials of Trk TKIs, such as larotrectinib and entrectinib, are under way in a range of sarcomas.
Epithelioid sarcoma. The FDA granted accelerated approval to tazemetostat for the treatment of adults and young people (16 years or older) with metastatic or locally advanced epithelioid sarcoma who are not eligible for complete resection (surgical removal of the tissue). Tazemetostat inhibits enhancer of zeste homolog 2 (EZH2); the biomarker is loss of integrase interactor 1 (INI1). Inflammatory myofibroblastic tumor. In around 50% of cases, and particularly in children, these rare tumors harbor rearrangements of chromosome 2p23. This is the site of ALK, which encodes an RTK that is upregulated on appropriate fusion to one of more than ten potential translocation partners. Specific TKI inhibitors, such as crizotinib, may be used for patients with inoperable/disseminated tumors. Molecular targeting in other non-GIST sarcomas. Pazopanib, an oral anti-angiogenic drug targeting vascular endothelial growth factor receptor (VEGFR), PDGFR, fibroblast growth factor receptor (FGFR), c-KIT and many other tyrosine kinases, was associated with improved PFS in patients with soft-tissue sarcomas other than liposarcomas (mainly leiomyosarcoma and synovial sarcoma) in the Phase III PALETTE study. There is, however, no biomarker to guide its use. Other TKIs used for treating GISTs, such sorafenib, sunitinib and regorafenib, have some effect against non-GI sarcomas, particularly in tumors with PDGFRA mutations. Other pathways investigated include the PI3K/AKT/mTOR pathway. The mTOR inhibitor ridaforolimus showed prolongation of PFS (though the drug was not approved by the FDA), while the mTOR inhibitor sirolimus is effective in the treatment of perivascular epithelioid tumors (PEComas), which are known to show mTOR pathway activation. Well-differentiated and dedifferentiated liposarcomas show amplification of MDM2 and CDK4 /. While the cyclin-dependent kinase 4/6 (CDK4/6) inhibitor palbociclib shows only modest results, there is some evidence that dual inhibition of CDK4/6 and MDM2 proto-oncogene (MDM2) may be synergistic. Summaries of molecular biomarkers and related therapies in sarcomas and targeted therapies in soft-tissue tumors are provided in Tables 5.6 and 5.7.
Prevention and management. By far the most research attention has been paid to the treatment of depression, with relatively few studies focusing on approaches to prevent its onset. However, knowledge derived from epidemiology and predictive studies has provided a basis for identifying people who are at increased risk of becoming depressed, and for tackling a range of psychosocial risk factors that are implicated in depression onset. Preventive strategies may be directed toward. the whole population - 'universal prevention'. selected high-risk groups - 'selective prevention'. people who have already developed some clinical features but not the full-blown disorder - 'indicated prevention'. Universal prevention. An area where universal programs appear to be beneficial is mental health promotion for young people using a whole-school approach. The most recent review findings (2017) indicate that such programs have modest beneficial effects, which appear to be maintained at 12-month follow-up. Universal programs have also sought to improve 'mental health literacy' (public knowledge and beliefs about mental health and mental disorders) to assist the recognition and professional or self-management of problems, and so address the high level of unmet need in this area. Initiatives have been conducted in many countries including the USA, UK, Germany, Norway and, most extensively, Australia, and have mostly involved mass media health campaigns. Initiatives to challenge stigma and discrimination. Challenging the prevailing stigma and discriminatory behaviors associated with depression and other mental health problems is a critical aspect of these universal programs. The Time to Change campaign (www. time-to-change. uk) launched in the UK in 2007, and the work of the Australian beyondblue organization (www. au), which started in 2000, are key examples of this approach, which typically involves social marketing such as advertising campaigns, together with education for employers, schools and health professionals. A central facet of these approaches is the focus on supporting people with lived experience of mental health problems, allowing them to play a central role in developing initiatives and delivering activities. A global antistigma alliance to identify best practices and share learning and resources was set up in 2012. A key aim of this work is to enable individuals to speak about their experience of depression without shame and to encourage them to become involved in treatment. It also aims to encourage more healthy workplaces where people with depression can have treatment support without their jobs being imperiled. This work has been aided considerably by celebrities and opinion leaders talking openly about their experience of depression and their recovery, exemplified recently by Prince Harry's comments about his experience of grief after his mother's death. Other universal programs. A related development in Australia is a 12-hour training course (Mental Health First Aid, www.
au), which was developed in 2000 to teach skills in recognizing and helping mental health problems such as depression, anxiety and psychosis. This program has been adopted in a number of other countries including Canada, Hong Kong, Finland, England, Wales and Scotland. Reviews suggest that, overall, these programs result in modest improvements in knowledge and attitudes toward depression or suicide (and other mental health problems), but most evaluations have not assessed the durability of these changes. There is limited evidence to date that campaigns or training increase appropriate help-seeking or decrease suicidal behavior. Universal versus selected prevention approaches. Although the science of depression prevention is at an early stage, the developing evidence base indicates promising findings, and a systematic review has identified good evidence for prevention among at-risk individuals and those with early features. In general, it seems that although universally directed services are a vital part of delivering health and social care, there are limits to broad-based community education programs. The most efficient and effective approaches for depression prevention may be those that target populations at risk rather than broader approaches. Selective prevention approaches that enhance protective factors using particular psychotherapy approaches appear able to delay the onset of depression in a range of at-risk groups. Current evidence indicates that interventions based on cognitive-behavior therapy (CBT) and interpersonal psychotherapy (IPT) are useful approaches. For instance, a Cochrane review () provides promising evidence for the effectiveness of individually delivered support for postnatal women. Intensive home visits by nurses and health visitors and telephone peer support were both found to reduce depression onset when targeted to those at risk. Preventing depression onset in patients with stroke has also been evaluated, and both antidepressant medication and problem-solving approaches have been found to be effective. Other investigations have found a reduced incidence of depression in patients recently diagnosed with cancer following a brief psychological intervention, although effects were limited to those participants at highest risk. Similarly, a CBT intervention for adolescents at risk of depression (having parents with depression together with a personal history of depression or subthreshold symptoms) has shown significant preventive benefits. Overall, findings from trials conducted in a range of settings indicate that preventive interventions may reduce the incidence of depressive disorders by around 20% compared with treatment-as-usual control groups. The extent of effect means that prevention should play a larger role in the further reduction of the disease burden of depressive disorders. Low participation rates in depression prevention programs have been noted as a problem, but it seems likely that the increased use of communications technologies such as mobile phones and the internet, together with appropriate marketing approaches, will enhance involvement.
The attention given to improving recognition of depression in primary care explored in Chapter 4 is encouraging. However, a central problem with innovations to improve detection is that they have not necessarily led to more effective depression management. It appears that many people who are recognized as being depressed are not provided with appropriate treatment, and many of those prescribed antidepressants discontinue them too soon. These management problems relate to the availability, acceptability and effectiveness of current treatments for depression: although considerable progress has been made over recent decades, difficulties and limitations remain in all these areas. A central issue in relation to depression management is the heterogeneity of this condition. As discussed earlier, depression covers a spectrum of symptoms, and different approaches are appropriate for differing levels and types of presentation. As well as matching different approaches according to severity, a number of different treatments appear to be equally effective for managing depression. People receiving the same treatments show a range of differing responses, with 25-30% of people treated showing no improvement with initial treatment. The clinical benefits of treatment are likely to be incomplete for a substantial proportion of people, with many requiring changes and combinations of treatments. At present around one-third of people achieve only partial remission from symptoms despite treatment. Types of treatment. There are two main approaches to the management of depression: pharmacotherapy and psychological therapy, and evidence shows that both are effective. Psychological or pharmacological therapy? Many people prefer psychological treatments, and reviews indicate that drop-out rates may be lower for psychological treatment than for drug treatment. There is some evidence that, for people with less severe forms of depression, psychological treatments may be more effective than pharmacotherapy. Conversely, antidepressant treatment is essential for melancholia, persistent depressive disorder and moderate to severe depression. The forms of psychological therapy best supported by current evidence are CBT, behavioral activation and IPT. In addition, electroconvulsive therapy is a well-established treatment with good evidence for effectiveness in severe depression; it is generally reserved for those who have not responded to other treatment or when the condition is potentially life-threatening. Self-help treatment options. Several other approaches have benefits for particular levels or presentations of depression - there is some evidence to support the use of exercise, relaxation training and a range of self-help approaches, as well as befriending, for milder levels. Exposure to light is effective for seasonal affective disorder and may be useful as an adjunct to other treatments for non-seasonal depression. Complementary therapies. Some complementary treatments may be useful - there is evidence that the herbal treatment St John's wort is equivalent to standard antidepressants in mild and moderate depression (although there are important problems that limit its usefulness, which are discussed on page 95). The nutritional supplement omega-3 polyunsaturated fatty acids may be useful as an adjunctive treatment for depressive symptoms in bipolar disorder but the effect is uncertain for unipolar depression. Studies of acupuncture for depression have shown mixed results: compared with sham control there is little evidence of benefit, but there may be an additive benefit when acupuncture is combined with medication. Validity of findings. The evidence for the effectiveness of all these approaches is derived from randomized controlled trials (RCTs) and systematic reviews of such trials. The RCT is a research method designed to ensure findings are free from bias and confounding influences, but it must be noted that there is a trade-off in determining effect by these methods. The high internal validity of RCTs (freedom from bias) is offset by problems in generalizing the findings to wider populations. Trials are typically conducted over relatively short time frames compared with treatment in the real world, and the interventions are often delivered and monitored by experts and evaluated in a selected population, the members of which are likely to be free of coexisting conditions and unrepresentative of certain groups, such as women, older people, minority ethnic group and the socioeconomically disadvantaged, compared with the general population. 
Organizational approaches. The effectiveness of individual treatments is clearly important in tackling depression, but much depends on the way that services are organized and enabled to respond to the variability of depression presentation and need for support within populations. Hence the development and evaluation of ways of delivering services is a key part of depression management. Current evidence suggests that better outcomes could be achieved if primary care practices were better organized to identify and respond to those experiencing depression. Stepped care is a model of healthcare based on matching treatments of differing intensity to the needs of the individual, so that the least restrictive approaches (in terms of intensity, inconvenience and cost) are used. Allied with this is the systematic monitoring of progress so that management can be altered (stepped up) if response is inadequate. This way of organizing practice involves standardizing the procedures and indications, and it has been used for the management of diverse conditions from migraine and back pain to diabetes and hypertension. More recently, it has been advocated for mental health problems such as anxiety, bulimia and depression. A stepped care framework for providing identification and treatment options appropriate to the differing needs of people with depression has been adopted in the UK and is integral to systems for depression care developed in health maintenance organizations in the USA. The stepped care model advocated in the guidance for depression developed by the National Institute for Health and Care Excellence (NICE) for use in the National Health Service in England, Wales and Northern Ireland is shown in Table 6.1. The subsequent sections of this chapter review treatment approaches for depression with reference to this stepped approach. The Royal Australian and New Zealand College of Psychiatrists also uses a similar stepped care model (Figure 6.1), with a focus on addressing lifestyle factors that may be contributing to the depression along with psychoeducation comprising the initial steps, before moving on to more specific interventions. Chronic care models. Many of the central elements of chronic care models (Table 6.2) have been integrated in the management of depression in adults and older people, and also for people with depression and coexisting physical illnesses. Much of the evaluation of these approaches has been conducted in the USA, but some evidence for effectiveness has also emerged from studies in the UK and Europe. Enhanced care packages. It appears that the use of an 'enhanced care' package including stepped care, case management and collaborative care is beneficial and cost-effective for those experiencing moderate to severe major depression. An important element of this care appears to be case management - a specific care-coordinator role, usually carried out by a nurse, social worker or psychologist. This appears to be beneficial because it enables more responsive and proactive treatment, allows consistent psychosocial support and assists with understanding and adhering to treatments, as well as developing self-management. Telephone as well as face-to-face support is commonly used, and outcomes are routinely measured - using instruments such as the Patient Health Questionnaire (see page 57). Subclinical and mild to moderate depression. Many presentations of depressive symptoms are likely to be relatively short lived and uncomplicated: spontaneous remission and placebo response are relatively common, with around 30% of people improving without active treatment or showing response to inactive management approaches. These presentations do not require formal professional treatment. However, this does not mean that such episodes are insignificant as they may be a precursor to a more serious disorder, and in the early stages of a depressive episode limited indicators may reliably guide the individual or clinician to the likely course. For this reason, the most appropriate approaches for initial depression onset and for mild depression involve establishing a therapeutic relationship (use of active listening, non-judgmental approach, ensuring confidentiality) and providing simple psychoeducation, while actively monitoring symptoms ('watchful waiting'). If a depressive episode of mild to moderate severity develops, the first options for treatment should include low-intensity interventions such as guided self-help, activity scheduling, brief psychological interventions, supervised exercise programs and computerized CBT.
Self-help and bibliotherapy involves the use of written materials, such as books or self-help manuals (increasingly, other media may be used but the majority of programs are in book form) that provide information about depression, symptom management, condition monitoring and strategies to improve mood and function. These materials are based on evidence, usually a cognitive-behavioral approach, and may involve guidance and support from a health professional (guided self-help), which can be delivered individually or in a group setting. Typically, a health professional introduces and explains the materials and monitors and reviews the treatment outcome via limited further contacts. These may be face-to-face, or by telephone or email. Several systematic reviews have indicated this approach to be effective in reducing depression scores in comparison with waiting-list controls or treatment as usual, and that effectiveness is maintained even where there is minimal contact with a professional or the resources are entirely self-administered. There is good evidence to support the use of these interventions for mild depressive symptoms or mild depression as a first-line intervention. Based on this evidence, schemes such as the Reading Well Books on Prescription in England have been established (see Useful Resources), whereby books that have been reviewed and endorsed by health experts (health professionals and patient and carer representatives) are made available and promoted by public libraries. Around 200 controlled evaluations of the effect of exercise for depression indicate a moderate effect on depressive symptoms; however, the evidence is inconclusive, primarily because of weaknesses in the design of many of the studies. Additionally, observational studies commonly find that people self-report that they use exercise to avoid depressive symptoms and prevent relapse, although robust evidence to support these claims is currently lacking. Various types of physical activity have been examined including aerobic (training of cardiorespiratory capacity) and non-aerobic (such as muscular strength or endurance training) exercise, ranging from water aerobics, ballroom dancing, jogging, running, walking and Qigong (a Chinese system of prescribed physical exercises involving breathing, coordination and relaxation performed in a meditative state). The most recent Cochrane review () based on 39 randomized trials found that physical exercise improves depressive symptoms in people with a diagnosis of mild or moderate depression, but the extent of the effect diminishes to borderline significance when evidence from only the six most robust trials is used. A systematic review of yoga () found the intervention potentially useful for adults with depression, although the conclusions were limited by the poor quality of the studies. While current evidence indicates that exercise is superior to waiting-list or no-treatment controls, findings are inconclusive concerning equivalence or superiority to either antidepressants or psychological therapy. It appears that discontinuation rates are lower for exercise than for antidepressants, although the rate of drop out from exercise programs may be substantial. Important questions remain concerning the most effective type, frequency and duration of exercise: the studies that comprise the evidence base involve a wide range of types of physical activity conducted for periods from 10 days to 16 weeks, conducted indoors and outdoors, and including both individual and group approaches, and supervised and unsupervised delivery. Reviewers have found that both insufficient data and inadequately reported intervention details make it difficult to identify the characteristics of exercise most likely to be associated with benefit. It appears likely that mixed rather than purely aerobic exercise is most effective, and that forms with higher intensity and longer duration are most likely to be beneficial, although this is based on very limited data. Not only does physical exercise appear to be beneficial for depression, it also has the important additional advantage of delivering health gains in other areas. Although encouragement of any increase in physical activity is likely to be useful for people with depression, current evidence indicates that a regular program of activity is necessary for maintenance of effects. The most recent () NICE guidance notes that a typical intervention is likely to involve supervised exercise for 45 minutes to 1 hour, undertaken two to three times each week for 10-14 weeks.
Given the lack of evidence of superiority of particular kinds of physical activity or modes of delivery, it seems reasonable to encourage people with depression to choose forms of exercise they most enjoy, as this is likely to maximize continuation in the long term. Problem-solving therapy (PST) is a discrete, structured, time-limited psychological intervention defined as a goal-oriented collaborative and active process that involves identifying and prioritizing particular problems from among the person's current difficulties. It aims to help people use their own skills and resources to improve their functioning. The treatment usually consists of six sessions, the first of which lasts 1 hour, with subsequent half-hour sessions. It is based on establishing a link between current problems and psychological functioning; success in solving and establishing control over these problems is associated with improvement in depressive symptoms. Important strengths of this approach are its brevity, accessibility and feasibility for delivery by a range of professionals and paraprofessionals within primary care. Several studies have supported the effectiveness of PST in primary care, but the extent and robustness of the evidence are limited. A recent trial of PST delivered by community mental health nurses compared with usual physician-led primary care found no significant differences in effectiveness. A systematic review of psychosocial interventions delivered in primary care by primary care physicians indicated that this approach was the most promising. Relaxation training. A variety of techniques are available for inducing relaxation, including progressive muscle relaxation, relaxation imagery, autogenic training and methods adapted from yoga and meditation. These approaches are generally well received by clients, with low drop-out rates evident in studies. One 2008 systematic review (which included 11 trials in the meta-analysis) indicated that relaxation training was more effective at reducing self-rated depressive symptoms than waiting-list, no-treatment or minimal-treatment controls, although outcome measurement by clinician-rated depressive symptoms provided inconclusive differences. Relaxation is less effective than established psychotherapies such as CBT but, because it requires only brief training, it may be useful as an initial element in a stepped care approach. It may, however, be better practice to invest in approaches with stronger evidence of effect such as guided self-help and computer- or internet-delivered treatments. Sleep hygiene. Disturbed sleep is one of the most common symptoms of depression, occurring in around three-quarters of episodes; it is also a common prodromal feature of depression, preceding depressive episodes in about 40% of cases. It causes subjective distress and can aggravate other symptoms of depression. Assisting people with the sleep difficulties that form a part of the spectrum of depressive features is an important part of initial intervention. 'Sleep hygiene' relates to the promotion of sleep that is appropriately timed and effective. Good sleep hygiene involves adhering to all elements of sleep hygiene guidance (Table 6.3), with particular emphasis on the importance of getting up at the same time each day. Psychological approaches using the computer and internet. One of the main barriers to patients receiving appropriate psychological treatments, such as CBT or PST, is a shortage of clinicians who can deliver them. Additionally, there may be problems for individuals scheduling appointments with family, work or carer commitments. Internet-based CBT programs, such as MoodGym and the Sadness Program in Australia, make use of the same techniques as face-to-face therapy but these are delivered in a more accessible and cost-effective way via a website. Evaluations of a range of computer- and internet-based CBT programs indicate significant improvements in the symptoms of depression compared with control patients. There is some indication that improvements in symptoms are maintained over time (up to 12 months). The use of such innovations in delivery may be particularly useful in rural and remote settings where access to clinicians may be limited.
Moderate and moderate to severe major depression. The main options for the treatment of moderate and severe depression involve pharmacological and psychological interventions. Treatment choice depends on judgments of severity, previous response to interventions and the person's preferences. Both pharmacological and psychological interventions increase response and remission to a similar extent compared with controls, and trials indicate an improvement for around two-thirds of those in any actively treated group. Lower rates of drop out from treatment are usually seen for psychological interventions. Pharmacological treatments. In the early 1950s the first antidepressant drugs were developed: the monoamine oxidase inhibitors (MAOIs) and, later, the tricyclic antidepressants (TCAs). These drugs act by enhancement of neurotransmission of serotonin and norepinephrine (noradrenaline), as well as other neurotransmitters. Problematically, TCAs have marked anticholinergic and antihistamine effects, and are toxic in overdose, while MAOIs have interactions with foods containing tyramine (red wines, cheese, liver, yeast extract) that cause a potentially fatal hypertensive reaction. For this reason MAOIs are not commonly used, and are rarely prescribed in primary care; TCAs are more frequently used in primary and secondary care but, because of their interactions and side effects, they are contraindicated in a number of physical illnesses, in particular cardiac disease. They are not recommended as a first-line treatment because of their side effects, interaction, and toxicity: importantly, TCAs should be avoided where there appears to be a risk of suicide because (except for lofepramine) they are associated with greatest risk in overdose. In the late 1980s, more selective drugs were developed with a primary focus of activity on the serotonergic system, giving rise to the selective serotonin-reuptake inhibitors (SSRIs). Drugs targeting the noradrenergic systems were also developed and antidepressants that combine action on both systems, the serotonin-norepinephrine-reuptake inhibitors, are now available and may be more effective for more severe episodes of depression. New antidepressants are being researched and developed that better enhance neurotransmission and influence allied mechanisms; one example is agomelatine, which exploits the synergy between melatonergic agonism (MT1 and MT2 receptors) and serotonergic 5HT2c antagonism; the latter enhances the release of dopamine and norepinephrine in the prefrontal cortex. In addition, stimulation of MT1 and MT2 receptors causes resynchronization of circadian rhythms. Circadian rhythms ensure that we have the most energy and cognitive capacity during the daylight hours when we are at work, and low energy at night time when we sleep. Circadian rhythms are controlled by the circadian clock located in the suprachiasmatic nucleus and modulated by the hormone melatonin, which is only released during the dark. The circadian clock keeps us in tune with the external environment and the seasonal changes in the length of day (photoperiod), with daylight suppressing melatonin production. It is recognized that circadian rhythms become desynchronized in some forms of depression; for example, the common symptom of waking early in the morning and the changing mood throughout the day are most likely the result of the desynchronization of these daily rhythms. The synergistic action of agomelatine results in release of dopamine and norepinephrine in the prefrontal cortex without increasing serotonin levels, providing the antidepressant efficacy without causing common serotonin-related side effects (sexual dysfunction, emotional blunting and a discontinuation syndrome). Agomelatine can be used as a first-line treatment, especially when the depression is accompanied by sleep problems or poor hedonic drives. It can also be used for patients who have experienced emotional blunting or sexual dysfunction while on an SSRI or SNRI. Are antidepressants effective? Some recent reviews have questioned the extent of benefit of antidepressants, and there seems little doubt that, as in many other areas of medical treatment, publication bias (wherein studies with positive findings are most likely to be published) has led to an inflated view of antidepressant effectiveness. It also appears that pharmaceutical industry sponsorship of studies is linked to biases in the reporting of findings. Furthermore, in recent years, the difference in effect between antidepressant drugs and placebos in RCTs has been becoming smaller. This has led some clinicians to question the effectiveness of antidepressants. However, the diminishing drug-placebo difference is more likely to be related to clinical trial methodology, with selection of participants with milder episodes of depression that would be likely to have a high placebo response.
Despite these important limitations, the majority of well-conducted meta-analyses provide evidence of the effectiveness of antidepressants for moderate and severe major depression, as well as for persistent subthreshold or mild depression. It seems, in general, that the more severe the symptoms, the greater the benefit of antidepressant treatment: this accords with general clinical experience, although there is conflicting research evidence about a relationship between medication response and symptom severity in moderate to severe depression. A very large number of studies have shown that, around 20% of people with depression of at least moderate severity, will improve without any treatment and 30% will respond to placebo treatment, while up to 50% will respond to antidepressant treatment. Choosing an antidepressant. In the USA, more than 40 antidepressant medications approved by the Food and Drug Administration are noted in current National Institute of Mental Health literature, while 29 are listed in the current British National Formulary. This diversity of treatment is not necessarily associated with increased effectiveness: there are relatively modest differences between the individual drugs or classes of drug. The main advantage from developments in antidepressants is in terms of tolerability and adverse effects, and notably the risk of harm in overdose. The SSRIs are far safer in overdose than TCAs, and are generally better tolerated than antidepressants from other classes, although they do have significant side effects. Systematic reviews conducted for recent clinical guidelines indicate that, for people with depression, including those with comorbid physical illness, SSRIs are an appropriate first-line treatment. Among this class of drugs, the NICE guideline for depression and chronic physical illnesses () notes that sertraline and citalopram appear to have the lowest interaction potential and may be protective against cardiac events, with evidence of benefits in reducing the risk of cardiac morbidity and mortality among people both with and without pre-existing ischemic heart disease. Another systematic review that compared the relative effects of 12 antidepressants (SSRIs and newer third-generation antidepressants) found sertraline and escitalopram (the S -stereoisomer of citalopram) to have the most favorable balance between efficacy and acceptability. The balance between efficacy and tolerability is an important consideration in deciding which antidepressant to use (Figure 6.2). Monitoring use of antidepressants. For people whose symptoms respond poorly to antidepressants, reviewing side effects, treatment preferences and adherence is warranted. Many people discontinue their antidepressant treatment before the recommended adequate treatment period (6 months continuation after symptom remission): findings from the USA indicate that 42% of patients stop taking antidepressants within the first month of prescribing, and 72% within the first 3 months. Patient education. Communication between clinician and patient about how antidepressants work (psychoeducation), especially that they need to be taken daily (and not just when feeling depressed) for at least 2 weeks before a response will be seen, and discussing treatment concerns (such as worries that antidepressants may be addictive), risk and experience of side effects, and likelihood of relapse, are helpful in facilitating treatment adherence. Regimen changes. Increasing the dose may be beneficial, but there is limited evidence that exceeding the recommended dose has additional benefit. Alternatively, switching to another medication, either within class when side effects may be the problem, or to another class of antidepressants when the response is limited, can be considered, with choice guided by factors such as previous treatment history and the potential of the drug to cause side effects. Switching between antidepressants because of poor response to initial treatment is more likely to be needed for people with more chronic depression and more coexisting health problems. If there is poor response to these strategies, augmentation with another psychotropic drug such as lithium, a second-generation antipsychotic or another antidepressant may be considered. This is usually undertaken in consultation with a consultant psychiatrist or within a specialist mental health service. Adverse effects of antidepressants (Table 6.4). All antidepressants may cause side effects, but TCAs are associated with a greater burden of anticholinergic side effects such as dry mouth, blurred vision and related cardiovascular effects such as hypotension, tachycardia and QT c prolongation; they also cause sedation and drowsiness. The SSRIs are associated with headache and gastrointestinal (GI) symptoms (nausea and bleeds). Sexual dysfunction is a common side effect with these agents but is rarely mentioned by patients, so it should be asked about explicitly, especially delayed ejaculation.
Emotional blunting can also occur with the SSRIs and SNRIs, in which patients are no longer depressed but do not feel a full range of emotions. Cardiovascular disease and depression are strongly associated, and a number of antidepressants can increase the risk of adverse events in this patient group. TCAs are particularly noted for their cardiotoxicity, while the SSRIs do not appear to be linked to increased risks for cardiovascular events and may exert a protective effect, although some are linked with QTs prolongation. Bleeding effects. SSRIs affect platelet aggregation and vaso-constriction, leading to increased bleeding times: the use of these antidepressants for people with risk factors for GI hemorrhage, or who are concurrently taking non-steroidal anti-inflammatory drugs, should therefore be avoided, and special monitoring and the use of gastro-protective agents is required where such treatments cannot be avoided. Suicidal ideas and acts may be increased in the early stages of antidepressant treatment. This is a small but very important effect that is most evident in children and adolescents but remains for those aged below 25 years. Careful consideration of treatment options and close monitoring of people at risk of suicide is essential. If antidepressants are used for people judged to be at risk of suicide, frequent monitoring and reviews are essential. Direct contact is recommended within 1 week of prescribing and thereafter at regular intervals depending on the level of risk and response to treatment. Telephone reviews may usefully supplement direct contacts. The quantity and toxicity of the prescribed antidepressant is an important consideration, and judgment of risk by assessing depression features, conducting a comprehensive suicide risk assessment (Table 6.5), and consideration of social situation and supports are all key factors in deciding whether to involve specialist mental health services. Antidepressant discontinuation symptoms may occur when stopping treatment and can affect one-third of people. Although this response may occur with any antidepressant (other than agomelatine), it is most commonly linked to paroxetine, venlafaxine, duloxetine and amitriptyline. Typical features are sweating, irritability, headache, dizziness, nausea and symptoms that are sometimes indistinguishable from depression itself. To avoid discontinuation symptoms, antidepressants should be withdrawn slowly, with a gradual dose reduction over a 4-week period. Although some people experience severe and prolonged discontinuation symptoms, especially when the antidepressant has been stopped abruptly, symptoms are usually mild and self-limiting if withdrawal is done slowly. The risk of these withdrawal features should be part of the discussion and explanation in the initial treatment planning and ongoing monitoring with the patient. If there is a history of discontinuation symptoms, a longer timeframe may be required and should be negotiated with the patient. There is no evidence of tolerance or cravings developing for antidepressants such as those commonly seen with drugs such as the benzodiazepines. Serotonin syndrome is a drug toxicity reaction that involves shivering, sweating, agitation, confusion, delirium, changes in blood pressure, pyrexia and myoclonus. Onset may be rapid, symptoms may range from mild to severe, and the condition may be fatal. It is caused most usually by medicine combinations that increase synaptic serotonin. Management involves the immediate cessation of precipitating medicines and the use of serotonin antagonists and benzodiazepines where necessary. Antidepressant continuation. For those who meet the criteria for an episode of major depression, antidepressant treatment should be continued for at least 6 months after the remission of an episode of depression (and for 12 months in people aged 60 and over). Stopping antidepressant treatment should involve a discussion between the patient and clinician, and this review should consider the possible need for continued treatment. There is consistent evidence that continuing treatment with antidepressants greatly reduces the risks of depression relapse, and for patients who are at high risk of recurrence, prescribing should continue for at least 2 years. People at risk of relapse for whom longer-term treatment must be considered include those who have experienced two or more recent episodes of depression, or who have residual symptoms or prolonged or severe episodes. The dose of medication for this maintenance should be at the same level as the acute treatment. Some psychological treatments have promising effects in the prevention of relapse and recurrence, even after the discontinuation of antidepressants.
Other biological treatments. St John's wort (Hypericum perforatum) extracts have been used for centuries as a depression treatment in Germany and have become popular in many other countries, including the UK. It has similar effectiveness to standard antidepressants for mild and moderate depression, and has less frequent and less serious side effects. However, inconsistencies in preparations and uncertainties about dose limit its usefulness, and there are serious interactions with frequently used drugs such as oral contraceptives, anticonvulsants and anticoagulants. Transcranial magnetic stimulation is a non-invasive process that uses an electromagnetic coil to excite or inhibit cortical areas of the brain. It has been used for physiological studies of neurological conditions and as a treatment for depression. A growing number of studies indicate effectiveness for depressive symptoms, although recent reviews report the strength of evidence to be low and uncertainty about the duration of benefits. Electroconvulsive therapy (ECT) involves inducing a therapeutic seizure by applying an electric current under general anesthesia and muscle relaxation. It is usually administered 2-3 times per week, with a total of 6-12 treatments typically being prescribed. Although it is highly effective in the acute episode, the relapse rate is high in the following 6 months, so maintenance antidepressant treatment is usually needed. ECT is an important and effective treatment for severe depression, particularly in people in whom other treatments have been ineffective, or who have a high suicide risk or pronounced psychomotor retardation. Acute confusion can sometimes follow treatment, and there is a risk of memory loss, particularly personal (autobiographical) memory and recall of recent events. Some degree of memory impairment affects at least one-third of people treated with ECT, but the extent and duration are variable. Newer forms of ECT (brief pulse ECT) have gone some way to reduce memory impairment. Nutrition, diet and dietary supplements. Omega-3 polyunsaturated fatty acids (PUFA) (predominantly found in oily fish and widely available as a nutritional supplement) form part of the Mediterranean diet and may be beneficial for a number of medical conditions. Studies have examined effects for depression and, on the basis of promising findings derived from observational studies, around 60 experimental studies have been conducted, although many of these are of poor quality. Most recent systematic review evidence (2016) shows a beneficial effect on depression symptoms for study participants with depression as defined by the Diagnostic and Statistical Manual of Mental Disorders of the American Psychiatric Association; this effect is evident where PUFA was the sole treatment and also (and with larger effect) when it was added to antidepressant treatment. 5-Hydroxytryptophan and tryptophan are dietary supplements regarded as natural alternatives to antidepressants. Studies suggest they are better than placebo at alleviating depression, but they may pose a risk for a potentially fatal condition (eosinophilia-myalgia syndrome). This, allied with the fact that their mode of action is the same as effective and well-tolerated antidepressants, makes their usefulness very limited. Psychological treatments. Many people may prefer a non-drug treatment for depression or one that is consistent with their own view of their problem. Psychological treatments have a lengthy history in the treatment of depression. There are many approaches and the six main treatment types are described in the next subsections. There is good evidence that psychological therapies are beneficial for depression, and several reviews have found only modest differences between these treatment types. However, systematic reviews with the most rigorously applied criteria have consistently identified differences, with approaches that are directive and teach new skills (IPT, CBT, behavioral activation [BA] and mindfulness-based cognitive therapy [MBCT]) appearing to be more effective than counseling and psychodynamic therapy. These psychological treatments are effective in both specialist and primary care settings, although one review identified lower effects in primary care, and suggested that selection for therapy by systematic screening rather than referral by the primary care physician was responsible for this lower effect size. Alongside the widely voiced concerns about the role of pharmaceutical companies in promoting and emphasizing the benefits of antidepressants, the effects of psychological therapy for adult depression have been rigorously re-evaluated. The findings indicate that both publication bias and limitations in quality selection of studies for meta-analysis have overestimated the effectiveness of psychological treatments. Their effects are significant, and clinically important, but seem likely to be smaller than has been previously assumed. Psychodynamic and psychoanalytic approaches to therapy originate from the work of Freud in the early 20th century and involve exploring conscious and unconscious conflicts and examining how these are played out in relationships, including the relationship between therapist and client.
As such, they are non-directive approaches. These therapies are often long term, but treatments for depression that have been evaluated are generally short-term psychodynamic psychotherapies (STPP) conducted over 10-30 sessions. The most recent Cochrane review of STPP () for common mental health problems showed significant improvements in depression compared with control for short- and medium-term, but not long-term, outcomes. Counseling refers to a range of interventions, often delivered in primary care and by voluntary sector organizations, that are based on the humanistic psychology model and work of Carl Rogers in the 1950s and 60s. Contemporary counseling, although rooted in these theories, draws on a variety of other sources such as behavioral, psychodynamic and cognitive models. Systematic review evidence indicates that counseling is better than usual GP care in improving mental health outcomes in the short term, although the advantages are modest and long-term benefit is not apparent. Behavioral treatments for depression are based on learning theory, and were initially developed in the 1950s. The BA approach involves encouraging the person to alter patterns of negative reinforcement by scheduling pleasurable activities, particularly with others, and to extend activities to encompass more rewarding behaviors. This has been part of CBT, but interest and application of BA as a therapy in its own right has increased over the past decade. A Cochrane review () comparing behavioral treatment for depression versus other psychological approaches indicated that this approach, which is relatively simple to deliver, was equally effective and acceptable. Cognitive-behavior therapy originated from the work of Aaron Beck in the 1950s and was formalized into a depression treatment in the late 1970s. It has subsequently been further refined and developed for various other conditions. The key focus is on the ways of thinking that are characteristic of depression and reflect underlying negative beliefs. CBT helps the person to recognize and modify negative thinking through structured collaborative work in therapy sessions and homework assignments. It can be delivered at an individual or group level, typically for 5-20 sessions of around 40 minutes each. Numerous studies and systematic reviews demonstrate good evidence for the effectiveness of CBT: it is consistently more effective than 'waiting list' or 'no treatment' controls. The review evidence indicates CBT to be either equally effective or superior to other psychological approaches. Interpersonal therapy is a more recent therapy, developed for depression treatment in the 1980s. It focuses on current personal relationships and their effects on mood. Mindfulness-based cognitive therapy was developed from mindfulness-based stress reduction, an approach used to help patients cope with symptoms related to a broad range of chronic illnesses. It is derived from the Eastern meditative practice of mindfulness, combined with elements of CBT. A central focus of this technique is to interrupt the negative thought patterns that are linked to the development of depressive ruminations and mood changes. This is achieved by learning and practicing engagement with internal and external experiences in the present, rather than focusing on past events or future possibilities. MBCT is typically delivered as an 8-week group program with a further four follow-up sessions in the year after therapy. The most consistent evidence for MBCT is for its effect in reducing the risk of further relapse among people with a history of depression relapses. Systematic reviews comparing MBCT with standard treatment also indicate significant benefits in symptom reduction. Residual symptoms, chronic depression and combination treatments. For a large group of people, remission from depression is partial, with features persisting after initial treatments. This limits function and quality of life and is an indicator of greater risk of relapse. Poor treatment response is more likely for people with coexisting medical problems and with concurrent mental health problems, such as anxiety disorders or substance misuse, and negative social circumstances, and for those whose depression is more chronic. Ongoing treatment steps are successful in improving remission rates - a large-scale study in the USA (the STAR-D study) has shown that around two-thirds of people achieve remission with up to four successive treatments. When response to antidepressant treatment is suboptimal, switching to CBT is likely to achieve outcomes as good as those achieved by changing to a different antidepressant, and this option may be better tolerated. It is also clear that when response to single treatment types is suboptimal, a combination of antidepressants and CBT is more effective than either treatment alone. MBCT appears to be a useful adjunct to antidepressant treatment, as well as a relapse prevention approach in its own right. Combinations of psychological therapy and antidepressants are also indicated for depression at the most serious end of the severity spectrum. SSRIs together with CBT are currently the best-evaluated approach.
