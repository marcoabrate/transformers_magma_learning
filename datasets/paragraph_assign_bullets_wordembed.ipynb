{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "magma_dir = '/home/marco/epfl/magma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 16184,
     "status": "ok",
     "timestamp": 1610463826089,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "ClE5D523OTZG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, magma_dir)\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 16184,
     "status": "ok",
     "timestamp": 1610463826092,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "82WSp6khIcua"
   },
   "outputs": [],
   "source": [
    "MODEL = 'pegasus'\n",
    "\n",
    "RE_SPLITTER = '\\n'              # do we split sentences of paragraphs?\n",
    "                                # use '\\.(?!\\d)|\\n' or '\\n', respectively\n",
    "\n",
    "TOKEN_MAX_LEN = 99              # max length of a word\n",
    "PARA_MIN_LENGTH = 2             # minimum length for a sentence or\n",
    "                                # a paragraph, in tokens\n",
    "\n",
    "RECALL_THRESHOLD = 0.7\n",
    "\n",
    "# Output path\n",
    "OUTPUT_PATH = magma_dir+'datasets/karger_books_para_wordembed/'+MODEL+'/'\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "tb7fAfzaK4es"
   },
   "source": [
    "### **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 16443,
     "status": "ok",
     "timestamp": 1610463826354,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "hidden": true,
    "id": "wvbMlPBxk45S"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from textwrap import fill\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "if 'pegasus' in MODEL:\n",
    "    from transformers import PegasusTokenizer\n",
    "    tokenizer =\\\n",
    "        PegasusTokenizer.from_pretrained('google/pegasus-large')\n",
    "elif 'bart' in MODEL:\n",
    "    from transformers import BartTokenizer\n",
    "    tokenizer =\\\n",
    "        BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "elif 't5' in MODEL:\n",
    "    from transformers import T5Tokenizer\n",
    "    tokenizer =\\\n",
    "        T5Tokenizer.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQGq4WLu3Gei"
   },
   "source": [
    "## **Karger Books Base Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Z0lbkScg0a7j"
   },
   "outputs": [],
   "source": [
    "base_dataset = magma_dir+'datasets/karger_books_base/df.csv'\n",
    "df = pd.read_csv(base_dataset)\n",
    "df = df.set_index(['book', 'chapter', 'section', 'subsection'])\n",
    "df.bullets = df.bullets.map(eval, na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "tSHT0mxuvkEp"
   },
   "source": [
    "## **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "-eRnW74aH95b"
   },
   "source": [
    "#### Preprocessing\n",
    "\n",
    "* Split based on RE_SPLITTER\n",
    "* Explode the dataset\n",
    "* Remove unwanted chars at beginning or end of sentence\n",
    "* Remove multiple spaces\n",
    "* Remove long words (> TOKEN_MAX_LEN chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "id": "CDsT33j-wPCw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split in sentences / paragraphs based on RE_SPLITTER\n",
    "df.text =\\\n",
    "    df.text.map(lambda x: [p.strip() for p in re.split(RE_SPLITTER, x) if p!=''],\n",
    "                na_action='ignore')\n",
    "    \n",
    "# explode to get one row for each paragraph /sentence\n",
    "df = df.explode('text')\n",
    "df = df.rename(columns={'text': 'para'})\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove unwanted chars at beginning or end of sentence\n",
    "df.para = df.para.map(lambda p: p.lstrip('.,;:-)] \\n'))\n",
    "df.para = df.para.map(lambda p: p.rstrip('.,;:-([ \\n'))\n",
    "\n",
    "# Remove multiple spaces\n",
    "df.para = df.para.map(lambda p:\n",
    "    re.sub('\\s+', ' ', p).strip())\n",
    "\n",
    "# Remove long words (> TOKEN_MAX_LEN chars)\n",
    "def para2words(para):\n",
    "    return gensim.utils.simple_preprocess(\n",
    "        para, deacc=True, max_len=TOKEN_MAX_LEN)\n",
    "df['para_proc'] = df.para.map(para2words)\n",
    "df['bullets_proc'] = df.bullets.map(lambda bs: [para2words(b) for b in bs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Further Preprocessing\n",
    "\n",
    "* Remove stop words\n",
    "* Remove short sentences / paragraphs (< PARA_MIN_LENGTH tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "df.para_proc = df.para_proc.map(lambda p:\n",
    "    [w for w in p if w not in stop_words])\n",
    "df.bullets_proc = df.bullets_proc.map(lambda bs:\n",
    "    [[w for w in b if w not in stop_words] for b in bs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove short sentences / paragraphs (< PARA_MIN_LENGTH tokens)\n",
    "df.loc[df.para_proc.map(len) <\\\n",
    "    PARA_MIN_LENGTH, 'para_proc'] = np.nan\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.para = df.para.map(lambda p: p+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Assign Bullets to Best Para and Expand Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_best_metric_para(df, col_metric):\n",
    "    df['best_match'] = False\n",
    "\n",
    "    for idx, para  in df.groupby('bullets').progress_apply(\n",
    "        lambda g: g.iloc[g[col_metric].argmax()]).para.iteritems():\n",
    "        \n",
    "        df.loc[\\\n",
    "            (df['bullets'] == idx) &\\\n",
    "            (df['para'] == para), 'best_match'] = True\n",
    "    \n",
    "    para_too_short =\\\n",
    "        df[(df['compression_ratio'] >= config.MAX_RATIO) & df['best_match']]\n",
    "    print('Percentage of paragraphs which are too short to be summarized: %.2f %%'\\\n",
    "        %(len(para_too_short)/len(df[df['best_match']])*100))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_up_down(df, col_metric):\n",
    "    # for each bullet\n",
    "    for bul in tqdm(set(df.bullets.tolist())):\n",
    "        df_bul = df[df['bullets'] == bul]\n",
    "        \n",
    "        # get book and chapter where this bullet is\n",
    "        book = df_bul.index.get_level_values(0)[0]\n",
    "        cpt = df_bul.index.get_level_values(1)[0]\n",
    "\n",
    "        df_bul = df_bul.reset_index()\n",
    "        # get best match index\n",
    "        best_match_idx = np.where(df_bul['best_match'])[0][0]\n",
    "        merged_para_idx = [best_match_idx]\n",
    "\n",
    "        bul_num_tok = df_bul.loc[best_match_idx, 'bullets_num_tokens']\n",
    "        merged_para_num_tok = df_bul.loc[best_match_idx, 'para_num_tokens']\n",
    "        comp_ratio = df_bul.loc[best_match_idx, 'compression_ratio']\n",
    "        num_bul_cpt = len(set(df.loc[book, cpt].bullets.tolist()))\n",
    "        max_idx = len(df_bul)-1\n",
    "        \n",
    "        while comp_ratio > config.MAX_RATIO and\\\n",
    "            merged_para_num_tok < tokenizer.model_max_length:\n",
    "            \n",
    "            # if we already merged all possible paragraphs\n",
    "            if (0 in merged_para_idx) and (max_idx in merged_para_idx):\n",
    "                break\n",
    "                \n",
    "            # if we already merged the first paragraph\n",
    "            elif 0 in merged_para_idx:\n",
    "                new_para_idx = max(merged_para_idx)+1\n",
    "                \n",
    "            # if we already merged the last paragraph\n",
    "            elif max_idx in merged_para_idx:\n",
    "                new_para_idx = min(merged_para_idx)-1\n",
    "                \n",
    "            # otherwise check for best metric inclusion\n",
    "            else:\n",
    "                if df_bul.loc[min(merged_para_idx)-1, col_metric] <\\\n",
    "                    df_bul.loc[max(merged_para_idx)+1, col_metric]:\n",
    "                    # merge down\n",
    "                    new_para_idx = max(merged_para_idx)+1\n",
    "                    \n",
    "                else: # merge up\n",
    "                    new_para_idx = min(merged_para_idx)-1       \n",
    "\n",
    "            df_bul.loc[new_para_idx, 'best_match'] = True\n",
    "            merged_para_idx.append(new_para_idx)\n",
    "            \n",
    "            merged_para_num_tok += df_bul.loc[new_para_idx, 'para_num_tokens']\n",
    "            comp_ratio = bul_num_tok / merged_para_num_tok\n",
    "\n",
    "        for p, b in zip(df_bul.loc[merged_para_idx]['para'].tolist(),\n",
    "            df_bul.loc[merged_para_idx]['bullets'].tolist()):\n",
    "            df.loc[(df['para'] == p) &\n",
    "                (df['bullets'] == b), 'best_match'] = True\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_up_down_numtok_th(df, col_metric):\n",
    "    # for each bullet\n",
    "    for bul in tqdm(set(df.bullets.tolist())):\n",
    "        df_bul = df[df['bullets'] == bul]\n",
    "        \n",
    "        # get book and chapter where this bullet is\n",
    "        book = df_bul.index.get_level_values(0)[0]\n",
    "        cpt = df_bul.index.get_level_values(1)[0]\n",
    "\n",
    "        df_bul = df_bul.reset_index()\n",
    "        # get best match index\n",
    "        best_match_idx = np.where(df_bul['best_match'])[0][0]\n",
    "        merged_para_idx = [best_match_idx]\n",
    "\n",
    "        bul_num_tok = df_bul.loc[best_match_idx, 'bullets_num_tokens']\n",
    "        merged_para_num_tok = df_bul.loc[best_match_idx, 'para_num_tokens']\n",
    "        num_bul_cpt = len(set(df.loc[book, cpt].bullets.tolist()))\n",
    "        num_tok_threshold = min(df_bul.para_num_tokens.sum() / num_bul_cpt,\n",
    "                                0.9*tokenizer.model_max_length)\n",
    "        max_idx = len(df_bul)-1\n",
    "        \n",
    "        while merged_para_num_tok < num_tok_threshold:\n",
    "            \n",
    "            # if we already merged all possible paragraphs\n",
    "            if (0 in merged_para_idx) and (max_idx in merged_para_idx):\n",
    "                break\n",
    "                \n",
    "            # if we already merged the first paragraph\n",
    "            elif 0 in merged_para_idx:\n",
    "                new_para_idx = max(merged_para_idx)+1\n",
    "                \n",
    "            # if we already merged the last paragraph\n",
    "            elif max_idx in merged_para_idx:\n",
    "                new_para_idx = min(merged_para_idx)-1\n",
    "                \n",
    "            # otherwise check for best metric inclusion\n",
    "            else:\n",
    "                if df_bul.loc[min(merged_para_idx)-1, col_metric] <\\\n",
    "                    df_bul.loc[max(merged_para_idx)+1, col_metric]:\n",
    "                    # merge down\n",
    "                    new_para_idx = max(merged_para_idx)+1\n",
    "                else: # merge up\n",
    "                    new_para_idx = min(merged_para_idx)-1   \n",
    "\n",
    "            df_bul.loc[new_para_idx, 'best_match'] = True\n",
    "            merged_para_idx.append(new_para_idx)\n",
    "            merged_para_num_tok += df_bul.loc[new_para_idx, 'para_num_tokens']\n",
    "\n",
    "        for p, b in zip(df_bul.loc[merged_para_idx]['para'].tolist(),\n",
    "            df_bul.loc[merged_para_idx]['bullets'].tolist()):\n",
    "            df.loc[(df['para'] == p) &\n",
    "                (df['bullets'] == b), 'best_match'] = True\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(df):\n",
    "    num_para_tot = 18822\n",
    "    num_para_kept = np.sum(df.groupby('para')['best_match'].apply(np.any).tolist())\n",
    "    print('%d out of %d paragraphs are considered using this method.'%(num_para_kept, num_para_tot), end=' ')\n",
    "    print('Thus, %.2f %%'%(100*num_para_kept/num_para_tot))\n",
    "    \n",
    "    print()\n",
    "    df_count_tokens = df.groupby('para', sort=False).agg({\n",
    "        'best_match': lambda bm: np.any(list(bm)),\n",
    "        'para_num_tokens': lambda pnt: list(pnt)[0]})\n",
    "    num_tok_kept = df_count_tokens[df_count_tokens['best_match']].para_num_tokens.sum()\n",
    "    num_tok_tot = df_count_tokens.para_num_tokens.sum()\n",
    "\n",
    "    print('%d out of %d tokens are considered using this method.'%(num_tok_kept, num_tok_tot), end=' ')\n",
    "    print('Thus, %.2f %%'%(100*num_tok_kept/num_tok_tot))\n",
    "\n",
    "def print_stats_after_merge(df):\n",
    "    para_too_short = df[df['compression_ratio'] > config.MAX_RATIO]\n",
    "    print('Percentage of paragraphs which are too short to be summarized: %.2f %%'\\\n",
    "        %(len(para_too_short)/len(df)*100))\n",
    "    \n",
    "    print()\n",
    "    print('Paragraphs which are too long to fit into the model: %d paragraphs.'%\\\n",
    "          len(df[df['para_num_tokens'] > tokenizer.model_max_length]))\n",
    "    print(df[df['para_num_tokens'] > tokenizer.model_max_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## **Word2Vec Book Level**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Create Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "op = OUTPUT_PATH + 'w2v/'\n",
    "if not os.path.exists(op):\n",
    "    os.makedirs(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v_book = df_w2v.groupby('book', sort=False).agg({\n",
    "    'para_proc': lambda pp: list(pp),\n",
    "    'bullets_proc': lambda bp: list(bp)[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v_book['corpus'] = df_w2v_book.para_proc + df_w2v_book.bullets_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:38<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "df_w2v_book['w2v'] = df_w2v_book.corpus.progress_map(lambda c:\\\n",
    "    gensim.models.Word2Vec(\n",
    "        c,\n",
    "        #size=128,\n",
    "        #window=3,\n",
    "        min_count=1,\n",
    "        sg=1, # 1 for skip-gram; otherwise CBOW.\n",
    "        seed = config.SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18773/18773 [00:01<00:00, 11339.62it/s]\n",
      "100%|██████████| 18773/18773 [00:00<00:00, 31967.78it/s]\n"
     ]
    }
   ],
   "source": [
    "def assign_word_vectors(r, col):\n",
    "    book = r.name[0]\n",
    "    wv = df_w2v_book.loc[book, 'w2v'].wv\n",
    "    wv_list = []\n",
    "    for x in r[col]:\n",
    "        try:\n",
    "            v = wv[x]\n",
    "        except:\n",
    "            continue\n",
    "        wv_list.append(v)\n",
    "    return wv_list\n",
    "\n",
    "df_w2v['para_wv'] = df_w2v.progress_apply(lambda row: assign_word_vectors(row, 'para_proc'), axis=1)\n",
    "\n",
    "# taking the average of the w2v vector of each paragraph\n",
    "df_w2v.para_wv = df_w2v.para_wv.progress_map(lambda p_wv: np.mean(p_wv, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [para, bullets, para_proc, bullets_proc, para_wv]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df_w2v[df_w2v.para_wv.isna()])\n",
    "df_w2v = df_w2v.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Explode, preprocess, w2v bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114277/114277 [00:10<00:00, 10942.81it/s]\n",
      "100%|██████████| 114277/114277 [00:06<00:00, 16737.64it/s]\n"
     ]
    }
   ],
   "source": [
    "df_w2v = df_w2v.explode('bullets')\n",
    "\n",
    "df_w2v['bullets_proc'] = df_w2v.bullets.progress_map(para2words)\n",
    "df_w2v.bullets_proc = df_w2v.bullets_proc.progress_map(lambda b:\n",
    "    [w for w in b if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114277/114277 [00:06<00:00, 18080.65it/s]\n",
      "100%|██████████| 114277/114277 [00:02<00:00, 42885.92it/s]\n"
     ]
    }
   ],
   "source": [
    "df_w2v['bullets_wv'] = df_w2v.progress_apply(lambda row: assign_word_vectors(row, 'bullets_proc'), axis=1)\n",
    "\n",
    "# taking the average of the w2v vector of each bullet\n",
    "df_w2v.bullets_wv = df_w2v.bullets_wv.progress_map(lambda b_wv: np.mean(b_wv, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114277/114277 [00:55<00:00, 2062.96it/s]\n",
      "100%|██████████| 114277/114277 [00:46<00:00, 2453.12it/s]\n"
     ]
    }
   ],
   "source": [
    "df_w2v['para_num_tokens'] = df_w2v.para.progress_map(lambda p: len(tokenizer.tokenize(p)))\n",
    "df_w2v['bullets_num_tokens'] = df_w2v.bullets.progress_map(lambda b: len(tokenizer.tokenize(b)))\n",
    "\n",
    "df_w2v['compression_ratio'] = df_w2v.bullets_num_tokens / df_w2v.para_num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calculate cosine similarity between each couple bullet-para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114277/114277 [00:04<00:00, 25128.01it/s]\n"
     ]
    }
   ],
   "source": [
    "df_w2v['cosine_sim'] = df_w2v[['para_wv', 'bullets_wv']].progress_apply(lambda row:\\\n",
    "    cosine_sim(row[0], row[1]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Find Best Match and Expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2556/2556 [00:01<00:00, 1651.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of paragraphs which are too short to be summarized: 53.79 %\n"
     ]
    }
   ],
   "source": [
    "# find best match bullet-para for each bullet\n",
    "df_w2v = assign_best_metric_para(df_w2v, 'cosine_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2556 [00:00<?, ?it/s]/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/ipykernel_launcher.py:18: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "100%|██████████| 2556/2556 [01:40<00:00, 25.51it/s]\n"
     ]
    }
   ],
   "source": [
    "df_w2v = expand_up_down(df_w2v, 'cosine_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3961 out of 18822 paragraphs are considered using this method. Thus, 21.04 %\n",
      "\n",
      "345838 out of 1229678 tokens are considered using this method. Thus, 28.12 %\n"
     ]
    }
   ],
   "source": [
    "print_stats(df_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v_merge = df_w2v[df_w2v['best_match']].reset_index().groupby(['book', 'chapter', 'bullets'], sort=False)\\\n",
    ".agg({\n",
    "    'para': lambda p: ' '.join(list(p)),\n",
    "    'para_num_tokens': sum,\n",
    "    'bullets_num_tokens': lambda bnt: list(bnt)[0]\n",
    "}).reset_index(level='bullets')\n",
    "df_w2v_merge = df_w2v_merge.rename(columns={'para': 'text'})\n",
    "\n",
    "df_w2v_merge['compression_ratio'] = df_w2v_merge.bullets_num_tokens / df_w2v_merge.para_num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of paragraphs which are too short to be summarized: 0.00 %\n",
      "\n",
      "Paragraphs which are too long to fit into the model: 0 paragraphs.\n",
      "Empty DataFrame\n",
      "Columns: [bullets, text, para_num_tokens, bullets_num_tokens, compression_ratio]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print_stats_after_merge(df_w2v_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    0.0\n",
       "mean     NaN\n",
       "std      NaN\n",
       "min      NaN\n",
       "25%      NaN\n",
       "50%      NaN\n",
       "75%      NaN\n",
       "max      NaN\n",
       "Name: para_num_tokens, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w2v_merge[df_w2v_merge['para_num_tokens'] > tokenizer.model_max_length].para_num_tokens.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v_merge.to_csv(op+'df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Create train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_w2v_merge = df_w2v_merge.groupby(level=[0, 1], sort=False).agg({\n",
    "    'bullets': lambda b: list(b),\n",
    "    'text': lambda t: list(t),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 408\n"
     ]
    }
   ],
   "source": [
    "df_w2v_merge = df_w2v_merge.sample(frac=1, random_state=config.SEED)\n",
    "df_w2v_merge['num_bulls'] = df_w2v_merge.bullets.map(len).cumsum()\n",
    "tot_bulls = df_w2v_merge.num_bulls.iloc[-1]\n",
    "split1 = np.where(df_w2v_merge.num_bulls > int(tot_bulls*0.8))[0][0]+1\n",
    "split2 = np.where(df_w2v_merge.num_bulls > int(tot_bulls*0.9))[0][0]+1\n",
    "print(split1, split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train, val, test =\\\n",
    "    df_w2v_merge.iloc[:split1].explode('bullets'),\\\n",
    "    df_w2v_merge.iloc[split1:split2].explode('bullets'),\\\n",
    "    df_w2v_merge.iloc[split2:].explode('bullets')\n",
    "\n",
    "train['text'] = df_w2v_merge.iloc[:split1].explode('text')['text']\n",
    "val['text'] = df_w2v_merge.iloc[split1:split2].explode('text')['text']\n",
    "test['text'] = df_w2v_merge.iloc[split2:].explode('text')['text']\n",
    "\n",
    "train.to_csv(op+'train.csv')\n",
    "val.to_csv(op+'val.csv')\n",
    "test.to_csv(op+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(op+'train.source', 'w') as tr_s,\\\n",
    "    open(op+'train.target', 'w') as tr_t,\\\n",
    "    open(op+'train.index', 'w') as tr_i:\n",
    "    for idx, row in train[['text', 'bullets']].iterrows():\n",
    "        tr_i.write(str(idx) + '\\n')\n",
    "        tr_s.write(row.text + '\\n')\n",
    "        tr_t.write(row.bullets + '\\n')\n",
    "        \n",
    "with open(op+'val.source', 'w') as va_s,\\\n",
    "    open(op+'val.target', 'w') as va_t,\\\n",
    "    open(op+'val.index', 'w') as va_i:\n",
    "    for idx, row in val[['text', 'bullets']].iterrows():\n",
    "        va_i.write(str(idx) + '\\n')\n",
    "        va_s.write(row.text + '\\n')\n",
    "        va_t.write(row.bullets + '\\n')\n",
    "        \n",
    "with open(op+'test.source', 'w') as te_s,\\\n",
    "    open(op+'test.target', 'w') as te_t,\\\n",
    "    open(op+'test.index', 'w') as te_i:\n",
    "    for idx, row in test[['text', 'bullets']].iterrows():\n",
    "        te_i.write(str(idx) + '\\n')\n",
    "        te_s.write(row.text + '\\n')\n",
    "        te_t.write(row.bullets + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## **Doc2Vec Book Level**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Create Doc Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "op = OUTPUT_PATH + 'd2v/'\n",
    "if not os.path.exists(op):\n",
    "    os.makedirs(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_d2v = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_d2v_book = df_d2v.groupby('book', sort=False).agg({\n",
    "    'para_proc': lambda pp: list(pp),\n",
    "    'bullets_proc': lambda bp: list(bp)[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_d2v_book['corpus'] = df_d2v_book.para_proc + df_d2v_book.bullets_proc\n",
    "df_d2v_book['tagged_corpus'] = df_d2v_book.corpus.map(lambda c:\n",
    "    [gensim.models.doc2vec.TaggedDocument(para, [i]) for i, para in enumerate(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:36<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "df_d2v_book['d2v'] = df_d2v_book.tagged_corpus.progress_map(lambda tc:\\\n",
    "    gensim.models.Doc2Vec(\n",
    "        tc,\n",
    "        dm=1, # 1 for PV-DM; otherwise PV-DBOW\n",
    "        #vector_size=128,\n",
    "        #window=3,\n",
    "        #epochs=5,\n",
    "        min_count=1,\n",
    "        seed = config.SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Explode and preprocess bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114277/114277 [00:07<00:00, 14568.97it/s]\n",
      "100%|██████████| 114277/114277 [00:04<00:00, 23445.69it/s]\n"
     ]
    }
   ],
   "source": [
    "df_d2v = df_d2v.explode('bullets')\n",
    "\n",
    "df_d2v['bullets_proc'] = df_d2v.bullets.progress_map(para2words)\n",
    "df_d2v.bullets_proc = df_d2v.bullets_proc.progress_map(lambda b:\n",
    "    [w for w in b if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114277/114277 [00:52<00:00, 2184.74it/s]\n",
      "100%|██████████| 114277/114277 [00:43<00:00, 2650.27it/s]\n"
     ]
    }
   ],
   "source": [
    "df_d2v['para_num_tokens'] = df_d2v.para.progress_map(lambda p: len(tokenizer.tokenize(p)))\n",
    "df_d2v['bullets_num_tokens'] = df_d2v.bullets.progress_map(lambda b: len(tokenizer.tokenize(b)))\n",
    "\n",
    "df_d2v['compression_ratio'] = df_d2v.bullets_num_tokens / df_d2v.para_num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calculate similarity between each couple bullet-para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114277/114277 [02:40<00:00, 711.70it/s]\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "def d2v_similarity(r):\n",
    "    book = r.name[0]\n",
    "    d2v = df_d2v_book.loc[book, 'd2v']\n",
    "    dv_para = d2v.infer_vector(r.para_proc)\n",
    "    dv_bullets = d2v.infer_vector(r.bullets_proc)\n",
    "    \n",
    "    return cosine_sim(dv_para, dv_bullets)\n",
    "    \n",
    "df_d2v['d2v_sim'] = df_d2v.progress_apply(lambda row: d2v_similarity(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Find Best Match and Expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2556/2556 [00:01<00:00, 1365.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of paragraphs which are too short to be summarized: 56.57 %\n"
     ]
    }
   ],
   "source": [
    "# find best match bullet-para for each bullet\n",
    "df_d2v = assign_best_metric_para(df_d2v, 'd2v_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2556 [00:00<?, ?it/s]/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/ipykernel_launcher.py:18: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "100%|██████████| 2556/2556 [01:40<00:00, 25.32it/s]\n"
     ]
    }
   ],
   "source": [
    "df_d2v_expand = expand_up_down(df_d2v, 'd2v_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4105 out of 18822 paragraphs are considered using this method. Thus, 21.81 %\n",
      "\n",
      "354731 out of 1229678 tokens are considered using this method. Thus, 28.85 %\n"
     ]
    }
   ],
   "source": [
    "print_stats(df_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_d2v_merge = df_d2v[df_d2v['best_match']].reset_index().groupby(['book', 'chapter', 'bullets'], sort=False)\\\n",
    ".agg({\n",
    "    'para': lambda p: ' '.join(list(p)),\n",
    "    'para_num_tokens': sum,\n",
    "    'bullets_num_tokens': lambda bnt: list(bnt)[0]\n",
    "}).reset_index(level='bullets')\n",
    "df_d2v_merge = df_d2v_merge.rename(columns={'para': 'text'})\n",
    "\n",
    "df_d2v_merge['compression_ratio'] = df_d2v_merge.bullets_num_tokens / df_d2v_merge.para_num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of paragraphs which are too short to be summarized: 0.00 %\n",
      "\n",
      "Paragraphs which are too long to fit into the model: 0 paragraphs.\n",
      "Empty DataFrame\n",
      "Columns: [bullets, text, para_num_tokens, bullets_num_tokens, compression_ratio]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print_stats_after_merge(df_d2v_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_d2v_merge.to_csv(op+'df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Create train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_d2v_merge = df_d2v_merge.groupby(level=[0, 1], sort=False).agg({\n",
    "    'bullets': lambda b: list(b),\n",
    "    'text': lambda t: list(t),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 408\n"
     ]
    }
   ],
   "source": [
    "df_d2v_merge = df_d2v_merge.sample(frac=1, random_state=config.SEED)\n",
    "df_d2v_merge['num_bulls'] = df_d2v_merge.bullets.map(len).cumsum()\n",
    "tot_bulls = df_d2v_merge.num_bulls.iloc[-1]\n",
    "split1 = np.where(df_d2v_merge.num_bulls > int(tot_bulls*0.8))[0][0]+1\n",
    "split2 = np.where(df_d2v_merge.num_bulls > int(tot_bulls*0.9))[0][0]+1\n",
    "print(split1, split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train, val, test =\\\n",
    "    df_d2v_merge.iloc[:split1].explode('bullets'),\\\n",
    "    df_d2v_merge.iloc[split1:split2].explode('bullets'),\\\n",
    "    df_d2v_merge.iloc[split2:].explode('bullets')\n",
    "\n",
    "train['text'] = df_d2v_merge.iloc[:split1].explode('text')['text']\n",
    "val['text'] = df_d2v_merge.iloc[split1:split2].explode('text')['text']\n",
    "test['text'] = df_d2v_merge.iloc[split2:].explode('text')['text']\n",
    "\n",
    "train.to_csv(op+'train.csv')\n",
    "val.to_csv(op+'val.csv')\n",
    "test.to_csv(op+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(op+'train.source', 'w') as tr_s,\\\n",
    "    open(op+'train.target', 'w') as tr_t,\\\n",
    "    open(op+'train.index', 'w') as tr_i:\n",
    "    for idx, row in train[['text', 'bullets']].iterrows():\n",
    "        tr_i.write(str(idx) + '\\n')\n",
    "        tr_s.write(row.text + '\\n')\n",
    "        tr_t.write(row.bullets + '\\n')\n",
    "        \n",
    "with open(op+'val.source', 'w') as va_s,\\\n",
    "    open(op+'val.target', 'w') as va_t,\\\n",
    "    open(op+'val.index', 'w') as va_i:\n",
    "    for idx, row in val[['text', 'bullets']].iterrows():\n",
    "        va_i.write(str(idx) + '\\n')\n",
    "        va_s.write(row.text + '\\n')\n",
    "        va_t.write(row.bullets + '\\n')\n",
    "        \n",
    "with open(op+'test.source', 'w') as te_s,\\\n",
    "    open(op+'test.target', 'w') as te_t,\\\n",
    "    open(op+'test.index', 'w') as te_i:\n",
    "    for idx, row in test[['text', 'bullets']].iterrows():\n",
    "        te_i.write(str(idx) + '\\n')\n",
    "        te_s.write(row.text + '\\n')\n",
    "        te_t.write(row.bullets + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sentence-Transformers Book Level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = OUTPUT_PATH + 'st/'\n",
    "if not os.path.exists(op):\n",
    "    os.makedirs(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_st = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create embedding vectors for para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18773/18773 [20:25<00:00, 15.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# might want to try 'msmarco-distilbert-base-v2' too\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "df_st['para_enc'] = df_st.para.progress_map(model.encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explode bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st = df_st.explode('bullets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114277/114277 [00:48<00:00, 2332.50it/s]\n",
      "100%|██████████| 114277/114277 [00:42<00:00, 2687.26it/s]\n"
     ]
    }
   ],
   "source": [
    "df_st['para_num_tokens'] = df_st.para.progress_map(lambda p: len(tokenizer.tokenize(p)))\n",
    "df_st['bullets_num_tokens'] = df_st.bullets.progress_map(lambda b: len(tokenizer.tokenize(b)))\n",
    "\n",
    "df_st['compression_ratio'] = df_st.bullets_num_tokens / df_st.para_num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create embedding vectors for bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2556/2556 [01:50<00:00, 23.21it/s]\n"
     ]
    }
   ],
   "source": [
    "bull_to_embed = df_st.groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'bullets': lambda b: list(set(b))\n",
    "}).explode('bullets')\n",
    "\n",
    "bull_to_embed['bullets_enc'] = bull_to_embed.bullets.progress_map(model.encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Calculate similarity between each couple bullet-para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114277/114277 [01:36<00:00, 1180.34it/s]\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "def sentence_transformers_sim(r):\n",
    "    book = r.name[0]\n",
    "    b2e = bull_to_embed.loc[book]\n",
    "    para_enc = r.para_enc\n",
    "    bullets_enc = b2e.loc[(b2e.bullets == r.bullets), 'bullets_enc']\n",
    "    assert len(bullets_enc) == 1\n",
    "    bullets_enc = bullets_enc[0]\n",
    "    \n",
    "    return cosine_sim(para_enc, bullets_enc)\n",
    "    \n",
    "df_st['st_sim'] = df_st.progress_apply(sentence_transformers_sim, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Best Match and Expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2556/2556 [00:01<00:00, 1609.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of paragraphs which are too short to be summarized: 63.50 %\n"
     ]
    }
   ],
   "source": [
    "# find best match bullet-para for each bullet\n",
    "df_st = assign_best_metric_para(df_st, 'st_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2556 [00:00<?, ?it/s]/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/ipykernel_launcher.py:18: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "100%|██████████| 2556/2556 [01:53<00:00, 22.59it/s]\n"
     ]
    }
   ],
   "source": [
    "df_st_base = expand_up_down(df_st.copy(), 'st_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2556 [00:00<?, ?it/s]/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/ipykernel_launcher.py:17: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "100%|██████████| 2556/2556 [04:58<00:00,  8.56it/s]\n"
     ]
    }
   ],
   "source": [
    "df_st_th = expand_up_down_numtok_th(df_st.copy(), 'st_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4558 out of 18822 paragraphs are considered using this method. Thus, 24.22 %\n",
      "\n",
      "353137 out of 1229678 tokens are considered using this method. Thus, 28.72 %\n",
      "\n",
      "11315 out of 18822 paragraphs are considered using this method. Thus, 60.12 %\n",
      "\n",
      "804751 out of 1229678 tokens are considered using this method. Thus, 65.44 %\n"
     ]
    }
   ],
   "source": [
    "print_stats(df_st_base)\n",
    "print()\n",
    "print_stats(df_st_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study Overlaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overlap_matrix(r):\n",
    "    num_bulls = len(r.selected_para)\n",
    "    #assert num_bulls == len(r.bullets)\n",
    "    overlap_matrix = np.zeros((num_bulls,num_bulls))\n",
    "    \n",
    "    def list_overlap(a, b):\n",
    "        return list( set(a).intersection(set(b)) )\n",
    "    \n",
    "    for i in range(num_bulls):\n",
    "        for j in range(num_bulls):\n",
    "            if i == j : continue\n",
    "            num_tok_i = np.sum(r.para_num_tokens[r.selected_para[i]])\n",
    "            overlap = list_overlap(\n",
    "                r.selected_para[i], r.selected_para[j])\n",
    "            num_tok_overlap = np.sum(r.para_num_tokens[overlap])\n",
    "            assert num_tok_overlap <= num_tok_i\n",
    "            \n",
    "            overlap_matrix[i, j] = round(num_tok_overlap/num_tok_i*100, 2)\n",
    "    \n",
    "    return overlap_matrix\n",
    "\n",
    "def find_big_overlap(r, threshold):\n",
    "    om = r.overlap_matrix\n",
    "    big_overlap_idx = np.argwhere(om >= threshold)\n",
    "    big_overlap_idx = set([frozenset(t) for t in big_overlap_idx])\n",
    "    merged = set()\n",
    "    to_be_merged = set()\n",
    "    for idx in big_overlap_idx:\n",
    "        idx = tuple(idx)\n",
    "        i, j = idx[0], idx[1]\n",
    "        if i not in merged and j not in merged:\n",
    "            to_be_merged.add(idx)\n",
    "            merged.add(i)\n",
    "            merged.add(j)\n",
    "    return to_be_merged\n",
    "\n",
    "def merge_bullets(r):\n",
    "    for i, j in r.to_be_merged:\n",
    "        r.bullets[i] += (' '+r.bullets[j])\n",
    "        r.bullets_num_tokens[i] += r.bullets_num_tokens[j]\n",
    "        r.selected_para[i] = np.array(list(set(\n",
    "            np.concatenate((r.selected_para[i], r.selected_para[j])))))\n",
    "        \n",
    "        r.bullets[j] = None\n",
    "        r.bullets_num_tokens[j] = None\n",
    "        r.selected_para[j] = None\n",
    "    r.bullets = [b for b in r.bullets if b is not None]\n",
    "    r.bullets_num_tokens = [bnt for bnt in r.bullets_num_tokens if bnt is not None]\n",
    "    r.selected_para = [sp for sp in r.selected_para if sp is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_st_base_overlap = df_st_base.groupby(['book', 'chapter', 'bullets'], sort=False).agg({\n",
    "    'para': lambda p: list(p),\n",
    "    'para_num_tokens': lambda pnt: list(pnt),\n",
    "    'bullets_num_tokens': lambda bnt: list(bnt)[0],\n",
    "    'best_match': lambda bm: list(bm)\n",
    "}).reset_index('bullets')\n",
    "df_st_base_overlap.best_match = df_st_base_overlap.best_match.map(lambda bm: np.where(bm)[0])\n",
    "df_st_base_overlap = df_st_base_overlap.groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'bullets': lambda b: list(b),\n",
    "    'para': lambda p: list(p)[0],\n",
    "    'para_num_tokens': lambda pnt: list(pnt)[0],\n",
    "    'bullets_num_tokens': lambda bnt: list(bnt),\n",
    "    'best_match': lambda bm: list(bm)\n",
    "}).rename(columns={'best_match': 'selected_para'})\n",
    "df_st_base_overlap.para_num_tokens = df_st_base_overlap.para_num_tokens.map(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of bullets: 2556\n",
      "Bullets to be merged: 412\n",
      "\n",
      "Total number of bullets: 2144\n",
      "Bullets to be merged: 64\n",
      "\n",
      "Total number of bullets: 2080\n",
      "Bullets to be merged: 9\n",
      "\n",
      "Total number of bullets: 2071\n",
      "Bullets to be merged: 2\n",
      "\n",
      "Total number of bullets: 2069\n",
      "Bullets to be merged: 0\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print('\\nTotal number of bullets: %d'%(df_st_base_overlap.bullets.map(len).sum()))\n",
    "    df_st_base_overlap['overlap_matrix'] = df_st_base_overlap.apply(create_overlap_matrix, axis=1)\n",
    "\n",
    "    df_st_base_overlap['to_be_merged'] = df_st_base_overlap.apply(lambda row: find_big_overlap(row, 90), axis=1)\n",
    "\n",
    "    num_to_be_merged = df_st_base_overlap.to_be_merged.map(len).sum()\n",
    "    print('Bullets to be merged: %d'%num_to_be_merged)\n",
    "    if (num_to_be_merged <= 0) : break\n",
    "\n",
    "    df_st_base_overlap.apply(merge_bullets, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Num Tok Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_st_th_overlap = df_st_th.groupby(['book', 'chapter', 'bullets'], sort=False).agg({\n",
    "    'para': lambda p: list(p),\n",
    "    'para_num_tokens': lambda pnt: list(pnt),\n",
    "    'bullets_num_tokens': lambda bnt: list(bnt)[0],\n",
    "    'best_match': lambda bm: list(bm)\n",
    "}).reset_index('bullets')\n",
    "df_st_th_overlap.best_match = df_st_th_overlap.best_match.map(lambda bm: np.where(bm)[0])\n",
    "df_st_th_overlap = df_st_th_overlap.groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'bullets': lambda b: list(b),\n",
    "    'para': lambda p: list(p)[0],\n",
    "    'para_num_tokens': lambda pnt: list(pnt)[0],\n",
    "    'bullets_num_tokens': lambda bnt: list(bnt),\n",
    "    'best_match': lambda bm: list(bm)\n",
    "}).rename(columns={'best_match': 'selected_para'})\n",
    "df_st_th_overlap.para_num_tokens = df_st_th_overlap.para_num_tokens.map(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of bullets: 2556\n",
      "Bullets to be merged: 532\n",
      "\n",
      "Total number of bullets: 2024\n",
      "Bullets to be merged: 118\n",
      "\n",
      "Total number of bullets: 1906\n",
      "Bullets to be merged: 3\n",
      "\n",
      "Total number of bullets: 1903\n",
      "Bullets to be merged: 0\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print('\\nTotal number of bullets: %d'%(df_st_th_overlap.bullets.map(len).sum()))\n",
    "    df_st_th_overlap['overlap_matrix'] = df_st_th_overlap.apply(lambda row: create_overlap_matrix(row), axis=1)\n",
    "    df_st_th_overlap.overlap_matrix.map(lambda om: np.sum(om > 90)).describe()\n",
    "\n",
    "    df_st_th_overlap['to_be_merged'] = df_st_th_overlap.apply(lambda row: find_big_overlap(row, 90), axis=1)\n",
    "\n",
    "    num_to_be_merged = df_st_th_overlap.to_be_merged.map(len).sum()\n",
    "    print('Bullets to be merged: %d'%num_to_be_merged)\n",
    "    if (num_to_be_merged <= 0) : break\n",
    "\n",
    "    df_st_th_overlap.apply(merge_bullets, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Things Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st_base_merge =\\\n",
    "    df_st_base[df_st_base['best_match']].reset_index().groupby(['book', 'chapter', 'bullets'], sort=False)\\\n",
    "    .agg({\n",
    "        'para': lambda p: ' '.join(list(p)),\n",
    "        'para_num_tokens': sum,\n",
    "        'bullets_num_tokens': lambda bnt: list(bnt)[0]\n",
    "    }).reset_index(level='bullets')\n",
    "df_st_base_merge = df_st_base_merge.rename(columns={'para': 'text'})\n",
    "\n",
    "df_st_base_merge['compression_ratio'] = df_st_base_merge.bullets_num_tokens / df_st_base_merge.para_num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2556.000000\n",
       "mean       28.862285\n",
       "std        16.298578\n",
       "min         2.000000\n",
       "25%        18.000000\n",
       "50%        25.000000\n",
       "75%        36.000000\n",
       "max       223.000000\n",
       "Name: bullets_num_tokens, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st_base_merge.bullets_num_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2556.000000\n",
       "mean      170.105634\n",
       "std        79.163032\n",
       "min         8.000000\n",
       "25%       115.000000\n",
       "50%       156.000000\n",
       "75%       212.000000\n",
       "max       988.000000\n",
       "Name: para_num_tokens, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st_base_merge.para_num_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of paragraphs which are too short to be summarized: 0.00 %\n",
      "\n",
      "Paragraphs which are too long to fit into the model: 0 paragraphs.\n",
      "Empty DataFrame\n",
      "Columns: [bullets, text, para_num_tokens, bullets_num_tokens, compression_ratio]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print_stats_after_merge(df_st_base_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Num Tok Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st_th_merge =\\\n",
    "    df_st_th[df_st_th['best_match']].reset_index().groupby(['book', 'chapter', 'bullets'], sort=False)\\\n",
    "    .agg({\n",
    "        'para': lambda p: ' '.join(list(p)),\n",
    "        'para_num_tokens': sum,\n",
    "        'bullets_num_tokens': lambda bnt: list(bnt)[0]\n",
    "    }).reset_index(level='bullets')\n",
    "df_st_th_merge = df_st_th_merge.rename(columns={'para': 'text'})\n",
    "\n",
    "df_st_th_merge['compression_ratio'] =\\\n",
    "    df_st_th_merge.bullets_num_tokens / df_st_th_merge.para_num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2556.000000\n",
       "mean       28.862285\n",
       "std        16.298578\n",
       "min         2.000000\n",
       "25%        18.000000\n",
       "50%        25.000000\n",
       "75%        36.000000\n",
       "max       223.000000\n",
       "Name: bullets_num_tokens, dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st_th_merge.bullets_num_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2556.000000\n",
       "mean      513.486698\n",
       "std       223.103295\n",
       "min        94.000000\n",
       "25%       339.000000\n",
       "50%       466.000000\n",
       "75%       662.000000\n",
       "max      1134.000000\n",
       "Name: para_num_tokens, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st_th_merge.para_num_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of paragraphs which are too short to be summarized: 0.86 %\n",
      "\n",
      "Paragraphs which are too long to fit into the model: 31 paragraphs.\n",
      "                                                                  bullets  \\\n",
      "book          chapter                                                       \n",
      "9781908541178 ch_6      Piriformis tenderness is an area of local tend...   \n",
      "              ch_6      Facet arthritis and trochanteric bursal irrita...   \n",
      "              ch_6      Radiographic spinal stenosis, facet arthrosis ...   \n",
      "              ch_8      Surgery is usually effective for refractory sc...   \n",
      "9781908541406 ch_6      Thrombocytopenia may occur with some inherited...   \n",
      "              ch_6      In immune thrombocytopenia, bleeding is uncomm...   \n",
      "              ch_6      Platelet transfusion has limited indications a...   \n",
      "9781908541420 ch_6      Drugs used in the management of asthma can be ...   \n",
      "9781908541703 ch_5      Triple assessment - clinical examination plus ...   \n",
      "              ch_7      Trial evidence supports a further 5 years of h...   \n",
      "              ch_7      Taxane-based chemotherapy is recommended for w...   \n",
      "9781908541796 chapter6  Other low-grade lymphomas are hard to diagnose...   \n",
      "9781908541901 ch_6      Guidelines uniformly recommend starting all UA...   \n",
      "9781910797006 ch07      Bronchodilators are the first-line treatment i...   \n",
      "              ch07      Inhaled glucocorticosteroids can improve airfl...   \n",
      "9781910797150 ch03      Corticosteroids (dexamethasone in particular) ...   \n",
      "9781910797211 ch04      The treatment of non-motor symptoms is importa...   \n",
      "9781910797426 ch02      Imaging tests with varied diagnostic performan...   \n",
      "9781910797495 chp3      Research evidence to support the effectiveness...   \n",
      "              chp3      Opioids play a major role in cancer pain but d...   \n",
      "9781910797631 chp3      Depression is related to a range of biological...   \n",
      "              chp3      At least half of all people affected by depres...   \n",
      "              chp6      Selective serotonin-reuptake inhibitor antidep...   \n",
      "              chp8      Suicide is strongly associated with depression...   \n",
      "              chp8      In addition to previous self-harm, risk factor...   \n",
      "              chp8      Assessment practice following self-harm needs ...   \n",
      "9781912776153 chp12     Pacemakers are the treatment of choice in pati...   \n",
      "              chp12     Patients with pacemakers can live a near-norma...   \n",
      "              chp12     Patients need regular review (usually annually...   \n",
      "9781912776238 ch10      Lack of a uniform approach to the design of cl...   \n",
      "              ch10      Extension of patent protection, as occurs when...   \n",
      "\n",
      "                                                                     text  \\\n",
      "book          chapter                                                       \n",
      "9781908541178 ch_6      Many anatomic factors can play a primary or a ...   \n",
      "              ch_6      Facet (zygapophyseal) joints (Figure 3.5) are ...   \n",
      "              ch_6      The sensory neural components of the nerve roo...   \n",
      "              ch_8      Sacroiliac joint injection. Despite the ambigu...   \n",
      "9781908541406 ch_6      Inherited disorders. Although uncommon, congen...   \n",
      "              ch_6      Thrombotic microangiopathies include thrombocy...   \n",
      "              ch_6      Thrombocytopenia associated with hepatitis C v...   \n",
      "9781908541420 ch_6      Concern has been expressed over the long-term ...   \n",
      "9781908541703 ch_5      MRI may aid the diagnosis and management of br...   \n",
      "              ch_7      The Breast International Group (BIG) 1-98 tria...   \n",
      "              ch_7      Selection. There is little doubt that postmeno...   \n",
      "9781908541796 chapter6  Interestingly in normal cells, both MALT1 and ...   \n",
      "9781908541901 ch_6      Apixaban is an oral, selective, reversible and...   \n",
      "9781910797006 ch07      The treatment strategy for stable COPD recomme...   \n",
      "              ch07      Glucocorticosteroids. Oral corticosteroids sho...   \n",
      "9781910797150 ch03      Efficacy in HEC. A randomized, double-blind, p...   \n",
      "9781910797211 ch04      reduce the likelihood of side effects. Theoret...   \n",
      "9781910797426 ch02      The physical examination is complex and depend...   \n",
      "9781910797495 chp3      Many patients do well with minimal resources; ...   \n",
      "              chp3      For chronic cancer pain. Opioids remain the fo...   \n",
      "9781910797631 chp3      Epidemiology and impact. The origins of depres...   \n",
      "              chp3      Relapse, recovery and recurrence. At least hal...   \n",
      "              chp6      Are antidepressants effective? Some recent rev...   \n",
      "              chp8      Self-harm and suicide. Suicide is a major publ...   \n",
      "              chp8      Self-harm and suicide. Suicide is a major publ...   \n",
      "              chp8      People who die by suicide represent only a fra...   \n",
      "9781912776153 chp12     Cardiac devices: pacemakers and defibrillators...   \n",
      "              chp12     Single- and dual-chamber devices. Early pacema...   \n",
      "              chp12     Demand-pacing. Very early pacemakers were life...   \n",
      "9781912776238 ch10      While these trial designs can provide the phar...   \n",
      "              ch10      While these trial designs can provide the phar...   \n",
      "\n",
      "                        para_num_tokens  bullets_num_tokens  compression_ratio  \n",
      "book          chapter                                                           \n",
      "9781908541178 ch_6                 1051                  53           0.050428  \n",
      "              ch_6                 1034                  22           0.021277  \n",
      "              ch_6                 1108                  25           0.022563  \n",
      "              ch_8                 1083                  36           0.033241  \n",
      "9781908541406 ch_6                 1074                  33           0.030726  \n",
      "              ch_6                 1029                  42           0.040816  \n",
      "              ch_6                 1039                  25           0.024062  \n",
      "9781908541420 ch_6                 1080                  54           0.050000  \n",
      "9781908541703 ch_5                 1032                  21           0.020349  \n",
      "              ch_7                 1053                  39           0.037037  \n",
      "              ch_7                 1046                  22           0.021033  \n",
      "9781908541796 chapter6             1086                  34           0.031308  \n",
      "9781908541901 ch_6                 1029                  26           0.025267  \n",
      "9781910797006 ch07                 1041                  31           0.029779  \n",
      "              ch07                 1036                  23           0.022201  \n",
      "9781910797150 ch03                 1045                  43           0.041148  \n",
      "9781910797211 ch04                 1029                  36           0.034985  \n",
      "9781910797426 ch02                 1106                  32           0.028933  \n",
      "9781910797495 chp3                 1066                  39           0.036585  \n",
      "              chp3                 1063                  27           0.025400  \n",
      "9781910797631 chp3                 1038                  37           0.035645  \n",
      "              chp3                 1026                  27           0.026316  \n",
      "              chp6                 1029                  31           0.030126  \n",
      "              chp8                 1042                  44           0.042226  \n",
      "              chp8                 1042                  37           0.035509  \n",
      "              chp8                 1045                  49           0.046890  \n",
      "9781912776153 chp12                1051                  16           0.015224  \n",
      "              chp12                1075                  12           0.011163  \n",
      "              chp12                1045                  16           0.015311  \n",
      "9781912776238 ch10                 1134                  39           0.034392  \n",
      "              ch10                 1134                  28           0.024691  \n"
     ]
    }
   ],
   "source": [
    "print_stats_after_merge(df_st_th_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base No Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_st_merge_no_overlap = \n",
    "df_st_base_overlap['bp'] = df_st_base_overlap.apply(lambda r:\\\n",
    "    [(b, sp, bnt) for b, sp, bnt in zip(r.bullets, r.selected_para, r.bullets_num_tokens)], axis=1)\n",
    "df_st_base_over_merge = df_st_base_overlap.explode('bp')\n",
    "df_st_base_over_merge.bullets = df_st_base_over_merge.bp.map(lambda t: t[0])\n",
    "df_st_base_over_merge.selected_para = df_st_base_over_merge.bp.map(lambda t: t[1])\n",
    "df_st_base_over_merge.bullets_num_tokens = df_st_base_over_merge.bp.map(lambda t: t[2])\n",
    "\n",
    "df_st_base_over_merge['para'] = df_st_base_over_merge.apply(lambda row:\\\n",
    "    ' '.join([p for i, p in enumerate(row.para) if i in row.selected_para]), axis=1)\n",
    "\n",
    "df_st_base_over_merge['para_num_tokens'] = df_st_base_over_merge.apply(lambda row:\\\n",
    "    sum([p for i, p in enumerate(row.para_num_tokens) if i in row.selected_para]), axis=1)\n",
    "\n",
    "df_st_base_over_merge = df_st_base_over_merge.drop(\n",
    "    columns=['overlap_matrix', 'to_be_merged', 'bp', 'selected_para']).rename(columns={'para': 'text'})\n",
    "\n",
    "df_st_base_over_merge['compression_ratio'] =\\\n",
    "    df_st_base_over_merge.bullets_num_tokens / df_st_base_over_merge.para_num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2069.000000\n",
       "mean       35.655872\n",
       "std        26.291229\n",
       "min         2.000000\n",
       "25%        20.000000\n",
       "50%        29.000000\n",
       "75%        43.000000\n",
       "max       361.000000\n",
       "Name: bullets_num_tokens, dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st_base_over_merge.bullets_num_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2069.000000\n",
       "mean      176.131948\n",
       "std        81.374665\n",
       "min         8.000000\n",
       "25%       120.000000\n",
       "50%       163.000000\n",
       "75%       220.000000\n",
       "max       988.000000\n",
       "Name: para_num_tokens, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st_base_over_merge.para_num_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bullets</th>\n",
       "      <th>text</th>\n",
       "      <th>para_num_tokens</th>\n",
       "      <th>bullets_num_tokens</th>\n",
       "      <th>compression_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book</th>\n",
       "      <th>chapter</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">9781905832729</th>\n",
       "      <th>ch_2</th>\n",
       "      <td>The bladder operates as a low-pressure high-vo...</td>\n",
       "      <td>Continence is maintained by a complex interact...</td>\n",
       "      <td>184</td>\n",
       "      <td>56</td>\n",
       "      <td>0.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch_4</th>\n",
       "      <td>The surgical procedure of choice in women is i...</td>\n",
       "      <td>insertion of an artificial urinary sphincter. ...</td>\n",
       "      <td>164</td>\n",
       "      <td>61</td>\n",
       "      <td>0.371951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch_7</th>\n",
       "      <td>Blood in the urine can originate from anywhere...</td>\n",
       "      <td>Hematuria can originate from anywhere along th...</td>\n",
       "      <td>96</td>\n",
       "      <td>35</td>\n",
       "      <td>0.364583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch_8</th>\n",
       "      <td>Recurrent urinary tract infection (UTI) is def...</td>\n",
       "      <td>Recurrent urinary tract infection. Recurrent U...</td>\n",
       "      <td>174</td>\n",
       "      <td>63</td>\n",
       "      <td>0.362069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch_8</th>\n",
       "      <td>Urinalysis is quick and easy. A positive test ...</td>\n",
       "      <td>Diagnosis. Urinalysis, using dipstick tests fo...</td>\n",
       "      <td>132</td>\n",
       "      <td>44</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9783318067095</th>\n",
       "      <th>ch9</th>\n",
       "      <td>Verification evaluates the capture and transfe...</td>\n",
       "      <td>Once you have arrived at a construct to measur...</td>\n",
       "      <td>801</td>\n",
       "      <td>282</td>\n",
       "      <td>0.352060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">9783318068207</th>\n",
       "      <th>hh-6</th>\n",
       "      <td>A basket trial is a biomarker-driven study in ...</td>\n",
       "      <td>Traditionally, oncology Phase I clinical trial...</td>\n",
       "      <td>161</td>\n",
       "      <td>56</td>\n",
       "      <td>0.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hh-7</th>\n",
       "      <td>The molecular status of, at least, EGFR, ALK, ...</td>\n",
       "      <td>NGS testing for predictive biomarkers. EGFR ac...</td>\n",
       "      <td>229</td>\n",
       "      <td>92</td>\n",
       "      <td>0.401747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hh-7</th>\n",
       "      <td>PD-L1 expression also needs to be tested in pa...</td>\n",
       "      <td>Programmed death-ligand 1. In addition to the ...</td>\n",
       "      <td>96</td>\n",
       "      <td>38</td>\n",
       "      <td>0.395833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hh-7</th>\n",
       "      <td>The combination of BRAF and MEK inhibitors has...</td>\n",
       "      <td>c-KIT inhibitors. Early data suggest that muco...</td>\n",
       "      <td>119</td>\n",
       "      <td>48</td>\n",
       "      <td>0.403361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>295 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 bullets  \\\n",
       "book          chapter                                                      \n",
       "9781905832729 ch_2     The bladder operates as a low-pressure high-vo...   \n",
       "              ch_4     The surgical procedure of choice in women is i...   \n",
       "              ch_7     Blood in the urine can originate from anywhere...   \n",
       "              ch_8     Recurrent urinary tract infection (UTI) is def...   \n",
       "              ch_8     Urinalysis is quick and easy. A positive test ...   \n",
       "...                                                                  ...   \n",
       "9783318067095 ch9      Verification evaluates the capture and transfe...   \n",
       "9783318068207 hh-6     A basket trial is a biomarker-driven study in ...   \n",
       "              hh-7     The molecular status of, at least, EGFR, ALK, ...   \n",
       "              hh-7     PD-L1 expression also needs to be tested in pa...   \n",
       "              hh-7     The combination of BRAF and MEK inhibitors has...   \n",
       "\n",
       "                                                                    text  \\\n",
       "book          chapter                                                      \n",
       "9781905832729 ch_2     Continence is maintained by a complex interact...   \n",
       "              ch_4     insertion of an artificial urinary sphincter. ...   \n",
       "              ch_7     Hematuria can originate from anywhere along th...   \n",
       "              ch_8     Recurrent urinary tract infection. Recurrent U...   \n",
       "              ch_8     Diagnosis. Urinalysis, using dipstick tests fo...   \n",
       "...                                                                  ...   \n",
       "9783318067095 ch9      Once you have arrived at a construct to measur...   \n",
       "9783318068207 hh-6     Traditionally, oncology Phase I clinical trial...   \n",
       "              hh-7     NGS testing for predictive biomarkers. EGFR ac...   \n",
       "              hh-7     Programmed death-ligand 1. In addition to the ...   \n",
       "              hh-7     c-KIT inhibitors. Early data suggest that muco...   \n",
       "\n",
       "                       para_num_tokens  bullets_num_tokens  compression_ratio  \n",
       "book          chapter                                                          \n",
       "9781905832729 ch_2                 184                  56           0.304348  \n",
       "              ch_4                 164                  61           0.371951  \n",
       "              ch_7                  96                  35           0.364583  \n",
       "              ch_8                 174                  63           0.362069  \n",
       "              ch_8                 132                  44           0.333333  \n",
       "...                                ...                 ...                ...  \n",
       "9783318067095 ch9                  801                 282           0.352060  \n",
       "9783318068207 hh-6                 161                  56           0.347826  \n",
       "              hh-7                 229                  92           0.401747  \n",
       "              hh-7                  96                  38           0.395833  \n",
       "              hh-7                 119                  48           0.403361  \n",
       "\n",
       "[295 rows x 5 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st_base_over_merge[df_st_base_over_merge['compression_ratio'] > config.MAX_RATIO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of paragraphs which are too short to be summarized: 14.26 %\n",
      "\n",
      "Paragraphs which are too long to fit into the model: 0 paragraphs.\n",
      "Empty DataFrame\n",
      "Columns: [bullets, text, para_num_tokens, bullets_num_tokens, compression_ratio]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print_stats_after_merge(df_st_base_over_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Num Tok Threshold No Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_st_merge_no_overlap = \n",
    "df_st_th_overlap['bp'] = df_st_th_overlap.apply(lambda r:\\\n",
    "    [(b, sp, bnt) for b, sp, bnt in zip(r.bullets, r.selected_para, r.bullets_num_tokens)], axis=1)\n",
    "df_st_th_over_merge = df_st_th_overlap.explode('bp')\n",
    "df_st_th_over_merge.bullets = df_st_th_over_merge.bp.map(lambda t: t[0])\n",
    "df_st_th_over_merge.selected_para = df_st_th_over_merge.bp.map(lambda t: t[1])\n",
    "df_st_th_over_merge.bullets_num_tokens = df_st_th_over_merge.bp.map(lambda t: t[2])\n",
    "\n",
    "df_st_th_over_merge['para'] = df_st_th_over_merge.apply(lambda row:\\\n",
    "    ' '.join([p for i, p in enumerate(row.para) if i in row.selected_para]), axis=1)\n",
    "\n",
    "df_st_th_over_merge['para_num_tokens'] = df_st_th_over_merge.apply(lambda row:\\\n",
    "    sum([p for i, p in enumerate(row.para_num_tokens) if i in row.selected_para]), axis=1)\n",
    "\n",
    "df_st_th_over_merge = df_st_th_over_merge.drop(\n",
    "    columns=['overlap_matrix', 'to_be_merged', 'bp', 'selected_para']).rename(columns={'para': 'text'})\n",
    "\n",
    "df_st_th_over_merge['compression_ratio'] =\\\n",
    "    df_st_th_over_merge.bullets_num_tokens / df_st_th_over_merge.para_num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1903.000000\n",
       "mean       38.766159\n",
       "std        26.499201\n",
       "min         3.000000\n",
       "25%        21.000000\n",
       "50%        31.000000\n",
       "75%        48.000000\n",
       "max       246.000000\n",
       "Name: bullets_num_tokens, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st_th_over_merge.bullets_num_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1903.000000\n",
       "mean      522.982659\n",
       "std       222.127554\n",
       "min       106.000000\n",
       "25%       349.000000\n",
       "50%       473.000000\n",
       "75%       676.000000\n",
       "max      1134.000000\n",
       "Name: para_num_tokens, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st_th_over_merge.para_num_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bullets</th>\n",
       "      <th>text</th>\n",
       "      <th>para_num_tokens</th>\n",
       "      <th>bullets_num_tokens</th>\n",
       "      <th>compression_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book</th>\n",
       "      <th>chapter</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9781905832729</th>\n",
       "      <th>ch_9</th>\n",
       "      <td>Treatment of nocturia is largely on the basis ...</td>\n",
       "      <td>Treatment of nocturia should be on the basis o...</td>\n",
       "      <td>290</td>\n",
       "      <td>74</td>\n",
       "      <td>0.255172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">9781908541024</th>\n",
       "      <th>ch_3</th>\n",
       "      <td>Initial symptoms and signs of a brain tumor ar...</td>\n",
       "      <td>Patients with brain tumors typically develop n...</td>\n",
       "      <td>290</td>\n",
       "      <td>73</td>\n",
       "      <td>0.251724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch_6</th>\n",
       "      <td>Concurrent and adjuvant temozolomide chemother...</td>\n",
       "      <td>Management. Anaplastic oligodendrogliomas have...</td>\n",
       "      <td>532</td>\n",
       "      <td>184</td>\n",
       "      <td>0.345865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">9781908541277</th>\n",
       "      <th>ch_4</th>\n",
       "      <td>Epilepsy is the most common serious neurologic...</td>\n",
       "      <td>Epilepsy is the most common serious neurologic...</td>\n",
       "      <td>168</td>\n",
       "      <td>45</td>\n",
       "      <td>0.267857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch_11</th>\n",
       "      <td>Oral contraceptives containing at least 50 µg ...</td>\n",
       "      <td>Contraception. Carbamazepine (CBZ), eslicarbaz...</td>\n",
       "      <td>335</td>\n",
       "      <td>84</td>\n",
       "      <td>0.250746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">9783318066241</th>\n",
       "      <th>ch5</th>\n",
       "      <td>Early liquid peripancreatic collections withou...</td>\n",
       "      <td>An APFC is defined as peripancreatic fluid (a ...</td>\n",
       "      <td>280</td>\n",
       "      <td>84</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch5</th>\n",
       "      <td>In the revised Atlanta classification, acute p...</td>\n",
       "      <td>According to the number of organs involved: th...</td>\n",
       "      <td>259</td>\n",
       "      <td>85</td>\n",
       "      <td>0.328185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">9783318067095</th>\n",
       "      <th>ch6</th>\n",
       "      <td>As advances in technology enable digital tools...</td>\n",
       "      <td>For example, there is a lot of excitement in t...</td>\n",
       "      <td>587</td>\n",
       "      <td>152</td>\n",
       "      <td>0.258944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch7</th>\n",
       "      <td>'Effective, unambiguous communication is essen...</td>\n",
       "      <td>Digital biomarkers and clinical outcomes. Are ...</td>\n",
       "      <td>415</td>\n",
       "      <td>152</td>\n",
       "      <td>0.366265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch9</th>\n",
       "      <td>Validation ensures that the technology is meas...</td>\n",
       "      <td>Verification is the assessment of sensor accur...</td>\n",
       "      <td>368</td>\n",
       "      <td>208</td>\n",
       "      <td>0.565217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 bullets  \\\n",
       "book          chapter                                                      \n",
       "9781905832729 ch_9     Treatment of nocturia is largely on the basis ...   \n",
       "9781908541024 ch_3     Initial symptoms and signs of a brain tumor ar...   \n",
       "              ch_6     Concurrent and adjuvant temozolomide chemother...   \n",
       "9781908541277 ch_4     Epilepsy is the most common serious neurologic...   \n",
       "              ch_11    Oral contraceptives containing at least 50 µg ...   \n",
       "...                                                                  ...   \n",
       "9783318066241 ch5      Early liquid peripancreatic collections withou...   \n",
       "              ch5      In the revised Atlanta classification, acute p...   \n",
       "9783318067095 ch6      As advances in technology enable digital tools...   \n",
       "              ch7      'Effective, unambiguous communication is essen...   \n",
       "              ch9      Validation ensures that the technology is meas...   \n",
       "\n",
       "                                                                    text  \\\n",
       "book          chapter                                                      \n",
       "9781905832729 ch_9     Treatment of nocturia should be on the basis o...   \n",
       "9781908541024 ch_3     Patients with brain tumors typically develop n...   \n",
       "              ch_6     Management. Anaplastic oligodendrogliomas have...   \n",
       "9781908541277 ch_4     Epilepsy is the most common serious neurologic...   \n",
       "              ch_11    Contraception. Carbamazepine (CBZ), eslicarbaz...   \n",
       "...                                                                  ...   \n",
       "9783318066241 ch5      An APFC is defined as peripancreatic fluid (a ...   \n",
       "              ch5      According to the number of organs involved: th...   \n",
       "9783318067095 ch6      For example, there is a lot of excitement in t...   \n",
       "              ch7      Digital biomarkers and clinical outcomes. Are ...   \n",
       "              ch9      Verification is the assessment of sensor accur...   \n",
       "\n",
       "                       para_num_tokens  bullets_num_tokens  compression_ratio  \n",
       "book          chapter                                                          \n",
       "9781905832729 ch_9                 290                  74           0.255172  \n",
       "9781908541024 ch_3                 290                  73           0.251724  \n",
       "              ch_6                 532                 184           0.345865  \n",
       "9781908541277 ch_4                 168                  45           0.267857  \n",
       "              ch_11                335                  84           0.250746  \n",
       "...                                ...                 ...                ...  \n",
       "9783318066241 ch5                  280                  84           0.300000  \n",
       "              ch5                  259                  85           0.328185  \n",
       "9783318067095 ch6                  587                 152           0.258944  \n",
       "              ch7                  415                 152           0.366265  \n",
       "              ch9                  368                 208           0.565217  \n",
       "\n",
       "[84 rows x 5 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st_th_over_merge[df_st_th_over_merge['compression_ratio'] > config.MAX_RATIO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of paragraphs which are too short to be summarized: 4.41 %\n",
      "\n",
      "Paragraphs which are too long to fit into the model: 30 paragraphs.\n",
      "                                                                  bullets  \\\n",
      "book          chapter                                                       \n",
      "9781908541178 ch_6      Radiographic spinal stenosis, facet arthrosis ...   \n",
      "              ch_6      Piriformis tenderness is an area of local tend...   \n",
      "              ch_6      Facet arthritis and trochanteric bursal irrita...   \n",
      "              ch_8      Surgery is usually effective for refractory sc...   \n",
      "9781908541406 ch_6      Thrombocytopenia may occur with some inherited...   \n",
      "              ch_6      In immune thrombocytopenia, bleeding is uncomm...   \n",
      "              ch_6      Platelet transfusion has limited indications a...   \n",
      "9781908541420 ch_6      Drugs used in the management of asthma can be ...   \n",
      "9781908541703 ch_5      Triple assessment - clinical examination plus ...   \n",
      "              ch_7      For women with strongly estrogen-receptor(ER) ...   \n",
      "              ch_7      Trial evidence supports a further 5 years of h...   \n",
      "9781908541796 chapter6  Other low-grade lymphomas are hard to diagnose...   \n",
      "9781908541901 ch_6      Guidelines uniformly recommend starting all UA...   \n",
      "9781910797006 ch07      Bronchodilators are the first-line treatment i...   \n",
      "              ch07      Inhaled glucocorticosteroids can improve airfl...   \n",
      "9781910797150 ch03      Corticosteroids (dexamethasone in particular) ...   \n",
      "9781910797211 ch04      The treatment of non-motor symptoms is importa...   \n",
      "9781910797426 ch02      Imaging tests with varied diagnostic performan...   \n",
      "9781910797495 chp3      The specific disciplines of healthcare provide...   \n",
      "              chp3      Opioids play a major role in cancer pain but d...   \n",
      "9781910797631 chp3      In high- and middle-income countries, depressi...   \n",
      "              chp3      Depression is related to a range of biological...   \n",
      "              chp4      Interventions to improve recognition that do n...   \n",
      "              chp6      Selective serotonin-reuptake inhibitor antidep...   \n",
      "              chp8      Suicide is strongly associated with depression...   \n",
      "              chp8      Self-harm is most common in adolescent girls a...   \n",
      "9781912776153 chp12     Pacemakers are the treatment of choice in pati...   \n",
      "              chp12     Patients with pacemakers can live a near-norma...   \n",
      "              chp12     Patients need regular review (usually annually...   \n",
      "9781912776238 ch10      Lack of a uniform approach to the design of cl...   \n",
      "\n",
      "                                                                     text  \\\n",
      "book          chapter                                                       \n",
      "9781908541178 ch_6      The sensory neural components of the nerve roo...   \n",
      "              ch_6      Many anatomic factors can play a primary or a ...   \n",
      "              ch_6      Facet (zygapophyseal) joints (Figure 3.5) are ...   \n",
      "              ch_8      Sacroiliac joint injection. Despite the ambigu...   \n",
      "9781908541406 ch_6      Inherited disorders. Although uncommon, congen...   \n",
      "              ch_6      Thrombotic microangiopathies include thrombocy...   \n",
      "              ch_6      Thrombocytopenia associated with hepatitis C v...   \n",
      "9781908541420 ch_6      Concern has been expressed over the long-term ...   \n",
      "9781908541703 ch_5      MRI may aid the diagnosis and management of br...   \n",
      "              ch_7      Selection. There is little doubt that postmeno...   \n",
      "              ch_7      The Breast International Group (BIG) 1-98 tria...   \n",
      "9781908541796 chapter6  Interestingly in normal cells, both MALT1 and ...   \n",
      "9781908541901 ch_6      Apixaban is an oral, selective, reversible and...   \n",
      "9781910797006 ch07      The treatment strategy for stable COPD recomme...   \n",
      "              ch07      Glucocorticosteroids. Oral corticosteroids sho...   \n",
      "9781910797150 ch03      Efficacy in HEC. A randomized, double-blind, p...   \n",
      "9781910797211 ch04      reduce the likelihood of side effects. Theoret...   \n",
      "9781910797426 ch02      The physical examination is complex and depend...   \n",
      "9781910797495 chp3      Many patients do well with minimal resources; ...   \n",
      "              chp3      For chronic cancer pain. Opioids remain the fo...   \n",
      "9781910797631 chp3      Relapse, recovery and recurrence. At least hal...   \n",
      "              chp3      Epidemiology and impact. The origins of depres...   \n",
      "              chp4      The limitations of consultation duration are a...   \n",
      "              chp6      Are antidepressants effective? Some recent rev...   \n",
      "              chp8      Self-harm and suicide. Suicide is a major publ...   \n",
      "              chp8      In common with the general population, male se...   \n",
      "9781912776153 chp12     Cardiac devices: pacemakers and defibrillators...   \n",
      "              chp12     Single- and dual-chamber devices. Early pacema...   \n",
      "              chp12     Demand-pacing. Very early pacemakers were life...   \n",
      "9781912776238 ch10      While these trial designs can provide the phar...   \n",
      "\n",
      "                        para_num_tokens  bullets_num_tokens  compression_ratio  \n",
      "book          chapter                                                           \n",
      "9781908541178 ch_6                 1108                  25           0.022563  \n",
      "              ch_6                 1051                  53           0.050428  \n",
      "              ch_6                 1034                  22           0.021277  \n",
      "              ch_8                 1083                  36           0.033241  \n",
      "9781908541406 ch_6                 1074                  71           0.066108  \n",
      "              ch_6                 1029                  42           0.040816  \n",
      "              ch_6                 1039                  25           0.024062  \n",
      "9781908541420 ch_6                 1080                  54           0.050000  \n",
      "9781908541703 ch_5                 1032                  21           0.020349  \n",
      "              ch_7                 1046                  54           0.051625  \n",
      "              ch_7                 1053                  39           0.037037  \n",
      "9781908541796 chapter6             1086                  34           0.031308  \n",
      "9781908541901 ch_6                 1029                  26           0.025267  \n",
      "9781910797006 ch07                 1041                  31           0.029779  \n",
      "              ch07                 1036                  23           0.022201  \n",
      "9781910797150 ch03                 1045                 197           0.188517  \n",
      "9781910797211 ch04                 1029                  36           0.034985  \n",
      "9781910797426 ch02                 1106                  32           0.028933  \n",
      "9781910797495 chp3                 1066                 121           0.113508  \n",
      "              chp3                 1063                  27           0.025400  \n",
      "9781910797631 chp3                 1079                  69           0.063948  \n",
      "              chp3                 1038                  37           0.035645  \n",
      "              chp4                 1088                  66           0.060662  \n",
      "              chp6                 1029                  31           0.030126  \n",
      "              chp8                 1042                  81           0.077735  \n",
      "              chp8                 1111                 106           0.095410  \n",
      "9781912776153 chp12                1051                  16           0.015224  \n",
      "              chp12                1075                  12           0.011163  \n",
      "              chp12                1045                  16           0.015311  \n",
      "9781912776238 ch10                 1134                  67           0.059083  \n"
     ]
    }
   ],
   "source": [
    "print_stats_after_merge(df_st_th_over_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_st_base_selected_para = df_st_base.groupby(['book', 'chapter', 'para'], sort=False).agg({\n",
    "    'best_match': lambda b: np.any(list(b))\n",
    "}).reset_index('para').groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'para': lambda p: list(p),\n",
    "    'best_match': lambda b: list(b)\n",
    "})\n",
    "df_st_base_selected_para.best_match = df_st_base_selected_para.best_match.map(lambda b: list(np.where(b)[0]))\n",
    "df_st_base_selected_para.to_csv(op+'df_base_selected_para.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_st_th_selected_para = df_st_th.groupby(['book', 'chapter', 'para'], sort=False).agg({\n",
    "    'best_match': lambda b: np.any(list(b))\n",
    "}).reset_index('para').groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'para': lambda p: list(p),\n",
    "    'best_match': lambda b: list(b)\n",
    "})\n",
    "df_st_th_selected_para.best_match = df_st_th_selected_para.best_match.map(lambda b: list(np.where(b)[0]))\n",
    "df_st_th_selected_para.to_csv(op+'df_th_selected_para.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 408\n",
      "361 408\n",
      "362 408\n",
      "361 408\n"
     ]
    }
   ],
   "source": [
    "for d, df_merge in zip(\n",
    "    ['base', 'th', 'base_overlap', 'th_overlap'],\n",
    "    [df_st_base_merge, df_st_th_merge, df_st_base_over_merge, df_st_th_over_merge]):\n",
    "\n",
    "    op = OUTPUT_PATH + 'st/'+d+'/'\n",
    "    if not os.path.exists(op):\n",
    "        os.makedirs(op)\n",
    "        \n",
    "    df_merge.to_csv(op+'df.csv')\n",
    "        \n",
    "    df_merge = df_merge.groupby(level=[0, 1], sort=False).agg({\n",
    "        'bullets': lambda b: list(b),\n",
    "        'text': lambda t: list(t),\n",
    "    })\n",
    "    \n",
    "    df_merge = df_merge.sample(frac=1, random_state=config.SEED)\n",
    "    df_merge['num_bulls'] = df_merge.bullets.map(len).cumsum()\n",
    "    tot_bulls = df_merge.num_bulls.iloc[-1]\n",
    "    split1 = np.where(df_merge.num_bulls > int(tot_bulls*0.8))[0][0]+1\n",
    "    split2 = np.where(df_merge.num_bulls > int(tot_bulls*0.9))[0][0]+1\n",
    "    print(split1, split2)\n",
    "    \n",
    "    train, val, test =\\\n",
    "    df_merge.iloc[:split1].explode('bullets'),\\\n",
    "    df_merge.iloc[split1:split2].explode('bullets'),\\\n",
    "    df_merge.iloc[split2:].explode('bullets')\n",
    "\n",
    "    train['text'] = df_merge.iloc[:split1].explode('text')['text']\n",
    "    val['text'] = df_merge.iloc[split1:split2].explode('text')['text']\n",
    "    test['text'] = df_merge.iloc[split2:].explode('text')['text']\n",
    "\n",
    "    train.to_csv(op+'train.csv')\n",
    "    val.to_csv(op+'val.csv')\n",
    "    test.to_csv(op+'test.csv')\n",
    "    \n",
    "    with open(op+'train.source', 'w') as tr_s,\\\n",
    "        open(op+'train.target', 'w') as tr_t,\\\n",
    "        open(op+'train.index', 'w') as tr_i:\n",
    "        for idx, row in train[['text', 'bullets']].iterrows():\n",
    "            tr_i.write(str(idx) + '\\n')\n",
    "            tr_s.write(row.text + '\\n')\n",
    "            tr_t.write(row.bullets + '\\n')\n",
    "        \n",
    "    with open(op+'val.source', 'w') as va_s,\\\n",
    "        open(op+'val.target', 'w') as va_t,\\\n",
    "        open(op+'val.index', 'w') as va_i:\n",
    "        for idx, row in val[['text', 'bullets']].iterrows():\n",
    "            va_i.write(str(idx) + '\\n')\n",
    "            va_s.write(row.text + '\\n')\n",
    "            va_t.write(row.bullets + '\\n')\n",
    "\n",
    "    with open(op+'test.source', 'w') as te_s,\\\n",
    "        open(op+'test.target', 'w') as te_t,\\\n",
    "        open(op+'test.index', 'w') as te_i:\n",
    "        for idx, row in test[['text', 'bullets']].iterrows():\n",
    "            te_i.write(str(idx) + '\\n')\n",
    "            te_s.write(row.text + '\\n')\n",
    "            te_t.write(row.bullets + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO7BPitFH8Th"
   },
   "source": [
    "### **Print Some Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_print(idx, bull, list_text, list_text_num_tok, list_method):\n",
    "    print(idx)\n",
    "    print()\n",
    "    print('Bullet:')\n",
    "    print(fill(bull, 100))\n",
    "    print()\n",
    "    for t, tok, m in zip(list_text, list_text_num_tok, list_method):\n",
    "        print(m+' (' +str(tok)+'):')\n",
    "        print(fill(t, 100))\n",
    "        print()\n",
    "    \n",
    "    print(''.join(['#']*100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### W2V vs D2V vs Sentence-Transformers vs Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1610463902969,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "hidden": true,
    "id": "tvWJmbjVH-qN",
    "outputId": "861d6b76-0978-4aec-bb7a-86609006f34e",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/marco/epfl/magma/datasets/karger_books_para_wordembed/pegasus/st/df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-166eb624505b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'w2v/df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'book'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chapter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_d2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'd2v/df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'book'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chapter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'st/df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'book'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chapter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf_rouge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagma_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'datasets/karger_books_para/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'book'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chapter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/marco/epfl/magma/datasets/karger_books_para_wordembed/pegasus/st/df.csv'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "df_w2v = pd.read_csv(OUTPUT_PATH+'w2v/df.csv').set_index(['book', 'chapter'])\n",
    "df_d2v = pd.read_csv(OUTPUT_PATH+'d2v/df.csv').set_index(['book', 'chapter'])\n",
    "df_st = pd.read_csv(OUTPUT_PATH+'st/df.csv').set_index(['book', 'chapter'])\n",
    "df_rouge = pd.read_csv(magma_dir+'datasets/karger_books_para/'+MODEL+'/df.csv').set_index(['book', 'chapter'])\n",
    "\n",
    "random.seed(config.SEED)\n",
    "\n",
    "bullet_examples = random.sample(df_w2v.bullets.tolist(), 10)\n",
    "print(bullet_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_method = ['W2V', 'D2V', 'Sentence-Transformers', 'ROUGE']\n",
    "\n",
    "for bull in bullet_examples:\n",
    "    idx = df_w2v.loc[df_w2v.bullets == bull].index.tolist()[0]\n",
    "    \n",
    "    list_text = [df_w2v.loc[df_w2v.bullets == bull, 'text'].tolist()[0],\n",
    "        df_d2v.loc[df_d2v.bullets == bull, 'text'].tolist()[0],\n",
    "        df_st.loc[df_st.bullets == bull, 'text'].tolist()[0],\n",
    "        df_rouge.loc[df_rouge.bullets == bull, 'text'].tolist()[0]]\n",
    "    \n",
    "    list_text_num_tok = [df_w2v.loc[df_w2v.bullets == bull, 'para_num_tokens'].tolist()[0],\n",
    "        df_d2v.loc[df_d2v.bullets == bull, 'para_num_tokens'].tolist()[0],\n",
    "        df_st.loc[df_st.bullets == bull, 'para_num_tokens'].tolist()[0],\n",
    "        df_rouge.loc[df_rouge.bullets == bull, 'para_num_tokens'].tolist()[0]]\n",
    "    \n",
    "    nice_print(idx, bull, list_text, list_text_num_tok, list_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ST Base vs Th vs Base-no-overlap vs Th-no-overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1610463902969,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "tvWJmbjVH-qN",
    "outputId": "861d6b76-0978-4aec-bb7a-86609006f34e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "df_st_base = pd.read_csv(OUTPUT_PATH+'st/base/df.csv').set_index(['book', 'chapter'])\n",
    "df_st_th = pd.read_csv(OUTPUT_PATH+'st/th/df.csv').set_index(['book', 'chapter'])\n",
    "df_st_base_noov = pd.read_csv(OUTPUT_PATH+'st/base_overlap/df.csv').set_index(['book', 'chapter'])\n",
    "df_st_th_noov = pd.read_csv(OUTPUT_PATH+'st/th_overlap/df.csv').set_index(['book', 'chapter'])\n",
    "\n",
    "random.seed(config.SEED)\n",
    "\n",
    "bullet_examples = random.sample(df_st_base.bullets.tolist(), 10)\n",
    "print(bullet_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_method = ['BASE', 'THRESHOLD', 'BASE-NOOV', 'THRESHOLD-NOOV']\n",
    "\n",
    "for bull in bullet_examples:\n",
    "    idx = df_st_base.loc[df_st_base.bullets == bull].index.tolist()[0]\n",
    "    \n",
    "    list_text = [df_st_base.loc[df_st_base.bullets == bull, 'text'].tolist()[0],\n",
    "        df_st_th.loc[df_st_th.bullets == bull, 'text'].tolist()[0],\n",
    "        df_st_base_noov.loc[df_st_base_noov.bullets == bull, 'text'].tolist()[0],\n",
    "        df_st_th_noov.loc[df_st_th_noov.bullets == bull, 'text'].tolist()[0]]\n",
    "    \n",
    "    list_text_num_tok = [df_st_base.loc[df_st_base.bullets == bull, 'para_num_tokens'].tolist()[0],\n",
    "        df_st_th.loc[df_st_th.bullets == bull, 'para_num_tokens'].tolist()[0],\n",
    "        df_st_base_noov.loc[df_st_base_noov.bullets == bull, 'para_num_tokens'].tolist()[0],\n",
    "        df_st_th_noov.loc[df_st_th_noov.bullets == bull, 'para_num_tokens'].tolist()[0]]\n",
    "    \n",
    "    nice_print(idx, bull, list_text, list_text_num_tok, list_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "mFAC31paODFl",
    "S0FByNNOIRvG",
    "tb7fAfzaK4es",
    "eQGq4WLu3Gei",
    "tSHT0mxuvkEp",
    "-eRnW74aH95b",
    "X2xp7jJNwB6b",
    "2Eb-_Ud3vxeY",
    "VndEUBoDjjkV",
    "8_li_hFKF_Ws"
   ],
   "name": "paragraph_assign_bullets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
