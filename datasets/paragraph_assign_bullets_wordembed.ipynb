{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "magma_dir = '/home/marco/epfl/magma/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 16184,
     "status": "ok",
     "timestamp": 1610463826089,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "ClE5D523OTZG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, magma_dir)\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 16184,
     "status": "ok",
     "timestamp": 1610463826092,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "82WSp6khIcua"
   },
   "outputs": [],
   "source": [
    "MODEL = 'bart'\n",
    "\n",
    "RE_SPLITTER = '\\n'              # do we split sentences of paragraphs?\n",
    "                                # use '\\.(?!\\d)|\\n' or '\\n', respectively\n",
    "\n",
    "TOKEN_MAX_LEN = 99              # max length of a word\n",
    "PARA_MIN_LENGTH = 2             # minimum length for a sentence or\n",
    "                                # a paragraph, in tokens\n",
    "\n",
    "RECALL_THRESHOLD = 0.7\n",
    "\n",
    "# Output path\n",
    "OUTPUT_PATH = magma_dir+'datasets/karger_books_para_wordembed/'+MODEL+'/'\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb7fAfzaK4es"
   },
   "source": [
    "### **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 16443,
     "status": "ok",
     "timestamp": 1610463826354,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "wvbMlPBxk45S"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from textwrap import fill\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "if 'pegasus' in MODEL:\n",
    "    from transformers import PegasusTokenizer\n",
    "    tokenizer =\\\n",
    "        PegasusTokenizer.from_pretrained('google/pegasus-large')\n",
    "elif 'bart' in MODEL:\n",
    "    from transformers import BartTokenizer\n",
    "    tokenizer =\\\n",
    "        BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQGq4WLu3Gei"
   },
   "source": [
    "### **Karger Books Base Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Z0lbkScg0a7j"
   },
   "outputs": [],
   "source": [
    "base_dataset = magma_dir+'datasets/karger_books_base/df.csv'\n",
    "df = pd.read_csv(base_dataset)\n",
    "df = df.set_index(['book', 'chapter', 'section', 'subsection'])\n",
    "df.bullets = df.bullets.map(eval, na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSHT0mxuvkEp"
   },
   "source": [
    "### **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eRnW74aH95b"
   },
   "source": [
    "#### Preprocessing\n",
    "\n",
    "* Split based on RE_SPLITTER\n",
    "* Explode the dataset\n",
    "* Remove unwanted chars at beginning or end of sentence\n",
    "* Remove multiple spaces\n",
    "* Remove long words (> TOKEN_MAX_LEN chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CDsT33j-wPCw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split in sentences / paragraphs based on RE_SPLITTER\n",
    "df.text =\\\n",
    "    df.text.map(lambda x: [p.strip() for p in re.split(RE_SPLITTER, x) if p!=''],\n",
    "                na_action='ignore')\n",
    "    \n",
    "# explode to get one row for each paragraph /sentence\n",
    "df = df.explode('text')\n",
    "df = df.rename(columns={'text': 'para'})\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove unwanted chars at beginning or end of sentence\n",
    "df.para = df.para.map(lambda p: p.lstrip('.,;:-)] \\n'))\n",
    "df.para = df.para.map(lambda p: p.rstrip('.,;:-([ \\n'))\n",
    "\n",
    "# Remove multiple spaces\n",
    "df.para = df.para.map(lambda p:\n",
    "    re.sub('\\s+', ' ', p).strip())\n",
    "\n",
    "# Remove long words (> TOKEN_MAX_LEN chars)\n",
    "def para2words(para):\n",
    "    return gensim.utils.simple_preprocess(\n",
    "        para, deacc=True, max_len=TOKEN_MAX_LEN)\n",
    "df['para_proc'] = df.para.map(para2words)\n",
    "df['bullets_proc'] = df.bullets.map(lambda bs: [para2words(b) for b in bs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Preprocessing\n",
    "\n",
    "* Remove stop words\n",
    "* Remove short sentences / paragraphs (< PARA_MIN_LENGTH tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "df.para_proc = df.para_proc.map(lambda p:\n",
    "    [w for w in p if w not in stop_words])\n",
    "df.bullets_proc = df.bullets_proc.map(lambda bs:\n",
    "    [[w for w in b if w not in stop_words] for b in bs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove short sentences / paragraphs (< PARA_MIN_LENGTH tokens)\n",
    "df.loc[df.para_proc.map(len) <\\\n",
    "    PARA_MIN_LENGTH, 'para_proc'] = np.nan\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.para = df.para.map(lambda p: p+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Assign Bullets to Best Para and Expand Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_best_metric_para(df, col_metric):\n",
    "    df['best_match'] = False\n",
    "\n",
    "    for idx, para  in df.groupby('bullets').progress_apply(\n",
    "        lambda g: g.iloc[g[col_metric].argmax()]).para.iteritems():\n",
    "        \n",
    "        df.loc[\\\n",
    "            (df['bullets'] == idx) &\\\n",
    "            (df['para'] == para), 'best_match'] = True\n",
    "    \n",
    "    para_too_short =\\\n",
    "        df[(df['compression_ratio'] >= config.MAX_RATIO) & df['best_match']]\n",
    "    print('Percentage of paragraphs which are too short to be summarized: %.2f %%'\\\n",
    "        %(len(para_too_short)/len(df[df['best_match']])*100))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_up_down(df, col_metric, verbose=False):\n",
    "    # for each bullet\n",
    "    for bul in tqdm(set(df.bullets.tolist())):\n",
    "        if verbose : print(bul)\n",
    "        df_bul = df[df['bullets'] == bul]\n",
    "        \n",
    "        # get book and chapter where this bullet is\n",
    "        book = df_bul.index.get_level_values(0)[0]\n",
    "        cpt = df_bul.index.get_level_values(1)[0]\n",
    "\n",
    "        df_bul = df_bul.reset_index()\n",
    "        if verbose : print(df_bul[df_bul['best_match']].para.tolist())\n",
    "        # get best match index\n",
    "        best_match_idx = np.where(df_bul['best_match'])[0][0]\n",
    "        merged_para_idx = [best_match_idx]\n",
    "\n",
    "        bul_num_tok = df_bul.loc[best_match_idx].bullets_num_tokens\n",
    "        comp_ratio = df_bul.loc[best_match_idx].compression_ratio\n",
    "        if verbose:\n",
    "            print('Book %s, Chapter %s'%(book, cpt))\n",
    "            print('Paragraphs in this chapter:', len(df_bul))\n",
    "            print('Location of best_bul index:', best_match_idx)\n",
    "            print('Compression ratio before merging: %.2f %%'%comp_ratio)\n",
    "            print()\n",
    "        while comp_ratio > config.MAX_RATIO:\n",
    "            if 0 in merged_para_idx:\n",
    "                if verbose : print('merge down')\n",
    "                new_para_idx = max(merged_para_idx)+1\n",
    "                df_bul.loc[new_para_idx, 'best_match'] = True\n",
    "                merged_para_idx.append(new_para_idx)\n",
    "\n",
    "            elif (len(df_bul)-1) in merged_para_idx:\n",
    "                if verbose : print('merge up')\n",
    "                new_para_idx = min(merged_para_idx)-1\n",
    "                df_bul.loc[new_para_idx, 'best_match'] = True\n",
    "                merged_para_idx.append(new_para_idx)\n",
    "\n",
    "            else:\n",
    "                if verbose : print('based on metric %s '%col_metric, end='')\n",
    "\n",
    "                if df_bul.loc[min(merged_para_idx)-1, col_metric] <\\\n",
    "                    df_bul.loc[max(merged_para_idx)+1, col_metric]:\n",
    "                    if verbose : print('merge down')\n",
    "                    new_para_idx = max(merged_para_idx)+1\n",
    "                    df_bul.loc[new_para_idx, 'best_match'] = True\n",
    "                    merged_para_idx.append(new_para_idx)\n",
    "                else:\n",
    "                    if verbose : print('merge up')\n",
    "                    new_para_idx = min(merged_para_idx)-1\n",
    "                    df_bul.loc[new_para_idx, 'best_match'] = True\n",
    "                    merged_para_idx.append(new_para_idx)         \n",
    "\n",
    "            comp_ratio = bul_num_tok / np.sum(df_bul[df_bul['best_match']].para_num_tokens.tolist())\n",
    "            if verbose:\n",
    "                print(df_bul[df_bul['best_match']].para.tolist())\n",
    "                print('Compression ratio: %.2f %%'%comp_ratio)\n",
    "                print()\n",
    "\n",
    "        for p, b in zip(df_bul.loc[merged_para_idx]['para'].tolist(),\n",
    "            df_bul.loc[merged_para_idx]['bullets'].tolist()):\n",
    "            df.loc[(df['para'] == p) &\n",
    "                (df['bullets'] == b), 'best_match'] = True\n",
    "        if verbose : print()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(df):\n",
    "    num_para_tot = 18822\n",
    "    num_para_kept = np.sum(df.groupby('para')['best_match'].apply(np.any).tolist())\n",
    "    print('%d out of %d paragraphs are considered using this method.'%(num_para_kept, num_para_tot), end=' ')\n",
    "    print('Thus, %.2f %%'%(100*num_para_kept/num_para_tot))\n",
    "    \n",
    "    print()\n",
    "    df_count_tokens = df.groupby('para', sort=False).agg({\n",
    "        'best_match': lambda bm: np.any(list(bm)),\n",
    "        'para_num_tokens': lambda pnt: list(pnt)[0]})\n",
    "    num_tok_kept = df_count_tokens[df_count_tokens['best_match']].para_num_tokens.sum()\n",
    "    num_tok_tot = df_count_tokens.para_num_tokens.sum()\n",
    "\n",
    "    print('%d out of %d tokens are considered using this method.'%(num_tok_kept, num_tok_tot), end=' ')\n",
    "    print('Thus, %.2f %%'%(100*num_tok_kept/num_tok_tot))\n",
    "\n",
    "def print_stats_after_merge(df):\n",
    "    para_too_short = df[df['compression_ratio'] > config.MAX_RATIO]\n",
    "    print('Percentage of paragraphs which are too short to be summarized: %.2f %%'\\\n",
    "        %(len(para_too_short)/len(df)*100))\n",
    "    \n",
    "    print()\n",
    "    print('Paragraphs which are too long to fit into the model:')\n",
    "    print(df[df['para_num_tokens'] > config.MODEL_MAX_LEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### **Word2Vec Book Level**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Create Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "op = OUTPUT_PATH + 'w2v/'\n",
    "if not os.path.exists(op):\n",
    "    os.makedirs(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v_book = df_w2v.groupby('book', sort=False).agg({\n",
    "    'para_proc': lambda pp: list(pp),\n",
    "    'bullets_proc': lambda bp: list(bp)[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v_book['corpus'] = df_w2v_book.para_proc + df_w2v_book.bullets_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v_book['w2v'] = df_w2v_book.corpus.progress_map(lambda c:\\\n",
    "    gensim.models.Word2Vec(\n",
    "        c,\n",
    "        #size=128,\n",
    "        #window=3,\n",
    "        min_count=1,\n",
    "        sg=1, # 1 for skip-gram; otherwise CBOW.\n",
    "        seed = config.SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def assign_word_vectors(r, col):\n",
    "    book = r.name[0]\n",
    "    wv = df_w2v_book.loc[book, 'w2v'].wv\n",
    "    wv_list = []\n",
    "    for x in r[col]:\n",
    "        try:\n",
    "            v = wv[x]\n",
    "        except:\n",
    "            continue\n",
    "        wv_list.append(v)\n",
    "    return wv_list\n",
    "\n",
    "df_w2v['para_wv'] = df_w2v.progress_apply(lambda row: assign_word_vectors(row, 'para_proc'), axis=1)\n",
    "\n",
    "# taking the average of the w2v vector of each paragraph\n",
    "df_w2v.para_wv = df_w2v.para_wv.progress_map(lambda p_wv: np.mean(p_wv, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(df_w2v[df_w2v.para_wv.isna()])\n",
    "df_w2v = df_w2v.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Explode, preprocess, w2v bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v = df_w2v.explode('bullets')\n",
    "\n",
    "df_w2v['bullets_proc'] = df_w2v.bullets.progress_map(para2words)\n",
    "df_w2v.bullets_proc = df_w2v.bullets_proc.progress_map(lambda b:\n",
    "    [w for w in b if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v['bullets_wv'] = df_w2v.progress_apply(lambda row: assign_word_vectors(row, 'bullets_proc'), axis=1)\n",
    "\n",
    "# taking the average of the w2v vector of each bullet\n",
    "df_w2v.bullets_wv = df_w2v.bullets_wv.progress_map(lambda b_wv: np.mean(b_wv, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v['para_num_tokens'] = df_w2v.para.progress_map(lambda p: len(tokenizer.tokenize(p)))\n",
    "df_w2v['bullets_num_tokens'] = df_w2v.bullets.progress_map(lambda b: len(tokenizer.tokenize(b)))\n",
    "\n",
    "df_w2v['compression_ratio'] = df_w2v.bullets_num_tokens / df_w2v.para_num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calculate cosine similarity between each couple bullet-para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_w2v['cosine_sim'] = df_w2v[['para_wv', 'bullets_wv']].progress_apply(lambda row:\\\n",
    "    cosine_sim(row[0], row[1]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Find Best Match and Expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find best match bullet-para for each bullet\n",
    "df_w2v = assign_best_metric_para(df_w2v, 'cosine_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_w2v = expand_up_down(df_w2v, 'cosine_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_stats(df_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v_merge = df_w2v[df_w2v['best_match']].reset_index().groupby(['book', 'chapter', 'bullets'], sort=False)\\\n",
    ".agg({\n",
    "    'para': lambda p: ' '.join(list(p)),\n",
    "    'para_num_tokens': sum,\n",
    "    'bullets_num_tokens': lambda bnt: list(bnt)[0]\n",
    "}).reset_index(level='bullets')\n",
    "df_w2v_merge = df_w2v_merge.rename(columns={'para': 'text'})\n",
    "\n",
    "df_w2v_merge['compression_ratio'] = df_w2v_merge.bullets_num_tokens / df_w2v_merge.para_num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_stats_after_merge(df_w2v_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v_merge.to_csv(op+'df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Create train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_w2v_merge = df_w2v_merge.groupby(level=[0, 1], sort=False).agg({\n",
    "    'bullets': lambda b: list(b),\n",
    "    'text': lambda t: list(t),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_w2v_merge = df_w2v_merge.sample(frac=1, random_state=config.SEED)\n",
    "df_w2v_merge['num_bulls'] = df_w2v_merge.bullets.map(len).cumsum()\n",
    "tot_bulls = df_w2v_merge.num_bulls.iloc[-1]\n",
    "split1 = np.where(df_w2v_merge.num_bulls > int(tot_bulls*0.8))[0][0]+1\n",
    "split2 = np.where(df_w2v_merge.num_bulls > int(tot_bulls*0.9))[0][0]+1\n",
    "print(split1, split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train, val, test =\\\n",
    "    df_w2v_merge.iloc[:split1].explode('bullets'),\\\n",
    "    df_w2v_merge.iloc[split1:split2].explode('bullets'),\\\n",
    "    df_w2v_merge.iloc[split2:].explode('bullets')\n",
    "\n",
    "train['text'] = df_w2v_merge.iloc[:split1].explode('text')['text']\n",
    "val['text'] = df_w2v_merge.iloc[split1:split2].explode('text')['text']\n",
    "test['text'] = df_w2v_merge.iloc[split2:].explode('text')['text']\n",
    "\n",
    "train.to_csv(op+'train.csv')\n",
    "val.to_csv(op+'val.csv')\n",
    "test.to_csv(op+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(op+'train.source', 'w') as tr_s,\\\n",
    "    open(op+'train.target', 'w') as tr_t,\\\n",
    "    open(op+'train.index', 'w') as tr_i:\n",
    "    for idx, row in train[['text', 'bullets']].iterrows():\n",
    "        tr_i.write(str(idx) + '\\n')\n",
    "        tr_s.write(row.text + '\\n')\n",
    "        tr_t.write(row.bullets + '\\n')\n",
    "        \n",
    "with open(op+'val.source', 'w') as va_s,\\\n",
    "    open(op+'val.target', 'w') as va_t,\\\n",
    "    open(op+'val.index', 'w') as va_i:\n",
    "    for idx, row in val[['text', 'bullets']].iterrows():\n",
    "        va_i.write(str(idx) + '\\n')\n",
    "        va_s.write(row.text + '\\n')\n",
    "        va_t.write(row.bullets + '\\n')\n",
    "        \n",
    "with open(op+'test.source', 'w') as te_s,\\\n",
    "    open(op+'test.target', 'w') as te_t,\\\n",
    "    open(op+'test.index', 'w') as te_i:\n",
    "    for idx, row in test[['text', 'bullets']].iterrows():\n",
    "        te_i.write(str(idx) + '\\n')\n",
    "        te_s.write(row.text + '\\n')\n",
    "        te_t.write(row.bullets + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Doc2Vec Book Level**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Create Doc Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "op = OUTPUT_PATH + 'd2v/'\n",
    "if not os.path.exists(op):\n",
    "    os.makedirs(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_d2v = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_d2v_book = df_d2v.groupby('book', sort=False).agg({\n",
    "    'para_proc': lambda pp: list(pp),\n",
    "    'bullets_proc': lambda bp: list(bp)[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_d2v_book['corpus'] = df_d2v_book.para_proc + df_d2v_book.bullets_proc\n",
    "df_d2v_book['tagged_corpus'] = df_d2v_book.corpus.map(lambda c:\n",
    "    [gensim.models.doc2vec.TaggedDocument(para, [i]) for i, para in enumerate(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:35<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "df_d2v_book['d2v'] = df_d2v_book.tagged_corpus.progress_map(lambda tc:\\\n",
    "    gensim.models.Doc2Vec(\n",
    "        tc,\n",
    "        dm=1, # 1 for PV-DM; otherwise PV-DBOW\n",
    "        #vector_size=128,\n",
    "        #window=3,\n",
    "        #epochs=5,\n",
    "        min_count=1,\n",
    "        seed = config.SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Explode and preprocess bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114277/114277 [00:07<00:00, 14571.06it/s]\n",
      "100%|██████████| 114277/114277 [00:04<00:00, 23807.62it/s]\n"
     ]
    }
   ],
   "source": [
    "df_d2v = df_d2v.explode('bullets')\n",
    "\n",
    "df_d2v['bullets_proc'] = df_d2v.bullets.progress_map(para2words)\n",
    "df_d2v.bullets_proc = df_d2v.bullets_proc.progress_map(lambda b:\n",
    "    [w for w in b if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114277/114277 [00:22<00:00, 5116.23it/s]\n",
      "100%|██████████| 114277/114277 [00:12<00:00, 9148.06it/s]\n"
     ]
    }
   ],
   "source": [
    "df_d2v['para_num_tokens'] = df_d2v.para.progress_map(lambda p: len(tokenizer.tokenize(p)))\n",
    "df_d2v['bullets_num_tokens'] = df_d2v.bullets.progress_map(lambda b: len(tokenizer.tokenize(b)))\n",
    "\n",
    "df_d2v['compression_ratio'] = df_d2v.bullets_num_tokens / df_d2v.para_num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Calculate similarity between each couple bullet-para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114277/114277 [02:04<00:00, 916.41it/s] \n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "def d2v_similarity(r):\n",
    "    book = r.name[0]\n",
    "    d2v = df_d2v_book.loc[book, 'd2v']\n",
    "    dv_para = d2v.infer_vector(r.para_proc)\n",
    "    dv_bullets = d2v.infer_vector(r.bullets_proc)\n",
    "    \n",
    "    return cosine_sim(dv_para, dv_bullets)\n",
    "    \n",
    "df_d2v['d2v_sim'] = df_d2v.progress_apply(lambda row: d2v_similarity(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Best Match and Expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2556/2556 [00:01<00:00, 2105.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of paragraphs which are too short to be summarized: 55.44 %\n"
     ]
    }
   ],
   "source": [
    "# find best match bullet-para for each bullet\n",
    "df_d2v = assign_best_metric_para(df_d2v, 'd2v_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2556/2556 [01:27<00:00, 29.28it/s]\n"
     ]
    }
   ],
   "source": [
    "df_d2v_expand = expand_up_down(df_d2v, 'd2v_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4001 out of 18822 paragraphs are considered using this method. Thus, 21.26 %\n",
      "\n",
      "378698 out of 1335844 tokens are considered using this method. Thus, 28.35 %\n"
     ]
    }
   ],
   "source": [
    "print_stats(df_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d2v_merge = df_d2v[df_d2v['best_match']].reset_index().groupby(['book', 'chapter', 'bullets'], sort=False)\\\n",
    ".agg({\n",
    "    'para': lambda p: ' '.join(list(p)),\n",
    "    'para_num_tokens': sum,\n",
    "    'bullets_num_tokens': lambda bnt: list(bnt)[0]\n",
    "}).reset_index(level='bullets')\n",
    "df_d2v_merge = df_d2v_merge.rename(columns={'para': 'text'})\n",
    "\n",
    "df_d2v_merge['compression_ratio'] = df_d2v_merge.bullets_num_tokens / df_d2v_merge.para_num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of paragraphs which are too short to be summarized: 0.00 %\n",
      "\n",
      "Paragraphs which are too long to fit into the model:\n",
      "Empty DataFrame\n",
      "Columns: [bullets, text, para_num_tokens, bullets_num_tokens, compression_ratio]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print_stats_after_merge(df_d2v_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_d2v_merge.to_csv(op+'df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Create train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_d2v_merge = df_d2v_merge.groupby(level=[0, 1], sort=False).agg({\n",
    "    'bullets': lambda b: list(b),\n",
    "    'text': lambda t: list(t),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 408\n"
     ]
    }
   ],
   "source": [
    "df_d2v_merge = df_d2v_merge.sample(frac=1, random_state=config.SEED)\n",
    "df_d2v_merge['num_bulls'] = df_d2v_merge.bullets.map(len).cumsum()\n",
    "tot_bulls = df_d2v_merge.num_bulls.iloc[-1]\n",
    "split1 = np.where(df_d2v_merge.num_bulls > int(tot_bulls*0.8))[0][0]+1\n",
    "split2 = np.where(df_d2v_merge.num_bulls > int(tot_bulls*0.9))[0][0]+1\n",
    "print(split1, split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train, val, test =\\\n",
    "    df_d2v_merge.iloc[:split1].explode('bullets'),\\\n",
    "    df_d2v_merge.iloc[split1:split2].explode('bullets'),\\\n",
    "    df_d2v_merge.iloc[split2:].explode('bullets')\n",
    "\n",
    "train['text'] = df_d2v_merge.iloc[:split1].explode('text')['text']\n",
    "val['text'] = df_d2v_merge.iloc[split1:split2].explode('text')['text']\n",
    "test['text'] = df_d2v_merge.iloc[split2:].explode('text')['text']\n",
    "\n",
    "train.to_csv(op+'train.csv')\n",
    "val.to_csv(op+'val.csv')\n",
    "test.to_csv(op+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(op+'train.source', 'w') as tr_s,\\\n",
    "    open(op+'train.target', 'w') as tr_t,\\\n",
    "    open(op+'train.index', 'w') as tr_i:\n",
    "    for idx, row in train[['text', 'bullets']].iterrows():\n",
    "        tr_i.write(str(idx) + '\\n')\n",
    "        tr_s.write(row.text + '\\n')\n",
    "        tr_t.write(row.bullets + '\\n')\n",
    "        \n",
    "with open(op+'val.source', 'w') as va_s,\\\n",
    "    open(op+'val.target', 'w') as va_t,\\\n",
    "    open(op+'val.index', 'w') as va_i:\n",
    "    for idx, row in val[['text', 'bullets']].iterrows():\n",
    "        va_i.write(str(idx) + '\\n')\n",
    "        va_s.write(row.text + '\\n')\n",
    "        va_t.write(row.bullets + '\\n')\n",
    "        \n",
    "with open(op+'test.source', 'w') as te_s,\\\n",
    "    open(op+'test.target', 'w') as te_t,\\\n",
    "    open(op+'test.index', 'w') as te_i:\n",
    "    for idx, row in test[['text', 'bullets']].iterrows():\n",
    "        te_i.write(str(idx) + '\\n')\n",
    "        te_s.write(row.text + '\\n')\n",
    "        te_t.write(row.bullets + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### **Sentence-Transformers Book Level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "op = OUTPUT_PATH + 'st/'\n",
    "if not os.path.exists(op):\n",
    "    os.makedirs(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_st = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Create embedding vectors for para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# might want to try 'msmarco-distilbert-base-v2' too\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "df_st['para_enc'] = df_st.para.progress_map(model.encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Explode bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_st = df_st.explode('bullets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_st['para_num_tokens'] = df_st.para.progress_map(lambda p: len(tokenizer.tokenize(p)))\n",
    "df_st['bullets_num_tokens'] = df_st.bullets.progress_map(lambda b: len(tokenizer.tokenize(b)))\n",
    "\n",
    "df_st['compression_ratio'] = df_st.bullets_num_tokens / df_st.para_num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Create embedding vectors for bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bull_to_embed = df_st.groupby(['book', 'chapter'], sort=False).agg({\n",
    "    'bullets': lambda b: list(set(b))\n",
    "}).explode('bullets')\n",
    "\n",
    "bull_to_embed['bullets_enc'] = bull_to_embed.bullets.progress_map(model.encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calculate similarity between each couple bullet-para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "def sentence_transformers_sim(r):\n",
    "    book = r.name[0]\n",
    "    b2e = bull_to_embed.loc[book]\n",
    "    para_enc = r.para_enc\n",
    "    bullets_enc = b2e.loc[(b2e.bullets == r.bullets), 'bullets_enc']\n",
    "    assert len(bullets_enc) == 1\n",
    "    bullets_enc = bullets_enc[0]\n",
    "    \n",
    "    return cosine_sim(para_enc, bullets_enc)\n",
    "    \n",
    "df_st['st_sim'] = df_st.progress_apply(sentence_transformers_sim, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Find Best Match and Expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find best match bullet-para for each bullet\n",
    "df_st = assign_best_metric_para(df_st, 'st_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_st = expand_up_down(df_st, 'st_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_stats(df_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_st_merge = df_st[df_st['best_match']].reset_index().groupby(['book', 'chapter', 'bullets'], sort=False)\\\n",
    ".agg({\n",
    "    'para': lambda p: ' '.join(list(p)),\n",
    "    'para_num_tokens': sum,\n",
    "    'bullets_num_tokens': lambda bnt: list(bnt)[0]\n",
    "}).reset_index(level='bullets')\n",
    "df_st_merge = df_st_merge.rename(columns={'para': 'text'})\n",
    "\n",
    "df_st_merge['compression_ratio'] = df_st_merge.bullets_num_tokens / df_st_merge.para_num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_stats_after_merge(df_st_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_st_merge.to_csv(op+'df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Create train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_st_merge = df_st_merge.groupby(level=[0, 1], sort=False).agg({\n",
    "    'bullets': lambda b: list(b),\n",
    "    'text': lambda t: list(t),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_st_merge = df_st_merge.sample(frac=1, random_state=config.SEED)\n",
    "df_st_merge['num_bulls'] = df_st_merge.bullets.map(len).cumsum()\n",
    "tot_bulls = df_st_merge.num_bulls.iloc[-1]\n",
    "split1 = np.where(df_st_merge.num_bulls > int(tot_bulls*0.8))[0][0]+1\n",
    "split2 = np.where(df_st_merge.num_bulls > int(tot_bulls*0.9))[0][0]+1\n",
    "print(split1, split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train, val, test =\\\n",
    "    df_st_merge.iloc[:split1].explode('bullets'),\\\n",
    "    df_st_merge.iloc[split1:split2].explode('bullets'),\\\n",
    "    df_st_merge.iloc[split2:].explode('bullets')\n",
    "\n",
    "train['text'] = df_st_merge.iloc[:split1].explode('text')['text']\n",
    "val['text'] = df_st_merge.iloc[split1:split2].explode('text')['text']\n",
    "test['text'] = df_st_merge.iloc[split2:].explode('text')['text']\n",
    "\n",
    "train.to_csv(op+'train.csv')\n",
    "val.to_csv(op+'val.csv')\n",
    "test.to_csv(op+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(op+'train.source', 'w') as tr_s,\\\n",
    "    open(op+'train.target', 'w') as tr_t,\\\n",
    "    open(op+'train.index', 'w') as tr_i:\n",
    "    for idx, row in train[['text', 'bullets']].iterrows():\n",
    "        tr_i.write(str(idx) + '\\n')\n",
    "        tr_s.write(row.text + '\\n')\n",
    "        tr_t.write(row.bullets + '\\n')\n",
    "        \n",
    "with open(op+'val.source', 'w') as va_s,\\\n",
    "    open(op+'val.target', 'w') as va_t,\\\n",
    "    open(op+'val.index', 'w') as va_i:\n",
    "    for idx, row in val[['text', 'bullets']].iterrows():\n",
    "        va_i.write(str(idx) + '\\n')\n",
    "        va_s.write(row.text + '\\n')\n",
    "        va_t.write(row.bullets + '\\n')\n",
    "        \n",
    "with open(op+'test.source', 'w') as te_s,\\\n",
    "    open(op+'test.target', 'w') as te_t,\\\n",
    "    open(op+'test.index', 'w') as te_i:\n",
    "    for idx, row in test[['text', 'bullets']].iterrows():\n",
    "        te_i.write(str(idx) + '\\n')\n",
    "        te_s.write(row.text + '\\n')\n",
    "        te_t.write(row.bullets + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO7BPitFH8Th"
   },
   "source": [
    "### **Print Some Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_print(idx, bull, list_text, list_text_num_tok, list_method):\n",
    "    print(idx)\n",
    "    print()\n",
    "    print('Bullet:')\n",
    "    print(fill(bull, 100))\n",
    "    print()\n",
    "    for t, tok, m in zip(list_text, list_text_num_tok, list_method):\n",
    "        print(m+' (' +str(tok)+'):')\n",
    "        print(fill(t, 100))\n",
    "        print()\n",
    "    \n",
    "    print(''.join(['#']*100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W2V vs D2V vs Sentence-Transformers vs Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1610463902969,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "tvWJmbjVH-qN",
    "outputId": "861d6b76-0978-4aec-bb7a-86609006f34e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hypertension is both an important cause and a consequence of renal disease.', 'Neuronal tumors are uncommon brain neoplasms typically diagnosed in children and young adults.', 'The prevention of nausea has been much less successful with currently approved agents.', 'Surgical removal of large bullae and lung volume reduction surgery may improve lung function and symptoms in carefully selected patients.', 'Acne presents with both inflammatory and comedonal lesions in most patients.', 'Decreased waist circumference in the absence of weight loss can keep a patient motivated.', 'Causes of acute asthma include viral respiratory infections, acute allergen exposure, food allergies and some medications such as acetylsalicylic acid (aspirin) and non-steroidal anti-inflammatory drugs.', 'Biosimilars are highly similar, but not identical, to their reference (originator) biologic.', 'Unlike hemophilia A, the level of FXI is not a good predictor of bleeding risk.', 'Most patients are diagnosed based on typical pain and increased serum pancreatic enzyme activity greater than three times the upper limit of normal.']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "df_w2v = pd.read_csv(OUTPUT_PATH+'w2v/df.csv').set_index(['book', 'chapter'])\n",
    "df_d2v = pd.read_csv(OUTPUT_PATH+'d2v/df.csv').set_index(['book', 'chapter'])\n",
    "df_st = pd.read_csv(OUTPUT_PATH+'st/df.csv').set_index(['book', 'chapter'])\n",
    "df_rouge = pd.read_csv(magma_dir+'datasets/karger_books_para/'+MODEL+'/df.csv').set_index(['book', 'chapter'])\n",
    "\n",
    "random.seed(config.SEED)\n",
    "\n",
    "bullet_examples = random.sample(df_w2v.bullets.tolist(), 10)\n",
    "print(bullet_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9781908541468, 'ch_7')\n",
      "\n",
      "Bullet:\n",
      "Hypertension is both an important cause and a consequence of renal disease.\n",
      "\n",
      "W2V (67):\n",
      "Hypertension associated with parenchymal kidney disease represents a potent vicious cycle; it is\n",
      "both a consequence of CKD and a cause of progressive kidney damage. Evidence from many studies shows\n",
      "that treatment of hypertension is crucial to slowing progression of renal disease, particularly\n",
      "among those with significant albuminuria (> 1 g/24 hours).\n",
      "\n",
      "D2V (125):\n",
      "Management. Antihypertensive therapy is beneficial in reducing both cardiovascular and renal events,\n",
      "as well as lowering mortality. In patients with CKD, treating hypertension is important in slowing\n",
      "disease progression and in reducing cardiovascular risk. In patients with ESKD, the focus should be\n",
      "directed towards reducing cardiovascular morbidity. Patients with albuminuric CKD (> 500 mg\n",
      "albumin/24 hours) require aggressive management of blood pressure, with a target of less than 130/80\n",
      "mmHg. A more relaxed target of less than 140/90 mmHg is recommended for patients who are\n",
      "normoalbuminuric.\n",
      "\n",
      "Sentence-Transformers (67):\n",
      "Hypertension associated with parenchymal kidney disease represents a potent vicious cycle; it is\n",
      "both a consequence of CKD and a cause of progressive kidney damage. Evidence from many studies shows\n",
      "that treatment of hypertension is crucial to slowing progression of renal disease, particularly\n",
      "among those with significant albuminuria (> 1 g/24 hours).\n",
      "\n",
      "ROUGE (261):\n",
      "Hypertension during pregnancy is defined as any rise in systolic blood pressure of more than 30 mmHg\n",
      "or a rise in diastolic blood pressure of more than 15 mmHg above baseline, or the use of\n",
      "antihypertensive agents. It is classified according to its presentation (Table 5.5). Chronic\n",
      "hypertension is more common in multiparous women, and is present at the first antenatal visit. On\n",
      "the other hand, pre-eclampsia is more common in primigravidas (in 10% of first pregnancies), and\n",
      "represents an important cause of maternal and perinatal mortality. It usually presents only after 20\n",
      "weeks of gestation, with or without proteinuria and a raised serum urate. Elevated levels of soluble\n",
      "fms-like tyrosine kinase-1 (sFlt-1 or sVEGFR-1) and endoglin have been reported in pre-eclampsia and\n",
      "have been implicated in disease pathogenesis. Pre-eclampsia may progress to full-blown eclampsia,\n",
      "which is characterized by seizures and also associated with acute kidney injury (AKI). Management of\n",
      "eclampsia comprises immediate delivery, and magnesium sulfate, anticonvulsant and antihypertensive\n",
      "therapy.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "(9781908541024, 'ch_8')\n",
      "\n",
      "Bullet:\n",
      "Neuronal tumors are uncommon brain neoplasms typically diagnosed in children and young adults.\n",
      "\n",
      "W2V (101):\n",
      "Neuronal tumors are very uncommon brain neoplasms that tend to affect children and young adults\n",
      "(Table 7.1). There are often few signs and symptoms other than intractable epilepsy. The rarity of\n",
      "these neoplasms has been a source of considerable confusion and controversy, and it is only over the\n",
      "past two decades that many of these neoplasms have been recognized as distinct clinicopathological\n",
      "entities. General management guidelines are shown in Figure 7.1, and specific treatment issues are\n",
      "discussed below.\n",
      "\n",
      "D2V (78):\n",
      "Dysplastic gangliocytoma is a rare disorder combining neoplastic and malformative features.\n",
      "Macroscopically, megaloencephaly is a common finding, with the cerebellum demonstrating thickened\n",
      "folia. Microscopic examination of the cerebellum reveals disruption of cortical lamination by\n",
      "dysplastic neurons, and reduction or absence of white matter.\n",
      "\n",
      "Sentence-Transformers (101):\n",
      "Neuronal tumors are very uncommon brain neoplasms that tend to affect children and young adults\n",
      "(Table 7.1). There are often few signs and symptoms other than intractable epilepsy. The rarity of\n",
      "these neoplasms has been a source of considerable confusion and controversy, and it is only over the\n",
      "past two decades that many of these neoplasms have been recognized as distinct clinicopathological\n",
      "entities. General management guidelines are shown in Figure 7.1, and specific treatment issues are\n",
      "discussed below.\n",
      "\n",
      "ROUGE (101):\n",
      "Neuronal tumors are very uncommon brain neoplasms that tend to affect children and young adults\n",
      "(Table 7.1). There are often few signs and symptoms other than intractable epilepsy. The rarity of\n",
      "these neoplasms has been a source of considerable confusion and controversy, and it is only over the\n",
      "past two decades that many of these neoplasms have been recognized as distinct clinicopathological\n",
      "entities. General management guidelines are shown in Figure 7.1, and specific treatment issues are\n",
      "discussed below.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "(9781910797150, 'ch02')\n",
      "\n",
      "Bullet:\n",
      "The prevention of nausea has been much less successful with currently approved agents.\n",
      "\n",
      "W2V (71):\n",
      "The use of antiemetic agents, as recommended by international guidelines, has been shown to prevent\n",
      "emesis in approximately 50-70% of patients receiving either highly or moderately emetogenic\n",
      "chemotherapy. - The prevention of nausea has been much less successful with currently approved\n",
      "agents., New agents and new combinations of agents are necessary to adequately prevent chemotherapy-\n",
      "induced nausea.\n",
      "\n",
      "D2V (76):\n",
      "Patient characteristics also influence the potential for CINV (Table 2.2). Young women with a\n",
      "history of motion sickness, emesis during pregnancy and no history of alcohol consumption have the\n",
      "highest risk of developing significant CINV. These patients should receive the most effective\n",
      "prophylactic antiemetic regimen available based on the international antiemetic guidelines (see\n",
      "Chapter 4).\n",
      "\n",
      "Sentence-Transformers (71):\n",
      "The use of antiemetic agents, as recommended by international guidelines, has been shown to prevent\n",
      "emesis in approximately 50-70% of patients receiving either highly or moderately emetogenic\n",
      "chemotherapy. - The prevention of nausea has been much less successful with currently approved\n",
      "agents., New agents and new combinations of agents are necessary to adequately prevent chemotherapy-\n",
      "induced nausea.\n",
      "\n",
      "ROUGE (71):\n",
      "The use of antiemetic agents, as recommended by international guidelines, has been shown to prevent\n",
      "emesis in approximately 50-70% of patients receiving either highly or moderately emetogenic\n",
      "chemotherapy. - The prevention of nausea has been much less successful with currently approved\n",
      "agents., New agents and new combinations of agents are necessary to adequately prevent chemotherapy-\n",
      "induced nausea.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "(9781910797006, 'ch07')\n",
      "\n",
      "Bullet:\n",
      "Surgical removal of large bullae and lung volume reduction surgery may improve lung function and\n",
      "symptoms in carefully selected patients.\n",
      "\n",
      "W2V (162):\n",
      "Lung volume reduction surgery has been shown to improve FEV, decrease total lung capacity, and\n",
      "improve exercise tolerance and quality of life; these effects may last for more than 2 years. In\n",
      "addition, longer-term follow-up has shown that lung volume reduction surgery leads to an improvement\n",
      "in maximal work capacity and health-related quality of life, a reduction in exacerbation frequency\n",
      "and improved survival. These beneficial effects are largely seen in those patients with predominant\n",
      "upper-zone emphysema and poor exercise tolerance. The efficacy of surgical and bronchoscopic lung\n",
      "volume reduction also depends on the presence of collateral ventilation to the diseased lobe\n",
      "(detected by the presence of an incomplete fissure between the lung lobes on CT scanning). Lung\n",
      "volume reduction surgery is expensive and should be reserved for carefully selected patients.\n",
      "\n",
      "D2V (104):\n",
      "Adverse effects of inhaled corticosteroids are far fewer than those associated with systemic\n",
      "administration. Local side effects include thrush, dysphonia and oral candidiasis. Systemic effects,\n",
      "including skin fragility and bruising, are observed with some preparations. Adverse effects on bone\n",
      "density are controversial and have not been shown in most studies. Several meta-analyses have shown\n",
      "an association between inhaled corticosteroids and increased pneumonia risk. Agents that are cleared\n",
      "more rapidly from the circulation have fewer systemic side effects.\n",
      "\n",
      "Sentence-Transformers (162):\n",
      "Lung volume reduction surgery has been shown to improve FEV, decrease total lung capacity, and\n",
      "improve exercise tolerance and quality of life; these effects may last for more than 2 years. In\n",
      "addition, longer-term follow-up has shown that lung volume reduction surgery leads to an improvement\n",
      "in maximal work capacity and health-related quality of life, a reduction in exacerbation frequency\n",
      "and improved survival. These beneficial effects are largely seen in those patients with predominant\n",
      "upper-zone emphysema and poor exercise tolerance. The efficacy of surgical and bronchoscopic lung\n",
      "volume reduction also depends on the presence of collateral ventilation to the diseased lobe\n",
      "(detected by the presence of an incomplete fissure between the lung lobes on CT scanning). Lung\n",
      "volume reduction surgery is expensive and should be reserved for carefully selected patients.\n",
      "\n",
      "ROUGE (162):\n",
      "Lung volume reduction surgery has been shown to improve FEV, decrease total lung capacity, and\n",
      "improve exercise tolerance and quality of life; these effects may last for more than 2 years. In\n",
      "addition, longer-term follow-up has shown that lung volume reduction surgery leads to an improvement\n",
      "in maximal work capacity and health-related quality of life, a reduction in exacerbation frequency\n",
      "and improved survival. These beneficial effects are largely seen in those patients with predominant\n",
      "upper-zone emphysema and poor exercise tolerance. The efficacy of surgical and bronchoscopic lung\n",
      "volume reduction also depends on the presence of collateral ventilation to the diseased lobe\n",
      "(detected by the presence of an incomplete fissure between the lung lobes on CT scanning). Lung\n",
      "volume reduction surgery is expensive and should be reserved for carefully selected patients.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "(9781908541994, 'ch03')\n",
      "\n",
      "Bullet:\n",
      "Acne presents with both inflammatory and comedonal lesions in most patients.\n",
      "\n",
      "W2V (97):\n",
      "Postinflammatory erythema and/or pigmentary changes. In some patients, particularly those with skin\n",
      "of color and Fitzpatrick skin types IV-VI, hyper- (Figure 3.7) or hypopigmented macules may persist\n",
      "following resolution of inflammatory acne lesions. Patients may think that these resolving lesions\n",
      "are active acne lesions and may have the erroneous impression that their acne is not improving. It\n",
      "is important to reassure patients that these dark areas are healing lesions and not active acne.\n",
      "\n",
      "D2V (133):\n",
      "Defining acne severity helps with the selection of the most suitable initial therapy and enables\n",
      "response to treatment to be monitored. There are many scales available for assessing the severity of\n",
      "acne, but no consensus on a gold standard. Severity can be determined according to the type, number,\n",
      "distribution or location of lesions, or a combination of each of these. Acne can be classified as\n",
      "comedonal, mild, moderate or severe based on the type of lesions (Table 3.6). Alternatively, an\n",
      "assessment tool such as the Leeds Revised Acne Grading System allows grading of severity based on\n",
      "photographic comparisons of acne on the face (Figure 3.18), back and chest.\n",
      "\n",
      "Sentence-Transformers (82):\n",
      "Most patients have a mixture of non-inflammatory and inflammatory lesions. Non-inflammatory lesions\n",
      "are called comedones (see Chapter 2). Comedones may be microscopic (microcomedones) or visible as\n",
      "blackheads (open comedones) or whiteheads (closed comedones) (Figure 3.2). Microcomedones, the\n",
      "precursors of all acne lesions, may develop into whiteheads or blackheads.\n",
      "\n",
      "ROUGE (164):\n",
      "Acne conglobata is a very severe form of inflammatory acne characterized by grouped comedones,\n",
      "cysts, abscesses, draining sinus tracts and scars (Figure 3.8). The majority of affected patients\n",
      "are males who present with lesions on the back, buttocks, chest and face. The axilla and inguinal\n",
      "areas can also be involved. The grouped comedones often have multiple openings. The inflammatory\n",
      "lesions are large, tender and red to violaceous in color; they often drain a serous or purulent\n",
      "material. Deep-seated sinus tracts often develop, as does keloidal scarring. Secondary infection\n",
      "with staphylococci or streptococci can occur, although many lesions are colonized by\n",
      "Propionibacterium acnes (P. acnes) only.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "(9781908541680, 'ch_2')\n",
      "\n",
      "Bullet:\n",
      "Decreased waist circumference in the absence of weight loss can keep a patient motivated.\n",
      "\n",
      "W2V (70):\n",
      "Waist circumference is also a useful and tangible marker of weight loss (or gain) for patients. An\n",
      "individual who has increased their physical activity and maintained their dietary intake will lose\n",
      "centimeters around the waist even though they may gain weight as muscle mass increases. Monitoring\n",
      "waist circumference will therefore keep them motivated when lack of change in bodyweight might prove\n",
      "demoralizing.\n",
      "\n",
      "D2V (122):\n",
      "The examination should include measurement of height, weight and waist circumference (see Assessment\n",
      "of body fat). Body fat analysis is optional but is useful for identifying patients with a normal\n",
      "body mass index (BMI) but high percentage of fat and for monitoring progress, in particular\n",
      "providing encouragement to patients who are improving their health by increased activity but who\n",
      "fail to lose weight. A general visual assessment of a person's fat distribution and body morphology\n",
      "is useful to define a central, peripheral or mixed pattern of fat distribution. The presence of any\n",
      "dysmorphic features should be noted, as these may suggest the presence of an obesity syndrome.\n",
      "\n",
      "Sentence-Transformers (70):\n",
      "Waist circumference is also a useful and tangible marker of weight loss (or gain) for patients. An\n",
      "individual who has increased their physical activity and maintained their dietary intake will lose\n",
      "centimeters around the waist even though they may gain weight as muscle mass increases. Monitoring\n",
      "waist circumference will therefore keep them motivated when lack of change in bodyweight might prove\n",
      "demoralizing.\n",
      "\n",
      "ROUGE (91):\n",
      "A patient attending a diabetes or cardiovascular clinic can be asked whether they have lost or\n",
      "gained weight recently. Any patient can be offered a 'well patient check' during a consultation,\n",
      "which will include measurement of weight, height and waist circumference. 'Best practice in weight\n",
      "management' can be performed in the last 2 minutes of any consultation, provided that appropriate\n",
      "follow-up is offered. These 2 minutes are the most important part of the entire weight-loss program.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "(9781908541420, 'ch_8')\n",
      "\n",
      "Bullet:\n",
      "Causes of acute asthma include viral respiratory infections, acute allergen exposure, food allergies\n",
      "and some medications such as acetylsalicylic acid (aspirin) and non-steroidal anti-inflammatory\n",
      "drugs.\n",
      "\n",
      "W2V (405):\n",
      "Other causes of acute asthma include acute allergen exposure, when an allergic person is exposed to\n",
      "an allergen not usually encountered in the environment. Examples of this include 'thunderstorm\n",
      "asthma' in people who usually have allergic rhinitis due to grass pollens, or individuals with\n",
      "allergy to molds. Inhalation of small allergen particles into the lower respiratory tract can cause\n",
      "a severe asthma episode and weather patterns can lead to epidemics of asthma in such circumstances.\n",
      "Similarly, severe food allergies (exempli gratia to peanuts or shellfish) may trigger asthma. While\n",
      "individuals with such a food allergy often present with signs of anaphylaxis, acute asthma is a\n",
      "major component of a severe food reaction for some and it requires recognition and treatment in its\n",
      "own right. Some medications, such as beta-blockers - even in eye drops - may also be responsible for\n",
      "an acute exacerbation of asthma. Individuals with acetylsalicylic acid (ASA; aspirin)-sensitive\n",
      "asthma may have severe asthma after taking ASA or other non-steroidal anti-inflammatory drug (NSAID)\n",
      "that has cyclooxygenase (COX)-1 activity such as indometacin, ibuprofen, naproxen, piroxicam, and\n",
      "nabumetone. The mechanism probably relates to inhibition of formation of protective prostaglandins\n",
      "such as prostaglandin E (PGE). COX-2 inhibitors such as celecoxib are much less likely to cause\n",
      "reactions. NSAID reactions are most often described in individuals with non-allergic asthma who have\n",
      "nasal polyposis. Individuals with this type of asthma should avoid ASA and preferably all NSAIDs,\n",
      "although if absolutely necessary (exempli gratia a patient with severe rheumatoid arthritis and\n",
      "asthma), a supervised trial of a COX-2 inhibitor might be considered. It is important to be aware of\n",
      "other preventable causes of exacerbations such as occupational and sometimes unusual allergen\n",
      "exposures.\n",
      "\n",
      "D2V (405):\n",
      "Other causes of acute asthma include acute allergen exposure, when an allergic person is exposed to\n",
      "an allergen not usually encountered in the environment. Examples of this include 'thunderstorm\n",
      "asthma' in people who usually have allergic rhinitis due to grass pollens, or individuals with\n",
      "allergy to molds. Inhalation of small allergen particles into the lower respiratory tract can cause\n",
      "a severe asthma episode and weather patterns can lead to epidemics of asthma in such circumstances.\n",
      "Similarly, severe food allergies (exempli gratia to peanuts or shellfish) may trigger asthma. While\n",
      "individuals with such a food allergy often present with signs of anaphylaxis, acute asthma is a\n",
      "major component of a severe food reaction for some and it requires recognition and treatment in its\n",
      "own right. Some medications, such as beta-blockers - even in eye drops - may also be responsible for\n",
      "an acute exacerbation of asthma. Individuals with acetylsalicylic acid (ASA; aspirin)-sensitive\n",
      "asthma may have severe asthma after taking ASA or other non-steroidal anti-inflammatory drug (NSAID)\n",
      "that has cyclooxygenase (COX)-1 activity such as indometacin, ibuprofen, naproxen, piroxicam, and\n",
      "nabumetone. The mechanism probably relates to inhibition of formation of protective prostaglandins\n",
      "such as prostaglandin E (PGE). COX-2 inhibitors such as celecoxib are much less likely to cause\n",
      "reactions. NSAID reactions are most often described in individuals with non-allergic asthma who have\n",
      "nasal polyposis. Individuals with this type of asthma should avoid ASA and preferably all NSAIDs,\n",
      "although if absolutely necessary (exempli gratia a patient with severe rheumatoid arthritis and\n",
      "asthma), a supervised trial of a COX-2 inhibitor might be considered. It is important to be aware of\n",
      "other preventable causes of exacerbations such as occupational and sometimes unusual allergen\n",
      "exposures.\n",
      "\n",
      "Sentence-Transformers (405):\n",
      "Other causes of acute asthma include acute allergen exposure, when an allergic person is exposed to\n",
      "an allergen not usually encountered in the environment. Examples of this include 'thunderstorm\n",
      "asthma' in people who usually have allergic rhinitis due to grass pollens, or individuals with\n",
      "allergy to molds. Inhalation of small allergen particles into the lower respiratory tract can cause\n",
      "a severe asthma episode and weather patterns can lead to epidemics of asthma in such circumstances.\n",
      "Similarly, severe food allergies (exempli gratia to peanuts or shellfish) may trigger asthma. While\n",
      "individuals with such a food allergy often present with signs of anaphylaxis, acute asthma is a\n",
      "major component of a severe food reaction for some and it requires recognition and treatment in its\n",
      "own right. Some medications, such as beta-blockers - even in eye drops - may also be responsible for\n",
      "an acute exacerbation of asthma. Individuals with acetylsalicylic acid (ASA; aspirin)-sensitive\n",
      "asthma may have severe asthma after taking ASA or other non-steroidal anti-inflammatory drug (NSAID)\n",
      "that has cyclooxygenase (COX)-1 activity such as indometacin, ibuprofen, naproxen, piroxicam, and\n",
      "nabumetone. The mechanism probably relates to inhibition of formation of protective prostaglandins\n",
      "such as prostaglandin E (PGE). COX-2 inhibitors such as celecoxib are much less likely to cause\n",
      "reactions. NSAID reactions are most often described in individuals with non-allergic asthma who have\n",
      "nasal polyposis. Individuals with this type of asthma should avoid ASA and preferably all NSAIDs,\n",
      "although if absolutely necessary (exempli gratia a patient with severe rheumatoid arthritis and\n",
      "asthma), a supervised trial of a COX-2 inhibitor might be considered. It is important to be aware of\n",
      "other preventable causes of exacerbations such as occupational and sometimes unusual allergen\n",
      "exposures.\n",
      "\n",
      "ROUGE (405):\n",
      "Other causes of acute asthma include acute allergen exposure, when an allergic person is exposed to\n",
      "an allergen not usually encountered in the environment. Examples of this include 'thunderstorm\n",
      "asthma' in people who usually have allergic rhinitis due to grass pollens, or individuals with\n",
      "allergy to molds. Inhalation of small allergen particles into the lower respiratory tract can cause\n",
      "a severe asthma episode and weather patterns can lead to epidemics of asthma in such circumstances.\n",
      "Similarly, severe food allergies (exempli gratia to peanuts or shellfish) may trigger asthma. While\n",
      "individuals with such a food allergy often present with signs of anaphylaxis, acute asthma is a\n",
      "major component of a severe food reaction for some and it requires recognition and treatment in its\n",
      "own right. Some medications, such as beta-blockers - even in eye drops - may also be responsible for\n",
      "an acute exacerbation of asthma. Individuals with acetylsalicylic acid (ASA; aspirin)-sensitive\n",
      "asthma may have severe asthma after taking ASA or other non-steroidal anti-inflammatory drug (NSAID)\n",
      "that has cyclooxygenase (COX)-1 activity such as indometacin, ibuprofen, naproxen, piroxicam, and\n",
      "nabumetone. The mechanism probably relates to inhibition of formation of protective prostaglandins\n",
      "such as prostaglandin E (PGE). COX-2 inhibitors such as celecoxib are much less likely to cause\n",
      "reactions. NSAID reactions are most often described in individuals with non-allergic asthma who have\n",
      "nasal polyposis. Individuals with this type of asthma should avoid ASA and preferably all NSAIDs,\n",
      "although if absolutely necessary (exempli gratia a patient with severe rheumatoid arthritis and\n",
      "asthma), a supervised trial of a COX-2 inhibitor might be considered. It is important to be aware of\n",
      "other preventable causes of exacerbations such as occupational and sometimes unusual allergen\n",
      "exposures.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "(9781912776238, 'ch1')\n",
      "\n",
      "Bullet:\n",
      "Biosimilars are highly similar, but not identical, to their reference (originator) biologic.\n",
      "\n",
      "W2V (124):\n",
      "The manufacturing process can affect important aspects of the structure of a biological drug; a\n",
      "copied biologic can never therefore be entirely identical to the original reference product. Thus,\n",
      "the active substance of a biosimilar and its reference medicine is almost the same biological\n",
      "substance, but there may be minor differences as a result of their complex nature and production\n",
      "methods. Like the reference (originator) biologic, the biosimilar has a degree of natural\n",
      "variability. For a biosimilar to be approved, this variability, and any differences between the\n",
      "biosimilar and the reference biologic, have to have been shown not to affect safety or\n",
      "effectiveness.\n",
      "\n",
      "D2V (198):\n",
      "Manufacturing process change. Biotechnology evolves quickly; each year brings developments in\n",
      "analytics, higher yielding cell lines and more efficient manufacturing processes that offer\n",
      "manufacturers the chance to make better versions of established biologics at lower prices. For this\n",
      "reason, most biological drugs undergo regular manufacturing process changes over the lifetime of a\n",
      "drug and so the product may not always remain identical over time. The manufacturing process can\n",
      "affect important aspects of the structure of a biological drug; a copied biologic can never\n",
      "therefore be entirely identical to the original reference product. Thus, the active substance of a\n",
      "biosimilar and its reference medicine is almost the same biological substance, but there may be\n",
      "minor differences as a result of their complex nature and production methods. Like the reference\n",
      "(originator) biologic, the biosimilar has a degree of natural variability. For a biosimilar to be\n",
      "approved, this variability, and any differences between the biosimilar and the reference biologic,\n",
      "have to have been shown not to affect safety or effectiveness.\n",
      "\n",
      "Sentence-Transformers (124):\n",
      "The manufacturing process can affect important aspects of the structure of a biological drug; a\n",
      "copied biologic can never therefore be entirely identical to the original reference product. Thus,\n",
      "the active substance of a biosimilar and its reference medicine is almost the same biological\n",
      "substance, but there may be minor differences as a result of their complex nature and production\n",
      "methods. Like the reference (originator) biologic, the biosimilar has a degree of natural\n",
      "variability. For a biosimilar to be approved, this variability, and any differences between the\n",
      "biosimilar and the reference biologic, have to have been shown not to affect safety or\n",
      "effectiveness.\n",
      "\n",
      "ROUGE (124):\n",
      "The manufacturing process can affect important aspects of the structure of a biological drug; a\n",
      "copied biologic can never therefore be entirely identical to the original reference product. Thus,\n",
      "the active substance of a biosimilar and its reference medicine is almost the same biological\n",
      "substance, but there may be minor differences as a result of their complex nature and production\n",
      "methods. Like the reference (originator) biologic, the biosimilar has a degree of natural\n",
      "variability. For a biosimilar to be approved, this variability, and any differences between the\n",
      "biosimilar and the reference biologic, have to have been shown not to affect safety or\n",
      "effectiveness.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "(9781908541406, 'ch_10')\n",
      "\n",
      "Bullet:\n",
      "Unlike hemophilia A, the level of FXI is not a good predictor of bleeding risk.\n",
      "\n",
      "W2V (195):\n",
      "Diagnosis. The deficiency is classified as severe if the FXI level is less than 20 IU/dL and partial\n",
      "at 20-70 IU/dL; the lower limit of the normal range is 60-70 IU/dL. Unlike hemophilia A and B, the\n",
      "correlation between the plasma FXI level and the propensity to bleed is poor. Individuals with\n",
      "similar FXI levels may have very different hemorrhagic potential; furthermore, this may vary over\n",
      "time. Bleeding usually arises following surgery, particularly dental extraction, tonsillectomy,\n",
      "prostatectomy and appendectomy. Treatment. The management of bleeding and surgery in patients with\n",
      "FXI deficiency is not straightforward due to the lack of correlation between hemorrhagic potential\n",
      "and plasma FXI level, and the lack of an effective and entirely safe FXI concentrate. Before surgery\n",
      "the postoperative bleeding risk should be assessed based on the patient's hemorrhagic history after\n",
      "previous surgery, FXI level and severity of the impending operation.\n",
      "\n",
      "D2V (378):\n",
      "Treatment. The management of bleeding and surgery in patients with FXI deficiency is not\n",
      "straightforward due to the lack of correlation between hemorrhagic potential and plasma FXI level,\n",
      "and the lack of an effective and entirely safe FXI concentrate. Before surgery the postoperative\n",
      "bleeding risk should be assessed based on the patient's hemorrhagic history after previous surgery,\n",
      "FXI level and severity of the impending operation. For those with a partial deficiency it may be\n",
      "appropriate to undertake surgery with only careful attention to hemostasis and to use tranexamic\n",
      "acid for dental extractions. The FXI level may need to be raised in those with a severe deficiency\n",
      "who are undergoing major surgery. As the half-life of FXI is approximately 45 hours, therapeutic\n",
      "infusions of plasma or concentrate should be given daily or on alternate days. Virally inactivated\n",
      "fresh frozen plasma can be used but solvent/detergent-treated plasma has a lower FXI content and is\n",
      "therefore relatively ineffective. FXI concentrates can be used, but these have been associated with\n",
      "both arterial and venous thromboembolism. They are therefore contraindicated in individuals with\n",
      "clinical atherosclerosis and in patients whose hemostatic mechanisms are activated, such as pregnant\n",
      "women and those with liver disease. Concentrates should not normally be used in doses above 30 IU/kg\n",
      "and the plasma level of FXI should not exceed 50-70 IU/dL. Fibrinolytic inhibitors should never be\n",
      "given concomitantly with a FXI concentrate, because of their propensity to predispose to thrombosis\n",
      "and DIC. Patients with the type 2 defect often develop inhibitory antibodies when exposed to normal\n",
      "FXI. Such antibodies may induce resistance to further treatment with FXI, in which case activated\n",
      "prothrombin complex concentrate or recombinant activated human FVIIa should be used to control\n",
      "bleeding.\n",
      "\n",
      "Sentence-Transformers (116):\n",
      "Diagnosis. The deficiency is classified as severe if the FXI level is less than 20 IU/dL and partial\n",
      "at 20-70 IU/dL; the lower limit of the normal range is 60-70 IU/dL. Unlike hemophilia A and B, the\n",
      "correlation between the plasma FXI level and the propensity to bleed is poor. Individuals with\n",
      "similar FXI levels may have very different hemorrhagic potential; furthermore, this may vary over\n",
      "time. Bleeding usually arises following surgery, particularly dental extraction, tonsillectomy,\n",
      "prostatectomy and appendectomy.\n",
      "\n",
      "ROUGE (299):\n",
      "For those with a partial deficiency it may be appropriate to undertake surgery with only careful\n",
      "attention to hemostasis and to use tranexamic acid for dental extractions. The FXI level may need to\n",
      "be raised in those with a severe deficiency who are undergoing major surgery. As the half-life of\n",
      "FXI is approximately 45 hours, therapeutic infusions of plasma or concentrate should be given daily\n",
      "or on alternate days. Virally inactivated fresh frozen plasma can be used but solvent/detergent-\n",
      "treated plasma has a lower FXI content and is therefore relatively ineffective. FXI concentrates can\n",
      "be used, but these have been associated with both arterial and venous thromboembolism. They are\n",
      "therefore contraindicated in individuals with clinical atherosclerosis and in patients whose\n",
      "hemostatic mechanisms are activated, such as pregnant women and those with liver disease.\n",
      "Concentrates should not normally be used in doses above 30 IU/kg and the plasma level of FXI should\n",
      "not exceed 50-70 IU/dL. Fibrinolytic inhibitors should never be given concomitantly with a FXI\n",
      "concentrate, because of their propensity to predispose to thrombosis and DIC. Patients with the type\n",
      "2 defect often develop inhibitory antibodies when exposed to normal FXI. Such antibodies may induce\n",
      "resistance to further treatment with FXI, in which case activated prothrombin complex concentrate or\n",
      "recombinant activated human FVIIa should be used to control bleeding.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "(9783318066241, 'ch3')\n",
      "\n",
      "Bullet:\n",
      "Most patients are diagnosed based on typical pain and increased serum pancreatic enzyme activity\n",
      "greater than three times the upper limit of normal.\n",
      "\n",
      "W2V (168):\n",
      "Hypertriglyceridemia (> 500 mg/mL) may be associated with falsely low levels of amylase and lipase\n",
      "activity. This is likely to be due to interference with the assay and so, in patients with lipemic\n",
      "plasma and typical symptoms of AP but unexpectedly low levels of pancreatic enzyme activity, the\n",
      "plasma can be serially diluted and the enzymes measured again, or imaging should be performed.\n",
      "Increased serum pancreatic enzyme activity may also be due to other conditions (Table 3.1 and Table\n",
      "3.2) and about 1 in 10 patients may be wrongly diagnosed as having AP by blood or urinary tests. As\n",
      "indicated in Table 3.1, any cause of acute abdomen may be associated with increased enzyme activity\n",
      "in blood; in general, the more elevated the pancreatic enzyme activity level, the greater the\n",
      "specificity for AP.\n",
      "\n",
      "D2V (228):\n",
      "Blood and urine tests. Amylase is mainly produced in the pancreas and salivary glands, and is\n",
      "cleared by the reticuloendothelial system (75%) and the kidneys (25%). The pancreas is the major\n",
      "source of lipase in adults. Most patients with AP have amylase and/or lipase serum activity greater\n",
      "than three times the upper level of normal. The amylase increase is first detected 2-12 hours after\n",
      "the onset of symptoms of AP; the level peaks at 12-72 hours and usually normalizes within 5 days.\n",
      "Serum lipase levels increase within 4-8 hours after the onset of symptoms; they peak at 24 hours,\n",
      "and return to normal after 8-14 days. The half-life of lipase is longer than amylase, so patients\n",
      "with a delayed presentation in the emergency room may show normal amylase activity with increased\n",
      "lipase activity. Urinary trypsinogen-2 and urinary amylase are also increased in AP. There is no\n",
      "evidence supporting the superiority of any one serum or urinary test over another in terms of\n",
      "accuracy.\n",
      "\n",
      "Sentence-Transformers (110):\n",
      "abdominal pain consistent with AP (acute onset of a persistent, severe, epigastric pain, often\n",
      "radiating to the back). serum amylase or lipase activity at least three times greater than the upper\n",
      "limit of normal. characteristic findings of AP on a contrast-enhanced CT scan or, less commonly, MRI\n",
      "or transabdominal ultrasonography. Most patients can be diagnosed on the basis of the first two\n",
      "criteria, with imaging required only in individuals with atypical symptoms or low serum enzyme\n",
      "activity.\n",
      "\n",
      "ROUGE (171):\n",
      "Most patients with AP have amylase and/or lipase serum activity greater than three times the upper\n",
      "level of normal. The amylase increase is first detected 2-12 hours after the onset of symptoms of\n",
      "AP; the level peaks at 12-72 hours and usually normalizes within 5 days. Serum lipase levels\n",
      "increase within 4-8 hours after the onset of symptoms; they peak at 24 hours, and return to normal\n",
      "after 8-14 days. The half-life of lipase is longer than amylase, so patients with a delayed\n",
      "presentation in the emergency room may show normal amylase activity with increased lipase activity.\n",
      "Urinary trypsinogen-2 and urinary amylase are also increased in AP. There is no evidence supporting\n",
      "the superiority of any one serum or urinary test over another in terms of accuracy.\n",
      "\n",
      "####################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_method = ['W2V', 'D2V', 'Sentence-Transformers', 'ROUGE']\n",
    "\n",
    "for bull in bullet_examples:\n",
    "    idx = df_w2v.loc[df_w2v.bullets == bull].index.tolist()[0]\n",
    "    \n",
    "    list_text = [df_w2v.loc[df_w2v.bullets == bull, 'text'].tolist()[0],\n",
    "        df_d2v.loc[df_d2v.bullets == bull, 'text'].tolist()[0],\n",
    "        df_st.loc[df_st.bullets == bull, 'text'].tolist()[0],\n",
    "        df_rouge.loc[df_rouge.bullets == bull, 'text'].tolist()[0]]\n",
    "    \n",
    "    list_text_num_tok = [df_w2v.loc[df_w2v.bullets == bull, 'para_num_tokens'].tolist()[0],\n",
    "        df_d2v.loc[df_d2v.bullets == bull, 'para_num_tokens'].tolist()[0],\n",
    "        df_st.loc[df_st.bullets == bull, 'para_num_tokens'].tolist()[0],\n",
    "        df_rouge.loc[df_rouge.bullets == bull, 'para_num_tokens'].tolist()[0]]\n",
    "    \n",
    "    nice_print(idx, bull, list_text, list_text_num_tok, list_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "mFAC31paODFl",
    "S0FByNNOIRvG",
    "tb7fAfzaK4es",
    "eQGq4WLu3Gei",
    "tSHT0mxuvkEp",
    "-eRnW74aH95b",
    "X2xp7jJNwB6b",
    "2Eb-_Ud3vxeY",
    "VndEUBoDjjkV",
    "8_li_hFKF_Ws"
   ],
   "name": "paragraph_assign_bullets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
