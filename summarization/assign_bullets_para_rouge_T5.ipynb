{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 18956,
     "status": "ok",
     "timestamp": 1610616869075,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "au5Z9XQAC7C-"
   },
   "outputs": [],
   "source": [
    "magma_dir = '/home/marco/epfl/magma/'\n",
    "cache_dir = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "magma_dir = '/home/ubuntu/magma/'\n",
    "bucket_dir = '/home/ubuntu/s3/'\n",
    "transformers_dir = '/home/ubuntu/transformers/'\n",
    "cache_dir = bucket_dir+'.cache/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 45120,
     "status": "ok",
     "timestamp": 1610616895249,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "3BHImWfNKpDN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, magma_dir)\n",
    "import config\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 45119,
     "status": "ok",
     "timestamp": 1610616895251,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "82WSp6khIcua"
   },
   "outputs": [],
   "source": [
    "MODEL = 't5'\n",
    "MODELS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 45117,
     "status": "ok",
     "timestamp": 1610616895252,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "5ti6XKYnFzSr"
   },
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "data_dir = magma_dir + 'datasets/karger_books_para_rouge/'+MODEL+'/'\n",
    "\n",
    "# Output path\n",
    "OUTPUT_PATH = magma_dir+'summarization/assign_bullets_para_rouge/'+MODEL+'/'\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFd0ppeJyX1o"
   },
   "source": [
    "### **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 45115,
     "status": "ok",
     "timestamp": 1610616895253,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "yCzod0OizR5U"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from textwrap import fill\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dllOnKR9Os5i"
   },
   "source": [
    "### **Function Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2mpXoSaQiQE"
   },
   "source": [
    "##### Import Model and Tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 45113,
     "status": "ok",
     "timestamp": 1610616895255,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "XhBGMJVFOs5l"
   },
   "outputs": [],
   "source": [
    "def import_model_tok(model_name_or_path, verbose=False):\n",
    "    global MODELS\n",
    "\n",
    "    if model_name_or_path in MODELS.keys():\n",
    "        if verbose : print('[+] model already present in cache\\n')\n",
    "        return MODELS[model_name_or_path]\n",
    "    if verbose : print('[*] importing the model\\n')\n",
    "\n",
    "    if 'bart' in MODEL:\n",
    "        from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "        model = BartForConditionalGeneration.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "        tokenizer = BartTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "    elif 'pegasus' in MODEL:\n",
    "        from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "        model = PegasusForConditionalGeneration.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "        tokenizer = PegasusTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "    elif 't5' in MODEL:\n",
    "        from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "\n",
    "    if verbose : print(model.config)\n",
    "    MODELS[model_name_or_path] = model, tokenizer\n",
    "    if verbose : print('[+] the model is now present in cache\\n')\n",
    "    return MODELS[model_name_or_path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiYIkI5xN2VA"
   },
   "source": [
    "##### Print Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 45112,
     "status": "ok",
     "timestamp": 1610616895257,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "dxsXYs1kN5HX"
   },
   "outputs": [],
   "source": [
    "def print_examples(model_name_list, df, n_examples=10):\n",
    "    \n",
    "    df_examples = df.sample(n_examples, axis='index', random_state=config.SEED)\n",
    "    \n",
    "    for idx, row in df_examples.iterrows():\n",
    "        print(idx)\n",
    "        print(fill(row.text, 100))\n",
    "        print()\n",
    "        for model_name in model_name_list:\n",
    "            model, tokenizer = import_model_tok(model_name)\n",
    "            model = model.to(device)\n",
    "            \n",
    "            summ_enc = model.generate(\n",
    "                tokenizer.encode(row.text, return_tensors='pt').to(device),\n",
    "                min_length = config.ONE_BULLET_MIN_LEN,\n",
    "                max_length = config.ONE_BULLET_MAX_LEN,\n",
    "                length_penalty = config.LENGTH_PENALTY,\n",
    "                num_beams = config.NUM_BEAMS,\n",
    "                no_repeat_ngram_size = config.NO_REPEAT_NGRAM_SIZE,\n",
    "                early_stopping = True)[0]\n",
    "            summ_num_tok = len(summ_enc)\n",
    "            summ = tokenizer.decode(summ_enc, skip_special_tokens=True)\n",
    "\n",
    "            print('Prediction\\n%s (%d tok):\\n'%(model_name, summ_num_tok))\n",
    "            print(fill(summ, 100))\n",
    "            print()\n",
    "            \n",
    "        print('Reference:')\n",
    "        print(fill(row.bullets, 100))\n",
    "        print()\n",
    "        print(''.join(['#']*100))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KV669nVZQnzT"
   },
   "source": [
    "##### Plot Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 45379,
     "status": "ok",
     "timestamp": 1610616895526,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "PrKd0tI7PBi5"
   },
   "outputs": [],
   "source": [
    "def plot_evaluation_bullet_by_bullet(model_name_or_path):\n",
    "    df = pd.read_csv(OUTPUT_PATH+model_name_or_path.replace('/', '?')+\\\n",
    "        '_bullet_by_bullet.csv').set_index(['book', 'chapter'])\n",
    "\n",
    "    prf = ['precision', 'recall', 'fmeasure']\n",
    "    num_rouge = len(config.ROUGE_TYPES)\n",
    "\n",
    "    from matplotlib.cm import get_cmap\n",
    "    color = get_cmap('tab10')(range(num_rouge))\n",
    "    def set_box_color(b, c):\n",
    "        for k in b.keys():\n",
    "            plt.setp(b[k], color=c)\n",
    "    \n",
    "    xticks = 2*np.array(np.arange(1, num_rouge+3))\n",
    "    \n",
    "    box_plt_list = []\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for r, var in zip(prf, np.linspace(-0.15*num_rouge, 0.15*num_rouge, num_rouge)):\n",
    "    \n",
    "        box_plt_list.append(\n",
    "            plt.boxplot(\n",
    "            [df[rouge+'_'+r].tolist() for rouge in config.ROUGE_TYPES],\n",
    "            positions= xticks[:-2]+var,\n",
    "            sym='+',\n",
    "            widths=0.4,\n",
    "            patch_artist=False,\n",
    "            meanline=True,\n",
    "            showmeans=True))\n",
    "        \n",
    "    box_plt_list.append(\n",
    "        plt.boxplot(\n",
    "        df['st_cosine_sim'].tolist(),\n",
    "        positions=[xticks[-2]],\n",
    "        sym='+',\n",
    "        widths=0.4,\n",
    "        patch_artist=False,\n",
    "        meanline=True,\n",
    "        showmeans=True))\n",
    "    \n",
    "    box_plt_list.append(\n",
    "        plt.boxplot(\n",
    "        df['w2v_cosine_sim'].tolist(),\n",
    "        positions=[xticks[-1]],\n",
    "        sym='+',\n",
    "        widths=0.4,\n",
    "        patch_artist=False,\n",
    "        meanline=True,\n",
    "        showmeans=True))\n",
    "\n",
    "    for i, bp in enumerate(box_plt_list[:-2]):\n",
    "        set_box_color(bp, color[i])\n",
    "        plt.plot([], c=color[i], label=prf[i])\n",
    "    plt.legend()\n",
    "\n",
    "    ax.grid(True, axis='y', alpha=0.7, linestyle='--')\n",
    "    ax.set_title('Evaluation Results', fontsize='xx-large')\n",
    "    ax.set_ylabel('Rouge', fontsize='x-large')\n",
    "    plt.xticks(xticks, config.ROUGE_TYPES+['ST Cosine', 'W2V Cosine'], fontsize='x-large')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_grouping_bullets(model_name_or_path):\n",
    "    df = pd.read_csv(OUTPUT_PATH+model_name_or_path.replace('/', '?')+\\\n",
    "        '_grouped.csv').set_index(['book', 'chapter'])\n",
    "\n",
    "    prf = ['precision', 'recall', 'fmeasure']\n",
    "    num_rouge = len(config.ROUGE_TYPES)\n",
    "\n",
    "    from matplotlib.cm import get_cmap\n",
    "    color = get_cmap('tab10')(range(num_rouge))\n",
    "    def set_box_color(b, c):\n",
    "        for k in b.keys():\n",
    "            plt.setp(b[k], color=c)\n",
    "    \n",
    "    xticks = 2*np.array(np.arange(1, num_rouge+3))\n",
    "    \n",
    "    box_plt_list = []\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for r, var in zip(prf, np.linspace(-0.15*num_rouge, 0.15*num_rouge, num_rouge)):\n",
    "    \n",
    "        box_plt_list.append(\n",
    "            plt.boxplot(\n",
    "            [df[rouge+'_'+r].tolist() for rouge in config.ROUGE_TYPES],\n",
    "            positions= xticks[:-2]+var,\n",
    "            sym='+',\n",
    "            widths=0.4,\n",
    "            patch_artist=False,\n",
    "            meanline=True,\n",
    "            showmeans=True))\n",
    "    \n",
    "    box_plt_list.append(\n",
    "        plt.boxplot(\n",
    "        df['st_cosine_sim'].tolist(),\n",
    "        positions=[xticks[-2]],\n",
    "        sym='+',\n",
    "        widths=0.4,\n",
    "        patch_artist=False,\n",
    "        meanline=True,\n",
    "        showmeans=True))\n",
    "    \n",
    "    box_plt_list.append(\n",
    "        plt.boxplot(\n",
    "        df['w2v_cosine_sim'].tolist(),\n",
    "        positions=[xticks[-1]],\n",
    "        sym='+',\n",
    "        widths=0.4,\n",
    "        patch_artist=False,\n",
    "        meanline=True,\n",
    "        showmeans=True))\n",
    "\n",
    "    for i, bp in enumerate(box_plt_list[:-2]):\n",
    "        set_box_color(bp, color[i])\n",
    "        plt.plot([], c=color[i], label=prf[i])\n",
    "    plt.legend()\n",
    "\n",
    "    ax.grid(True, axis='y', alpha=0.7, linestyle='--')\n",
    "    ax.set_title('Evaluation Results', fontsize='xx-large')\n",
    "    ax.set_ylabel('Rouge', fontsize='x-large')\n",
    "    plt.xticks(xticks, config.ROUGE_TYPES+['ST Cosine', 'W2V Cosine'], fontsize='x-large')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb_MdivVauzb"
   },
   "source": [
    "## **Karger Books Para**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 18765,
     "status": "ok",
     "timestamp": 1610616898502,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "K98cwOLys0XX"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(data_dir + 'train.csv').set_index(['book', 'chapter'])\n",
    "df_val = pd.read_csv(data_dir + 'val.csv').set_index(['book', 'chapter'])\n",
    "df_test = pd.read_csv(data_dir + 'test.csv').set_index(['book', 'chapter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpNmnjXn3u2U"
   },
   "source": [
    "### **Print and Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "WIiEnpvbMkJa"
   },
   "source": [
    "##### Print Train Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "addb9bfe4b084b61a77f19962f2919d1",
      "01e3a51fe914437187fb471f042cbc4e",
      "8ef14e9327d84807a95686427993ebcd",
      "0795dec75e4f4a9cad293877eddef047",
      "37bb8348b38e45b981c8e2c580de608b"
     ]
    },
    "executionInfo": {
     "elapsed": 78849,
     "status": "ok",
     "timestamp": 1610617032580,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "hidden": true,
    "id": "wgZKNh2f3u2l",
    "outputId": "fabe9770-ee41-42bc-9726-a13e121e11ab",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9781912776696, 'hh-5')\n",
      "Kaplan-Meier survival curves are often used to compare the data between two groups of subjects.\n",
      "Figure 2.2 shows Kaplan-Meier curves for OS in a randomized study of patients with human epidermal\n",
      "growth factor receptor 2 (HER2)-positive metastatic breast cancer treated either with or without\n",
      "trastuzumab. The Kaplan-Meier curve steps down at time points at which deaths occur, while censored\n",
      "observations are denoted by notches on the curve. In this study, the follow-up period ranged from 3\n",
      "months to 74 months. The Kaplan-Meier curve plots the probability of being event free over time,\n",
      "with these probabilities being estimated from the data in the study. Note that the curve for\n",
      "patients who received trastuzumab is consistently above the curve for those who did not receive\n",
      "trastuzumab, indicating a higher survival probability in that group.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't set num_hidden_layers with value 16 for T5Config {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 1,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 256,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization_aeslc\": {\n",
      "      \"length_penalty\": 0.6,\n",
      "      \"max_length\": 32,\n",
      "      \"max_position_embeddings\": 512\n",
      "    },\n",
      "    \"summarization_arxiv\": {\n",
      "      \"length_penalty\": 0.8,\n",
      "      \"max_length\": 256,\n",
      "      \"max_position_embeddings\": 1024\n",
      "    },\n",
      "    \"summarization_big_patent\": {\n",
      "      \"length_penalty\": 0.7,\n",
      "      \"max_length\": 256,\n",
      "      \"max_position_embeddings\": 1024\n",
      "    },\n",
      "    \"summarization_billsum\": {\n",
      "      \"length_penalty\": 0.6,\n",
      "      \"max_length\": 256,\n",
      "      \"max_position_embeddings\": 1024\n",
      "    },\n",
      "    \"summarization_cnn_dailymail\": {\n",
      "      \"length_penalty\": 0.8,\n",
      "      \"max_length\": 128,\n",
      "      \"max_position_embeddings\": 1024\n",
      "    },\n",
      "    \"summarization_gigaword\": {\n",
      "      \"length_penalty\": 0.6,\n",
      "      \"max_length\": 32,\n",
      "      \"max_position_embeddings\": 128\n",
      "    },\n",
      "    \"summarization_large\": {\n",
      "      \"length_penalty\": 0.8,\n",
      "      \"max_length\": 256,\n",
      "      \"max_position_embeddings\": 1024\n",
      "    },\n",
      "    \"summarization_multi_news\": {\n",
      "      \"length_penalty\": 0.8,\n",
      "      \"max_length\": 256,\n",
      "      \"max_position_embeddings\": 1024\n",
      "    },\n",
      "    \"summarization_newsroom\": {\n",
      "      \"length_penalty\": 0.8,\n",
      "      \"max_length\": 128,\n",
      "      \"max_position_embeddings\": 512\n",
      "    },\n",
      "    \"summarization_pubmed\": {\n",
      "      \"length_penalty\": 0.8,\n",
      "      \"max_length\": 256,\n",
      "      \"max_position_embeddings\": 1024\n",
      "    },\n",
      "    \"summarization_reddit_tifu\": {\n",
      "      \"length_penalty\": 0.6,\n",
      "      \"max_length\": 128,\n",
      "      \"max_position_embeddings\": 512\n",
      "    },\n",
      "    \"summarization_wikihow\": {\n",
      "      \"length_penalty\": 0.6,\n",
      "      \"max_length\": 256,\n",
      "      \"max_position_embeddings\": 512\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 0.8,\n",
      "      \"max_length\": 64,\n",
      "      \"max_position_embeddings\": 512\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.3.0.dev0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9fabde3aa2a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m print_examples([\n\u001b[1;32m      2\u001b[0m     'google/pegasus-large'],\n\u001b[0;32m----> 3\u001b[0;31m     df_train)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-9e6d0b1796ec>\u001b[0m in \u001b[0;36mprint_examples\u001b[0;34m(model_name_list, df, n_examples)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_name_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_model_tok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3a8c89623eac>\u001b[0m in \u001b[0;36mimport_model_tok\u001b[0;34m(model_name_or_path, verbose)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    960\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m                 \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m             )\n\u001b[1;32m    964\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \"\"\"\n\u001b[1;32m    372\u001b[0m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, config_dict, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mreturn_unused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"return_unused_kwargs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pruned_heads\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/t5/configuration_t5.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, d_model, d_kv, d_ff, num_layers, num_decoder_layers, num_heads, relative_attention_num_buckets, dropout_rate, layer_norm_epsilon, initializer_factor, feed_forward_proj, is_encoder_decoder, use_cache, pad_token_id, eos_token_id, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mis_encoder_decoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         )\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't set {} with value {} for {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                 \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't set {} with value {} for {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "print_examples([\n",
    "    'google/pegasus-large'],\n",
    "    df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "fSqOx7kdMoNJ"
   },
   "source": [
    "##### Print Val Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7134,
     "status": "ok",
     "timestamp": 1610617100643,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "hidden": true,
    "id": "f7k8RzJgMoNK",
    "outputId": "34bc6750-7383-42d4-8a7a-f30b9e35a3bb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_examples([\n",
    "    'google/pegasus-large'],\n",
    "    df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "UTUY8QgQa2WM"
   },
   "source": [
    "##### Print Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7745,
     "status": "ok",
     "timestamp": 1610617121036,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "hidden": true,
    "id": "D9rpcejOa2WO",
    "outputId": "c86cf9ed-eca7-4f92-ce7a-91f8661fdd29",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_examples([\n",
    "    'google/pegasus-large'],\n",
    "    df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKYnUXhvMszX"
   },
   "source": [
    "##### Summarize Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1610617165073,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "t3EV4tDfQ1hM"
   },
   "outputs": [],
   "source": [
    "def summarize(model_name_or_path, df, batch_size):\n",
    "    model, tokenizer = import_model_tok(model_name_or_path)\n",
    "    model = model.to(device)\n",
    "\n",
    "    input_ids = tokenizer(df.text.tolist(), return_tensors='pt', truncation=True, padding=True).input_ids\n",
    "    input_ids = input_ids.split(batch_size)\n",
    "\n",
    "    summs = []\n",
    "\n",
    "    pbar = tqdm(total=len(input_ids), \n",
    "                position=0,\n",
    "                leave=True,\n",
    "                file=sys.stdout)\n",
    "    for batch in input_ids:\n",
    "\n",
    "        summ_enc = model.generate(\n",
    "            batch.to(device),\n",
    "            min_length = config.ONE_BULLET_MIN_LEN,\n",
    "            max_length = config.ONE_BULLET_MAX_LEN,\n",
    "            length_penalty = config.LENGTH_PENALTY,\n",
    "            num_beams = config.NUM_BEAMS,\n",
    "            no_repeat_ngram_size = config.NO_REPEAT_NGRAM_SIZE,\n",
    "            early_stopping = True)\n",
    "        summ = tokenizer.batch_decode(summ_enc, skip_special_tokens=True)\n",
    "        summs += summ\n",
    "\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    df['summary'] = summs\n",
    "    \n",
    "    df.to_csv(OUTPUT_PATH+model_name_or_path.replace('/', '?')+'.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89831,
     "status": "ok",
     "timestamp": 1610617255307,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "xQ0KCVkXMszZ",
    "outputId": "d55a74ed-6e68-4e49-8577-9e6125282fb5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 23/49 [1:34:47<1:51:41, 257.75s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3816d4824f39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't5-large'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-a30b98d88eaa>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(model_name_or_path, df, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mnum_beams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_BEAMS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mno_repeat_ngram_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNO_REPEAT_NGRAM_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             early_stopping = True)\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0msumm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumm_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0msumms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msumm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m                 \u001b[0moutput_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m             )\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m             )\n\u001b[1;32m   1588\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1553\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1556\u001b[0m         )\n\u001b[1;32m   1557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, encoder_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    955\u001b[0m                 \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m             )\n\u001b[1;32m    959\u001b[0m             \u001b[0;31m# layer_outputs is a tuple with:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, encoder_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    666\u001b[0m                 \u001b[0mquery_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m             )\n\u001b[1;32m    670\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0mquery_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         )\n\u001b[1;32m    585\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;31m# compute scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         scores = torch.matmul(\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_test = summarize('t5-large', df_test, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoxYNE9nOSYL"
   },
   "source": [
    "### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "referenced_widgets": [
      "a77961a700f2489f8af86af7aecb7a36"
     ]
    },
    "executionInfo": {
     "elapsed": 2055,
     "status": "ok",
     "timestamp": 1610618117862,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "1d6eIfp0OWYh",
    "outputId": "e85b66fd-c178-41a6-a465-394c29fa4ac8"
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_distilroberta = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIcAg1ss4qIR"
   },
   "source": [
    "#### Evaluate summaries bullet by bullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1296,
     "status": "ok",
     "timestamp": 1610618120072,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "NN-p4XvvxPLi"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_bullet_by_bullet(model_name_or_path):\n",
    "    df_eval = pd.read_csv(OUTPUT_PATH+model_name_or_path.replace('/', '?')+\\\n",
    "        '.csv').set_index(['book', 'chapter'])\n",
    "    \n",
    "    rouge_res =\\\n",
    "        df_eval[['bullets', 'summary']]\\\n",
    "        .apply(lambda row:\n",
    "        metric.compute(\n",
    "            predictions = [row[1]],\n",
    "            references = [row[0]],\n",
    "            rouge_types = config.ROUGE_TYPES,\n",
    "            use_agregator = False), axis=1)\n",
    "    for r in config.ROUGE_TYPES:\n",
    "        for i, prf in enumerate(['precision', 'recall', 'fmeasure']):\n",
    "            df_eval[r+'_'+prf] =\\\n",
    "                rouge_res.map(lambda score: 100*score[r][0][i])\n",
    "    \n",
    "    cosine_sim = lambda a, b: (np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b)))\n",
    "    df_eval['st_cosine_sim'] =\\\n",
    "        df_eval[['bullets', 'summary']]\\\n",
    "        .apply(lambda row:\n",
    "        100*cosine_sim(\n",
    "            sentence_distilroberta.encode(row[1]),\n",
    "            sentence_distilroberta.encode(row[0])), axis=1)\n",
    "    \n",
    "    def cosine_sim_w2v(s, b):\n",
    "        s = gensim.utils.simple_preprocess(s, deacc=True)\n",
    "        b = gensim.utils.simple_preprocess(b, deacc=True)\n",
    "        corpus = [s, b]\n",
    "        w2v = gensim.models.Word2Vec(\n",
    "            corpus,\n",
    "            min_count=1,\n",
    "            sg=1,\n",
    "            seed = config.SEED)\n",
    "        s_embed = np.mean([w2v.wv[word] for word in s], axis=0)\n",
    "        b_embed = np.mean([w2v.wv[word] for word in b], axis=0)\n",
    "        return cosine_sim(s_embed, b_embed)\n",
    "        \n",
    "    df_eval['w2v_cosine_sim'] =\\\n",
    "        df_eval[['bullets', 'summary']]\\\n",
    "        .apply(lambda row:\n",
    "        100*cosine_sim_w2v(row[1], row[0]), axis=1)\n",
    "            \n",
    "    df_eval.to_csv(OUTPUT_PATH+model_name_or_path.replace('/', '?')+'_bullet_by_bullet.csv')\n",
    "    \n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model_bullet_by_bullet('t5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate summaries grouping bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_grouping_bullets(model_name_or_path):\n",
    "    df_eval = pd.read_csv(OUTPUT_PATH+model_name_or_path.replace('/', '?')+\\\n",
    "        '.csv').set_index(['book', 'chapter'])\n",
    "    \n",
    "    df_eval = df_eval.groupby(['book', 'chapter'], sort=False).agg({\n",
    "        'text': lambda t: ' '.join(list(t)),\n",
    "        'bullets': lambda b: ' '.join(list(b)),\n",
    "        'summary': lambda s: ' '.join(list(s))})\n",
    "    \n",
    "    rouge_res =\\\n",
    "        df_eval[['bullets', 'summary']]\\\n",
    "        .apply(lambda row:\n",
    "        metric.compute(\n",
    "            predictions = [row[1]],\n",
    "            references = [row[0]],\n",
    "            rouge_types = config.ROUGE_TYPES,\n",
    "            use_agregator = False), axis=1)\n",
    "    for r in config.ROUGE_TYPES:\n",
    "        for i, prf in enumerate(['precision', 'recall', 'fmeasure']):\n",
    "            df_eval[r+'_'+prf] =\\\n",
    "                rouge_res.map(lambda score: 100*score[r][0][i])\n",
    "            \n",
    "    cosine_sim = lambda a, b: (np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b)))\n",
    "    df_eval['st_cosine_sim'] =\\\n",
    "    df_eval[['bullets', 'summary']]\\\n",
    "        .apply(lambda row:\n",
    "        100*cosine_sim(\n",
    "            sentence_distilroberta.encode(row[1]),\n",
    "            sentence_distilroberta.encode(row[0])), axis=1)\n",
    "    \n",
    "    def cosine_sim_w2v(s, b):\n",
    "        s = gensim.utils.simple_preprocess(s, deacc=True)\n",
    "        b = gensim.utils.simple_preprocess(b, deacc=True)\n",
    "        corpus = [s, b]\n",
    "        w2v = gensim.models.Word2Vec(\n",
    "            corpus,\n",
    "            min_count=1,\n",
    "            sg=1,\n",
    "            seed = config.SEED)\n",
    "        s_embed = np.mean([w2v.wv[word] for word in s], axis=0)\n",
    "        b_embed = np.mean([w2v.wv[word] for word in b], axis=0)\n",
    "        return cosine_sim(s_embed, b_embed)\n",
    "        \n",
    "    df_eval['w2v_cosine_sim'] =\\\n",
    "        df_eval[['bullets', 'summary']]\\\n",
    "        .apply(lambda row:\n",
    "        100*cosine_sim_w2v(row[1], row[0]), axis=1)\n",
    "            \n",
    "    df_eval.to_csv(OUTPUT_PATH+model_name_or_path.replace('/', '?')+'_grouped.csv')\n",
    "    \n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model_grouping_bullets('t5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot evaluation bullet by bullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_evaluation_bullet_by_bullet('t5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot evaluation grouping bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation_grouping_bullets('t5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "tvSGvNzvKbvP",
    "S0FByNNOIRvG",
    "JFd0ppeJyX1o",
    "dllOnKR9Os5i",
    "U2mpXoSaQiQE",
    "k0qONrX4Qkkm",
    "JiYIkI5xN2VA",
    "KV669nVZQnzT",
    "WIiEnpvbMkJa",
    "fSqOx7kdMoNJ",
    "UTUY8QgQa2WM",
    "bKYnUXhvMszX",
    "mIcAg1ss4qIR",
    "GlnC1NYkRrQH"
   ],
   "name": "assign_bullets_para.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01e3a51fe914437187fb471f042cbc4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a503362a1d6c49958365ce7e93f4d67d",
       "IPY_MODEL_1249b68a0e2c459eae68dfede2dc5b34"
      ],
      "layout": "IPY_MODEL_9b00208076e345faaa7c1d3b38df79c4"
     }
    },
    "0795dec75e4f4a9cad293877eddef047": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cdd09db5faa14b609970f2a3d37dc430",
       "IPY_MODEL_efc306b702aa4dc0a5e6257cc71ac3aa"
      ],
      "layout": "IPY_MODEL_ec56f7bbfb29414f89c168dfdddd3465"
     }
    },
    "37bb8348b38e45b981c8e2c580de608b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c2fd32ed07b47a89011f42fe95af039",
       "IPY_MODEL_5d0caa7b27f044cb851b38b299d0de86"
      ],
      "layout": "IPY_MODEL_88830eabf77a4c4aa1130c37dc2ffa5d"
     }
    },
    "8ef14e9327d84807a95686427993ebcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3c8709747e444e2e99d5f5d4bd0c4513",
       "IPY_MODEL_8a9da00575624af09987d6936e79f41b"
      ],
      "layout": "IPY_MODEL_e0202968e0864b7ea3ce26fabdde768d"
     }
    },
    "a77961a700f2489f8af86af7aecb7a36": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_23e74f006e7d44e7aee69e3a3394a37d",
       "IPY_MODEL_aa51b0fd1e9648719c0c2b39f252a199"
      ],
      "layout": "IPY_MODEL_66be567c4a0a41e086ac00af108ec408"
     }
    },
    "addb9bfe4b084b61a77f19962f2919d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e6baed9b364643e2a628ec2fa20eda17",
       "IPY_MODEL_1af98602915a4304a38cb231c85076b4"
      ],
      "layout": "IPY_MODEL_0fee59c682d04276b99c86a112570588"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
