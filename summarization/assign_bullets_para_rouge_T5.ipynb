{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 18956,
     "status": "ok",
     "timestamp": 1610616869075,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "au5Z9XQAC7C-"
   },
   "outputs": [],
   "source": [
    "magma_dir = '/home/marco/epfl/magma/'\n",
    "cache_dir = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "magma_dir = '/home/ubuntu/magma/'\n",
    "bucket_dir = '/home/ubuntu/s3/'\n",
    "transformers_dir = '/home/ubuntu/transformers/'\n",
    "cache_dir = bucket_dir+'.cache/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 45120,
     "status": "ok",
     "timestamp": 1610616895249,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "3BHImWfNKpDN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, magma_dir)\n",
    "import config\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 45119,
     "status": "ok",
     "timestamp": 1610616895251,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "82WSp6khIcua"
   },
   "outputs": [],
   "source": [
    "MODEL = 't5'\n",
    "MODELS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 45117,
     "status": "ok",
     "timestamp": 1610616895252,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "5ti6XKYnFzSr"
   },
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "data_dir = magma_dir + 'datasets/karger_books_para_rouge/'+MODEL+'/'\n",
    "\n",
    "# Output path\n",
    "OUTPUT_PATH = magma_dir+'summarization/assign_bullets_para_rouge/'+MODEL+'/'\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFd0ppeJyX1o"
   },
   "source": [
    "### **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 45115,
     "status": "ok",
     "timestamp": 1610616895253,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "yCzod0OizR5U"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from textwrap import fill\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dllOnKR9Os5i"
   },
   "source": [
    "### **Function Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2mpXoSaQiQE"
   },
   "source": [
    "##### Import Model and Tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 45113,
     "status": "ok",
     "timestamp": 1610616895255,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "XhBGMJVFOs5l"
   },
   "outputs": [],
   "source": [
    "def import_model_tok(model_name_or_path, verbose=False):\n",
    "    global MODELS\n",
    "\n",
    "    if model_name_or_path in MODELS.keys():\n",
    "        if verbose : print('[+] model already present in cache\\n')\n",
    "        return MODELS[model_name_or_path]\n",
    "    if verbose : print('[*] importing the model\\n')\n",
    "\n",
    "    if 'bart' in MODEL:\n",
    "        from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "        model = BartForConditionalGeneration.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "        tokenizer = BartTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "    elif 'pegasus' in MODEL:\n",
    "        from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "        model = PegasusForConditionalGeneration.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "        tokenizer = PegasusTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "    elif 't5' in MODEL:\n",
    "        from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "\n",
    "    if verbose : print(model.config)\n",
    "    MODELS[model_name_or_path] = model, tokenizer\n",
    "    if verbose : print('[+] the model is now present in cache\\n')\n",
    "    return MODELS[model_name_or_path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiYIkI5xN2VA"
   },
   "source": [
    "##### Print Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 45112,
     "status": "ok",
     "timestamp": 1610616895257,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "dxsXYs1kN5HX"
   },
   "outputs": [],
   "source": [
    "def print_examples(model_name_list, df, n_examples=10):\n",
    "    \n",
    "    df_examples = df.sample(n_examples, axis='index', random_state=config.SEED)\n",
    "    \n",
    "    for idx, row in df_examples.iterrows():\n",
    "        print(idx)\n",
    "        print(fill(row.text, 100))\n",
    "        print()\n",
    "        for model_name in model_name_list:\n",
    "            model, tokenizer = import_model_tok(model_name)\n",
    "            model = model.to(device)\n",
    "            \n",
    "            summ_enc = model.generate(\n",
    "                tokenizer.encode('summarize: '+row.text,\n",
    "                    return_tensors='pt', truncation=True).to(device),\n",
    "                min_length = config.ONE_BULLET_MIN_LEN,\n",
    "                max_length = config.ONE_BULLET_MAX_LEN,\n",
    "                length_penalty = config.LENGTH_PENALTY,\n",
    "                num_beams = config.NUM_BEAMS,\n",
    "                no_repeat_ngram_size = config.NO_REPEAT_NGRAM_SIZE,\n",
    "                early_stopping = True)[0]\n",
    "            summ_num_tok = len(summ_enc)\n",
    "            summ = tokenizer.decode(summ_enc, skip_special_tokens=True)\n",
    "\n",
    "            print('Prediction\\n%s (%d tok):\\n'%(model_name, summ_num_tok))\n",
    "            print(fill(summ, 100))\n",
    "            print()\n",
    "            \n",
    "        print('Reference:')\n",
    "        print(fill(row.bullets, 100))\n",
    "        print()\n",
    "        print(''.join(['#']*100))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KV669nVZQnzT"
   },
   "source": [
    "##### Plot Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 45379,
     "status": "ok",
     "timestamp": 1610616895526,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "PrKd0tI7PBi5"
   },
   "outputs": [],
   "source": [
    "def plot_evaluation_bullet_by_bullet(model_name_or_path):\n",
    "    df = pd.read_csv(OUTPUT_PATH+model_name_or_path.replace('/', '?')+\\\n",
    "        '_bullet_by_bullet.csv').set_index(['book', 'chapter'])\n",
    "\n",
    "    prf = ['precision', 'recall', 'fmeasure']\n",
    "    num_rouge = len(config.ROUGE_TYPES)\n",
    "\n",
    "    from matplotlib.cm import get_cmap\n",
    "    color = get_cmap('tab10')(range(num_rouge))\n",
    "    def set_box_color(b, c):\n",
    "        for k in b.keys():\n",
    "            plt.setp(b[k], color=c)\n",
    "    \n",
    "    xticks = 2*np.array(np.arange(1, num_rouge+3))\n",
    "    \n",
    "    box_plt_list = []\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for r, var in zip(prf, np.linspace(-0.15*num_rouge, 0.15*num_rouge, num_rouge)):\n",
    "    \n",
    "        box_plt_list.append(\n",
    "            plt.boxplot(\n",
    "            [df[rouge+'_'+r].tolist() for rouge in config.ROUGE_TYPES],\n",
    "            positions= xticks[:-2]+var,\n",
    "            sym='+',\n",
    "            widths=0.4,\n",
    "            patch_artist=False,\n",
    "            meanline=True,\n",
    "            showmeans=True))\n",
    "        \n",
    "    box_plt_list.append(\n",
    "        plt.boxplot(\n",
    "        df['st_cosine_sim'].tolist(),\n",
    "        positions=[xticks[-2]],\n",
    "        sym='+',\n",
    "        widths=0.4,\n",
    "        patch_artist=False,\n",
    "        meanline=True,\n",
    "        showmeans=True))\n",
    "    \n",
    "    box_plt_list.append(\n",
    "        plt.boxplot(\n",
    "        df['w2v_cosine_sim'].tolist(),\n",
    "        positions=[xticks[-1]],\n",
    "        sym='+',\n",
    "        widths=0.4,\n",
    "        patch_artist=False,\n",
    "        meanline=True,\n",
    "        showmeans=True))\n",
    "\n",
    "    for i, bp in enumerate(box_plt_list[:-2]):\n",
    "        set_box_color(bp, color[i])\n",
    "        plt.plot([], c=color[i], label=prf[i])\n",
    "    plt.legend()\n",
    "\n",
    "    ax.grid(True, axis='y', alpha=0.7, linestyle='--')\n",
    "    ax.set_title('Evaluation Results', fontsize='xx-large')\n",
    "    ax.set_ylabel('Rouge', fontsize='x-large')\n",
    "    plt.xticks(xticks, config.ROUGE_TYPES+['ST Cosine', 'W2V Cosine'], fontsize='x-large')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_grouping_bullets(model_name_or_path):\n",
    "    df = pd.read_csv(OUTPUT_PATH+model_name_or_path.replace('/', '?')+\\\n",
    "        '_grouped.csv').set_index(['book', 'chapter'])\n",
    "\n",
    "    prf = ['precision', 'recall', 'fmeasure']\n",
    "    num_rouge = len(config.ROUGE_TYPES)\n",
    "\n",
    "    from matplotlib.cm import get_cmap\n",
    "    color = get_cmap('tab10')(range(num_rouge))\n",
    "    def set_box_color(b, c):\n",
    "        for k in b.keys():\n",
    "            plt.setp(b[k], color=c)\n",
    "    \n",
    "    xticks = 2*np.array(np.arange(1, num_rouge+3))\n",
    "    \n",
    "    box_plt_list = []\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for r, var in zip(prf, np.linspace(-0.15*num_rouge, 0.15*num_rouge, num_rouge)):\n",
    "    \n",
    "        box_plt_list.append(\n",
    "            plt.boxplot(\n",
    "            [df[rouge+'_'+r].tolist() for rouge in config.ROUGE_TYPES],\n",
    "            positions= xticks[:-2]+var,\n",
    "            sym='+',\n",
    "            widths=0.4,\n",
    "            patch_artist=False,\n",
    "            meanline=True,\n",
    "            showmeans=True))\n",
    "    \n",
    "    box_plt_list.append(\n",
    "        plt.boxplot(\n",
    "        df['st_cosine_sim'].tolist(),\n",
    "        positions=[xticks[-2]],\n",
    "        sym='+',\n",
    "        widths=0.4,\n",
    "        patch_artist=False,\n",
    "        meanline=True,\n",
    "        showmeans=True))\n",
    "    \n",
    "    box_plt_list.append(\n",
    "        plt.boxplot(\n",
    "        df['w2v_cosine_sim'].tolist(),\n",
    "        positions=[xticks[-1]],\n",
    "        sym='+',\n",
    "        widths=0.4,\n",
    "        patch_artist=False,\n",
    "        meanline=True,\n",
    "        showmeans=True))\n",
    "\n",
    "    for i, bp in enumerate(box_plt_list[:-2]):\n",
    "        set_box_color(bp, color[i])\n",
    "        plt.plot([], c=color[i], label=prf[i])\n",
    "    plt.legend()\n",
    "\n",
    "    ax.grid(True, axis='y', alpha=0.7, linestyle='--')\n",
    "    ax.set_title('Evaluation Results', fontsize='xx-large')\n",
    "    ax.set_ylabel('Rouge', fontsize='x-large')\n",
    "    plt.xticks(xticks, config.ROUGE_TYPES+['ST Cosine', 'W2V Cosine'], fontsize='x-large')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb_MdivVauzb"
   },
   "source": [
    "## **Karger Books Para**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 18765,
     "status": "ok",
     "timestamp": 1610616898502,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "K98cwOLys0XX"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(data_dir + 'train.csv').set_index(['book', 'chapter'])\n",
    "df_val = pd.read_csv(data_dir + 'val.csv').set_index(['book', 'chapter'])\n",
    "df_test = pd.read_csv(data_dir + 'test.csv').set_index(['book', 'chapter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpNmnjXn3u2U"
   },
   "source": [
    "### **Print and Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIiEnpvbMkJa"
   },
   "source": [
    "##### Print Train Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "addb9bfe4b084b61a77f19962f2919d1",
      "01e3a51fe914437187fb471f042cbc4e",
      "8ef14e9327d84807a95686427993ebcd",
      "0795dec75e4f4a9cad293877eddef047",
      "37bb8348b38e45b981c8e2c580de608b"
     ]
    },
    "executionInfo": {
     "elapsed": 78849,
     "status": "ok",
     "timestamp": 1610617032580,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "wgZKNh2f3u2l",
    "outputId": "fabe9770-ee41-42bc-9726-a13e121e11ab",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9781912776696, 'hh-5')\n",
      "Kaplan-Meier survival curves are often used to compare the data between two groups of subjects.\n",
      "Figure 2.2 shows Kaplan-Meier curves for OS in a randomized study of patients with human epidermal\n",
      "growth factor receptor 2 (HER2)-positive metastatic breast cancer treated either with or without\n",
      "trastuzumab. The Kaplan-Meier curve steps down at time points at which deaths occur, while censored\n",
      "observations are denoted by notches on the curve. In this study, the follow-up period ranged from 3\n",
      "months to 74 months. The Kaplan-Meier curve plots the probability of being event free over time,\n",
      "with these probabilities being estimated from the data in the study. Note that the curve for\n",
      "patients who received trastuzumab is consistently above the curve for those who did not receive\n",
      "trastuzumab, indicating a higher survival probability in that group.\n",
      "\n",
      "Prediction\n",
      "t5-large (60 tok):\n",
      "\n",
      "the Kaplan-Meier curve steps down at time points at which deaths occur. in this study, the follow-up\n",
      "period ranged from 3 months to 74 months. the curve for patients who received trastuzumab is\n",
      "consistently above the curve.\n",
      "\n",
      "Reference:\n",
      "Kaplan-Meier curves plot the probability of being event free over time. The curves from different\n",
      "treatment groups can be plotted against each other to show the differences in outcome.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "(9781908541062, 'ch_9')\n",
      "The acute and chronic consequences of rheumatoid arthritis (RA) result from persistent, misdirected\n",
      "and inadequately controlled inflammation that causes tissue destruction and loss of function.\n",
      "Consequently, management strategies for RA have become highly refined over the past 10 years. The\n",
      "concept of long-term therapy with non-steroidal anti-inflammatory drugs (NSAIDs) or corticosteroids\n",
      "while awaiting a natural remission is no longer acceptable. Rheumatologists today initiate disease-\n",
      "modifying antirheumatic drug (DMARD) therapy as soon as the diagnosis of RA is secure. Subsequently,\n",
      "therapy is aggressively adjusted until a state of low disease activity or remission is achieved. For\n",
      "this reason, it is critical that patients are referred for specialist assessment as soon as the\n",
      "diagnosis of RA is suspected, and even before the results of investigations are available. Early\n",
      "control of inflammation and the disease process is essential to minimize irreversible joint damage\n",
      "and functional disability. Most patients will commence therapy with methotrexate, either alone or\n",
      "ideally in combination with other DMARDs. With aggressive DMARD therapy a proportion of patients\n",
      "will achieve disease remission, and those who do not can now receive biological therapies (see\n",
      "Chapter 9) within a few months of diagnosis. The patient must be fully educated about treatment\n",
      "options and expectations from the outset, and decisions regarding drug therapy must be mutually\n",
      "agreed. Furthermore, the rheumatologist must serve as an advocate for the patient with regard to\n",
      "treatment and drug monitoring programs. In the USA this is particularly important, in light of\n",
      "potential restrictions due to changes in the healthcare reimbursement system and managed care.\n",
      "Economic considerations have also become important in the UK, although recent guidelines from the\n",
      "National Institute for Health and Clinical Excellence (NICE) have reduced some previous restrictions\n",
      "on the prescribing of biologics (see Chapter 9).\n",
      "\n",
      "Prediction\n",
      "t5-large (72 tok):\n",
      "\n",
      "rheumatologists today initiate disease-modifying antirheumatic drug (DMARD) therapy as soon as the\n",
      "diagnosis of RA is secure. with aggressive DMARD therapy a proportion of patients will achieve\n",
      "disease remission. the patient must be fully educated about treatment options and expectations from\n",
      "the start.\n",
      "\n",
      "Reference:\n",
      "Optimal therapy also includes access to allied health professionals such as nurse specialists,\n",
      "physiotherapists, occupational therapists and podiatrists.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "(9781910797495, 'chp8')\n",
      "Patient education. As the majority of PPSP is caused by nerve damage, the treatment options are\n",
      "similar to those for other neuropathic pain syndromes. Pharmacotherapy alone will rarely be\n",
      "sufficient. A biopsychosocial assessment should be made and treatment plans should aim to rectify\n",
      "maladaptive changes in physical, psychological and environmental domains. For example, patients are\n",
      "rarely told that PPSP is a possible complication of surgery. Thus, patients often feel angry and let\n",
      "down and find it difficult to move on unless the anger is addressed with careful explanation and\n",
      "reassurance. Patients may have been stigmatized at work because of a perceived unnecessarily long\n",
      "recovery, requiring input to the workplace. Preoperative patient education about the possibility of\n",
      "PPSP, during the informed consent procedure, may reduce the negative postoperative psychosocial\n",
      "consequences that can complicate treatment. In the authors' experience of patients with PPSP, and\n",
      "supported by several studies, psychosocial factors such as catastrophizing and the pain itself often\n",
      "activate stress and mood disorders, thus worsening pain and interfering with adaptive coping.\n",
      "\n",
      "Prediction\n",
      "t5-large (47 tok):\n",
      "\n",
      "majority of PPSP is caused by nerve damage. psychosocial factors such as catastrophizing and the\n",
      "pain itself often activate stress and mood disorders. pharmacotherapy alone will rarely be\n",
      "sufficient.\n",
      "\n",
      "Reference:\n",
      "The incidence of PPSP varies from about 10% for very limited peripheral surgery, such as inguinal\n",
      "hernia repair or joint replacement, to 10-30% for mastectomy and up to 50% for thoracotomy and\n",
      "amputation.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "(9781908541666, 'ch_12')\n",
      "Treatment. Immediate treatment of any identifiable cause of the DKA should be undertaken without\n",
      "delay. Until recently, standard management of DKA focused on reducing blood glucose levels with\n",
      "insulin and intravenous fluids. There is still universal agreement that the initial cornerstone of\n",
      "treatment is fluid replacement and insulin administration. The intravascular volume should be\n",
      "expanded to correct a typical deficit of 100 mL/kg of water with intravenous isotonic saline, which\n",
      "will also improve renal perfusion. Infusion rates of 1 L per hour for the first hour, 1 L for the\n",
      "next 2 hours and 1 L over the next 4 hours are usually adequate except in those patients with\n",
      "profound hypovolemia and hypotension, who may need twice this amount (Table 11.3). If the initial\n",
      "systolic blood pressure is below 90 mmHg, 500 mL of normal saline should be given over 15 minutes.\n",
      "In the presence of significant hypernatremia, isotonic saline can be replaced with 0.45% saline. The\n",
      "aim is to replace total body deficits of water and electrolytes, with half the estimated deficit\n",
      "being replaced in 12-24 hours (see Table 11.3). Once the blood glucose has fallen to about 14 mmol/L\n",
      "(252 mg/dL), saline should be replaced with 5% dextrose (or an infusion of 10% glucose given at 125\n",
      "mL/hour started alongside the infusion of normal saline to correct circulatory volume) to allow\n",
      "continued insulin administration to suppress ketogenesis while avoiding hypoglycemia.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2274952ea086>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m print_examples([\n\u001b[1;32m      2\u001b[0m     't5-large'],\n\u001b[0;32m----> 3\u001b[0;31m     df_train)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-88228a895e66>\u001b[0m in \u001b[0;36mprint_examples\u001b[0;34m(model_name_list, df, n_examples)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mnum_beams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_BEAMS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mno_repeat_ngram_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNO_REPEAT_NGRAM_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 early_stopping = True)[0]\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0msumm_num_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumm_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0msumm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumm_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m                 \u001b[0moutput_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m             )\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   1639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbeam_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_next_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m             \u001b[0mcur_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_len\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m             model_kwargs = self._update_model_kwargs_for_generation(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_examples([\n",
    "    't5-large'],\n",
    "    df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "fSqOx7kdMoNJ"
   },
   "source": [
    "##### Print Val Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7134,
     "status": "ok",
     "timestamp": 1610617100643,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "hidden": true,
    "id": "f7k8RzJgMoNK",
    "outputId": "34bc6750-7383-42d4-8a7a-f30b9e35a3bb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_examples([\n",
    "    'google/pegasus-large'],\n",
    "    df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "UTUY8QgQa2WM"
   },
   "source": [
    "##### Print Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7745,
     "status": "ok",
     "timestamp": 1610617121036,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "hidden": true,
    "id": "D9rpcejOa2WO",
    "outputId": "c86cf9ed-eca7-4f92-ce7a-91f8661fdd29",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_examples([\n",
    "    'google/pegasus-large'],\n",
    "    df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKYnUXhvMszX"
   },
   "source": [
    "##### Summarize Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1610617165073,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "t3EV4tDfQ1hM"
   },
   "outputs": [],
   "source": [
    "def summarize(model_name_or_path, df, batch_size):\n",
    "    model, tokenizer = import_model_tok(model_name_or_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    text_list = ['summarize: '+t for t in df.text.tolist()]\n",
    "\n",
    "    input_ids = tokenizer(text_list, return_tensors='pt', truncation=True, padding=True).input_ids\n",
    "    input_ids = input_ids.split(batch_size)\n",
    "\n",
    "    summs = []\n",
    "\n",
    "    pbar = tqdm(total=len(input_ids), \n",
    "                position=0,\n",
    "                leave=True,\n",
    "                file=sys.stdout)\n",
    "    for batch in input_ids:\n",
    "\n",
    "        summ_enc = model.generate(\n",
    "            batch.to(device),\n",
    "            min_length = config.ONE_BULLET_MIN_LEN,\n",
    "            max_length = config.ONE_BULLET_MAX_LEN,\n",
    "            length_penalty = config.LENGTH_PENALTY,\n",
    "            num_beams = config.NUM_BEAMS,\n",
    "            no_repeat_ngram_size = config.NO_REPEAT_NGRAM_SIZE,\n",
    "            early_stopping = True)\n",
    "        summ = tokenizer.batch_decode(summ_enc, skip_special_tokens=True)\n",
    "        summs += summ\n",
    "\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    df['summary'] = summs\n",
    "    \n",
    "    df.to_csv(OUTPUT_PATH+model_name_or_path.replace('/', '?')+'.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89831,
     "status": "ok",
     "timestamp": 1610617255307,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "xQ0KCVkXMszZ",
    "outputId": "d55a74ed-6e68-4e49-8577-9e6125282fb5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 23/49 [1:34:47<1:51:41, 257.75s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3816d4824f39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't5-large'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-a30b98d88eaa>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(model_name_or_path, df, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mnum_beams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_BEAMS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mno_repeat_ngram_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNO_REPEAT_NGRAM_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             early_stopping = True)\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0msumm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumm_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0msumms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msumm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m                 \u001b[0moutput_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m             )\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m             )\n\u001b[1;32m   1588\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1553\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1556\u001b[0m         )\n\u001b[1;32m   1557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, encoder_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    955\u001b[0m                 \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m             )\n\u001b[1;32m    959\u001b[0m             \u001b[0;31m# layer_outputs is a tuple with:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, encoder_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    666\u001b[0m                 \u001b[0mquery_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m             )\n\u001b[1;32m    670\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0mquery_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         )\n\u001b[1;32m    585\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;31m# compute scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         scores = torch.matmul(\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_test = summarize('t5-large', df_test, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoxYNE9nOSYL"
   },
   "source": [
    "### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "referenced_widgets": [
      "a77961a700f2489f8af86af7aecb7a36"
     ]
    },
    "executionInfo": {
     "elapsed": 2055,
     "status": "ok",
     "timestamp": 1610618117862,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "1d6eIfp0OWYh",
    "outputId": "e85b66fd-c178-41a6-a465-394c29fa4ac8"
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_distilroberta = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIcAg1ss4qIR"
   },
   "source": [
    "#### Evaluate summaries bullet by bullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1296,
     "status": "ok",
     "timestamp": 1610618120072,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "NN-p4XvvxPLi"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_bullet_by_bullet(model_name_or_path):\n",
    "    df_eval = pd.read_csv(OUTPUT_PATH+model_name_or_path.replace('/', '?')+\\\n",
    "        '.csv').set_index(['book', 'chapter'])\n",
    "    \n",
    "    rouge_res =\\\n",
    "        df_eval[['bullets', 'summary']]\\\n",
    "        .apply(lambda row:\n",
    "        metric.compute(\n",
    "            predictions = [row[1]],\n",
    "            references = [row[0]],\n",
    "            rouge_types = config.ROUGE_TYPES,\n",
    "            use_agregator = False), axis=1)\n",
    "    for r in config.ROUGE_TYPES:\n",
    "        for i, prf in enumerate(['precision', 'recall', 'fmeasure']):\n",
    "            df_eval[r+'_'+prf] =\\\n",
    "                rouge_res.map(lambda score: 100*score[r][0][i])\n",
    "    \n",
    "    cosine_sim = lambda a, b: (np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b)))\n",
    "    df_eval['st_cosine_sim'] =\\\n",
    "        df_eval[['bullets', 'summary']]\\\n",
    "        .apply(lambda row:\n",
    "        100*cosine_sim(\n",
    "            sentence_distilroberta.encode(row[1]),\n",
    "            sentence_distilroberta.encode(row[0])), axis=1)\n",
    "    \n",
    "    def cosine_sim_w2v(s, b):\n",
    "        s = gensim.utils.simple_preprocess(s, deacc=True)\n",
    "        b = gensim.utils.simple_preprocess(b, deacc=True)\n",
    "        corpus = [s, b]\n",
    "        w2v = gensim.models.Word2Vec(\n",
    "            corpus,\n",
    "            min_count=1,\n",
    "            sg=1,\n",
    "            seed = config.SEED)\n",
    "        s_embed = np.mean([w2v.wv[word] for word in s], axis=0)\n",
    "        b_embed = np.mean([w2v.wv[word] for word in b], axis=0)\n",
    "        return cosine_sim(s_embed, b_embed)\n",
    "        \n",
    "    df_eval['w2v_cosine_sim'] =\\\n",
    "        df_eval[['bullets', 'summary']]\\\n",
    "        .apply(lambda row:\n",
    "        100*cosine_sim_w2v(row[1], row[0]), axis=1)\n",
    "            \n",
    "    df_eval.to_csv(OUTPUT_PATH+model_name_or_path.replace('/', '?')+'_bullet_by_bullet.csv')\n",
    "    \n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model_bullet_by_bullet('t5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate summaries grouping bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_grouping_bullets(model_name_or_path):\n",
    "    df_eval = pd.read_csv(OUTPUT_PATH+model_name_or_path.replace('/', '?')+\\\n",
    "        '.csv').set_index(['book', 'chapter'])\n",
    "    \n",
    "    df_eval = df_eval.groupby(['book', 'chapter'], sort=False).agg({\n",
    "        'text': lambda t: ' '.join(list(t)),\n",
    "        'bullets': lambda b: ' '.join(list(b)),\n",
    "        'summary': lambda s: ' '.join(list(s))})\n",
    "    \n",
    "    rouge_res =\\\n",
    "        df_eval[['bullets', 'summary']]\\\n",
    "        .apply(lambda row:\n",
    "        metric.compute(\n",
    "            predictions = [row[1]],\n",
    "            references = [row[0]],\n",
    "            rouge_types = config.ROUGE_TYPES,\n",
    "            use_agregator = False), axis=1)\n",
    "    for r in config.ROUGE_TYPES:\n",
    "        for i, prf in enumerate(['precision', 'recall', 'fmeasure']):\n",
    "            df_eval[r+'_'+prf] =\\\n",
    "                rouge_res.map(lambda score: 100*score[r][0][i])\n",
    "            \n",
    "    cosine_sim = lambda a, b: (np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b)))\n",
    "    df_eval['st_cosine_sim'] =\\\n",
    "    df_eval[['bullets', 'summary']]\\\n",
    "        .apply(lambda row:\n",
    "        100*cosine_sim(\n",
    "            sentence_distilroberta.encode(row[1]),\n",
    "            sentence_distilroberta.encode(row[0])), axis=1)\n",
    "    \n",
    "    def cosine_sim_w2v(s, b):\n",
    "        s = gensim.utils.simple_preprocess(s, deacc=True)\n",
    "        b = gensim.utils.simple_preprocess(b, deacc=True)\n",
    "        corpus = [s, b]\n",
    "        w2v = gensim.models.Word2Vec(\n",
    "            corpus,\n",
    "            min_count=1,\n",
    "            sg=1,\n",
    "            seed = config.SEED)\n",
    "        s_embed = np.mean([w2v.wv[word] for word in s], axis=0)\n",
    "        b_embed = np.mean([w2v.wv[word] for word in b], axis=0)\n",
    "        return cosine_sim(s_embed, b_embed)\n",
    "        \n",
    "    df_eval['w2v_cosine_sim'] =\\\n",
    "        df_eval[['bullets', 'summary']]\\\n",
    "        .apply(lambda row:\n",
    "        100*cosine_sim_w2v(row[1], row[0]), axis=1)\n",
    "            \n",
    "    df_eval.to_csv(OUTPUT_PATH+model_name_or_path.replace('/', '?')+'_grouped.csv')\n",
    "    \n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model_grouping_bullets('t5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot evaluation bullet by bullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_evaluation_bullet_by_bullet('t5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot evaluation grouping bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation_grouping_bullets('t5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "tvSGvNzvKbvP",
    "S0FByNNOIRvG",
    "JFd0ppeJyX1o",
    "dllOnKR9Os5i",
    "U2mpXoSaQiQE",
    "k0qONrX4Qkkm",
    "JiYIkI5xN2VA",
    "KV669nVZQnzT",
    "WIiEnpvbMkJa",
    "fSqOx7kdMoNJ",
    "UTUY8QgQa2WM",
    "bKYnUXhvMszX",
    "mIcAg1ss4qIR",
    "GlnC1NYkRrQH"
   ],
   "name": "assign_bullets_para.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01e3a51fe914437187fb471f042cbc4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a503362a1d6c49958365ce7e93f4d67d",
       "IPY_MODEL_1249b68a0e2c459eae68dfede2dc5b34"
      ],
      "layout": "IPY_MODEL_9b00208076e345faaa7c1d3b38df79c4"
     }
    },
    "0795dec75e4f4a9cad293877eddef047": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cdd09db5faa14b609970f2a3d37dc430",
       "IPY_MODEL_efc306b702aa4dc0a5e6257cc71ac3aa"
      ],
      "layout": "IPY_MODEL_ec56f7bbfb29414f89c168dfdddd3465"
     }
    },
    "37bb8348b38e45b981c8e2c580de608b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c2fd32ed07b47a89011f42fe95af039",
       "IPY_MODEL_5d0caa7b27f044cb851b38b299d0de86"
      ],
      "layout": "IPY_MODEL_88830eabf77a4c4aa1130c37dc2ffa5d"
     }
    },
    "8ef14e9327d84807a95686427993ebcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3c8709747e444e2e99d5f5d4bd0c4513",
       "IPY_MODEL_8a9da00575624af09987d6936e79f41b"
      ],
      "layout": "IPY_MODEL_e0202968e0864b7ea3ce26fabdde768d"
     }
    },
    "a77961a700f2489f8af86af7aecb7a36": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_23e74f006e7d44e7aee69e3a3394a37d",
       "IPY_MODEL_aa51b0fd1e9648719c0c2b39f252a199"
      ],
      "layout": "IPY_MODEL_66be567c4a0a41e086ac00af108ec408"
     }
    },
    "addb9bfe4b084b61a77f19962f2919d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e6baed9b364643e2a628ec2fa20eda17",
       "IPY_MODEL_1af98602915a4304a38cb231c85076b4"
      ],
      "layout": "IPY_MODEL_0fee59c682d04276b99c86a112570588"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
