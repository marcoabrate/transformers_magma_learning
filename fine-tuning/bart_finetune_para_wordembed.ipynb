{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 30194,
     "status": "ok",
     "timestamp": 1610616621856,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "au5Z9XQAC7C-"
   },
   "outputs": [],
   "source": [
    "magma_dir = '/home/ubuntu/magma/'\n",
    "bucket_dir = '/home/ubuntu/s3/'\n",
    "transformers_dir = '/home/ubuntu/transformers/'\n",
    "cache_dir = bucket_dir+'.cache/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EddY1WDNsKlS"
   },
   "source": [
    "## **Fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 85455,
     "status": "ok",
     "timestamp": 1610616677129,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "2d6M41X9AKBi"
   },
   "outputs": [],
   "source": [
    "finetune_script = '\"'+transformers_dir+'examples/seq2seq/finetune_trainer.py\"'\n",
    "eval_script = '\"'+transformers_dir+'examples/seq2seq/run_eval.py\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "executionInfo": {
     "elapsed": 113930,
     "status": "ok",
     "timestamp": 1610616705611,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "82WSp6khIcua",
    "outputId": "54de6794-ef3c-41f4-b0e3-e108cfc4e333",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=finetune_para_wordembed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, magma_dir)\n",
    "import config\n",
    "\n",
    "import torch\n",
    "torch.manual_seed = config.SEED\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "project_name = 'finetune_para_wordembed'\n",
    "%env WANDB_PROJECT=$project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPZ7A-sBVOam"
   },
   "source": [
    "### Karger Books Para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 113928,
     "status": "ok",
     "timestamp": 1610616705613,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "vJOYl_g6F1e2"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = 'sshleifer/distilbart-cnn-12-6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 113926,
     "status": "ok",
     "timestamp": 1610616705616,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "4xs_XkbCVOar"
   },
   "outputs": [],
   "source": [
    "data_dir = '\"'+bucket_dir+'datasets/karger_books_para_wordembed/bart/st/\"'\n",
    "\n",
    "output_dir = '\"'+bucket_dir+'fine-tuning/'+\\\n",
    "    model_name_or_path.replace('/', '?')+'_karger_books_para_wordembed_train/\"'\n",
    "\n",
    "log_dir = bucket_dir + '/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "referenced_widgets": [
      "88734567ccb2435091eeab9f30b2de9b"
     ]
    },
    "executionInfo": {
     "elapsed": 120037,
     "status": "ok",
     "timestamp": 1610616711730,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "wtLC1O1ZJpU3",
    "outputId": "16632326-7ab5-4ef4-aa65-44efbdf45fb3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/%22/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_traincheckpoint-325%22/resolve/main/config.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for '\"/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_traincheckpoint-325\"'. Make sure that:\n\n- '\"/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_traincheckpoint-325\"' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '\"/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_traincheckpoint-325\"' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/transformers/src/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             )\n",
      "\u001b[0;32m~/transformers/src/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m         )\n",
      "\u001b[0;32m~/transformers/src/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/magma/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/%22/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_traincheckpoint-325%22/resolve/main/config.json",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ce8d7098e7d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mONE_BULLET_MIN_LEN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mONE_BULLET_MAX_LEN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLENGTH_PENALTY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/transformers/src/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m'foo'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/transformers/src/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             )\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for '\"/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_traincheckpoint-325\"'. Make sure that:\n\n- '\"/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_traincheckpoint-325\"' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '\"/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_traincheckpoint-325\"' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "model_config = AutoConfig.from_pretrained(model_name_or_path, use_cache=False)\n",
    "model_config.min_length = config.ONE_BULLET_MIN_LEN\n",
    "model_config.max_length = config.ONE_BULLET_MAX_LEN\n",
    "model_config.length_penalty = config.LENGTH_PENALTY\n",
    "model_config.no_repeat_ngram_size = config.NO_REPEAT_NGRAM_SIZE\n",
    "\n",
    "model_config.task_specific_params['summarization']['min_length'] = config.ONE_BULLET_MIN_LEN\n",
    "model_config.task_specific_params['summarization']['max_length'] = config.ONE_BULLET_MAX_LEN\n",
    "model_config.task_specific_params['summarization']['length_penalty'] = config.LENGTH_PENALTY\n",
    "model_config.task_specific_params['summarization']['no_repeat_ngram_size'] = config.NO_REPEAT_NGRAM_SIZE\n",
    "model_config_dir = '\"'+bucket_dir+'fine-tuning/'+\\\n",
    "    model_name_or_path.replace('/', '?')+'_config\"'\n",
    "model_config.save_pretrained(model_config_dir[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M58yiP1yVOav"
   },
   "source": [
    "##### Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = output_dir[:-1] + 'checkpoint-325\"'\n",
    "model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a66a5d76c946d995f384618342f3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379af8886a9b4bedb48fb304c828b037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0789825e11e4b0bbae91a067d16881c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325/tokenizer_config.json',\n",
       " '/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325/special_tokens_map.json',\n",
       " '/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325/vocab.json',\n",
       " '/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325/merges.txt',\n",
       " '/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6', use_cache=False)\n",
    "tok.save_pretrained(model_name_or_path[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2522396,
     "status": "ok",
     "timestamp": 1610555308072,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "K66kY1WOVOaw",
    "outputId": "11421fe6-93e5-461f-8b97-0615b4c2e7af",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/22/2021 11:53:54 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "01/22/2021 11:53:54 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jan22_11-53-54_ip-172-31-39-35', logging_first_step=False, logging_steps=10, save_steps=100, save_total_limit=3, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=25, dataloader_num_workers=0, past_index=-1, run_name='/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='rougeL', greater_is_better='True', ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False, label_smoothing=0.1, sortish_sampler=True, predict_with_generate=True, adafactor=False, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear')\n",
      "[INFO|configuration_utils.py:429] 2021-01-22 11:53:54,675 >> loading configuration file /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_config/config.json\n",
      "[INFO|configuration_utils.py:467] 2021-01-22 11:53:54,676 >> Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 150,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 10,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 5,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 1,\n",
      "      \"max_length\": 150,\n",
      "      \"min_length\": 10,\n",
      "      \"no_repeat_ngram_size\": 5,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:429] 2021-01-22 11:53:54,739 >> loading configuration file /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325/config.json\n",
      "[INFO|configuration_utils.py:467] 2021-01-22 11:53:54,740 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"sshleifer/distilbart-cnn-12-6\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 150,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 10,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 5,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 1,\n",
      "      \"max_length\": 150,\n",
      "      \"min_length\": 10,\n",
      "      \"no_repeat_ngram_size\": 5,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1721] 2021-01-22 11:53:54,740 >> Model name '/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming '/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[INFO|tokenization_utils_base.py:1754] 2021-01-22 11:53:54,811 >> Didn't find file /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1754] 2021-01-22 11:53:54,856 >> Didn't find file /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1800] 2021-01-22 11:53:54,882 >> loading file /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1800] 2021-01-22 11:53:54,882 >> loading file /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1800] 2021-01-22 11:53:54,882 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1800] 2021-01-22 11:53:54,882 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1800] 2021-01-22 11:53:54,882 >> loading file /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1800] 2021-01-22 11:53:54,882 >> loading file /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1022] 2021-01-22 11:53:55,191 >> loading weights file /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1140] 2021-01-22 11:54:46,609 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-01-22 11:54:46,609 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-325.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "01/22/2021 11:54:46 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 1, 'max_length': 150, 'min_length': 10, 'no_repeat_ngram_size': 5, 'num_beams': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/22/2021 11:54:50 - INFO - __main__ -   *** Train ***\n",
      "[INFO|trainer.py:703] 2021-01-22 11:55:21,580 >> ***** Running training *****\n",
      "[INFO|trainer.py:704] 2021-01-22 11:55:21,580 >>   Num examples = 2046\n",
      "[INFO|trainer.py:705] 2021-01-22 11:55:21,580 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:706] 2021-01-22 11:55:21,580 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:707] 2021-01-22 11:55:21,580 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:708] 2021-01-22 11:55:21,580 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:709] 2021-01-22 11:55:21,580 >>   Total optimization steps = 640\n",
      "[INFO|trainer.py:725] 2021-01-22 11:55:21,619 >>   Continuing training from checkpoint, will skip to saved global_step\n",
      "[INFO|trainer.py:726] 2021-01-22 11:55:21,619 >>   Continuing training from epoch 2\n",
      "[INFO|trainer.py:727] 2021-01-22 11:55:21,619 >>   Continuing training from global step 325\n",
      "[INFO|trainer.py:730] 2021-01-22 11:55:21,619 >>   Will skip the first 2 epochs then the first 69 batches in the first epoch.\n",
      "/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/\n",
      "[INFO|integrations.py:371] 2021-01-22 11:55:21,623 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/finetune_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/finetune_para_wordembed/runs/2mtdks3v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/magma/fine-tuning/wandb/run-20210122_115522-2mtdks3v\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      " 52%|█████▏    | 330/640 [00:02<00:01, 205.82it/s]125e-05, 'epoch': 2.578125}\n",
      " 53%|█████▎    | 340/640 [00:06<00:01, 205.82it/s]{'loss': 1802.1068359375, 'learning_rate': 1.40625e-05, 'epoch': 2.65625}\n",
      " 55%|█████▍    | 350/640 [00:09<00:09, 31.18it/s]{'loss': 1612.099609375, 'learning_rate': 1.359375e-05, 'epoch': 2.734375}\n",
      "[INFO|trainer.py:1412] 2021-01-22 11:55:32,546 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-22 11:55:32,547 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-22 11:55:32,548 >>   Batch size = 16\n",
      " 55%|█████▍    | 350/640 [00:19<00:09, 31.18it/s]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:17<02:09,  8.63s/it]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:26<02:05,  8.94s/it]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:42<02:27, 11.34s/it]\u001b[A\n",
      " 29%|██▉       | 5/17 [01:04<03:02, 15.17s/it]\u001b[A\n",
      " 35%|███▌      | 6/17 [01:11<02:18, 12.63s/it]\u001b[A\n",
      " 41%|████      | 7/17 [01:19<01:49, 10.94s/it]\u001b[A\n",
      " 47%|████▋     | 8/17 [01:29<01:36, 10.67s/it]\u001b[A\n",
      " 53%|█████▎    | 9/17 [01:40<01:26, 10.78s/it]\u001b[A\n",
      " 59%|█████▉    | 10/17 [01:51<01:16, 10.87s/it]\u001b[A\n",
      " 65%|██████▍   | 11/17 [02:09<01:19, 13.21s/it]\u001b[A\n",
      " 71%|███████   | 12/17 [02:24<01:08, 13.68s/it]\u001b[A\n",
      " 76%|███████▋  | 13/17 [02:34<00:50, 12.55s/it]\u001b[A\n",
      " 82%|████████▏ | 14/17 [02:43<00:33, 11.32s/it]\u001b[A\n",
      " 88%|████████▊ | 15/17 [02:53<00:21, 10.93s/it]\u001b[A\n",
      " 94%|█████████▍| 16/17 [03:03<00:10, 10.74s/it]\u001b[A\n",
      "{'eval_loss': 2075.361083984375, 'eval_rouge1': 32.4866, 'eval_rouge2': 14.0727, 'eval_rougeL': 26.0868, 'eval_rougeLsum': 26.6902, 'eval_gen_len': 34.5, 'epoch': 2.734375}\n",
      " 55%|█████▍    | 350/640 [03:35<00:09, 31.18it/s]\n",
      "100%|██████████| 17/17 [03:11<00:00,  9.53s/it]\u001b[A\n",
      "                                               \u001b[A\n",
      "[INFO|trainer.py:1226] 2021-01-22 11:58:58,555 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-350\n",
      "[INFO|configuration_utils.py:289] 2021-01-22 11:58:58,697 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-350/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-22 11:59:40,511 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-350/pytorch_model.bin\n",
      "[INFO|trainer.py:1285] 2021-01-22 12:00:11,567 >> Deleting older checkpoint [/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-350] due to args.save_total_limit\n",
      " 56%|█████▋    | 360/640 [04:52<07:08,  1.53s/it]{'loss': 1688.9291015625, 'learning_rate': 1.3125e-05, 'epoch': 2.8125}\n",
      "{'loss': 1854.6865234375, 'learning_rate': 1.2656250000000001e-05, 'epoch': 2.890625}\n",
      " 59%|█████▊    | 375/640 [04:59<03:23,  1.30it/s][INFO|trainer.py:1412] 2021-01-22 12:00:21,982 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-22 12:00:21,982 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-22 12:00:21,982 >>   Batch size = 16\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:15<01:57,  7.81s/it]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:29<02:26, 10.47s/it]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:49<03:00, 13.91s/it]\u001b[A\n",
      " 29%|██▉       | 5/17 [01:12<03:23, 16.95s/it]\u001b[A\n",
      " 35%|███▌      | 6/17 [01:24<02:48, 15.30s/it]\u001b[A\n",
      " 41%|████      | 7/17 [01:31<02:07, 12.76s/it]\u001b[A\n",
      " 47%|████▋     | 8/17 [01:41<01:47, 11.94s/it]\u001b[A\n",
      " 53%|█████▎    | 9/17 [01:55<01:39, 12.44s/it]\u001b[A\n",
      " 59%|█████▉    | 10/17 [02:10<01:32, 13.18s/it]\u001b[A\n",
      " 65%|██████▍   | 11/17 [02:28<01:29, 14.84s/it]\u001b[A\n",
      " 71%|███████   | 12/17 [02:43<01:14, 14.82s/it]\u001b[A\n",
      " 76%|███████▋  | 13/17 [02:56<00:57, 14.41s/it]\u001b[A\n",
      " 82%|████████▏ | 14/17 [03:09<00:41, 13.90s/it]\u001b[A\n",
      " 88%|████████▊ | 15/17 [03:22<00:27, 13.66s/it]\u001b[A\n",
      " 94%|█████████▍| 16/17 [03:36<00:13, 13.68s/it]\u001b[A\n",
      "                                                 A\n",
      " 59%|█████▊    | 375/640 [08:54<03:23,  1.30it/s]\n",
      "100%|██████████| 17/17 [03:44<00:00, 11.77s/it]\u001b[A\n",
      "                                               \u001b[A{'eval_loss': 2078.59228515625, 'eval_rouge1': 32.7486, 'eval_rouge2': 14.1637, 'eval_rougeL': 26.2771, 'eval_rougeLsum': 27.0272, 'eval_gen_len': 37.4, 'epoch': 2.9296875}\n",
      "[INFO|trainer.py:1226] 2021-01-22 12:04:17,584 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-375\n",
      "[INFO|configuration_utils.py:289] 2021-01-22 12:04:17,746 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-375/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-22 12:05:08,720 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-375/pytorch_model.bin\n",
      "[INFO|trainer.py:1285] 2021-01-22 12:05:38,816 >> Deleting older checkpoint [/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-375] due to args.save_total_limit\n",
      "                                                   {'loss': 1834.32265625, 'learning_rate': 1.21875e-05, 'epoch': 2.96875}\n",
      " 61%|██████    | 390/640 [10:23<05:11,  1.24s/it]{'loss': 1691.0076171875, 'learning_rate': 1.171875e-05, 'epoch': 3.046875}\n",
      "                                                 {'loss': 1565.751953125, 'learning_rate': 1.125e-05, 'epoch': 3.125}\n",
      " 62%|██████▎   | 400/640 [10:26<01:26,  2.77it/s][INFO|trainer.py:1412] 2021-01-22 12:05:49,501 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-22 12:05:49,501 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-22 12:05:49,501 >>   Batch size = 16\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:15<01:58,  7.87s/it]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:25<02:00,  8.60s/it]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:36<02:05,  9.64s/it]\u001b[A\n",
      " 29%|██▉       | 5/17 [00:58<02:47, 13.94s/it]\u001b[A\n",
      " 35%|███▌      | 6/17 [01:05<02:08, 11.68s/it]\u001b[A\n",
      " 41%|████      | 7/17 [01:14<01:45, 10.56s/it]\u001b[A\n",
      " 47%|████▋     | 8/17 [01:23<01:31, 10.20s/it]\u001b[A\n",
      " 53%|█████▎    | 9/17 [01:39<01:36, 12.11s/it]\u001b[A\n",
      " 59%|█████▉    | 10/17 [01:48<01:17, 11.00s/it]\u001b[A\n",
      " 65%|██████▍   | 11/17 [02:06<01:18, 13.13s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 12/17 [02:19<01:05, 13.05s/it]\u001b[A\n",
      " 76%|███████▋  | 13/17 [02:34<00:55, 13.77s/it]\u001b[A\n",
      " 82%|████████▏ | 14/17 [02:44<00:37, 12.65s/it]\u001b[A\n",
      " 88%|████████▊ | 15/17 [02:55<00:23, 11.95s/it]\u001b[A\n",
      " 94%|█████████▍| 16/17 [03:03<00:10, 10.99s/it]\u001b[A\n",
      "                                                 {'eval_loss': 2083.360107421875, 'eval_rouge1': 32.0908, 'eval_rouge2': 13.5278, 'eval_rougeL': 25.7769, 'eval_rougeLsum': 26.3739, 'eval_gen_len': 35.1, 'epoch': 3.125}\n",
      " 62%|██████▎   | 400/640 [13:53<01:26,  2.77it/s]\n",
      "100%|██████████| 17/17 [03:16<00:00, 11.32s/it]\u001b[A\n",
      "                                               \u001b[A\n",
      "[INFO|trainer.py:1226] 2021-01-22 12:09:16,909 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-400\n",
      "[INFO|configuration_utils.py:289] 2021-01-22 12:09:17,062 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-400/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-22 12:10:01,822 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-400/pytorch_model.bin\n",
      "[INFO|trainer.py:1285] 2021-01-22 12:10:36,969 >> Deleting older checkpoint [/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 1689.078125, 'learning_rate': 1.078125e-05, 'epoch': 3.203125}\n",
      "                                                 {'loss': 1678.342578125, 'learning_rate': 1.03125e-05, 'epoch': 3.28125}\n",
      " 66%|██████▋   | 425/640 [15:23<01:20,  2.68it/s][INFO|trainer.py:1412] 2021-01-22 12:10:46,256 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-22 12:10:46,256 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-22 12:10:46,256 >>   Batch size = 16\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:15<01:57,  7.83s/it]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:24<01:58,  8.45s/it]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:38<02:12, 10.20s/it]\u001b[A\n",
      " 29%|██▉       | 5/17 [01:00<02:51, 14.30s/it]\u001b[A\n",
      " 35%|███▌      | 6/17 [01:07<02:11, 11.92s/it]\u001b[A\n",
      " 41%|████      | 7/17 [01:16<01:50, 11.06s/it]\u001b[A\n",
      " 47%|████▋     | 8/17 [01:26<01:37, 10.84s/it]\u001b[A\n",
      " 53%|█████▎    | 9/17 [01:41<01:35, 11.94s/it]\u001b[A\n",
      " 59%|█████▉    | 10/17 [01:51<01:20, 11.47s/it]\u001b[A\n",
      " 65%|██████▍   | 11/17 [02:08<01:17, 13.00s/it]\u001b[A\n",
      " 71%|███████   | 12/17 [02:21<01:05, 13.04s/it]\u001b[A\n",
      " 76%|███████▋  | 13/17 [02:36<00:55, 13.76s/it]\u001b[A\n",
      " 82%|████████▏ | 14/17 [02:46<00:37, 12.64s/it]\u001b[A\n",
      " 88%|████████▊ | 15/17 [02:59<00:25, 12.62s/it]\u001b[A\n",
      " 94%|█████████▍| 16/17 [03:16<00:13, 13.88s/it]\u001b[A\n",
      "                                                 A\n",
      " 66%|██████▋   | 425/640 [19:00<01:20,  2.68it/s]\n",
      "100%|██████████| 17/17 [03:25<00:00, 12.14s/it]\u001b[A\n",
      "                                               \u001b[A{'eval_loss': 2080.270263671875, 'eval_rouge1': 32.5607, 'eval_rouge2': 13.8311, 'eval_rougeL': 26.0416, 'eval_rougeLsum': 26.6893, 'eval_gen_len': 36.9, 'epoch': 3.3203125}\n",
      "[INFO|trainer.py:1226] 2021-01-22 12:14:23,318 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-425\n",
      "[INFO|configuration_utils.py:289] 2021-01-22 12:14:23,485 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-425/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-22 12:15:18,275 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-425/pytorch_model.bin\n",
      "[INFO|trainer.py:1285] 2021-01-22 12:15:46,031 >> Deleting older checkpoint [/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-425] due to args.save_total_limit\n",
      " 67%|██████▋   | 430/640 [20:25<1:17:06, 22.03s/it]{'loss': 1620.6169921875, 'learning_rate': 9.84375e-06, 'epoch': 3.359375}\n",
      " 69%|██████▉   | 440/640 [20:29<03:13,  1.03it/s]{'loss': 1784.5498046875, 'learning_rate': 9.375000000000001e-06, 'epoch': 3.4375}\n",
      "{'loss': 1749.7634765625, 'learning_rate': 8.90625e-06, 'epoch': 3.515625}\n",
      " 70%|███████   | 450/640 [20:32<01:12,  2.63it/s][INFO|trainer.py:1412] 2021-01-22 12:15:55,957 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-22 12:15:55,957 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-22 12:15:55,957 >>   Batch size = 16\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:19<02:22,  9.52s/it]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:31<02:32, 10.86s/it]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:48<02:49, 13.01s/it]\u001b[A\n",
      " 29%|██▉       | 5/17 [01:10<03:12, 16.02s/it]\u001b[A\n",
      " 35%|███▌      | 6/17 [01:19<02:31, 13.81s/it]\u001b[A\n",
      " 41%|████      | 7/17 [01:38<02:33, 15.40s/it]\u001b[A\n",
      " 47%|████▋     | 8/17 [01:46<01:59, 13.32s/it]\u001b[A\n",
      " 53%|█████▎    | 9/17 [02:03<01:54, 14.29s/it]\u001b[A\n",
      " 59%|█████▉    | 10/17 [02:13<01:31, 13.11s/it]\u001b[A\n",
      " 65%|██████▍   | 11/17 [02:35<01:33, 15.56s/it]\u001b[A\n",
      " 71%|███████   | 12/17 [02:48<01:14, 14.87s/it]\u001b[A\n",
      " 76%|███████▋  | 13/17 [03:02<00:58, 14.70s/it]\u001b[A\n",
      " 82%|████████▏ | 14/17 [03:12<00:39, 13.30s/it]\u001b[A\n",
      " 88%|████████▊ | 15/17 [03:27<00:27, 13.76s/it]\u001b[A\n",
      " 94%|█████████▍| 16/17 [03:36<00:12, 12.29s/it]\u001b[A\n",
      "                                                 A\n",
      " 70%|███████   | 450/640 [24:28<01:12,  2.63it/s]31.5539, 'eval_rouge2': 13.1205, 'eval_rougeL': 24.8174, 'eval_rougeLsum': 25.6641, 'eval_gen_len': 35.9, 'epoch': 3.515625}\n",
      "100%|██████████| 17/17 [03:44<00:00, 10.65s/it]\u001b[A\n",
      "\n",
      "                                               \u001b[A[INFO|trainer.py:1226] 2021-01-22 12:19:51,939 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-450\n",
      "[INFO|configuration_utils.py:289] 2021-01-22 12:19:52,079 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-450/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-22 12:20:37,445 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-450/pytorch_model.bin\n",
      "[INFO|trainer.py:1285] 2021-01-22 12:21:06,605 >> Deleting older checkpoint [/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-450] due to args.save_total_limit\n",
      "                                                 {'loss': 1805.4365234375, 'learning_rate': 8.4375e-06, 'epoch': 3.59375}\n",
      " 73%|███████▎  | 470/640 [25:52<01:14,  2.29it/s]{'loss': 1686.7091796875, 'learning_rate': 7.96875e-06, 'epoch': 3.671875}\n",
      " 74%|███████▍  | 475/640 [25:53<01:00,  2.73it/s][INFO|trainer.py:1412] 2021-01-22 12:21:16,884 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-22 12:21:16,884 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-22 12:21:16,884 >>   Batch size = 16\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:14<01:47,  7.15s/it]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:27<02:13,  9.53s/it]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:47<02:54, 13.46s/it]\u001b[A\n",
      " 29%|██▉       | 5/17 [01:09<03:16, 16.40s/it]\u001b[A\n",
      " 35%|███▌      | 6/17 [01:16<02:28, 13.47s/it]\u001b[A\n",
      " 41%|████      | 7/17 [01:24<01:55, 11.53s/it]\u001b[A\n",
      " 47%|████▋     | 8/17 [01:32<01:36, 10.68s/it]\u001b[A\n",
      " 53%|█████▎    | 9/17 [01:49<01:39, 12.46s/it]\u001b[A\n",
      " 59%|█████▉    | 10/17 [02:02<01:29, 12.73s/it]\u001b[A\n",
      " 65%|██████▍   | 11/17 [02:15<01:17, 12.86s/it]\u001b[A\n",
      " 71%|███████   | 12/17 [02:28<01:04, 12.90s/it]\u001b[A\n",
      " 76%|███████▋  | 13/17 [02:41<00:51, 12.94s/it]\u001b[A\n",
      " 82%|████████▏ | 14/17 [02:50<00:35, 11.74s/it]\u001b[A\n",
      " 88%|████████▊ | 15/17 [03:03<00:24, 12.02s/it]\u001b[A\n",
      " 94%|█████████▍| 16/17 [03:11<00:10, 10.89s/it]\u001b[A\n",
      "                                                 {'eval_loss': 2074.907470703125, 'eval_rouge1': 32.8771, 'eval_rouge2': 14.1881, 'eval_rougeL': 26.4406, 'eval_rougeLsum': 27.0058, 'eval_gen_len': 35.8, 'epoch': 3.7109375}\n",
      " 74%|███████▍  | 475/640 [29:32<01:00,  2.73it/s]\n",
      "100%|██████████| 17/17 [03:18<00:00,  9.38s/it]\u001b[A\n",
      "                                               \u001b[A\n",
      "[INFO|trainer.py:1226] 2021-01-22 12:24:55,385 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:289] 2021-01-22 12:24:55,551 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-475/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-22 12:25:41,974 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-475/pytorch_model.bin\n",
      "[INFO|trainer.py:1285] 2021-01-22 12:26:11,372 >> Deleting older checkpoint [/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-475] due to args.save_total_limit\n",
      "{'loss': 1628.46650390625, 'learning_rate': 7.5e-06, 'epoch': 3.75}\n",
      "                                                 {'loss': 1957.0677734375, 'learning_rate': 7.03125e-06, 'epoch': 3.828125}\n",
      " 78%|███████▊  | 500/640 [30:59<00:50,  2.77it/s][INFO|trainer.py:1412] 2021-01-22 12:26:22,375 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-22 12:26:22,375 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-22 12:26:22,375 >>   Batch size = 16\n",
      "\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:21<02:38, 10.56s/it]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:31<02:24, 10.29s/it]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:47<02:45, 12.73s/it]\u001b[A\n",
      " 29%|██▉       | 5/17 [01:01<02:38, 13.21s/it]\u001b[A\n",
      " 35%|███▌      | 6/17 [01:09<02:03, 11.24s/it]\u001b[A\n",
      " 41%|████      | 7/17 [01:17<01:42, 10.27s/it]\u001b[A\n",
      " 47%|████▋     | 8/17 [01:32<01:44, 11.62s/it]\u001b[A\n",
      " 53%|█████▎    | 9/17 [01:45<01:38, 12.34s/it]\u001b[A\n",
      " 59%|█████▉    | 10/17 [01:59<01:28, 12.64s/it]\u001b[A\n",
      " 65%|██████▍   | 11/17 [02:15<01:22, 13.68s/it]\u001b[A\n",
      " 71%|███████   | 12/17 [02:25<01:03, 12.65s/it]\u001b[A\n",
      " 76%|███████▋  | 13/17 [02:36<00:48, 12.00s/it]\u001b[A\n",
      " 82%|████████▏ | 14/17 [02:48<00:36, 12.23s/it]\u001b[A\n",
      " 88%|████████▊ | 15/17 [03:01<00:24, 12.35s/it]\u001b[A\n",
      " 94%|█████████▍| 16/17 [03:09<00:11, 11.12s/it]\u001b[A\n",
      "                                                 {'eval_loss': 2075.878173828125, 'eval_rouge1': 32.635, 'eval_rouge2': 13.723, 'eval_rougeL': 25.8008, 'eval_rougeLsum': 26.4789, 'eval_gen_len': 35.2, 'epoch': 3.90625}\n",
      " 78%|███████▊  | 500/640 [34:31<00:50,  2.77it/s]\n",
      "100%|██████████| 17/17 [03:19<00:00, 10.43s/it]\u001b[A\n",
      "                                               \u001b[A\n",
      "[INFO|trainer.py:1226] 2021-01-22 12:29:54,988 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-500\n",
      "[INFO|configuration_utils.py:289] 2021-01-22 12:29:55,130 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-22 12:30:38,169 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-500/pytorch_model.bin\n",
      "[INFO|trainer.py:1285] 2021-01-22 12:31:08,676 >> Deleting older checkpoint [/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-500] due to args.save_total_limit\n",
      " 80%|███████▉  | 510/640 [35:49<08:18,  3.83s/it]{'loss': 1785.88671875, 'learning_rate': 6.09375e-06, 'epoch': 3.984375}\n",
      " 81%|████████▏ | 520/640 [35:54<01:02,  1.93it/s]{'loss': 1796.0900390625, 'learning_rate': 5.625e-06, 'epoch': 4.0625}\n",
      " 82%|████████▏ | 525/640 [35:56<01:13,  1.56it/s][INFO|trainer.py:1412] 2021-01-22 12:31:19,707 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-22 12:31:19,707 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-22 12:31:19,707 >>   Batch size = 16\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:21<02:38, 10.57s/it]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:33<02:36, 11.21s/it]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:50<02:54, 13.43s/it]\u001b[A\n",
      " 29%|██▉       | 5/17 [01:12<03:17, 16.44s/it]\u001b[A\n",
      " 35%|███▌      | 6/17 [01:24<02:44, 14.97s/it]\u001b[A\n",
      " 41%|████      | 7/17 [01:37<02:22, 14.21s/it]\u001b[A\n",
      " 47%|████▋     | 8/17 [01:47<01:55, 12.89s/it]\u001b[A\n",
      " 53%|█████▎    | 9/17 [02:03<01:52, 14.02s/it]\u001b[A\n",
      " 59%|█████▉    | 10/17 [02:16<01:35, 13.65s/it]\u001b[A\n",
      " 65%|██████▍   | 11/17 [02:34<01:30, 15.07s/it]\u001b[A\n",
      " 71%|███████   | 12/17 [02:47<01:12, 14.43s/it]\u001b[A\n",
      " 76%|███████▋  | 13/17 [03:02<00:57, 14.44s/it]\u001b[A\n",
      " 82%|████████▏ | 14/17 [03:15<00:41, 13.97s/it]\u001b[A\n",
      " 88%|████████▊ | 15/17 [03:27<00:27, 13.63s/it]\u001b[A\n",
      " 94%|█████████▍| 16/17 [03:36<00:12, 12.05s/it]\u001b[A\n",
      "                                                 A\n",
      " 82%|████████▏ | 525/640 [39:57<01:13,  1.56it/s]\n",
      "100%|██████████| 17/17 [03:47<00:00, 11.45s/it]\u001b[A\n",
      "                                               \u001b[A{'eval_loss': 2076.5732421875, 'eval_rouge1': 32.6141, 'eval_rouge2': 13.8786, 'eval_rougeL': 25.7717, 'eval_rougeLsum': 26.5848, 'eval_gen_len': 36.7, 'epoch': 4.1015625}\n",
      "[INFO|trainer.py:1226] 2021-01-22 12:35:20,956 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-525\n",
      "[INFO|configuration_utils.py:289] 2021-01-22 12:35:21,173 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-525/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-22 12:36:02,342 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-525/pytorch_model.bin\n",
      "[INFO|trainer.py:1285] 2021-01-22 12:36:30,704 >> Deleting older checkpoint [/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-525] due to args.save_total_limit\n",
      "                                                 {'loss': 1678.771875, 'learning_rate': 5.15625e-06, 'epoch': 4.140625}\n",
      "                                                 {'loss': 1543.9185546875, 'learning_rate': 4.6875000000000004e-06, 'epoch': 4.21875}\n",
      " 86%|████████▌ | 550/640 [41:17<00:33,  2.68it/s][INFO|trainer.py:1412] 2021-01-22 12:36:40,740 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-22 12:36:40,740 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-22 12:36:40,740 >>   Batch size = 16\n",
      "{'loss': 1733.0390625, 'learning_rate': 4.21875e-06, 'epoch': 4.296875}\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:21<02:38, 10.56s/it]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:34<02:42, 11.61s/it]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:49<02:50, 13.08s/it]\u001b[A\n",
      " 29%|██▉       | 5/17 [01:11<03:11, 15.93s/it]\u001b[A\n",
      " 35%|███▌      | 6/17 [01:18<02:23, 13.04s/it]\u001b[A\n",
      " 41%|████      | 7/17 [01:27<01:58, 11.85s/it]\u001b[A\n",
      " 47%|████▋     | 8/17 [01:36<01:37, 10.83s/it]\u001b[A\n",
      " 53%|█████▎    | 9/17 [01:52<01:40, 12.57s/it]\u001b[A\n",
      " 59%|█████▉    | 10/17 [02:05<01:29, 12.79s/it]\u001b[A\n",
      " 65%|██████▍   | 11/17 [02:24<01:26, 14.40s/it]\u001b[A\n",
      " 71%|███████   | 12/17 [02:36<01:09, 13.93s/it]\u001b[A\n",
      " 76%|███████▋  | 13/17 [02:50<00:54, 13.69s/it]\u001b[A\n",
      " 82%|████████▏ | 14/17 [02:58<00:36, 12.10s/it]\u001b[A\n",
      " 88%|████████▊ | 15/17 [03:10<00:24, 12.19s/it]\u001b[A\n",
      " 94%|█████████▍| 16/17 [03:18<00:10, 10.94s/it]\u001b[A\n",
      "                                                 {'eval_loss': 2080.430908203125, 'eval_rouge1': 32.2574, 'eval_rouge2': 13.5736, 'eval_rougeL': 25.5642, 'eval_rougeLsum': 26.206, 'eval_gen_len': 34.7, 'epoch': 4.296875}\n",
      "\n",
      " 86%|████████▌ | 550/640 [45:07<00:33,  2.68it/s]\n",
      "100%|██████████| 17/17 [03:30<00:00, 10.91s/it]\u001b[A\n",
      "                                               \u001b[A[INFO|trainer.py:1226] 2021-01-22 12:40:31,088 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-550\n",
      "[INFO|configuration_utils.py:289] 2021-01-22 12:40:31,262 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-550/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-22 12:41:12,815 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-550/pytorch_model.bin\n",
      "[INFO|trainer.py:1285] 2021-01-22 12:41:40,315 >> Deleting older checkpoint [/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-550] due to args.save_total_limit\n",
      "                                                 {'loss': 1594.58232421875, 'learning_rate': 3.75e-06, 'epoch': 4.375}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 570/640 [46:25<00:32,  2.18it/s]{'loss': 1750.7443359375, 'learning_rate': 3.28125e-06, 'epoch': 4.453125}\n",
      " 90%|████████▉ | 575/640 [46:26<00:23,  2.79it/s][INFO|trainer.py:1412] 2021-01-22 12:41:49,677 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-22 12:41:49,677 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-22 12:41:49,677 >>   Batch size = 16\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:15<01:57,  7.81s/it]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:28<02:18,  9.89s/it]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:44<02:36, 12.02s/it]\u001b[A\n",
      " 29%|██▉       | 5/17 [01:05<03:02, 15.22s/it]\u001b[A\n",
      " 35%|███▌      | 6/17 [01:12<02:18, 12.56s/it]\u001b[A\n",
      " 41%|████      | 7/17 [01:21<01:55, 11.58s/it]\u001b[A\n",
      " 47%|████▋     | 8/17 [01:30<01:35, 10.66s/it]\u001b[A\n",
      " 53%|█████▎    | 9/17 [01:47<01:39, 12.50s/it]\u001b[A\n",
      " 59%|█████▉    | 10/17 [01:56<01:20, 11.47s/it]\u001b[A\n",
      " 65%|██████▍   | 11/17 [02:12<01:17, 12.96s/it]\u001b[A\n",
      " 71%|███████   | 12/17 [02:25<01:04, 12.99s/it]\u001b[A\n",
      " 76%|███████▋  | 13/17 [02:39<00:52, 13.13s/it]\u001b[A\n",
      " 82%|████████▏ | 14/17 [02:47<00:34, 11.60s/it]\u001b[A\n",
      " 88%|████████▊ | 15/17 [03:00<00:23, 12.00s/it]\u001b[A\n",
      " 94%|█████████▍| 16/17 [03:08<00:10, 10.99s/it]\u001b[A\n",
      "                                                 A\n",
      "\u001b[A{'eval_loss': 2081.237060546875, 'eval_rouge1': 31.7977, 'eval_rouge2': 13.4768, 'eval_rougeL': 25.2941, 'eval_rougeLsum': 25.898, 'eval_gen_len': 34.6, 'epoch': 4.4921875}\n",
      " 90%|████████▉ | 575/640 [50:07<00:23,  2.79it/s]\n",
      "100%|██████████| 17/17 [03:21<00:00, 11.13s/it]\u001b[A\n",
      "                                               \u001b[A[INFO|trainer.py:1226] 2021-01-22 12:45:30,410 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-575\n",
      "[INFO|configuration_utils.py:289] 2021-01-22 12:45:30,587 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-575/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-22 12:46:15,774 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-575/pytorch_model.bin\n",
      "[INFO|trainer.py:1285] 2021-01-22 12:46:42,743 >> Deleting older checkpoint [/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-575] due to args.save_total_limit\n",
      "                                                 {'loss': 1620.6142578125, 'learning_rate': 2.8125e-06, 'epoch': 4.53125}\n",
      " 92%|█████████▏| 590/640 [51:26<00:47,  1.05it/s]{'loss': 1716.5837890625, 'learning_rate': 2.3437500000000002e-06, 'epoch': 4.609375}\n",
      "                                                 {'loss': 1735.8716796875, 'learning_rate': 1.875e-06, 'epoch': 4.6875}\n",
      " 94%|█████████▍| 600/640 [51:29<00:15,  2.57it/s][INFO|trainer.py:1412] 2021-01-22 12:46:52,958 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-22 12:46:52,958 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-22 12:46:52,958 >>   Batch size = 16\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:15<01:56,  7.76s/it]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:28<02:17,  9.83s/it]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:45<02:45, 12.73s/it]\u001b[A\n",
      " 29%|██▉       | 5/17 [00:58<02:33, 12.78s/it]\u001b[A\n",
      " 35%|███▌      | 6/17 [01:06<02:03, 11.26s/it]\u001b[A\n",
      " 41%|████      | 7/17 [01:16<01:46, 10.64s/it]\u001b[A\n",
      " 47%|████▋     | 8/17 [01:23<01:27,  9.68s/it]\u001b[A\n",
      " 53%|█████▎    | 9/17 [01:40<01:34, 11.83s/it]\u001b[A\n",
      " 59%|█████▉    | 10/17 [01:50<01:19, 11.37s/it]\u001b[A\n",
      " 65%|██████▍   | 11/17 [02:07<01:17, 13.00s/it]\u001b[A\n",
      " 71%|███████   | 12/17 [02:20<01:05, 13.01s/it]\u001b[A\n",
      " 76%|███████▋  | 13/17 [02:34<00:52, 13.22s/it]\u001b[A\n",
      " 82%|████████▏ | 14/17 [02:42<00:35, 11.86s/it]\u001b[A\n",
      " 88%|████████▊ | 15/17 [02:55<00:24, 12.16s/it]\u001b[A\n",
      " 94%|█████████▍| 16/17 [03:04<00:11, 11.04s/it]\u001b[A\n",
      "                                                 A\n",
      " 94%|█████████▍| 600/640 [55:10<00:15,  2.57it/s] 32.5186, 'eval_rouge2': 13.8384, 'eval_rougeL': 25.8052, 'eval_rougeLsum': 26.5288, 'eval_gen_len': 35.3, 'epoch': 4.6875}\n",
      "100%|██████████| 17/17 [03:21<00:00, 12.64s/it]\u001b[A\n",
      "                                               \u001b[A\n",
      "[INFO|trainer.py:1226] 2021-01-22 12:50:34,010 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-600\n",
      "[INFO|configuration_utils.py:289] 2021-01-22 12:50:34,226 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-600/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-22 12:51:15,663 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-600/pytorch_model.bin\n",
      "[INFO|trainer.py:1285] 2021-01-22 12:51:44,313 >> Deleting older checkpoint [/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-600] due to args.save_total_limit\n",
      " 95%|█████████▌| 610/640 [56:25<01:56,  3.89s/it]{'loss': 1671.3998046875, 'learning_rate': 1.40625e-06, 'epoch': 4.765625}\n",
      "                                                 {'loss': 1755.537890625, 'learning_rate': 9.375e-07, 'epoch': 4.84375}\n",
      " 98%|█████████▊| 625/640 [56:31<00:06,  2.49it/s][INFO|trainer.py:1412] 2021-01-22 12:51:54,029 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-22 12:51:54,029 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-22 12:51:54,029 >>   Batch size = 16\n",
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [00:15<01:56,  7.77s/it]\u001b[A\n",
      " 18%|█▊        | 3/17 [00:28<02:17,  9.81s/it]\u001b[A\n",
      " 24%|██▎       | 4/17 [00:45<02:45, 12.73s/it]\u001b[A\n",
      " 29%|██▉       | 5/17 [01:07<03:07, 15.66s/it]\u001b[A\n",
      " 35%|███▌      | 6/17 [01:16<02:30, 13.69s/it]\u001b[A\n",
      " 41%|████      | 7/17 [01:26<02:03, 12.30s/it]\u001b[A\n",
      " 47%|████▋     | 8/17 [01:36<01:45, 11.67s/it]\u001b[A\n",
      " 53%|█████▎    | 9/17 [01:53<01:45, 13.21s/it]\u001b[A\n",
      " 59%|█████▉    | 10/17 [02:03<01:26, 12.31s/it]\u001b[A\n",
      " 65%|██████▍   | 11/17 [02:19<01:20, 13.47s/it]\u001b[A\n",
      " 71%|███████   | 12/17 [02:32<01:07, 13.46s/it]\u001b[A\n",
      " 76%|███████▋  | 13/17 [02:46<00:53, 13.49s/it]\u001b[A\n",
      " 82%|████████▏ | 14/17 [02:59<00:40, 13.37s/it]\u001b[A\n",
      " 88%|████████▊ | 15/17 [03:12<00:26, 13.29s/it]\u001b[A\n",
      " 94%|█████████▍| 16/17 [03:25<00:13, 13.21s/it]\u001b[A\n",
      "                                                 {'eval_loss': 2081.04931640625, 'eval_rouge1': 32.3928, 'eval_rouge2': 13.676, 'eval_rougeL': 25.6553, 'eval_rougeLsum': 26.354, 'eval_gen_len': 36.1, 'epoch': 4.8828125}\n",
      " 98%|█████████▊| 625/640 [1:00:24<00:06,  2.49it/s]\n",
      "100%|██████████| 17/17 [03:33<00:00, 11.42s/it]\u001b[A\n",
      "                                               \u001b[A\n",
      "[INFO|trainer.py:1226] 2021-01-22 12:55:47,347 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-625\n",
      "[INFO|configuration_utils.py:289] 2021-01-22 12:55:47,496 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-625/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-22 12:56:28,525 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-625/pytorch_model.bin\n",
      "[INFO|trainer.py:1285] 2021-01-22 12:57:00,125 >> Deleting older checkpoint [/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-625] due to args.save_total_limit\n",
      " 98%|█████████▊| 630/640 [1:01:39<03:44, 22.48s/it]{'loss': 1655.384765625, 'learning_rate': 4.6875e-07, 'epoch': 4.921875}\n",
      "100%|██████████| 640/640 [1:01:43<00:00,  1.05s/it][INFO|trainer.py:862] 2021-01-22 12:57:06,578 >> h': 5.0}\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:865] 2021-01-22 12:57:06,578 >> Loading best model from /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-275 (score: 26.5396).\n",
      "\n",
      "[INFO|configuration_utils.py:429] 2021-01-22 12:57:06,701 >> loading configuration file /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-275/config.json\n",
      "[INFO|configuration_utils.py:467] 2021-01-22 12:57:06,701 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"sshleifer/distilbart-cnn-12-6\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 150,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 10,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 5,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 1,\n",
      "      \"max_length\": 150,\n",
      "      \"min_length\": 10,\n",
      "      \"no_repeat_ngram_size\": 5,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1022] 2021-01-22 12:57:06,714 >> loading weights file /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-275/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1140] 2021-01-22 12:57:58,569 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-01-22 12:57:58,569 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/checkpoint-275.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "{'epoch': 5.0}                                     \n",
      "100%|██████████| 640/640 [1:02:35<00:00,  5.87s/it]\n",
      "[INFO|trainer.py:1226] 2021-01-22 12:57:59,375 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/\n",
      "[INFO|configuration_utils.py:289] 2021-01-22 12:57:59,535 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-22 12:58:38,750 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/pytorch_model.bin\n",
      "01/22/2021 12:58:38 - INFO - __main__ -   ***** train metrics *****\n",
      "01/22/2021 12:58:38 - INFO - __main__ -     train_samples_per_second = -0.0\n",
      "01/22/2021 12:58:38 - INFO - __main__ -     train_runtime = 3788.657\n",
      "01/22/2021 12:58:38 - INFO - __main__ -     train_n_ojbs = -1\n",
      "01/22/2021 12:58:40 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:1412] 2021-01-22 12:58:40,181 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-22 12:58:40,181 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-22 12:58:40,181 >>   Batch size = 16\n",
      "100%|██████████| 17/17 [03:31<00:00, 12.45s/it]\n",
      "01/22/2021 13:02:26 - INFO - __main__ -   ***** val metrics *****\n",
      "01/22/2021 13:02:26 - INFO - __main__ -     val_loss = 2077.7104\n",
      "01/22/2021 13:02:26 - INFO - __main__ -     val_rouge1 = 32.47\n",
      "01/22/2021 13:02:26 - INFO - __main__ -     val_rouge2 = 14.7037\n",
      "01/22/2021 13:02:26 - INFO - __main__ -     val_rougeL = 26.4011\n",
      "01/22/2021 13:02:26 - INFO - __main__ -     val_rougeLsum = 27.0912\n",
      "01/22/2021 13:02:26 - INFO - __main__ -     val_gen_len = 34.9\n",
      "01/22/2021 13:02:26 - INFO - __main__ -     epoch = 5.0\n",
      "01/22/2021 13:02:26 - INFO - __main__ -     val_samples_per_second = -0.004\n",
      "01/22/2021 13:02:26 - INFO - __main__ -     val_runtime = 226.4723\n",
      "01/22/2021 13:02:26 - INFO - __main__ -     val_n_ojbs = -1\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 5787\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210122_115522-2mtdks3v/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210122_115522-2mtdks3v/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                        train/loss 1604.54063\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                               train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                       train/epoch 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                             _step 640\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                          _runtime 4025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                        _timestamp 1611320547\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                         eval/loss 2081.04932\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                       eval/rouge1 32.3928\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                       eval/rouge2 13.676\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                       eval/rougeL 25.6553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                    eval/rougeLsum 26.354\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                      eval/gen_len 36.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/total_flos 8384951673446400\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                    train/val_loss 2077.71045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/val_rouge1 32.47\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/val_rouge2 14.7037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/val_rougeL 26.4011\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                               train/val_rougeLsum 27.0912\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                 train/val_gen_len 34.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▁▇▇▇██▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/learning_rate ███▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/loss ▂▄█▆▁▁▂▃▆▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rouge1 ▆▇▄▆▁█▇▇▅▂▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rouge2 ▇█▄▆▁█▅▆▄▃▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rougeL ▆▇▅▆▁█▅▅▄▃▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/rougeLsum ▆█▅▆▁█▅▆▄▂▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/gen_len ▁█▂▇▄▄▃▆▁▁▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        train/val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rouge1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rouge2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rougeL ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/val_rougeLsum ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/val_gen_len ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_wordembed_train/\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/finetune_para_wordembed/runs/2mtdks3v\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 $finetune_script \\\n",
    "--model_name_or_path $model_name_or_path \\\n",
    "--config_name $model_config_dir \\\n",
    "--tokenizer_name $model_name_or_path \\\n",
    "--cache_dir $cache_dir \\\n",
    "--data_dir $data_dir \\\n",
    "--fp16 \\\n",
    "--learning_rate 3e-5 --label_smoothing 0.1 \\\n",
    "--freeze_embeds --freeze_encoder \\\n",
    "--sortish_sampler \\\n",
    "--task summarization \\\n",
    "--max_source_length 1024 \\\n",
    "--max_target_length $config.ONE_BULLET_MAX_LEN \\\n",
    "--val_max_target_length $config.ONE_BULLET_MAX_LEN \\\n",
    "--test_max_target_length $config.ONE_BULLET_MAX_LEN \\\n",
    "--do_train \\\n",
    "--num_train_epochs 5 \\\n",
    "--logging_steps 10 --save_steps 100 --save_total_limit 3 \\\n",
    "--per_device_train_batch_size 16 --per_device_eval_batch_size 16 --gradient_accumulation_steps 1 \\\n",
    "--do_eval --evaluation_strategy steps --eval_steps 25 --eval_beams 2 \\\n",
    "--metric_for_best_model rougeL --greater_is_better True \\\n",
    "--load_best_model_at_end \\\n",
    "--predict_with_generate \\\n",
    "--output_dir $output_dir \\\n",
    "--overwrite_output_dir \\\n",
    "--seed $config.SEED \\\n",
    "--run_name $output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeSSMVZwVOa1"
   },
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2YaHOesVOa4"
   },
   "outputs": [],
   "source": [
    "source_test_dir = data_dir[:-1] + '/test.source\"'\n",
    "reference_test_dir = data_dir[:-1] + '/test.target\"'\n",
    "\n",
    "save_dir = output_dir[:-1] + '/'+model_name_or_path.replace('/', '?')+'_test_karger_books_para.txt\"'\n",
    "score_dir = output_dir[:-1] + '/'+model_name_or_path.replace('/', '?')+'_test_karger_books_para.json\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1399,
     "status": "ok",
     "timestamp": 1610471442216,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "AgY3gO94VOa9",
    "outputId": "8436601c-f530-405f-e6c6-c025a7cf5b71"
   },
   "outputs": [],
   "source": [
    "!python3 $eval_script \\\n",
    "$output_dir \\\n",
    "$source_test_dir \\\n",
    "$save_dir \\\n",
    "--reference_path $reference_test_dir \\\n",
    "--score_path $score_dir \\\n",
    "--task summarization \\\n",
    "--bs 2 \\\n",
    "--length_penalty $config.LENAGTH_PENALTY \\\n",
    "--no_repeat_ngram_size $config.NO_REPEAT_NGRAM_SIZE \\\n",
    "--num_beams $config.NUM_BEAMS \\\n",
    "--dump-args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bbRUTtNDHth"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNa0Dk3mImrCcZO3VdQRayI",
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "L5sXxqeNCtkN",
    "S0FByNNOIRvG",
    "GPbOrCLWACbm",
    "siT4m5aYCFSh",
    "Dk1uGO5SCDNa",
    "WdDCBiMOBWiO",
    "pr_0J4xgBWiW",
    "l8hQT6ksBWin",
    "d5V0QCdf04Yx",
    "KQj3gt6s5ACz",
    "ah9sssub5DXX",
    "M58yiP1yVOav",
    "aeSSMVZwVOa1"
   ],
   "name": "bart_finetune.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "88734567ccb2435091eeab9f30b2de9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bd07faa8d1534ff9b1967f18e6fca2c2",
       "IPY_MODEL_43ac5522170c45f7856f7097ff9fee58"
      ],
      "layout": "IPY_MODEL_431d75cb2dd2403e88015f4e009080d4"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
