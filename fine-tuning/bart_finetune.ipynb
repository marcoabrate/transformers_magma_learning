{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bart_finetune.ipynb","provenance":[],"collapsed_sections":["P95DxvqWi_2Y","L5sXxqeNCtkN","S0FByNNOIRvG","GPbOrCLWACbm","siT4m5aYCFSh","Dk1uGO5SCDNa","WdDCBiMOBWiO","pr_0J4xgBWiW","l8hQT6ksBWin","d5V0QCdf04Yx","KQj3gt6s5ACz","ah9sssub5DXX","M58yiP1yVOav","aeSSMVZwVOa1"],"authorship_tag":"ABX9TyNa0Dk3mImrCcZO3VdQRayI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"88734567ccb2435091eeab9f30b2de9b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_431d75cb2dd2403e88015f4e009080d4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bd07faa8d1534ff9b1967f18e6fca2c2","IPY_MODEL_43ac5522170c45f7856f7097ff9fee58"]}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"P95DxvqWi_2Y"},"source":["#### For Colab"]},{"cell_type":"code","metadata":{"id":"GD_KFnI1H1ip","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1610616593465,"user_tz":-60,"elapsed":1819,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"c4399989-a071-40d4-d025-78508040eeeb"},"source":["\"\"\"\n","function ClickConnect(){\n","    console.log(\"Working\");\n","    document.querySelector(\"colab-toolbar-button\").click() \n","}\n","var i = setInterval(ClickConnect, 900000)\n","clearInterval(i)\n","\"\"\""],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nfunction ClickConnect(){\\n    console.log(\"Working\");\\n    document.querySelector(\"colab-toolbar-button\").click() \\n}\\nvar i = setInterval(ClickConnect, 900000)\\nclearInterval(i)\\n'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"RR-OcN_Wy1jE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610616621853,"user_tz":-60,"elapsed":30196,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"d9e7f11e-eb26-4fc4-daa7-3d942f5a3f9c"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"au5Z9XQAC7C-","executionInfo":{"status":"ok","timestamp":1610616621856,"user_tz":-60,"elapsed":30194,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}}},"source":["drive_dir = '/content/drive/My Drive/MAGMA: Summarization/'"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L5sXxqeNCtkN"},"source":["#### Install Libraries"]},{"cell_type":"code","metadata":{"id":"m6KSIQlYzjEy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610616677125,"user_tz":-60,"elapsed":85455,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"1aaa36ff-9815-45f6-decf-e667b120736d"},"source":["requirements_dir = (drive_dir + 'transformers/examples/seq2seq/').replace(' ', '\\ ')\n","requirements_file = requirements_dir + 'requirements.txt'\n","!cd $requirements_dir; pip install -r $requirements_file\n","\n","!pip install transformers==4.1.1\n","!pip install -U pyarrow\n","!pip install -U wandb"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (2.4.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 2)) (0.22.2.post1)\n","Collecting seqeval\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n","\r\u001b[K     |███████▌                        | 10kB 22.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 29.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 19.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 4)) (5.4.8)\n","Collecting sacrebleu\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n","\u001b[K     |████████████████████████████████| 71kB 7.7MB/s \n","\u001b[?25hCollecting rouge-score\n","  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n","Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 7)) (4.0.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 8)) (3.2.2)\n","Collecting git-python==1.0.3\n","  Downloading https://files.pythonhosted.org/packages/8a/de/0cc6353a45cdb1e137cffac5383097b300cc578e2e1133eeb847e23a1394/git_python-1.0.3-py2.py3-none-any.whl\n","Collecting faiss-cpu\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/f2/ea3c4ae49cd0d1bf21d01244025fd5cb3fb89768aecd5bfb4ef8453a0fdd/faiss_cpu-1.6.5-cp36-cp36m-manylinux2014_x86_64.whl (7.9MB)\n","\u001b[K     |████████████████████████████████| 7.9MB 44.3MB/s \n","\u001b[?25hCollecting streamlit\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/04/348652bb02a0b8b5f93951c228558e2021af89faeef7dec2aae2c63efb11/streamlit-0.74.1-py2.py3-none-any.whl (7.5MB)\n","\u001b[K     |████████████████████████████████| 7.5MB 46.7MB/s \n","\u001b[?25hCollecting elasticsearch\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/74/054342aa07121f7c82e30ae63e3f257a793a69ddc11c5065449252dcd8af/elasticsearch-7.10.1-py2.py3-none-any.whl (322kB)\n","\u001b[K     |████████████████████████████████| 327kB 55.8MB/s \n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 13)) (3.2.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 14)) (1.1.5)\n","Collecting datasets>=1.1.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/9b/d097f2238fc3c028495cf5f8c65378972b9f1b2cbb27f3c57c7219195aa9/datasets-1.2.1-py3-none-any.whl (159kB)\n","\u001b[K     |████████████████████████████████| 163kB 59.6MB/s \n","\u001b[?25hCollecting fire\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n","\u001b[K     |████████████████████████████████| 81kB 11.3MB/s \n","\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 17)) (3.6.4)\n","Collecting conllu\n","  Downloading https://files.pythonhosted.org/packages/25/1b/b1481ba63198eb7b88d715945682bb7fc986ec5dda2d26e313ecdea8a5f6/conllu-4.2.2-py2.py3-none-any.whl\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 45.2MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 20)) (3.12.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (0.4.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (3.3.3)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (0.36.2)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (51.1.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (1.17.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (1.7.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (2.23.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (1.32.0)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (1.19.5)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (0.10.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (1.15.0)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 2)) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 2)) (1.0.0)\n","Collecting portalocker\n","  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 7)) (20.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 7)) (0.16.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 7)) (0.8)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 7)) (4.41.1)\n","Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 7)) (2.3)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 7)) (0.26.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 7)) (1.1.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 7)) (0.3.3)\n","Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 7)) (4.1.1)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 7)) (0.1.5)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 8)) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 8)) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 8)) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 8)) (1.3.1)\n","Collecting gitpython\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/cb/ec98155c501b68dcb11314c7992cd3df6dce193fd763084338a117967d53/GitPython-3.1.12-py3-none-any.whl (159kB)\n","\u001b[K     |████████████████████████████████| 163kB 54.7MB/s \n","\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (4.2.0)\n","Collecting watchdog; platform_system != \"Darwin\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/d9/3d1f46b428fd7b646725896b58d2eddb84f79fd76912773e6193cf74263d/watchdog-1.0.2-py3-none-manylinux2014_x86_64.whl (72kB)\n","\u001b[K     |████████████████████████████████| 81kB 12.5MB/s \n","\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (7.1.2)\n","Collecting pydeck>=0.1.dev5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/9d/8fbf1f56cc5891e6c3295bf94fc176e9ab0a3ffdd090cc8b354ac2640f9a/pydeck-0.5.0-py2.py3-none-any.whl (4.5MB)\n","\u001b[K     |████████████████████████████████| 4.5MB 49.3MB/s \n","\u001b[?25hCollecting blinker\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/51/e2a9f3b757eb802f61dc1f2b09c8c99f6eb01cf06416c0671253536517b6/blinker-1.4.tar.gz (111kB)\n","\u001b[K     |████████████████████████████████| 112kB 55.5MB/s \n","\u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (1.5.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (20.8)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (7.0.0)\n","Collecting validators\n","  Downloading https://files.pythonhosted.org/packages/db/2f/7fed3ee94ad665ad2c1de87f858f10a7785251ff75b4fd47987888d07ef1/validators-0.18.2-py3-none-any.whl\n","Collecting base58\n","  Downloading https://files.pythonhosted.org/packages/b8/a1/d9f565e9910c09fd325dc638765e8843a19fa696275c16cc08cf3b0a3c25/base58-2.1.0-py3-none-any.whl\n","Requirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.14.1)\n","Requirement already satisfied: astor in /usr/local/lib/python3.6/dist-packages (from streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.8.1)\n","Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (4.1.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.10.2)\n","Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (5.1.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from elasticsearch->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 12)) (2020.12.5)\n","Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 12)) (1.24.3)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 14)) (2018.9)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 15)) (0.70.11.1)\n","Collecting xxhash\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n","\u001b[K     |████████████████████████████████| 245kB 56.4MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 15)) (3.3.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 17)) (8.6.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 17)) (0.7.1)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 17)) (1.4.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 17)) (1.10.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (1.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (4.6)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 7)) (1.52.0)\n","Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 7)) (3.4.0)\n","Collecting gitdb<5,>=4.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n","\u001b[K     |████████████████████████████████| 71kB 11.0MB/s \n","\u001b[?25hRequirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (7.6.3)\n","Requirement already satisfied: jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (2.11.2)\n","Collecting ipykernel>=5.1.2; python_version >= \"3.4\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/cc/e8908bbb2921732f6851ebbbe4b77b925aab62e644ab9402f21c84fa6107/ipykernel-5.4.3-py3-none-any.whl (120kB)\n","\u001b[K     |████████████████████████████████| 122kB 58.7MB/s \n","\u001b[?25hRequirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (4.3.3)\n","Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from validators->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (4.4.2)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.3)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.11.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (2.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 15)) (3.7.4.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (3.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 1)) (0.4.8)\n","Collecting smmap<4,>=3.0.1\n","  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n","Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (5.5.0)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (3.5.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (1.0.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (5.0.8)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.10.1->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (1.1.1)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (5.3.5)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.2.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (2.6.1)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.8.1)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (4.8.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (1.0.18)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.7.5)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (5.3.1)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (4.7.0)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (20.0.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.2.5)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (1.5.0)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.9.2)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (5.6.1)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (1.4.3)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.8.4)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.4.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (3.2.1)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.6.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r /content/drive/My Drive/MAGMA: Summarization/transformers/examples/seq2seq/requirements.txt (line 11)) (0.5.1)\n","Building wheels for collected packages: seqeval, fire, blinker\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=eb4ce93d222e3eb7bc895981a4ab163d03ef6bff51ba70f333df564ce1989f61\n","  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111006 sha256=8b32411534dae702985dc86ca162a3a4089e1994cdba3c5cc9c07e89496eb4a3\n","  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n","  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for blinker: filename=blinker-1.4-cp36-none-any.whl size=13450 sha256=1cd6b89197bf8eb179de0958ac77e1b1035e4701c328cd28eee2f63545f8bf14\n","  Stored in directory: /root/.cache/pip/wheels/92/a0/00/8690a57883956a301d91cf4ec999cc0b258b01e3f548f86e89\n","Successfully built seqeval fire blinker\n","\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.4.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datasets 1.2.1 has requirement pyarrow>=0.17.1, but you'll have pyarrow 0.14.1 which is incompatible.\u001b[0m\n","Installing collected packages: seqeval, portalocker, sacrebleu, rouge-score, smmap, gitdb, gitpython, git-python, faiss-cpu, watchdog, ipykernel, pydeck, blinker, validators, base58, streamlit, elasticsearch, xxhash, datasets, fire, conllu, sentencepiece\n","  Found existing installation: ipykernel 4.10.1\n","    Uninstalling ipykernel-4.10.1:\n","      Successfully uninstalled ipykernel-4.10.1\n","Successfully installed base58-2.1.0 blinker-1.4 conllu-4.2.2 datasets-1.2.1 elasticsearch-7.10.1 faiss-cpu-1.6.5 fire-0.3.1 git-python-1.0.3 gitdb-4.0.5 gitpython-3.1.12 ipykernel-5.4.3 portalocker-2.0.0 pydeck-0.5.0 rouge-score-0.0.4 sacrebleu-1.4.14 sentencepiece-0.1.95 seqeval-1.2.2 smmap-3.0.4 streamlit-0.74.1 validators-0.18.2 watchdog-1.0.2 xxhash-2.0.0\n","Collecting transformers==4.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 13.7MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (0.8)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (20.8)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 48.1MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (1.19.5)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 42.7MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (2019.12.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.1.1) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.1) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.1) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.1) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.1) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.1) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.1) (1.0.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=eb66b05a715f2132fd21ea365bce8c26438fb33abad9a0a67b31ccbf0a928018\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n","Collecting pyarrow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n","\u001b[K     |████████████████████████████████| 17.7MB 212kB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.19.5)\n","Installing collected packages: pyarrow\n","  Found existing installation: pyarrow 0.14.1\n","    Uninstalling pyarrow-0.14.1:\n","      Successfully uninstalled pyarrow-0.14.1\n","Successfully installed pyarrow-2.0.0\n","Collecting wandb\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/21/b1d29dca8187d8ebf5a0537b28247c837613bafc2940a840a987586c607f/wandb-0.10.13-py2.py3-none-any.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 23.3MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.1.12)\n","Collecting shortuuid>=0.5.0\n","  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n","Collecting subprocess32>=3.5.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n","\u001b[K     |████████████████████████████████| 102kB 11.6MB/s \n","\u001b[?25hCollecting watchdog<0.10.5,>=0.8.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/10/500580a0987363a0d9e1f3dd5cb1bba94a47e19266c6ce9dfb6cdd455758/watchdog-0.10.4.tar.gz (98kB)\n","\u001b[K     |████████████████████████████████| 102kB 11.6MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.3)\n","Collecting sentry-sdk>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/5c/018bf9a5c24343a664deaea70e61f33f53bb1bd3caf193110f827bfd07e2/sentry_sdk-0.19.5-py2.py3-none-any.whl (128kB)\n","\u001b[K     |████████████████████████████████| 133kB 50.7MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n","Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n","Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.12.4)\n","Collecting configparser>=3.8.1\n","  Downloading https://files.pythonhosted.org/packages/08/b2/ef713e0e67f6e7ec7d59aea3ee78d05b39c15930057e724cc6d362a8c3bb/configparser-5.0.1-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n","Collecting pathtools>=0.1.1\n","  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n","Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.0->wandb) (51.1.1)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied, skipping upgrade: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.4)\n","Building wheels for collected packages: subprocess32, watchdog, pathtools\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6490 sha256=628ccc97dea2bbaabf6bd819080ba8cdabe6b2e1eeb8f051e027193c2f3a8ccd\n","  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n","  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for watchdog: filename=watchdog-0.10.4-cp36-none-any.whl size=74842 sha256=9efbbf834ae4534e97cf904a54b70f2a9304998d9523374e3ddbe6d7963e5ca6\n","  Stored in directory: /root/.cache/pip/wheels/9e/11/04/5160b8815b0cc7cf574bdc6d053e510169ec264c8791b4ec3a\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8785 sha256=d19b06b45ce1a76b84da5ff002ffbd3d06ab8ab6e6787c4db121f3712dff1173\n","  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n","Successfully built subprocess32 watchdog pathtools\n","Installing collected packages: shortuuid, subprocess32, pathtools, watchdog, sentry-sdk, configparser, docker-pycreds, wandb\n","  Found existing installation: watchdog 1.0.2\n","    Uninstalling watchdog-1.0.2:\n","      Successfully uninstalled watchdog-1.0.2\n","Successfully installed configparser-5.0.1 docker-pycreds-0.4.0 pathtools-0.1.2 sentry-sdk-0.19.5 shortuuid-1.0.1 subprocess32-3.5.4 wandb-0.10.13 watchdog-0.10.4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EddY1WDNsKlS"},"source":["## **Fine-tuning**"]},{"cell_type":"code","metadata":{"id":"2d6M41X9AKBi","executionInfo":{"status":"ok","timestamp":1610616677129,"user_tz":-60,"elapsed":85455,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}}},"source":["finetune_script = '\"'+drive_dir+'transformers/examples/seq2seq/finetune_trainer.py\"'\n","eval_script = '\"'+drive_dir+'transformers/examples/seq2seq/run_eval.py\"'"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S0FByNNOIRvG"},"source":["### **Config**"]},{"cell_type":"code","metadata":{"id":"82WSp6khIcua","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1610616705611,"user_tz":-60,"elapsed":113930,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"54de6794-ef3c-41f4-b0e3-e108cfc4e333"},"source":["import sys\n","sys.path.insert(0, drive_dir)\n","import config\n","\n","import torch\n","torch.manual_seed = config.SEED\n","\n","import wandb\n","wandb.login()"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"],"name":"stderr"},{"output_type":"stream","text":["wandb: Paste an API key from your profile and hit enter: ··········\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"GPbOrCLWACbm"},"source":["### XSUM"]},{"cell_type":"code","metadata":{"id":"x5id4jqosMD8"},"source":["data_dir = '\"/content/drive/My Drive/MAGMA: Summarization/datasets/xsum\"'\n","\n","output_dir = '\"/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train\"'\n","log_dir = output_dir + '/logs'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"siT4m5aYCFSh"},"source":["##### Fine tune"]},{"cell_type":"code","metadata":{"id":"47rCgW8Ren_s"},"source":["#--warmup_steps 5\n","#--gradient_accumulation_steps 128\n","#--do_predict --n_test 20\n","#--dataloader_drop_last"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gy5VM1dYtVzB","executionInfo":{"status":"ok","timestamp":1609844375514,"user_tz":-60,"elapsed":371036,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"c36240b6-506f-4769-986c-16dd8ffe5db2"},"source":["!python3 $finetune_script \\\n","--model_name_or_path facebook/bart-large-cnn \\\n","--tokenizer_name facebook/bart-large-cnn \\\n","--data_dir $data_dir \\\n","--fp16 \\\n","--learning_rate 3e-5 --label_smoothing 0.1 \\\n","--sortish_sampler --freeze_embeds --adafactor \\\n","--task summarization \\\n","--max_source_length 1024 \\\n","--max_target_length 60 \\\n","--do_train \\\n","--n_train 80 --num_train_epochs 2 \\\n","--logging_steps 10 --save_steps 40 \\\n","--per_device_train_batch_size 2 --per_device_eval_batch_size 2 \\\n","--do_eval --n_val 20 --evaluation_strategy epoch --val_max_target_length 60 --eval_beams 2 \\\n","--metric_for_best_model rougeL --greater_is_better True \\\n","--predict_with_generate \\\n","--test_max_target_length 100 \\\n","--output_dir $output_dir \\\n","--overwrite_output_dir \\\n","--seed $config.SEED"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-01-05 10:53:25.802661: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n","01/05/2021 10:53:29 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n","01/05/2021 10:53:29 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jan05_10-53-29_14689f7f2f2b', logging_first_step=False, logging_steps=10, save_steps=40, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model='rougeL', greater_is_better='True', ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False, label_smoothing=0.1, sortish_sampler=True, predict_with_generate=True, adafactor=True, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear')\n","[INFO|configuration_utils.py:431] 2021-01-05 10:53:29,364 >> loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.30bea5d0a276dd70740d78f636e2c8e880d88bad362586009715702be8bda196\n","[INFO|configuration_utils.py:467] 2021-01-05 10:53:29,364 >> Model config BartConfig {\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"do_blenderbot_90_layernorm\": false,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","[INFO|configuration_utils.py:431] 2021-01-05 10:53:29,415 >> loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.30bea5d0a276dd70740d78f636e2c8e880d88bad362586009715702be8bda196\n","[INFO|configuration_utils.py:467] 2021-01-05 10:53:29,416 >> Model config BartConfig {\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"do_blenderbot_90_layernorm\": false,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","[INFO|tokenization_utils_base.py:1802] 2021-01-05 10:53:29,685 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|tokenization_utils_base.py:1802] 2021-01-05 10:53:29,685 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1802] 2021-01-05 10:53:29,685 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|modeling_utils.py:1024] 2021-01-05 10:53:29,823 >> loading weights file https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n","[INFO|modeling_utils.py:1140] 2021-01-05 10:54:33,008 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:1149] 2021-01-05 10:54:33,009 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n","01/05/2021 10:54:33 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n","01/05/2021 10:54:45 - INFO - __main__ -   *** Train ***\n","[INFO|trainer.py:703] 2021-01-05 10:54:45,426 >> ***** Running training *****\n","[INFO|trainer.py:704] 2021-01-05 10:54:45,426 >>   Num examples = 80\n","[INFO|trainer.py:705] 2021-01-05 10:54:45,426 >>   Num Epochs = 2\n","[INFO|trainer.py:706] 2021-01-05 10:54:45,426 >>   Instantaneous batch size per device = 2\n","[INFO|trainer.py:707] 2021-01-05 10:54:45,426 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n","[INFO|trainer.py:708] 2021-01-05 10:54:45,426 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:709] 2021-01-05 10:54:45,426 >>   Total optimization steps = 80\n","/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train\n","[INFO|integrations.py:371] 2021-01-05 10:54:45,461 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.12\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/huggingface/runs/33zbcsk6\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210105_105445-33zbcsk6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","  0%|          | 0/80 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","                                               \n"," 12%|█▎        | 10/80 [00:05<00:31,  2.23it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:506: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n","                                               \n","                                               {'loss': 205.1709716796875, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.75}\n"," 50%|█████     | 40/80 [00:25<00:26,  1.51it/s]{'loss': 170.9220703125, 'learning_rate': 1.5e-05, 'epoch': 1.0}\n","[INFO|trainer.py:1226] 2021-01-05 10:55:11,933 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train/checkpoint-40\n","[INFO|configuration_utils.py:289] 2021-01-05 10:55:12,189 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train/checkpoint-40/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 10:56:08,535 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train/checkpoint-40/pytorch_model.bin\n","[INFO|trainer.py:1412] 2021-01-05 10:56:09,816 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-05 10:56:09,816 >>   Num examples = 20\n","[INFO|trainer.py:1414] 2021-01-05 10:56:09,816 >>   Batch size = 2\n","\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 20%|██        | 2/10 [00:01<00:04,  1.84it/s]\u001b[A\n"," 30%|███       | 3/10 [00:02<00:05,  1.38it/s]\u001b[A\n"," 40%|████      | 4/10 [00:03<00:05,  1.09it/s]\u001b[A\n"," 50%|█████     | 5/10 [00:04<00:05,  1.02s/it]\u001b[A\n"," 60%|██████    | 6/10 [00:06<00:04,  1.09s/it]\u001b[A\n"," 70%|███████   | 7/10 [00:07<00:03,  1.16s/it]\u001b[A\n"," 80%|████████  | 8/10 [00:08<00:02,  1.26s/it]\u001b[A\n"," 90%|█████████ | 9/10 [00:10<00:01,  1.27s/it]\u001b[A\n","                                               \n","\u001b[A{'eval_loss': 154.70782470703125, 'eval_rouge1': 29.6743, 'eval_rouge2': 11.5784, 'eval_rougeL': 22.3402, 'eval_rougeLsum': 25.1819, 'eval_gen_len': 59.8, 'epoch': 1.0}\n"," 50%|█████     | 40/80 [01:36<00:26,  1.51it/s]\n","100%|██████████| 10/10 [00:11<00:00,  1.30s/it]\u001b[A\n","{'loss': 131.57991943359374, 'learning_rate': 1.125e-05, 'epoch': 1.25}\n"," 75%|███████▌  | 60/80 [01:50<00:14,  1.37it/s]{'loss': 142.9806640625, 'learning_rate': 7.5e-06, 'epoch': 1.5}\n","{'loss': 155.16646728515624, 'learning_rate': 3.75e-06, 'epoch': 1.75}\n","100%|██████████| 80/80 [02:03<00:00,  1.57it/s]\n","[INFO|trainer.py:1226] 2021-01-05 10:56:50,163 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train/checkpoint-80\n","[INFO|configuration_utils.py:289] 2021-01-05 10:56:50,324 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train/checkpoint-80/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 10:57:44,148 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train/checkpoint-80/pytorch_model.bin\n","[INFO|trainer.py:1412] 2021-01-05 10:57:46,951 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-05 10:57:46,951 >>   Num examples = 20\n","[INFO|trainer.py:1414] 2021-01-05 10:57:46,951 >>   Batch size = 2\n","\n","  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n"," 20%|██        | 2/10 [00:01<00:04,  1.85it/s]\u001b[A\n"," 30%|███       | 3/10 [00:02<00:05,  1.39it/s]\u001b[A\n"," 40%|████      | 4/10 [00:03<00:05,  1.12it/s]\u001b[A\n"," 50%|█████     | 5/10 [00:04<00:04,  1.03it/s]\u001b[A\n"," 60%|██████    | 6/10 [00:05<00:04,  1.00s/it]\u001b[A\n"," 70%|███████   | 7/10 [00:07<00:03,  1.10s/it]\u001b[A\n"," 80%|████████  | 8/10 [00:08<00:02,  1.22s/it]\u001b[A\n"," 90%|█████████ | 9/10 [00:09<00:01,  1.26s/it]\u001b[A\n","                                               \n","{'eval_loss': 153.82894897460938, 'eval_rouge1': 30.3907, 'eval_rouge2': 12.0226, 'eval_rougeL': 24.1699, 'eval_rougeLsum': 26.3853, 'eval_gen_len': 59.6, 'epoch': 2.0}\n","100%|██████████| 80/80 [03:12<00:00,  1.57it/s]\n","100%|██████████| 10/10 [00:11<00:00,  1.30s/it]\u001b[A\n","                                               \u001b[A[INFO|trainer.py:862] 2021-01-05 10:57:59,690 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'epoch': 2.0}\n","100%|██████████| 80/80 [03:12<00:00,  2.41s/it]\n","[INFO|trainer.py:1226] 2021-01-05 10:57:59,698 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train\n","[INFO|configuration_utils.py:289] 2021-01-05 10:58:00,090 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 10:59:05,052 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train/pytorch_model.bin\n","01/05/2021 10:59:05 - INFO - __main__ -   ***** train metrics *****\n","01/05/2021 10:59:05 - INFO - __main__ -     train_samples_per_second = 0.412\n","01/05/2021 10:59:05 - INFO - __main__ -     train_runtime = 194.2779\n","01/05/2021 10:59:05 - INFO - __main__ -     train_n_ojbs = 80\n","01/05/2021 10:59:06 - INFO - __main__ -   *** Evaluate ***\n","[INFO|trainer.py:1412] 2021-01-05 10:59:06,434 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-05 10:59:06,435 >>   Num examples = 20\n","[INFO|trainer.py:1414] 2021-01-05 10:59:06,435 >>   Batch size = 2\n","100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\n","01/05/2021 10:59:18 - INFO - __main__ -   ***** val metrics *****\n","01/05/2021 10:59:18 - INFO - __main__ -     val_loss = 153.8289\n","01/05/2021 10:59:18 - INFO - __main__ -     val_rouge1 = 30.3895\n","01/05/2021 10:59:18 - INFO - __main__ -     val_rouge2 = 11.8016\n","01/05/2021 10:59:18 - INFO - __main__ -     val_rougeL = 24.0503\n","01/05/2021 10:59:18 - INFO - __main__ -     val_rougeLsum = 26.5698\n","01/05/2021 10:59:18 - INFO - __main__ -     val_gen_len = 59.6\n","01/05/2021 10:59:18 - INFO - __main__ -     epoch = 2.0\n","01/05/2021 10:59:18 - INFO - __main__ -     val_samples_per_second = 1.601\n","01/05/2021 10:59:18 - INFO - __main__ -     val_runtime = 12.4884\n","01/05/2021 10:59:18 - INFO - __main__ -     val_n_ojbs = 20\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1260\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210105_105445-33zbcsk6/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210105_105445-33zbcsk6/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 152.28292\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/learning_rate 0.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:           train/epoch 2.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 80\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 274\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1609844359\n","\u001b[34m\u001b[1mwandb\u001b[0m:             eval/loss 153.82895\n","\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rouge1 30.3907\n","\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rouge2 12.0226\n","\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rougeL 24.1699\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/rougeLsum 26.3853\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/gen_len 59.6\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/total_flos 305556407451648\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train/val_loss 153.82895\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rouge1 30.3895\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rouge2 11.8016\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rougeL 24.0503\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/val_rougeLsum 26.5698\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train/val_gen_len 59.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▆▇█▅▁▂▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/learning_rate █▇▆▅▄▃▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           train/epoch ▁▂▃▄▅▆▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▂▃▄▅▆▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▃▄▄▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▃▄▄▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:             eval/loss █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rouge1 ▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rouge2 ▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rougeL ▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/rougeLsum ▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/gen_len █▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train/val_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rouge1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rouge2 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rougeL ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/val_rougeLsum ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train/val_gen_len ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_xsum_train\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/huggingface/runs/33zbcsk6\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Dk1uGO5SCDNa"},"source":["##### Evaluate"]},{"cell_type":"code","metadata":{"id":"hXNHzm0MBIv2"},"source":["source_test_dir = data_dir[:-1] + '/test.source\"'\n","reference_test_dir = data_dir[:-1] + '/test.target\"'\n","\n","save_dir = output_dir[:-1] + '/bart_test_xsum.txt\"'\n","score_dir = output_dir[:-1] + '/bart_test_xsum.json\"'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GuvhOoLCiL0","executionInfo":{"status":"ok","timestamp":1609773475165,"user_tz":-60,"elapsed":137458,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"cc3084c1-f389-40fc-b7a4-82a2387103be"},"source":["!python3 $eval_script \\\n","$output_dir \\\n","$source_test_dir \\\n","$save_dir \\\n","--reference_path $reference_test_dir \\\n","--score_path $score_dir \\\n","--task summarization \\\n","--n_obs 20 --bs 2 \\\n","--dump-args"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-01-04 15:15:39.475338: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n","100% 10/10 [00:40<00:00,  4.05s/it]\n","{'rouge1': 27.8322, 'rouge2': 10.8571, 'rougeL': 21.4738, 'rougeLsum': 23.8121, 'n_obs': 20, 'runtime': 40, 'seconds_per_sample': 2.0}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WdDCBiMOBWiO"},"source":["### Karger Books CC"]},{"cell_type":"code","metadata":{"id":"_OpjaL4EHAxM"},"source":["model_name_or_path = 'sshleifer/distilbart-cnn-12-6'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RczOAmNUBWiS"},"source":["data_dir = '\"/content/drive/My Drive/MAGMA: Summarization/datasets/karger_books_chunk_chapter\"'\n","\n","output_dir = '\"/content/drive/My Drive/MAGMA: Summarization/fine-tuning/'+\\\n","    model_name_or_path.replace('/', '?')+'_karger_books_cc_train\"'\n","log_dir = output_dir + '/logs'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pr_0J4xgBWiW"},"source":["##### Fine tune"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_vlJtSZBWiZ","outputId":"95a947c2-18eb-48a2-8e72-66f2a18e1090"},"source":["!python3 $finetune_script \\\n","--model_name_or_path facebook/bart-large-cnn \\\n","--tokenizer_name facebook/bart-large-cnn \\\n","--data_dir $data_dir \\\n","--learning_rate 3e-5 --label_smoothing 0.1 \\\n","--sortish_sampler --freeze_embeds --adafactor \\\n","--task summarization \\\n","--max_source_length 1024 \\\n","--max_target_length $config.BULLETS_MAX_LEN \\\n","--val_max_target_length $config.BULLETS_MAX_LEN \\\n","--test_max_target_length $config.BULLETS_MAX_LEN \\\n","--do_train \\\n","--logging_steps 10 --save_steps 200 --save_total_limit 3 \\\n","--per_device_train_batch_size 2 --per_device_eval_batch_size 2 \\\n","--do_eval --evaluation_strategy epoch --eval_beams 2 \\\n","--metric_for_best_model rougeL --greater_is_better True \\\n","--predict_with_generate \\\n","--output_dir $output_dir \\\n","--overwrite_output_dir \\\n","--seed $config.SEED"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-01-05 09:35:34.078850: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n","01/05/2021 09:35:39 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","01/05/2021 09:35:39 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jan05_09-35-39_14689f7f2f2b', logging_first_step=False, logging_steps=10, save_steps=200, save_total_limit=3, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model='rougeL', greater_is_better='True', ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False, label_smoothing=0.1, sortish_sampler=True, predict_with_generate=True, adafactor=True, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear')\n","01/05/2021 09:35:39 - INFO - filelock -   Lock 139985421790568 acquired on /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.30bea5d0a276dd70740d78f636e2c8e880d88bad362586009715702be8bda196.lock\n","[INFO|file_utils.py:1301] 2021-01-05 09:35:39,821 >> https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2nmj0h2h\n","Downloading: 100% 1.34k/1.34k [00:00<00:00, 1.83MB/s]\n","[INFO|file_utils.py:1305] 2021-01-05 09:35:39,871 >> storing https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.30bea5d0a276dd70740d78f636e2c8e880d88bad362586009715702be8bda196\n","[INFO|file_utils.py:1308] 2021-01-05 09:35:39,871 >> creating metadata file for /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.30bea5d0a276dd70740d78f636e2c8e880d88bad362586009715702be8bda196\n","01/05/2021 09:35:39 - INFO - filelock -   Lock 139985421790568 released on /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.30bea5d0a276dd70740d78f636e2c8e880d88bad362586009715702be8bda196.lock\n","[INFO|configuration_utils.py:431] 2021-01-05 09:35:39,872 >> loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.30bea5d0a276dd70740d78f636e2c8e880d88bad362586009715702be8bda196\n","[INFO|configuration_utils.py:467] 2021-01-05 09:35:39,873 >> Model config BartConfig {\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"do_blenderbot_90_layernorm\": false,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","[INFO|configuration_utils.py:431] 2021-01-05 09:35:39,921 >> loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.30bea5d0a276dd70740d78f636e2c8e880d88bad362586009715702be8bda196\n","[INFO|configuration_utils.py:467] 2021-01-05 09:35:39,922 >> Model config BartConfig {\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"do_blenderbot_90_layernorm\": false,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","01/05/2021 09:35:39 - INFO - filelock -   Lock 139985421788048 acquired on /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n","[INFO|file_utils.py:1301] 2021-01-05 09:35:39,978 >> https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpszrbekhm\n","Downloading: 100% 899k/899k [00:00<00:00, 11.7MB/s]\n","[INFO|file_utils.py:1305] 2021-01-05 09:35:40,110 >> storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|file_utils.py:1308] 2021-01-05 09:35:40,110 >> creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","01/05/2021 09:35:40 - INFO - filelock -   Lock 139985421788048 released on /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n","01/05/2021 09:35:40 - INFO - filelock -   Lock 139985421787656 acquired on /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n","[INFO|file_utils.py:1301] 2021-01-05 09:35:40,164 >> https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7z8a8mo5\n","Downloading: 100% 456k/456k [00:00<00:00, 7.37MB/s]\n","[INFO|file_utils.py:1305] 2021-01-05 09:35:40,283 >> storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|file_utils.py:1308] 2021-01-05 09:35:40,283 >> creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","01/05/2021 09:35:40 - INFO - filelock -   Lock 139985421787656 released on /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n","01/05/2021 09:35:40 - INFO - filelock -   Lock 139985421733336 acquired on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n","[INFO|file_utils.py:1301] 2021-01-05 09:35:40,344 >> https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp3v_11jdi\n","Downloading: 100% 1.36M/1.36M [00:00<00:00, 15.7MB/s]\n","[INFO|file_utils.py:1305] 2021-01-05 09:35:40,491 >> storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|file_utils.py:1308] 2021-01-05 09:35:40,491 >> creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","01/05/2021 09:35:40 - INFO - filelock -   Lock 139985421733336 released on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n","[INFO|tokenization_utils_base.py:1802] 2021-01-05 09:35:40,491 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|tokenization_utils_base.py:1802] 2021-01-05 09:35:40,491 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1802] 2021-01-05 09:35:40,491 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","01/05/2021 09:35:40 - INFO - filelock -   Lock 139985421732608 acquired on /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300.lock\n","[INFO|file_utils.py:1301] 2021-01-05 09:35:40,607 >> https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4ol_xa84\n","Downloading: 100% 1.63G/1.63G [00:19<00:00, 84.7MB/s]\n","[INFO|file_utils.py:1305] 2021-01-05 09:36:00,015 >> storing https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n","[INFO|file_utils.py:1308] 2021-01-05 09:36:00,015 >> creating metadata file for /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n","01/05/2021 09:36:00 - INFO - filelock -   Lock 139985421732608 released on /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300.lock\n","[INFO|modeling_utils.py:1024] 2021-01-05 09:36:00,016 >> loading weights file https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n","[INFO|modeling_utils.py:1140] 2021-01-05 09:36:19,210 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:1149] 2021-01-05 09:36:19,210 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n","01/05/2021 09:36:19 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n","01/05/2021 09:36:35 - INFO - __main__ -   *** Train ***\n","[INFO|trainer.py:703] 2021-01-05 09:36:35,187 >> ***** Running training *****\n","[INFO|trainer.py:704] 2021-01-05 09:36:35,187 >>   Num examples = 1431\n","[INFO|trainer.py:705] 2021-01-05 09:36:35,187 >>   Num Epochs = 3\n","[INFO|trainer.py:706] 2021-01-05 09:36:35,187 >>   Instantaneous batch size per device = 2\n","[INFO|trainer.py:707] 2021-01-05 09:36:35,187 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n","[INFO|trainer.py:708] 2021-01-05 09:36:35,187 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:709] 2021-01-05 09:36:35,187 >>   Total optimization steps = 2148\n","/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train\n","[INFO|integrations.py:371] 2021-01-05 09:36:35,217 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.12\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/huggingface/runs/exbr6eju\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210105_093635-exbr6eju\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","  0%|          | 0/2148 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:506: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n","  0%|          | 10/2148 [00:07<25:15,  1.41it/s]{'loss': 1682.92421875, 'learning_rate': 2.9860335195530727e-05, 'epoch': 0.013966480446927373}\n","  1%|          | 20/2148 [00:15<25:22,  1.40it/s]\n","                                                 \n","                                                 \n","  2%|▏         | 50/2148 [00:35<23:50,  1.47it/s]{'loss': 1590.43017578125, 'learning_rate': 2.9301675977653633e-05, 'epoch': 0.06983240223463687}\n","                                                 \n","  3%|▎         | 70/2148 [00:50<24:47,  1.40it/s]{'loss': 1516.40869140625, 'learning_rate': 2.9022346368715086e-05, 'epoch': 0.09776536312849161}\n","{'loss': 1645.2828125, 'learning_rate': 2.8882681564245812e-05, 'epoch': 0.11173184357541899}\n","  4%|▍         | 90/2148 [01:04<25:19,  1.35it/s]{'loss': 1684.296875, 'learning_rate': 2.8743016759776535e-05, 'epoch': 0.12569832402234637}\n","{'loss': 1828.2017578125, 'learning_rate': 2.8603351955307262e-05, 'epoch': 0.13966480446927373}\n","  5%|▌         | 110/2148 [01:20<23:12,  1.46it/s]{'loss': 1530.19990234375, 'learning_rate': 2.8463687150837992e-05, 'epoch': 0.15363128491620112}\n","                                                  \n","                                                  \n","{'loss': 1814.703125, 'learning_rate': 2.8044692737430168e-05, 'epoch': 0.19553072625698323}\n","{'loss': 1507.79228515625, 'learning_rate': 2.7905027932960894e-05, 'epoch': 0.20949720670391062}\n","{'loss': 1579.16943359375, 'learning_rate': 2.776536312849162e-05, 'epoch': 0.22346368715083798}\n","  8%|▊         | 170/2148 [02:04<25:09,  1.31it/s]{'loss': 1399.08515625, 'learning_rate': 2.7625698324022347e-05, 'epoch': 0.23743016759776536}\n","                                                  {'loss': 1414.25263671875, 'learning_rate': 2.7486033519553074e-05, 'epoch': 0.25139664804469275}\n","  9%|▉         | 190/2148 [02:18<22:17,  1.46it/s]{'loss': 1479.02119140625, 'learning_rate': 2.7346368715083797e-05, 'epoch': 0.26536312849162014}\n","{'loss': 1445.9916015625, 'learning_rate': 2.7206703910614527e-05, 'epoch': 0.27932960893854747}\n","  9%|▉         | 200/2148 [02:26<32:45,  1.01s/it][INFO|trainer.py:1226] 2021-01-05 09:39:03,043 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-200\n","[INFO|configuration_utils.py:289] 2021-01-05 09:39:03,052 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-200/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 09:39:13,457 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-200/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 09:39:13,782 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1600] due to args.save_total_limit\n","{'loss': 1473.55888671875, 'learning_rate': 2.7067039106145253e-05, 'epoch': 0.29329608938547486}\n"," 10%|█         | 220/2148 [02:51<24:09,  1.33it/s]{'loss': 1381.36640625, 'learning_rate': 2.692737430167598e-05, 'epoch': 0.30726256983240224}\n"," 11%|█         | 230/2148 [03:00<26:10,  1.22it/s]{'loss': 1602.31201171875, 'learning_rate': 2.6787709497206703e-05, 'epoch': 0.32122905027932963}\n"," 11%|█         | 240/2148 [03:08<24:04,  1.32it/s]{'loss': 1598.358984375, 'learning_rate': 2.6648044692737432e-05, 'epoch': 0.33519553072625696}\n","                                                  \n","{'loss': 1459.236328125, 'learning_rate': 2.6368715083798882e-05, 'epoch': 0.36312849162011174}\n"," 13%|█▎        | 270/2148 [03:29<23:12,  1.35it/s]{'loss': 1462.09404296875, 'learning_rate': 2.622905027932961e-05, 'epoch': 0.3770949720670391}\n","{'loss': 1357.1357421875, 'learning_rate': 2.6089385474860338e-05, 'epoch': 0.39106145251396646}\n","{'loss': 1523.86552734375, 'learning_rate': 2.594972067039106e-05, 'epoch': 0.40502793296089384}\n"," 14%|█▍        | 300/2148 [03:52<30:10,  1.02it/s]\n"," 14%|█▍        | 310/2148 [03:59<22:19,  1.37it/s]{'loss': 1470.3703125, 'learning_rate': 2.5670391061452514e-05, 'epoch': 0.4329608938547486}\n"," 15%|█▍        | 320/2148 [04:06<22:31,  1.35it/s]{'loss': 1628.3712890625, 'learning_rate': 2.553072625698324e-05, 'epoch': 0.44692737430167595}\n","                                                  \n"," 16%|█▌        | 340/2148 [04:20<21:00,  1.43it/s]{'loss': 1518.19853515625, 'learning_rate': 2.5251396648044694e-05, 'epoch': 0.4748603351955307}\n"," 16%|█▋        | 350/2148 [04:27<21:42,  1.38it/s]\n","{'loss': 1510.19111328125, 'learning_rate': 2.4972067039106143e-05, 'epoch': 0.5027932960893855}\n"," 17%|█▋        | 370/2148 [04:42<21:18,  1.39it/s]{'loss': 1290.8939453125, 'learning_rate': 2.4832402234636873e-05, 'epoch': 0.5167597765363129}\n","{'loss': 1559.25, 'learning_rate': 2.46927374301676e-05, 'epoch': 0.5307262569832403}\n","{'loss': 1484.6267578125, 'learning_rate': 2.4553072625698326e-05, 'epoch': 0.5446927374301676}\n","                                                  \n"," 19%|█▊        | 400/2148 [05:04<30:59,  1.06s/it][INFO|trainer.py:1226] 2021-01-05 09:41:41,081 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-400\n","[INFO|configuration_utils.py:289] 2021-01-05 09:41:41,096 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-400/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 09:41:51,113 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-400/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 09:41:51,283 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1800] due to args.save_total_limit\n"," 19%|█▉        | 410/2148 [05:22<25:26,  1.14it/s]{'loss': 1385.61640625, 'learning_rate': 2.427374301675978e-05, 'epoch': 0.5726256983240223}\n"," 20%|█▉        | 420/2148 [05:29<21:03,  1.37it/s]{'loss': 1431.134765625, 'learning_rate': 2.4134078212290505e-05, 'epoch': 0.5865921787709497}\n"," 20%|██        | 430/2148 [05:37<21:06,  1.36it/s]{'loss': 1480.61708984375, 'learning_rate': 2.399441340782123e-05, 'epoch': 0.6005586592178771}\n"," 20%|██        | 440/2148 [05:44<20:32,  1.39it/s]{'loss': 1326.96455078125, 'learning_rate': 2.3854748603351955e-05, 'epoch': 0.6145251396648045}\n"," 21%|██        | 450/2148 [05:51<21:47,  1.30it/s]{'loss': 1232.02333984375, 'learning_rate': 2.3715083798882685e-05, 'epoch': 0.6284916201117319}\n","{'loss': 1574.02421875, 'learning_rate': 2.3575418994413408e-05, 'epoch': 0.6424581005586593}\n","{'loss': 1356.91923828125, 'learning_rate': 2.3435754189944134e-05, 'epoch': 0.6564245810055865}\n"," 22%|██▏       | 480/2148 [06:13<20:01,  1.39it/s]{'loss': 1476.4509765625, 'learning_rate': 2.329608938547486e-05, 'epoch': 0.6703910614525139}\n","                                                  \n","                                                  \n","{'loss': 1386.1984375, 'learning_rate': 2.287709497206704e-05, 'epoch': 0.7122905027932961}\n","                                                  \n"," 25%|██▍       | 530/2148 [06:52<19:54,  1.35it/s]{'loss': 1409.81982421875, 'learning_rate': 2.259776536312849e-05, 'epoch': 0.7402234636871509}\n","{'loss': 1214.009375, 'learning_rate': 2.245810055865922e-05, 'epoch': 0.7541899441340782}\n","                                                  \n","                                                  \n"," 27%|██▋       | 570/2148 [07:22<20:27,  1.29it/s]{'loss': 1182.7611328125, 'learning_rate': 2.2039106145251395e-05, 'epoch': 0.7960893854748603}\n","{'loss': 1421.88798828125, 'learning_rate': 2.1899441340782125e-05, 'epoch': 0.8100558659217877}\n"," 27%|██▋       | 590/2148 [07:35<18:47,  1.38it/s]\n","                                                  \n"," 28%|██▊       | 600/2148 [07:43<24:38,  1.05it/s][INFO|trainer.py:1226] 2021-01-05 09:44:20,433 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-600\n","[INFO|configuration_utils.py:289] 2021-01-05 09:44:20,441 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-600/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 09:44:29,957 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-600/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 09:44:30,245 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-2000] due to args.save_total_limit\n"," 28%|██▊       | 610/2148 [08:06<26:09,  1.02s/it]{'loss': 1362.52412109375, 'learning_rate': 2.14804469273743e-05, 'epoch': 0.8519553072625698}\n","{'loss': 1200.7265625, 'learning_rate': 2.1340782122905028e-05, 'epoch': 0.8659217877094972}\n"," 29%|██▉       | 630/2148 [08:21<17:54,  1.41it/s]{'loss': 1154.9220703125, 'learning_rate': 2.1201117318435754e-05, 'epoch': 0.8798882681564246}\n"," 30%|██▉       | 640/2148 [08:29<19:10,  1.31it/s]{'loss': 1356.534375, 'learning_rate': 2.106145251396648e-05, 'epoch': 0.8938547486033519}\n","                                                  \n","{'loss': 1127.35146484375, 'learning_rate': 2.0782122905027933e-05, 'epoch': 0.9217877094972067}\n","                                                  \n","{'loss': 1281.20322265625, 'learning_rate': 2.0502793296089386e-05, 'epoch': 0.9497206703910615}\n","{'loss': 1344.1662109375, 'learning_rate': 2.0363128491620113e-05, 'epoch': 0.9636871508379888}\n","                                                  \n","{'loss': 1384.43681640625, 'learning_rate': 2.0083798882681566e-05, 'epoch': 0.9916201117318436}\n"," 33%|███▎      | 716/2148 [09:24<15:25,  1.55it/s][INFO|trainer.py:1412] 2021-01-05 09:46:01,047 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-05 09:46:01,047 >>   Num examples = 178\n","[INFO|trainer.py:1414] 2021-01-05 09:46:01,048 >>   Batch size = 2\n","\n","  0%|          | 0/89 [00:00<?, ?it/s]\u001b[A\n","  2%|▏         | 2/89 [00:04<03:31,  2.44s/it]\u001b[A\n","  3%|▎         | 3/89 [00:09<04:17,  2.99s/it]\u001b[A\n","  4%|▍         | 4/89 [00:11<04:07,  2.91s/it]\u001b[A\n","  6%|▌         | 5/89 [00:17<05:10,  3.70s/it]\u001b[A\n","  7%|▋         | 6/89 [00:20<04:49,  3.48s/it]\u001b[A\n","  8%|▊         | 7/89 [00:24<04:53,  3.58s/it]\u001b[A\n","  9%|▉         | 8/89 [00:28<05:02,  3.74s/it]\u001b[A\n"," 10%|█         | 9/89 [00:31<04:38,  3.48s/it]\u001b[A\n"," 11%|█         | 10/89 [00:34<04:39,  3.54s/it]\u001b[A\n"," 12%|█▏        | 11/89 [00:37<04:24,  3.39s/it]\u001b[A\n"," 13%|█▎        | 12/89 [00:41<04:15,  3.31s/it]\u001b[A\n"," 15%|█▍        | 13/89 [00:45<04:48,  3.79s/it]\u001b[A\n"," 16%|█▌        | 14/89 [00:48<04:26,  3.56s/it]\u001b[A\n"," 17%|█▋        | 15/89 [00:53<04:53,  3.97s/it]\u001b[A\n"," 18%|█▊        | 16/89 [00:58<04:53,  4.02s/it]\u001b[A\n"," 19%|█▉        | 17/89 [01:04<05:52,  4.90s/it]\u001b[A\n"," 20%|██        | 18/89 [01:10<05:51,  4.95s/it]\u001b[A\n"," 21%|██▏       | 19/89 [01:13<05:15,  4.51s/it]\u001b[A\n"," 22%|██▏       | 20/89 [01:17<05:03,  4.40s/it]\u001b[A\n"," 24%|██▎       | 21/89 [01:22<05:08,  4.54s/it]\u001b[A\n"," 25%|██▍       | 22/89 [01:26<04:47,  4.29s/it]\u001b[A\n"," 26%|██▌       | 23/89 [01:29<04:29,  4.08s/it]\u001b[A\n"," 27%|██▋       | 24/89 [01:35<05:01,  4.64s/it]\u001b[A\n"," 28%|██▊       | 25/89 [01:39<04:34,  4.30s/it]\u001b[A\n"," 29%|██▉       | 26/89 [01:42<04:16,  4.07s/it]\u001b[A\n"," 30%|███       | 27/89 [01:45<03:51,  3.73s/it]\u001b[A\n"," 31%|███▏      | 28/89 [01:50<04:09,  4.09s/it]\u001b[A\n"," 33%|███▎      | 29/89 [01:53<03:48,  3.81s/it]\u001b[A\n"," 34%|███▎      | 30/89 [01:59<04:22,  4.44s/it]\u001b[A\n"," 35%|███▍      | 31/89 [02:03<03:59,  4.13s/it]\u001b[A\n"," 36%|███▌      | 32/89 [02:09<04:24,  4.64s/it]\u001b[A\n"," 37%|███▋      | 33/89 [02:15<04:44,  5.07s/it]\u001b[A\n"," 38%|███▊      | 34/89 [02:18<04:03,  4.43s/it]\u001b[A\n"," 39%|███▉      | 35/89 [02:20<03:33,  3.95s/it]\u001b[A\n"," 40%|████      | 36/89 [02:24<03:20,  3.79s/it]\u001b[A\n"," 42%|████▏     | 37/89 [02:27<03:10,  3.66s/it]\u001b[A\n"," 43%|████▎     | 38/89 [02:30<02:56,  3.45s/it]\u001b[A\n"," 44%|████▍     | 39/89 [02:34<02:54,  3.49s/it]\u001b[A\n"," 45%|████▍     | 40/89 [02:39<03:25,  4.19s/it]\u001b[A\n"," 46%|████▌     | 41/89 [02:43<03:06,  3.90s/it]\u001b[A\n"," 47%|████▋     | 42/89 [02:46<02:57,  3.77s/it]\u001b[A\n"," 48%|████▊     | 43/89 [02:50<02:51,  3.74s/it]\u001b[A\n"," 49%|████▉     | 44/89 [02:56<03:16,  4.37s/it]\u001b[A\n"," 51%|█████     | 45/89 [03:00<03:15,  4.44s/it]\u001b[A\n"," 52%|█████▏    | 46/89 [03:06<03:23,  4.74s/it]\u001b[A\n"," 53%|█████▎    | 47/89 [03:12<03:38,  5.20s/it]\u001b[A\n"," 54%|█████▍    | 48/89 [03:17<03:34,  5.22s/it]\u001b[A\n"," 55%|█████▌    | 49/89 [03:22<03:26,  5.16s/it]\u001b[A\n"," 56%|█████▌    | 50/89 [03:28<03:23,  5.21s/it]\u001b[A\n"," 57%|█████▋    | 51/89 [03:33<03:14,  5.13s/it]\u001b[A\n"," 58%|█████▊    | 52/89 [03:36<02:54,  4.72s/it]\u001b[A\n"," 60%|█████▉    | 53/89 [03:40<02:42,  4.51s/it]\u001b[A\n"," 61%|██████    | 54/89 [03:47<02:55,  5.02s/it]\u001b[A\n"," 62%|██████▏   | 55/89 [03:53<03:02,  5.37s/it]\u001b[A\n"," 63%|██████▎   | 56/89 [03:57<02:43,  4.94s/it]\u001b[A\n"," 64%|██████▍   | 57/89 [04:02<02:45,  5.17s/it]\u001b[A\n"," 65%|██████▌   | 58/89 [04:07<02:39,  5.15s/it]\u001b[A\n"," 66%|██████▋   | 59/89 [04:12<02:27,  4.91s/it]\u001b[A\n"," 67%|██████▋   | 60/89 [04:17<02:24,  4.97s/it]\u001b[A\n"," 69%|██████▊   | 61/89 [04:20<01:58,  4.24s/it]\u001b[A\n"," 70%|██████▉   | 62/89 [04:24<01:55,  4.27s/it]\u001b[A\n"," 71%|███████   | 63/89 [04:29<01:59,  4.61s/it]\u001b[A\n"," 72%|███████▏  | 64/89 [04:32<01:43,  4.13s/it]\u001b[A\n"," 73%|███████▎  | 65/89 [04:37<01:41,  4.23s/it]\u001b[A\n"," 74%|███████▍  | 66/89 [04:41<01:34,  4.12s/it]\u001b[A\n"," 75%|███████▌  | 67/89 [04:45<01:29,  4.07s/it]\u001b[A\n"," 76%|███████▋  | 68/89 [04:51<01:39,  4.75s/it]\u001b[A\n"," 78%|███████▊  | 69/89 [04:55<01:33,  4.70s/it]\u001b[A\n"," 79%|███████▊  | 70/89 [04:59<01:24,  4.45s/it]\u001b[A\n"," 80%|███████▉  | 71/89 [05:04<01:20,  4.48s/it]\u001b[A\n"," 81%|████████  | 72/89 [05:08<01:15,  4.44s/it]\u001b[A\n"," 82%|████████▏ | 73/89 [05:13<01:14,  4.67s/it]\u001b[A\n"," 83%|████████▎ | 74/89 [05:18<01:10,  4.71s/it]\u001b[A\n"," 84%|████████▍ | 75/89 [05:22<01:03,  4.56s/it]\u001b[A\n"," 85%|████████▌ | 76/89 [05:27<01:00,  4.69s/it]\u001b[A\n"," 87%|████████▋ | 77/89 [05:33<00:57,  4.81s/it]\u001b[A\n"," 88%|████████▊ | 78/89 [05:38<00:56,  5.12s/it]\u001b[A\n"," 89%|████████▉ | 79/89 [05:43<00:48,  4.90s/it]\u001b[A\n"," 90%|████████▉ | 80/89 [05:48<00:44,  4.92s/it]\u001b[A\n"," 91%|█████████ | 81/89 [05:53<00:39,  4.95s/it]\u001b[A\n"," 92%|█████████▏| 82/89 [05:59<00:37,  5.36s/it]\u001b[A\n"," 93%|█████████▎| 83/89 [06:03<00:29,  4.93s/it]\u001b[A\n"," 94%|█████████▍| 84/89 [06:07<00:22,  4.57s/it]\u001b[A\n"," 96%|█████████▌| 85/89 [06:13<00:19,  4.96s/it]\u001b[A\n"," 97%|█████████▋| 86/89 [06:17<00:14,  4.70s/it]\u001b[A\n"," 98%|█████████▊| 87/89 [06:21<00:09,  4.61s/it]\u001b[A\n"," 99%|█████████▉| 88/89 [06:24<00:04,  4.07s/it]\u001b[A\n","                                                  {'eval_loss': 1217.4495849609375, 'eval_rouge1': 35.3328, 'eval_rouge2': 9.3389, 'eval_rougeL': 18.4634, 'eval_rougeLsum': 32.3447, 'eval_gen_len': 214.2, 'epoch': 1.0}\n","\n"," 33%|███▎      | 716/2148 [16:01<15:25,  1.55it/s]\n","100%|██████████| 89/89 [06:34<00:00,  4.48s/it]\u001b[A\n","{'loss': 1278.4517578125, 'learning_rate': 1.9944134078212292e-05, 'epoch': 1.005586592178771}\n"," 34%|███▍      | 730/2148 [16:12<43:35,  1.84s/it]{'loss': 1050.42353515625, 'learning_rate': 1.980446927374302e-05, 'epoch': 1.0195530726256983}\n"," 34%|███▍      | 740/2148 [16:19<17:59,  1.30it/s]\n","{'loss': 1113.14921875, 'learning_rate': 1.952513966480447e-05, 'epoch': 1.047486033519553}\n"," 35%|███▌      | 760/2148 [16:34<17:20,  1.33it/s]{'loss': 1258.016015625, 'learning_rate': 1.9385474860335198e-05, 'epoch': 1.0614525139664805}\n","                                                  \n","                                                  \n","{'loss': 1150.46328125, 'learning_rate': 1.8966480446927374e-05, 'epoch': 1.1033519553072626}\n"," 37%|███▋      | 800/2148 [17:04<23:31,  1.05s/it]{'loss': 1145.8517578125, 'learning_rate': 1.88268156424581e-05, 'epoch': 1.1173184357541899}\n","[INFO|trainer.py:1226] 2021-01-05 09:53:41,437 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-800\n","[INFO|configuration_utils.py:289] 2021-01-05 09:53:41,457 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-800/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 09:53:54,012 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-800/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 09:53:56,723 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-200] due to args.save_total_limit\n"," 38%|███▊      | 810/2148 [17:27<20:18,  1.10it/s]{'loss': 1174.7248046875, 'learning_rate': 1.8687150837988827e-05, 'epoch': 1.1312849162011174}\n","{'loss': 1154.27392578125, 'learning_rate': 1.8547486033519553e-05, 'epoch': 1.1452513966480447}\n","{'loss': 1249.80546875, 'learning_rate': 1.840782122905028e-05, 'epoch': 1.1592178770949721}\n","                                                  \n"," 40%|███▉      | 850/2148 [17:58<16:32,  1.31it/s]\n"," 40%|████      | 860/2148 [18:05<15:10,  1.41it/s]{'loss': 1244.2724609375, 'learning_rate': 1.798882681564246e-05, 'epoch': 1.2011173184357542}\n"," 41%|████      | 870/2148 [18:12<15:07,  1.41it/s]{'loss': 951.92470703125, 'learning_rate': 1.7849162011173182e-05, 'epoch': 1.2150837988826815}\n","                                                  {'loss': 1200.54716796875, 'learning_rate': 1.7709497206703912e-05, 'epoch': 1.229050279329609}\n","{'loss': 1096.41103515625, 'learning_rate': 1.756983240223464e-05, 'epoch': 1.2430167597765363}\n"," 42%|████▏     | 900/2148 [18:36<21:39,  1.04s/it]\n","                                                  \n"," 43%|████▎     | 920/2148 [18:50<15:20,  1.33it/s]{'loss': 1128.455859375, 'learning_rate': 1.7150837988826815e-05, 'epoch': 1.2849162011173183}\n"," 43%|████▎     | 930/2148 [18:58<15:12,  1.33it/s]{'loss': 1109.8013671875, 'learning_rate': 1.7011173184357544e-05, 'epoch': 1.2988826815642458}\n"," 44%|████▍     | 940/2148 [19:05<14:30,  1.39it/s]{'loss': 1114.0115234375, 'learning_rate': 1.6871508379888268e-05, 'epoch': 1.3128491620111733}\n","{'loss': 1039.58583984375, 'learning_rate': 1.6731843575418994e-05, 'epoch': 1.3268156424581006}\n","                                                  \n"," 45%|████▌     | 970/2148 [19:27<14:02,  1.40it/s]{'loss': 1069.4099609375, 'learning_rate': 1.6452513966480447e-05, 'epoch': 1.3547486033519553}\n"," 46%|████▌     | 980/2148 [19:34<13:42,  1.42it/s]{'loss': 1145.5923828125, 'learning_rate': 1.6312849162011173e-05, 'epoch': 1.3687150837988826}\n","                                                  {'loss': 1154.54384765625, 'learning_rate': 1.61731843575419e-05, 'epoch': 1.3826815642458101}\n","{'loss': 1047.57021484375, 'learning_rate': 1.6033519553072626e-05, 'epoch': 1.3966480446927374}\n"," 47%|████▋     | 1000/2148 [19:50<19:28,  1.02s/it][INFO|trainer.py:1226] 2021-01-05 09:56:27,001 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1000\n","[INFO|configuration_utils.py:289] 2021-01-05 09:56:27,010 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 09:56:37,401 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1000/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 09:56:37,634 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-400] due to args.save_total_limit\n"," 47%|████▋     | 1010/2148 [20:09<18:42,  1.01it/s]{'loss': 1179.84775390625, 'learning_rate': 1.5893854748603353e-05, 'epoch': 1.410614525139665}\n","{'loss': 1165.0462890625, 'learning_rate': 1.575418994413408e-05, 'epoch': 1.4245810055865922}\n","{'loss': 1144.07490234375, 'learning_rate': 1.5614525139664806e-05, 'epoch': 1.4385474860335195}\n"," 48%|████▊     | 1040/2148 [20:32<12:44,  1.45it/s]\n","{'loss': 1068.70087890625, 'learning_rate': 1.533519553072626e-05, 'epoch': 1.4664804469273742}\n","                                                   \n"," 50%|████▉     | 1070/2148 [20:54<12:41,  1.42it/s]{'loss': 1171.60244140625, 'learning_rate': 1.5055865921787711e-05, 'epoch': 1.494413407821229}\n","{'loss': 1133.15283203125, 'learning_rate': 1.4916201117318435e-05, 'epoch': 1.5083798882681565}\n"," 51%|█████     | 1090/2148 [21:09<14:02,  1.26it/s]{'loss': 1163.7044921875, 'learning_rate': 1.4776536312849163e-05, 'epoch': 1.5223463687150838}\n"," 51%|█████     | 1100/2148 [21:17<19:04,  1.09s/it]{'loss': 946.576171875, 'learning_rate': 1.4636871508379887e-05, 'epoch': 1.536312849162011}\n"," 52%|█████▏    | 1110/2148 [21:24<13:08,  1.32it/s]{'loss': 997.46240234375, 'learning_rate': 1.4497206703910616e-05, 'epoch': 1.5502793296089385}\n"," 52%|█████▏    | 1120/2148 [21:31<12:29,  1.37it/s]{'loss': 976.91552734375, 'learning_rate': 1.435754189944134e-05, 'epoch': 1.564245810055866}\n","{'loss': 1209.52919921875, 'learning_rate': 1.4217877094972069e-05, 'epoch': 1.5782122905027933}\n","                                                   \n","                                                   \n","{'loss': 1133.62197265625, 'learning_rate': 1.3798882681564246e-05, 'epoch': 1.6201117318435754}\n","{'loss': 983.85595703125, 'learning_rate': 1.3659217877094973e-05, 'epoch': 1.6340782122905027}\n","                                                   \n","                                                   \n","{'loss': 1077.9244140625, 'learning_rate': 1.324022346368715e-05, 'epoch': 1.675977653631285}\n"," 56%|█████▌    | 1200/2148 [22:31<16:08,  1.02s/it][INFO|trainer.py:1226] 2021-01-05 09:59:07,734 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1200\n","[INFO|configuration_utils.py:289] 2021-01-05 09:59:07,747 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1200/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 09:59:18,072 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1200/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 09:59:18,523 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-600] due to args.save_total_limit\n","{'loss': 914.71962890625, 'learning_rate': 1.3100558659217879e-05, 'epoch': 1.6899441340782122}\n","{'loss': 1041.3541015625, 'learning_rate': 1.2960893854748603e-05, 'epoch': 1.7039106145251397}\n"," 57%|█████▋    | 1230/2148 [23:04<11:18,  1.35it/s]\n"," 58%|█████▊    | 1240/2148 [23:12<10:58,  1.38it/s]{'loss': 1050.9951171875, 'learning_rate': 1.2681564245810056e-05, 'epoch': 1.7318435754189943}\n"," 58%|█████▊    | 1250/2148 [23:19<09:42,  1.54it/s]{'loss': 806.844287109375, 'learning_rate': 1.2541899441340781e-05, 'epoch': 1.7458100558659218}\n","{'loss': 1013.75771484375, 'learning_rate': 1.2402234636871509e-05, 'epoch': 1.7597765363128492}\n","{'loss': 1036.046484375, 'learning_rate': 1.2262569832402234e-05, 'epoch': 1.7737430167597765}\n"," 60%|█████▉    | 1280/2148 [23:41<09:59,  1.45it/s]{'loss': 1064.40234375, 'learning_rate': 1.2122905027932962e-05, 'epoch': 1.7877094972067038}\n","{'loss': 1242.92236328125, 'learning_rate': 1.1983240223463687e-05, 'epoch': 1.8016759776536313}\n"," 61%|██████    | 1300/2148 [23:56<13:59,  1.01it/s]{'loss': 982.23203125, 'learning_rate': 1.1843575418994415e-05, 'epoch': 1.8156424581005588}\n","                                                   \n","{'loss': 1125.01826171875, 'learning_rate': 1.1564245810055866e-05, 'epoch': 1.8435754189944134}\n"," 62%|██████▏   | 1330/2148 [24:18<09:27,  1.44it/s]{'loss': 1047.997265625, 'learning_rate': 1.1424581005586593e-05, 'epoch': 1.8575418994413408}\n","{'loss': 840.9642578125, 'learning_rate': 1.1284916201117319e-05, 'epoch': 1.8715083798882681}\n"," 63%|██████▎   | 1350/2148 [24:32<09:14,  1.44it/s]{'loss': 1012.25244140625, 'learning_rate': 1.1145251396648046e-05, 'epoch': 1.8854748603351954}\n","{'loss': 1075.9064453125, 'learning_rate': 1.1005586592178772e-05, 'epoch': 1.899441340782123}\n"," 64%|██████▍   | 1370/2148 [24:46<09:46,  1.33it/s]{'loss': 1093.4677734375, 'learning_rate': 1.0865921787709497e-05, 'epoch': 1.9134078212290504}\n"," 64%|██████▍   | 1380/2148 [24:54<09:15,  1.38it/s]{'loss': 990.504296875, 'learning_rate': 1.0726256983240223e-05, 'epoch': 1.9273743016759777}\n"," 65%|██████▍   | 1390/2148 [25:01<09:08,  1.38it/s]\n","{'loss': 1216.39765625, 'learning_rate': 1.0446927374301676e-05, 'epoch': 1.9553072625698324}\n"," 65%|██████▌   | 1400/2148 [25:09<12:38,  1.01s/it][INFO|trainer.py:1226] 2021-01-05 10:01:46,004 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1400\n","[INFO|configuration_utils.py:289] 2021-01-05 10:01:46,014 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1400/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 10:01:56,327 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1400/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 10:01:56,439 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-800] due to args.save_total_limit\n","{'loss': 1124.1408203125, 'learning_rate': 1.0307262569832403e-05, 'epoch': 1.9692737430167597}\n"," 66%|██████▌   | 1420/2148 [25:35<09:06,  1.33it/s]{'loss': 958.07275390625, 'learning_rate': 1.0167597765363127e-05, 'epoch': 1.983240223463687}\n"," 67%|██████▋   | 1430/2148 [25:42<09:13,  1.30it/s]\n"," 67%|██████▋   | 1432/2148 [25:44<08:10,  1.46it/s][INFO|trainer.py:1412] 2021-01-05 10:02:20,736 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-05 10:02:20,736 >>   Num examples = 178\n","[INFO|trainer.py:1414] 2021-01-05 10:02:20,736 >>   Batch size = 2\n","\n","  0%|          | 0/89 [00:00<?, ?it/s]\u001b[A\n","  2%|▏         | 2/89 [00:05<04:16,  2.95s/it]\u001b[A\n","  3%|▎         | 3/89 [00:10<04:57,  3.46s/it]\u001b[A\n","  4%|▍         | 4/89 [00:13<04:34,  3.23s/it]\u001b[A\n","  6%|▌         | 5/89 [00:19<05:43,  4.09s/it]\u001b[A\n","  7%|▋         | 6/89 [00:22<05:07,  3.71s/it]\u001b[A\n","  8%|▊         | 7/89 [00:28<06:05,  4.46s/it]\u001b[A\n","  9%|▉         | 8/89 [00:32<05:46,  4.27s/it]\u001b[A\n"," 10%|█         | 9/89 [00:36<05:41,  4.27s/it]\u001b[A\n"," 11%|█         | 10/89 [00:39<05:12,  3.96s/it]\u001b[A\n"," 12%|█▏        | 11/89 [00:43<05:09,  3.97s/it]\u001b[A\n"," 13%|█▎        | 12/89 [00:48<05:33,  4.32s/it]\u001b[A\n"," 15%|█▍        | 13/89 [00:54<06:06,  4.82s/it]\u001b[A\n"," 16%|█▌        | 14/89 [00:57<05:15,  4.20s/it]\u001b[A\n"," 17%|█▋        | 15/89 [01:03<05:56,  4.82s/it]\u001b[A\n"," 18%|█▊        | 16/89 [01:08<05:40,  4.67s/it]\u001b[A\n"," 19%|█▉        | 17/89 [01:14<06:10,  5.14s/it]\u001b[A\n"," 20%|██        | 18/89 [01:20<06:26,  5.44s/it]\u001b[A\n"," 21%|██▏       | 19/89 [01:26<06:38,  5.69s/it]\u001b[A\n"," 22%|██▏       | 20/89 [01:30<05:56,  5.17s/it]\u001b[A\n"," 24%|██▎       | 21/89 [01:36<06:10,  5.45s/it]\u001b[A\n"," 25%|██▍       | 22/89 [01:40<05:33,  4.98s/it]\u001b[A\n"," 26%|██▌       | 23/89 [01:44<05:03,  4.59s/it]\u001b[A\n"," 27%|██▋       | 24/89 [01:50<05:26,  5.02s/it]\u001b[A\n"," 28%|██▊       | 25/89 [01:56<05:33,  5.22s/it]\u001b[A\n"," 29%|██▉       | 26/89 [02:01<05:27,  5.20s/it]\u001b[A\n"," 30%|███       | 27/89 [02:06<05:21,  5.18s/it]\u001b[A\n"," 31%|███▏      | 28/89 [02:11<05:12,  5.13s/it]\u001b[A\n"," 33%|███▎      | 29/89 [02:16<05:04,  5.07s/it]\u001b[A\n"," 34%|███▎      | 30/89 [02:21<05:07,  5.20s/it]\u001b[A\n"," 35%|███▍      | 31/89 [02:28<05:23,  5.57s/it]\u001b[A\n"," 36%|███▌      | 32/89 [02:34<05:24,  5.69s/it]\u001b[A\n"," 37%|███▋      | 33/89 [02:40<05:30,  5.91s/it]\u001b[A\n"," 38%|███▊      | 34/89 [02:44<04:48,  5.25s/it]\u001b[A\n"," 39%|███▉      | 35/89 [02:48<04:18,  4.78s/it]\u001b[A\n"," 40%|████      | 36/89 [02:52<04:13,  4.78s/it]\u001b[A\n"," 42%|████▏     | 37/89 [02:57<04:08,  4.78s/it]\u001b[A\n"," 43%|████▎     | 38/89 [03:03<04:21,  5.12s/it]\u001b[A\n"," 44%|████▍     | 39/89 [03:09<04:30,  5.40s/it]\u001b[A\n"," 45%|████▍     | 40/89 [03:15<04:30,  5.52s/it]\u001b[A\n"," 46%|████▌     | 41/89 [03:20<04:19,  5.40s/it]\u001b[A\n"," 47%|████▋     | 42/89 [03:24<03:51,  4.93s/it]\u001b[A\n"," 48%|████▊     | 43/89 [03:30<04:04,  5.33s/it]\u001b[A\n"," 49%|████▉     | 44/89 [03:36<04:11,  5.60s/it]\u001b[A\n"," 51%|█████     | 45/89 [03:39<03:23,  4.62s/it]\u001b[A\n"," 52%|█████▏    | 46/89 [03:44<03:30,  4.89s/it]\u001b[A\n"," 53%|█████▎    | 47/89 [03:49<03:23,  4.85s/it]\u001b[A\n"," 54%|█████▍    | 48/89 [03:55<03:35,  5.25s/it]\u001b[A\n"," 55%|█████▌    | 49/89 [04:01<03:41,  5.55s/it]\u001b[A\n"," 56%|█████▌    | 50/89 [04:08<03:45,  5.77s/it]\u001b[A\n"," 57%|█████▋    | 51/89 [04:13<03:36,  5.69s/it]\u001b[A\n"," 58%|█████▊    | 52/89 [04:17<03:10,  5.15s/it]\u001b[A\n"," 60%|█████▉    | 53/89 [04:20<02:44,  4.57s/it]\u001b[A\n"," 61%|██████    | 54/89 [04:26<02:55,  5.03s/it]\u001b[A\n"," 62%|██████▏   | 55/89 [04:32<03:00,  5.30s/it]\u001b[A\n"," 63%|██████▎   | 56/89 [04:37<02:43,  4.95s/it]\u001b[A\n"," 64%|██████▍   | 57/89 [04:43<02:48,  5.28s/it]\u001b[A\n"," 65%|██████▌   | 58/89 [04:49<02:51,  5.53s/it]\u001b[A\n"," 66%|██████▋   | 59/89 [04:51<02:18,  4.62s/it]\u001b[A\n"," 67%|██████▋   | 60/89 [04:57<02:25,  5.03s/it]\u001b[A\n"," 69%|██████▊   | 61/89 [05:00<02:05,  4.48s/it]\u001b[A\n"," 70%|██████▉   | 62/89 [05:05<02:03,  4.57s/it]\u001b[A\n"," 71%|███████   | 63/89 [05:11<02:11,  5.06s/it]\u001b[A\n"," 72%|███████▏  | 64/89 [05:16<02:03,  4.94s/it]\u001b[A\n"," 73%|███████▎  | 65/89 [05:22<02:04,  5.20s/it]\u001b[A\n"," 74%|███████▍  | 66/89 [05:28<02:07,  5.54s/it]\u001b[A\n"," 75%|███████▌  | 67/89 [05:34<02:05,  5.68s/it]\u001b[A\n"," 76%|███████▋  | 68/89 [05:39<01:53,  5.42s/it]\u001b[A\n"," 78%|███████▊  | 69/89 [05:45<01:52,  5.65s/it]\u001b[A\n"," 79%|███████▊  | 70/89 [05:51<01:46,  5.62s/it]\u001b[A\n"," 80%|███████▉  | 71/89 [05:57<01:42,  5.72s/it]\u001b[A\n"," 81%|████████  | 72/89 [06:01<01:31,  5.40s/it]\u001b[A\n"," 82%|████████▏ | 73/89 [06:05<01:20,  5.03s/it]\u001b[A\n"," 83%|████████▎ | 74/89 [06:10<01:15,  5.02s/it]\u001b[A\n"," 84%|████████▍ | 75/89 [06:16<01:10,  5.06s/it]\u001b[A\n"," 85%|████████▌ | 76/89 [06:20<01:02,  4.84s/it]\u001b[A\n"," 87%|████████▋ | 77/89 [06:25<00:57,  4.76s/it]\u001b[A"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l8hQT6ksBWin"},"source":["##### Evaluate"]},{"cell_type":"code","metadata":{"id":"HtAWnVDlBWio"},"source":["source_test_dir = data_dir[:-1] + '/test.source\"'\n","reference_test_dir = data_dir[:-1] + '/test.target\"'\n","\n","save_dir = output_dir[:-1] + '/'+model_name_or_path.replace('/', '?')+'_test_karger_books_cc.txt\"'\n","score_dir = output_dir[:-1] + '/'+model_name_or_path.replace('/', '?')+'_test_karger_books_cc.json\"'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FIUZ_59SBWip"},"source":["!python3 $eval_script \\\n","$output_dir \\\n","$source_test_dir \\\n","$save_dir \\\n","--reference_path $reference_test_dir \\\n","--score_path $score_dir \\\n","--task summarization \\\n","--bs 2 \\\n","--length_penalty $config.LENAGTH_PENALTY \\\n","--no_repeat_ngram_size $config.NO_REPEAT_NGRAM_SIZE \\\n","--num_beams $config.NUM_BEAMS \\\n","--dump-args"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d5V0QCdf04Yx"},"source":["### Karger Books MOC"]},{"cell_type":"code","metadata":{"id":"tbkgvPA_Gsrm"},"source":["model_name_or_path = 'sshleifer/distilbart-cnn-12-6'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zjjlq58-5bnN"},"source":["data_dir = '\"/content/drive/My Drive/MAGMA: Summarization/datasets/karger_books_moc\"'\n","\n","output_dir = '\"/content/drive/My Drive/MAGMA: Summarization/fine-tuning/'+\\\n","    model_name_or_path.replace('/', '?')+'_karger_books_moc_train\"'\n","log_dir = output_dir + '/logs'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KQj3gt6s5ACz"},"source":["##### Fine tune"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GTE-A75XDtJc","executionInfo":{"status":"ok","timestamp":1609842933572,"user_tz":-60,"elapsed":3601138,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"95a947c2-18eb-48a2-8e72-66f2a18e1090"},"source":["!python3 $finetune_script \\\n","--model_name_or_path facebook/bart-large-cnn \\\n","--tokenizer_name facebook/bart-large-cnn \\\n","--data_dir $data_dir \\\n","--learning_rate 3e-5 --label_smoothing 0.1 \\\n","--sortish_sampler --freeze_embeds --adafactor \\\n","--task summarization \\\n","--max_source_length 1024 \\\n","--max_target_length $config.BULLETS_MAX_LEN \\\n","--val_max_target_length $config.BULLETS_MAX_LEN \\\n","--test_max_target_length $config.BULLETS_MAX_LEN \\\n","--do_train \\\n","--logging_steps 10 --save_steps 200 --save_total_limit 3 \\\n","--per_device_train_batch_size 2 --per_device_eval_batch_size 2 \\\n","--do_eval --evaluation_strategy epoch --eval_beams 2 \\\n","--metric_for_best_model rougeL --greater_is_better True \\\n","--predict_with_generate \\\n","--output_dir $output_dir \\\n","--overwrite_output_dir \\\n","--seed $config.SEED"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-01-05 09:35:34.078850: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n","01/05/2021 09:35:39 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","01/05/2021 09:35:39 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jan05_09-35-39_14689f7f2f2b', logging_first_step=False, logging_steps=10, save_steps=200, save_total_limit=3, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model='rougeL', greater_is_better='True', ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False, label_smoothing=0.1, sortish_sampler=True, predict_with_generate=True, adafactor=True, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear')\n","01/05/2021 09:35:39 - INFO - filelock -   Lock 139985421790568 acquired on /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.30bea5d0a276dd70740d78f636e2c8e880d88bad362586009715702be8bda196.lock\n","[INFO|file_utils.py:1301] 2021-01-05 09:35:39,821 >> https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2nmj0h2h\n","Downloading: 100% 1.34k/1.34k [00:00<00:00, 1.83MB/s]\n","[INFO|file_utils.py:1305] 2021-01-05 09:35:39,871 >> storing https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.30bea5d0a276dd70740d78f636e2c8e880d88bad362586009715702be8bda196\n","[INFO|file_utils.py:1308] 2021-01-05 09:35:39,871 >> creating metadata file for /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.30bea5d0a276dd70740d78f636e2c8e880d88bad362586009715702be8bda196\n","01/05/2021 09:35:39 - INFO - filelock -   Lock 139985421790568 released on /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.30bea5d0a276dd70740d78f636e2c8e880d88bad362586009715702be8bda196.lock\n","[INFO|configuration_utils.py:431] 2021-01-05 09:35:39,872 >> loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.30bea5d0a276dd70740d78f636e2c8e880d88bad362586009715702be8bda196\n","[INFO|configuration_utils.py:467] 2021-01-05 09:35:39,873 >> Model config BartConfig {\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"do_blenderbot_90_layernorm\": false,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","[INFO|configuration_utils.py:431] 2021-01-05 09:35:39,921 >> loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.30bea5d0a276dd70740d78f636e2c8e880d88bad362586009715702be8bda196\n","[INFO|configuration_utils.py:467] 2021-01-05 09:35:39,922 >> Model config BartConfig {\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"do_blenderbot_90_layernorm\": false,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","01/05/2021 09:35:39 - INFO - filelock -   Lock 139985421788048 acquired on /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n","[INFO|file_utils.py:1301] 2021-01-05 09:35:39,978 >> https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpszrbekhm\n","Downloading: 100% 899k/899k [00:00<00:00, 11.7MB/s]\n","[INFO|file_utils.py:1305] 2021-01-05 09:35:40,110 >> storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|file_utils.py:1308] 2021-01-05 09:35:40,110 >> creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","01/05/2021 09:35:40 - INFO - filelock -   Lock 139985421788048 released on /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n","01/05/2021 09:35:40 - INFO - filelock -   Lock 139985421787656 acquired on /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n","[INFO|file_utils.py:1301] 2021-01-05 09:35:40,164 >> https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7z8a8mo5\n","Downloading: 100% 456k/456k [00:00<00:00, 7.37MB/s]\n","[INFO|file_utils.py:1305] 2021-01-05 09:35:40,283 >> storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|file_utils.py:1308] 2021-01-05 09:35:40,283 >> creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","01/05/2021 09:35:40 - INFO - filelock -   Lock 139985421787656 released on /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n","01/05/2021 09:35:40 - INFO - filelock -   Lock 139985421733336 acquired on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n","[INFO|file_utils.py:1301] 2021-01-05 09:35:40,344 >> https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp3v_11jdi\n","Downloading: 100% 1.36M/1.36M [00:00<00:00, 15.7MB/s]\n","[INFO|file_utils.py:1305] 2021-01-05 09:35:40,491 >> storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|file_utils.py:1308] 2021-01-05 09:35:40,491 >> creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","01/05/2021 09:35:40 - INFO - filelock -   Lock 139985421733336 released on /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730.lock\n","[INFO|tokenization_utils_base.py:1802] 2021-01-05 09:35:40,491 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|tokenization_utils_base.py:1802] 2021-01-05 09:35:40,491 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1802] 2021-01-05 09:35:40,491 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","01/05/2021 09:35:40 - INFO - filelock -   Lock 139985421732608 acquired on /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300.lock\n","[INFO|file_utils.py:1301] 2021-01-05 09:35:40,607 >> https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4ol_xa84\n","Downloading: 100% 1.63G/1.63G [00:19<00:00, 84.7MB/s]\n","[INFO|file_utils.py:1305] 2021-01-05 09:36:00,015 >> storing https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n","[INFO|file_utils.py:1308] 2021-01-05 09:36:00,015 >> creating metadata file for /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n","01/05/2021 09:36:00 - INFO - filelock -   Lock 139985421732608 released on /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300.lock\n","[INFO|modeling_utils.py:1024] 2021-01-05 09:36:00,016 >> loading weights file https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n","[INFO|modeling_utils.py:1140] 2021-01-05 09:36:19,210 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:1149] 2021-01-05 09:36:19,210 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n","01/05/2021 09:36:19 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n","01/05/2021 09:36:35 - INFO - __main__ -   *** Train ***\n","[INFO|trainer.py:703] 2021-01-05 09:36:35,187 >> ***** Running training *****\n","[INFO|trainer.py:704] 2021-01-05 09:36:35,187 >>   Num examples = 1431\n","[INFO|trainer.py:705] 2021-01-05 09:36:35,187 >>   Num Epochs = 3\n","[INFO|trainer.py:706] 2021-01-05 09:36:35,187 >>   Instantaneous batch size per device = 2\n","[INFO|trainer.py:707] 2021-01-05 09:36:35,187 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n","[INFO|trainer.py:708] 2021-01-05 09:36:35,187 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:709] 2021-01-05 09:36:35,187 >>   Total optimization steps = 2148\n","/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train\n","[INFO|integrations.py:371] 2021-01-05 09:36:35,217 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.12\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/huggingface/runs/exbr6eju\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210105_093635-exbr6eju\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","  0%|          | 0/2148 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:506: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n","  0%|          | 10/2148 [00:07<25:15,  1.41it/s]{'loss': 1682.92421875, 'learning_rate': 2.9860335195530727e-05, 'epoch': 0.013966480446927373}\n","  1%|          | 20/2148 [00:15<25:22,  1.40it/s]\n","                                                 \n","                                                 \n","  2%|▏         | 50/2148 [00:35<23:50,  1.47it/s]{'loss': 1590.43017578125, 'learning_rate': 2.9301675977653633e-05, 'epoch': 0.06983240223463687}\n","                                                 \n","  3%|▎         | 70/2148 [00:50<24:47,  1.40it/s]{'loss': 1516.40869140625, 'learning_rate': 2.9022346368715086e-05, 'epoch': 0.09776536312849161}\n","{'loss': 1645.2828125, 'learning_rate': 2.8882681564245812e-05, 'epoch': 0.11173184357541899}\n","  4%|▍         | 90/2148 [01:04<25:19,  1.35it/s]{'loss': 1684.296875, 'learning_rate': 2.8743016759776535e-05, 'epoch': 0.12569832402234637}\n","{'loss': 1828.2017578125, 'learning_rate': 2.8603351955307262e-05, 'epoch': 0.13966480446927373}\n","  5%|▌         | 110/2148 [01:20<23:12,  1.46it/s]{'loss': 1530.19990234375, 'learning_rate': 2.8463687150837992e-05, 'epoch': 0.15363128491620112}\n","                                                  \n","                                                  \n","{'loss': 1814.703125, 'learning_rate': 2.8044692737430168e-05, 'epoch': 0.19553072625698323}\n","{'loss': 1507.79228515625, 'learning_rate': 2.7905027932960894e-05, 'epoch': 0.20949720670391062}\n","{'loss': 1579.16943359375, 'learning_rate': 2.776536312849162e-05, 'epoch': 0.22346368715083798}\n","  8%|▊         | 170/2148 [02:04<25:09,  1.31it/s]{'loss': 1399.08515625, 'learning_rate': 2.7625698324022347e-05, 'epoch': 0.23743016759776536}\n","                                                  {'loss': 1414.25263671875, 'learning_rate': 2.7486033519553074e-05, 'epoch': 0.25139664804469275}\n","  9%|▉         | 190/2148 [02:18<22:17,  1.46it/s]{'loss': 1479.02119140625, 'learning_rate': 2.7346368715083797e-05, 'epoch': 0.26536312849162014}\n","{'loss': 1445.9916015625, 'learning_rate': 2.7206703910614527e-05, 'epoch': 0.27932960893854747}\n","  9%|▉         | 200/2148 [02:26<32:45,  1.01s/it][INFO|trainer.py:1226] 2021-01-05 09:39:03,043 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-200\n","[INFO|configuration_utils.py:289] 2021-01-05 09:39:03,052 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-200/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 09:39:13,457 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-200/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 09:39:13,782 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1600] due to args.save_total_limit\n","{'loss': 1473.55888671875, 'learning_rate': 2.7067039106145253e-05, 'epoch': 0.29329608938547486}\n"," 10%|█         | 220/2148 [02:51<24:09,  1.33it/s]{'loss': 1381.36640625, 'learning_rate': 2.692737430167598e-05, 'epoch': 0.30726256983240224}\n"," 11%|█         | 230/2148 [03:00<26:10,  1.22it/s]{'loss': 1602.31201171875, 'learning_rate': 2.6787709497206703e-05, 'epoch': 0.32122905027932963}\n"," 11%|█         | 240/2148 [03:08<24:04,  1.32it/s]{'loss': 1598.358984375, 'learning_rate': 2.6648044692737432e-05, 'epoch': 0.33519553072625696}\n","                                                  \n","{'loss': 1459.236328125, 'learning_rate': 2.6368715083798882e-05, 'epoch': 0.36312849162011174}\n"," 13%|█▎        | 270/2148 [03:29<23:12,  1.35it/s]{'loss': 1462.09404296875, 'learning_rate': 2.622905027932961e-05, 'epoch': 0.3770949720670391}\n","{'loss': 1357.1357421875, 'learning_rate': 2.6089385474860338e-05, 'epoch': 0.39106145251396646}\n","{'loss': 1523.86552734375, 'learning_rate': 2.594972067039106e-05, 'epoch': 0.40502793296089384}\n"," 14%|█▍        | 300/2148 [03:52<30:10,  1.02it/s]\n"," 14%|█▍        | 310/2148 [03:59<22:19,  1.37it/s]{'loss': 1470.3703125, 'learning_rate': 2.5670391061452514e-05, 'epoch': 0.4329608938547486}\n"," 15%|█▍        | 320/2148 [04:06<22:31,  1.35it/s]{'loss': 1628.3712890625, 'learning_rate': 2.553072625698324e-05, 'epoch': 0.44692737430167595}\n","                                                  \n"," 16%|█▌        | 340/2148 [04:20<21:00,  1.43it/s]{'loss': 1518.19853515625, 'learning_rate': 2.5251396648044694e-05, 'epoch': 0.4748603351955307}\n"," 16%|█▋        | 350/2148 [04:27<21:42,  1.38it/s]\n","{'loss': 1510.19111328125, 'learning_rate': 2.4972067039106143e-05, 'epoch': 0.5027932960893855}\n"," 17%|█▋        | 370/2148 [04:42<21:18,  1.39it/s]{'loss': 1290.8939453125, 'learning_rate': 2.4832402234636873e-05, 'epoch': 0.5167597765363129}\n","{'loss': 1559.25, 'learning_rate': 2.46927374301676e-05, 'epoch': 0.5307262569832403}\n","{'loss': 1484.6267578125, 'learning_rate': 2.4553072625698326e-05, 'epoch': 0.5446927374301676}\n","                                                  \n"," 19%|█▊        | 400/2148 [05:04<30:59,  1.06s/it][INFO|trainer.py:1226] 2021-01-05 09:41:41,081 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-400\n","[INFO|configuration_utils.py:289] 2021-01-05 09:41:41,096 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-400/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 09:41:51,113 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-400/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 09:41:51,283 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1800] due to args.save_total_limit\n"," 19%|█▉        | 410/2148 [05:22<25:26,  1.14it/s]{'loss': 1385.61640625, 'learning_rate': 2.427374301675978e-05, 'epoch': 0.5726256983240223}\n"," 20%|█▉        | 420/2148 [05:29<21:03,  1.37it/s]{'loss': 1431.134765625, 'learning_rate': 2.4134078212290505e-05, 'epoch': 0.5865921787709497}\n"," 20%|██        | 430/2148 [05:37<21:06,  1.36it/s]{'loss': 1480.61708984375, 'learning_rate': 2.399441340782123e-05, 'epoch': 0.6005586592178771}\n"," 20%|██        | 440/2148 [05:44<20:32,  1.39it/s]{'loss': 1326.96455078125, 'learning_rate': 2.3854748603351955e-05, 'epoch': 0.6145251396648045}\n"," 21%|██        | 450/2148 [05:51<21:47,  1.30it/s]{'loss': 1232.02333984375, 'learning_rate': 2.3715083798882685e-05, 'epoch': 0.6284916201117319}\n","{'loss': 1574.02421875, 'learning_rate': 2.3575418994413408e-05, 'epoch': 0.6424581005586593}\n","{'loss': 1356.91923828125, 'learning_rate': 2.3435754189944134e-05, 'epoch': 0.6564245810055865}\n"," 22%|██▏       | 480/2148 [06:13<20:01,  1.39it/s]{'loss': 1476.4509765625, 'learning_rate': 2.329608938547486e-05, 'epoch': 0.6703910614525139}\n","                                                  \n","                                                  \n","{'loss': 1386.1984375, 'learning_rate': 2.287709497206704e-05, 'epoch': 0.7122905027932961}\n","                                                  \n"," 25%|██▍       | 530/2148 [06:52<19:54,  1.35it/s]{'loss': 1409.81982421875, 'learning_rate': 2.259776536312849e-05, 'epoch': 0.7402234636871509}\n","{'loss': 1214.009375, 'learning_rate': 2.245810055865922e-05, 'epoch': 0.7541899441340782}\n","                                                  \n","                                                  \n"," 27%|██▋       | 570/2148 [07:22<20:27,  1.29it/s]{'loss': 1182.7611328125, 'learning_rate': 2.2039106145251395e-05, 'epoch': 0.7960893854748603}\n","{'loss': 1421.88798828125, 'learning_rate': 2.1899441340782125e-05, 'epoch': 0.8100558659217877}\n"," 27%|██▋       | 590/2148 [07:35<18:47,  1.38it/s]\n","                                                  \n"," 28%|██▊       | 600/2148 [07:43<24:38,  1.05it/s][INFO|trainer.py:1226] 2021-01-05 09:44:20,433 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-600\n","[INFO|configuration_utils.py:289] 2021-01-05 09:44:20,441 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-600/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 09:44:29,957 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-600/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 09:44:30,245 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-2000] due to args.save_total_limit\n"," 28%|██▊       | 610/2148 [08:06<26:09,  1.02s/it]{'loss': 1362.52412109375, 'learning_rate': 2.14804469273743e-05, 'epoch': 0.8519553072625698}\n","{'loss': 1200.7265625, 'learning_rate': 2.1340782122905028e-05, 'epoch': 0.8659217877094972}\n"," 29%|██▉       | 630/2148 [08:21<17:54,  1.41it/s]{'loss': 1154.9220703125, 'learning_rate': 2.1201117318435754e-05, 'epoch': 0.8798882681564246}\n"," 30%|██▉       | 640/2148 [08:29<19:10,  1.31it/s]{'loss': 1356.534375, 'learning_rate': 2.106145251396648e-05, 'epoch': 0.8938547486033519}\n","                                                  \n","{'loss': 1127.35146484375, 'learning_rate': 2.0782122905027933e-05, 'epoch': 0.9217877094972067}\n","                                                  \n","{'loss': 1281.20322265625, 'learning_rate': 2.0502793296089386e-05, 'epoch': 0.9497206703910615}\n","{'loss': 1344.1662109375, 'learning_rate': 2.0363128491620113e-05, 'epoch': 0.9636871508379888}\n","                                                  \n","{'loss': 1384.43681640625, 'learning_rate': 2.0083798882681566e-05, 'epoch': 0.9916201117318436}\n"," 33%|███▎      | 716/2148 [09:24<15:25,  1.55it/s][INFO|trainer.py:1412] 2021-01-05 09:46:01,047 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-05 09:46:01,047 >>   Num examples = 178\n","[INFO|trainer.py:1414] 2021-01-05 09:46:01,048 >>   Batch size = 2\n","\n","  0%|          | 0/89 [00:00<?, ?it/s]\u001b[A\n","  2%|▏         | 2/89 [00:04<03:31,  2.44s/it]\u001b[A\n","  3%|▎         | 3/89 [00:09<04:17,  2.99s/it]\u001b[A\n","  4%|▍         | 4/89 [00:11<04:07,  2.91s/it]\u001b[A\n","  6%|▌         | 5/89 [00:17<05:10,  3.70s/it]\u001b[A\n","  7%|▋         | 6/89 [00:20<04:49,  3.48s/it]\u001b[A\n","  8%|▊         | 7/89 [00:24<04:53,  3.58s/it]\u001b[A\n","  9%|▉         | 8/89 [00:28<05:02,  3.74s/it]\u001b[A\n"," 10%|█         | 9/89 [00:31<04:38,  3.48s/it]\u001b[A\n"," 11%|█         | 10/89 [00:34<04:39,  3.54s/it]\u001b[A\n"," 12%|█▏        | 11/89 [00:37<04:24,  3.39s/it]\u001b[A\n"," 13%|█▎        | 12/89 [00:41<04:15,  3.31s/it]\u001b[A\n"," 15%|█▍        | 13/89 [00:45<04:48,  3.79s/it]\u001b[A\n"," 16%|█▌        | 14/89 [00:48<04:26,  3.56s/it]\u001b[A\n"," 17%|█▋        | 15/89 [00:53<04:53,  3.97s/it]\u001b[A\n"," 18%|█▊        | 16/89 [00:58<04:53,  4.02s/it]\u001b[A\n"," 19%|█▉        | 17/89 [01:04<05:52,  4.90s/it]\u001b[A\n"," 20%|██        | 18/89 [01:10<05:51,  4.95s/it]\u001b[A\n"," 21%|██▏       | 19/89 [01:13<05:15,  4.51s/it]\u001b[A\n"," 22%|██▏       | 20/89 [01:17<05:03,  4.40s/it]\u001b[A\n"," 24%|██▎       | 21/89 [01:22<05:08,  4.54s/it]\u001b[A\n"," 25%|██▍       | 22/89 [01:26<04:47,  4.29s/it]\u001b[A\n"," 26%|██▌       | 23/89 [01:29<04:29,  4.08s/it]\u001b[A\n"," 27%|██▋       | 24/89 [01:35<05:01,  4.64s/it]\u001b[A\n"," 28%|██▊       | 25/89 [01:39<04:34,  4.30s/it]\u001b[A\n"," 29%|██▉       | 26/89 [01:42<04:16,  4.07s/it]\u001b[A\n"," 30%|███       | 27/89 [01:45<03:51,  3.73s/it]\u001b[A\n"," 31%|███▏      | 28/89 [01:50<04:09,  4.09s/it]\u001b[A\n"," 33%|███▎      | 29/89 [01:53<03:48,  3.81s/it]\u001b[A\n"," 34%|███▎      | 30/89 [01:59<04:22,  4.44s/it]\u001b[A\n"," 35%|███▍      | 31/89 [02:03<03:59,  4.13s/it]\u001b[A\n"," 36%|███▌      | 32/89 [02:09<04:24,  4.64s/it]\u001b[A\n"," 37%|███▋      | 33/89 [02:15<04:44,  5.07s/it]\u001b[A\n"," 38%|███▊      | 34/89 [02:18<04:03,  4.43s/it]\u001b[A\n"," 39%|███▉      | 35/89 [02:20<03:33,  3.95s/it]\u001b[A\n"," 40%|████      | 36/89 [02:24<03:20,  3.79s/it]\u001b[A\n"," 42%|████▏     | 37/89 [02:27<03:10,  3.66s/it]\u001b[A\n"," 43%|████▎     | 38/89 [02:30<02:56,  3.45s/it]\u001b[A\n"," 44%|████▍     | 39/89 [02:34<02:54,  3.49s/it]\u001b[A\n"," 45%|████▍     | 40/89 [02:39<03:25,  4.19s/it]\u001b[A\n"," 46%|████▌     | 41/89 [02:43<03:06,  3.90s/it]\u001b[A\n"," 47%|████▋     | 42/89 [02:46<02:57,  3.77s/it]\u001b[A\n"," 48%|████▊     | 43/89 [02:50<02:51,  3.74s/it]\u001b[A\n"," 49%|████▉     | 44/89 [02:56<03:16,  4.37s/it]\u001b[A\n"," 51%|█████     | 45/89 [03:00<03:15,  4.44s/it]\u001b[A\n"," 52%|█████▏    | 46/89 [03:06<03:23,  4.74s/it]\u001b[A\n"," 53%|█████▎    | 47/89 [03:12<03:38,  5.20s/it]\u001b[A\n"," 54%|█████▍    | 48/89 [03:17<03:34,  5.22s/it]\u001b[A\n"," 55%|█████▌    | 49/89 [03:22<03:26,  5.16s/it]\u001b[A\n"," 56%|█████▌    | 50/89 [03:28<03:23,  5.21s/it]\u001b[A\n"," 57%|█████▋    | 51/89 [03:33<03:14,  5.13s/it]\u001b[A\n"," 58%|█████▊    | 52/89 [03:36<02:54,  4.72s/it]\u001b[A\n"," 60%|█████▉    | 53/89 [03:40<02:42,  4.51s/it]\u001b[A\n"," 61%|██████    | 54/89 [03:47<02:55,  5.02s/it]\u001b[A\n"," 62%|██████▏   | 55/89 [03:53<03:02,  5.37s/it]\u001b[A\n"," 63%|██████▎   | 56/89 [03:57<02:43,  4.94s/it]\u001b[A\n"," 64%|██████▍   | 57/89 [04:02<02:45,  5.17s/it]\u001b[A\n"," 65%|██████▌   | 58/89 [04:07<02:39,  5.15s/it]\u001b[A\n"," 66%|██████▋   | 59/89 [04:12<02:27,  4.91s/it]\u001b[A\n"," 67%|██████▋   | 60/89 [04:17<02:24,  4.97s/it]\u001b[A\n"," 69%|██████▊   | 61/89 [04:20<01:58,  4.24s/it]\u001b[A\n"," 70%|██████▉   | 62/89 [04:24<01:55,  4.27s/it]\u001b[A\n"," 71%|███████   | 63/89 [04:29<01:59,  4.61s/it]\u001b[A\n"," 72%|███████▏  | 64/89 [04:32<01:43,  4.13s/it]\u001b[A\n"," 73%|███████▎  | 65/89 [04:37<01:41,  4.23s/it]\u001b[A\n"," 74%|███████▍  | 66/89 [04:41<01:34,  4.12s/it]\u001b[A\n"," 75%|███████▌  | 67/89 [04:45<01:29,  4.07s/it]\u001b[A\n"," 76%|███████▋  | 68/89 [04:51<01:39,  4.75s/it]\u001b[A\n"," 78%|███████▊  | 69/89 [04:55<01:33,  4.70s/it]\u001b[A\n"," 79%|███████▊  | 70/89 [04:59<01:24,  4.45s/it]\u001b[A\n"," 80%|███████▉  | 71/89 [05:04<01:20,  4.48s/it]\u001b[A\n"," 81%|████████  | 72/89 [05:08<01:15,  4.44s/it]\u001b[A\n"," 82%|████████▏ | 73/89 [05:13<01:14,  4.67s/it]\u001b[A\n"," 83%|████████▎ | 74/89 [05:18<01:10,  4.71s/it]\u001b[A\n"," 84%|████████▍ | 75/89 [05:22<01:03,  4.56s/it]\u001b[A\n"," 85%|████████▌ | 76/89 [05:27<01:00,  4.69s/it]\u001b[A\n"," 87%|████████▋ | 77/89 [05:33<00:57,  4.81s/it]\u001b[A\n"," 88%|████████▊ | 78/89 [05:38<00:56,  5.12s/it]\u001b[A\n"," 89%|████████▉ | 79/89 [05:43<00:48,  4.90s/it]\u001b[A\n"," 90%|████████▉ | 80/89 [05:48<00:44,  4.92s/it]\u001b[A\n"," 91%|█████████ | 81/89 [05:53<00:39,  4.95s/it]\u001b[A\n"," 92%|█████████▏| 82/89 [05:59<00:37,  5.36s/it]\u001b[A\n"," 93%|█████████▎| 83/89 [06:03<00:29,  4.93s/it]\u001b[A\n"," 94%|█████████▍| 84/89 [06:07<00:22,  4.57s/it]\u001b[A\n"," 96%|█████████▌| 85/89 [06:13<00:19,  4.96s/it]\u001b[A\n"," 97%|█████████▋| 86/89 [06:17<00:14,  4.70s/it]\u001b[A\n"," 98%|█████████▊| 87/89 [06:21<00:09,  4.61s/it]\u001b[A\n"," 99%|█████████▉| 88/89 [06:24<00:04,  4.07s/it]\u001b[A\n","                                                  {'eval_loss': 1217.4495849609375, 'eval_rouge1': 35.3328, 'eval_rouge2': 9.3389, 'eval_rougeL': 18.4634, 'eval_rougeLsum': 32.3447, 'eval_gen_len': 214.2, 'epoch': 1.0}\n","\n"," 33%|███▎      | 716/2148 [16:01<15:25,  1.55it/s]\n","100%|██████████| 89/89 [06:34<00:00,  4.48s/it]\u001b[A\n","{'loss': 1278.4517578125, 'learning_rate': 1.9944134078212292e-05, 'epoch': 1.005586592178771}\n"," 34%|███▍      | 730/2148 [16:12<43:35,  1.84s/it]{'loss': 1050.42353515625, 'learning_rate': 1.980446927374302e-05, 'epoch': 1.0195530726256983}\n"," 34%|███▍      | 740/2148 [16:19<17:59,  1.30it/s]\n","{'loss': 1113.14921875, 'learning_rate': 1.952513966480447e-05, 'epoch': 1.047486033519553}\n"," 35%|███▌      | 760/2148 [16:34<17:20,  1.33it/s]{'loss': 1258.016015625, 'learning_rate': 1.9385474860335198e-05, 'epoch': 1.0614525139664805}\n","                                                  \n","                                                  \n","{'loss': 1150.46328125, 'learning_rate': 1.8966480446927374e-05, 'epoch': 1.1033519553072626}\n"," 37%|███▋      | 800/2148 [17:04<23:31,  1.05s/it]{'loss': 1145.8517578125, 'learning_rate': 1.88268156424581e-05, 'epoch': 1.1173184357541899}\n","[INFO|trainer.py:1226] 2021-01-05 09:53:41,437 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-800\n","[INFO|configuration_utils.py:289] 2021-01-05 09:53:41,457 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-800/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 09:53:54,012 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-800/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 09:53:56,723 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-200] due to args.save_total_limit\n"," 38%|███▊      | 810/2148 [17:27<20:18,  1.10it/s]{'loss': 1174.7248046875, 'learning_rate': 1.8687150837988827e-05, 'epoch': 1.1312849162011174}\n","{'loss': 1154.27392578125, 'learning_rate': 1.8547486033519553e-05, 'epoch': 1.1452513966480447}\n","{'loss': 1249.80546875, 'learning_rate': 1.840782122905028e-05, 'epoch': 1.1592178770949721}\n","                                                  \n"," 40%|███▉      | 850/2148 [17:58<16:32,  1.31it/s]\n"," 40%|████      | 860/2148 [18:05<15:10,  1.41it/s]{'loss': 1244.2724609375, 'learning_rate': 1.798882681564246e-05, 'epoch': 1.2011173184357542}\n"," 41%|████      | 870/2148 [18:12<15:07,  1.41it/s]{'loss': 951.92470703125, 'learning_rate': 1.7849162011173182e-05, 'epoch': 1.2150837988826815}\n","                                                  {'loss': 1200.54716796875, 'learning_rate': 1.7709497206703912e-05, 'epoch': 1.229050279329609}\n","{'loss': 1096.41103515625, 'learning_rate': 1.756983240223464e-05, 'epoch': 1.2430167597765363}\n"," 42%|████▏     | 900/2148 [18:36<21:39,  1.04s/it]\n","                                                  \n"," 43%|████▎     | 920/2148 [18:50<15:20,  1.33it/s]{'loss': 1128.455859375, 'learning_rate': 1.7150837988826815e-05, 'epoch': 1.2849162011173183}\n"," 43%|████▎     | 930/2148 [18:58<15:12,  1.33it/s]{'loss': 1109.8013671875, 'learning_rate': 1.7011173184357544e-05, 'epoch': 1.2988826815642458}\n"," 44%|████▍     | 940/2148 [19:05<14:30,  1.39it/s]{'loss': 1114.0115234375, 'learning_rate': 1.6871508379888268e-05, 'epoch': 1.3128491620111733}\n","{'loss': 1039.58583984375, 'learning_rate': 1.6731843575418994e-05, 'epoch': 1.3268156424581006}\n","                                                  \n"," 45%|████▌     | 970/2148 [19:27<14:02,  1.40it/s]{'loss': 1069.4099609375, 'learning_rate': 1.6452513966480447e-05, 'epoch': 1.3547486033519553}\n"," 46%|████▌     | 980/2148 [19:34<13:42,  1.42it/s]{'loss': 1145.5923828125, 'learning_rate': 1.6312849162011173e-05, 'epoch': 1.3687150837988826}\n","                                                  {'loss': 1154.54384765625, 'learning_rate': 1.61731843575419e-05, 'epoch': 1.3826815642458101}\n","{'loss': 1047.57021484375, 'learning_rate': 1.6033519553072626e-05, 'epoch': 1.3966480446927374}\n"," 47%|████▋     | 1000/2148 [19:50<19:28,  1.02s/it][INFO|trainer.py:1226] 2021-01-05 09:56:27,001 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1000\n","[INFO|configuration_utils.py:289] 2021-01-05 09:56:27,010 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 09:56:37,401 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1000/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 09:56:37,634 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-400] due to args.save_total_limit\n"," 47%|████▋     | 1010/2148 [20:09<18:42,  1.01it/s]{'loss': 1179.84775390625, 'learning_rate': 1.5893854748603353e-05, 'epoch': 1.410614525139665}\n","{'loss': 1165.0462890625, 'learning_rate': 1.575418994413408e-05, 'epoch': 1.4245810055865922}\n","{'loss': 1144.07490234375, 'learning_rate': 1.5614525139664806e-05, 'epoch': 1.4385474860335195}\n"," 48%|████▊     | 1040/2148 [20:32<12:44,  1.45it/s]\n","{'loss': 1068.70087890625, 'learning_rate': 1.533519553072626e-05, 'epoch': 1.4664804469273742}\n","                                                   \n"," 50%|████▉     | 1070/2148 [20:54<12:41,  1.42it/s]{'loss': 1171.60244140625, 'learning_rate': 1.5055865921787711e-05, 'epoch': 1.494413407821229}\n","{'loss': 1133.15283203125, 'learning_rate': 1.4916201117318435e-05, 'epoch': 1.5083798882681565}\n"," 51%|█████     | 1090/2148 [21:09<14:02,  1.26it/s]{'loss': 1163.7044921875, 'learning_rate': 1.4776536312849163e-05, 'epoch': 1.5223463687150838}\n"," 51%|█████     | 1100/2148 [21:17<19:04,  1.09s/it]{'loss': 946.576171875, 'learning_rate': 1.4636871508379887e-05, 'epoch': 1.536312849162011}\n"," 52%|█████▏    | 1110/2148 [21:24<13:08,  1.32it/s]{'loss': 997.46240234375, 'learning_rate': 1.4497206703910616e-05, 'epoch': 1.5502793296089385}\n"," 52%|█████▏    | 1120/2148 [21:31<12:29,  1.37it/s]{'loss': 976.91552734375, 'learning_rate': 1.435754189944134e-05, 'epoch': 1.564245810055866}\n","{'loss': 1209.52919921875, 'learning_rate': 1.4217877094972069e-05, 'epoch': 1.5782122905027933}\n","                                                   \n","                                                   \n","{'loss': 1133.62197265625, 'learning_rate': 1.3798882681564246e-05, 'epoch': 1.6201117318435754}\n","{'loss': 983.85595703125, 'learning_rate': 1.3659217877094973e-05, 'epoch': 1.6340782122905027}\n","                                                   \n","                                                   \n","{'loss': 1077.9244140625, 'learning_rate': 1.324022346368715e-05, 'epoch': 1.675977653631285}\n"," 56%|█████▌    | 1200/2148 [22:31<16:08,  1.02s/it][INFO|trainer.py:1226] 2021-01-05 09:59:07,734 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1200\n","[INFO|configuration_utils.py:289] 2021-01-05 09:59:07,747 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1200/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 09:59:18,072 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1200/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 09:59:18,523 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-600] due to args.save_total_limit\n","{'loss': 914.71962890625, 'learning_rate': 1.3100558659217879e-05, 'epoch': 1.6899441340782122}\n","{'loss': 1041.3541015625, 'learning_rate': 1.2960893854748603e-05, 'epoch': 1.7039106145251397}\n"," 57%|█████▋    | 1230/2148 [23:04<11:18,  1.35it/s]\n"," 58%|█████▊    | 1240/2148 [23:12<10:58,  1.38it/s]{'loss': 1050.9951171875, 'learning_rate': 1.2681564245810056e-05, 'epoch': 1.7318435754189943}\n"," 58%|█████▊    | 1250/2148 [23:19<09:42,  1.54it/s]{'loss': 806.844287109375, 'learning_rate': 1.2541899441340781e-05, 'epoch': 1.7458100558659218}\n","{'loss': 1013.75771484375, 'learning_rate': 1.2402234636871509e-05, 'epoch': 1.7597765363128492}\n","{'loss': 1036.046484375, 'learning_rate': 1.2262569832402234e-05, 'epoch': 1.7737430167597765}\n"," 60%|█████▉    | 1280/2148 [23:41<09:59,  1.45it/s]{'loss': 1064.40234375, 'learning_rate': 1.2122905027932962e-05, 'epoch': 1.7877094972067038}\n","{'loss': 1242.92236328125, 'learning_rate': 1.1983240223463687e-05, 'epoch': 1.8016759776536313}\n"," 61%|██████    | 1300/2148 [23:56<13:59,  1.01it/s]{'loss': 982.23203125, 'learning_rate': 1.1843575418994415e-05, 'epoch': 1.8156424581005588}\n","                                                   \n","{'loss': 1125.01826171875, 'learning_rate': 1.1564245810055866e-05, 'epoch': 1.8435754189944134}\n"," 62%|██████▏   | 1330/2148 [24:18<09:27,  1.44it/s]{'loss': 1047.997265625, 'learning_rate': 1.1424581005586593e-05, 'epoch': 1.8575418994413408}\n","{'loss': 840.9642578125, 'learning_rate': 1.1284916201117319e-05, 'epoch': 1.8715083798882681}\n"," 63%|██████▎   | 1350/2148 [24:32<09:14,  1.44it/s]{'loss': 1012.25244140625, 'learning_rate': 1.1145251396648046e-05, 'epoch': 1.8854748603351954}\n","{'loss': 1075.9064453125, 'learning_rate': 1.1005586592178772e-05, 'epoch': 1.899441340782123}\n"," 64%|██████▍   | 1370/2148 [24:46<09:46,  1.33it/s]{'loss': 1093.4677734375, 'learning_rate': 1.0865921787709497e-05, 'epoch': 1.9134078212290504}\n"," 64%|██████▍   | 1380/2148 [24:54<09:15,  1.38it/s]{'loss': 990.504296875, 'learning_rate': 1.0726256983240223e-05, 'epoch': 1.9273743016759777}\n"," 65%|██████▍   | 1390/2148 [25:01<09:08,  1.38it/s]\n","{'loss': 1216.39765625, 'learning_rate': 1.0446927374301676e-05, 'epoch': 1.9553072625698324}\n"," 65%|██████▌   | 1400/2148 [25:09<12:38,  1.01s/it][INFO|trainer.py:1226] 2021-01-05 10:01:46,004 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1400\n","[INFO|configuration_utils.py:289] 2021-01-05 10:01:46,014 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1400/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 10:01:56,327 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1400/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 10:01:56,439 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-800] due to args.save_total_limit\n","{'loss': 1124.1408203125, 'learning_rate': 1.0307262569832403e-05, 'epoch': 1.9692737430167597}\n"," 66%|██████▌   | 1420/2148 [25:35<09:06,  1.33it/s]{'loss': 958.07275390625, 'learning_rate': 1.0167597765363127e-05, 'epoch': 1.983240223463687}\n"," 67%|██████▋   | 1430/2148 [25:42<09:13,  1.30it/s]\n"," 67%|██████▋   | 1432/2148 [25:44<08:10,  1.46it/s][INFO|trainer.py:1412] 2021-01-05 10:02:20,736 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-05 10:02:20,736 >>   Num examples = 178\n","[INFO|trainer.py:1414] 2021-01-05 10:02:20,736 >>   Batch size = 2\n","\n","  0%|          | 0/89 [00:00<?, ?it/s]\u001b[A\n","  2%|▏         | 2/89 [00:05<04:16,  2.95s/it]\u001b[A\n","  3%|▎         | 3/89 [00:10<04:57,  3.46s/it]\u001b[A\n","  4%|▍         | 4/89 [00:13<04:34,  3.23s/it]\u001b[A\n","  6%|▌         | 5/89 [00:19<05:43,  4.09s/it]\u001b[A\n","  7%|▋         | 6/89 [00:22<05:07,  3.71s/it]\u001b[A\n","  8%|▊         | 7/89 [00:28<06:05,  4.46s/it]\u001b[A\n","  9%|▉         | 8/89 [00:32<05:46,  4.27s/it]\u001b[A\n"," 10%|█         | 9/89 [00:36<05:41,  4.27s/it]\u001b[A\n"," 11%|█         | 10/89 [00:39<05:12,  3.96s/it]\u001b[A\n"," 12%|█▏        | 11/89 [00:43<05:09,  3.97s/it]\u001b[A\n"," 13%|█▎        | 12/89 [00:48<05:33,  4.32s/it]\u001b[A\n"," 15%|█▍        | 13/89 [00:54<06:06,  4.82s/it]\u001b[A\n"," 16%|█▌        | 14/89 [00:57<05:15,  4.20s/it]\u001b[A\n"," 17%|█▋        | 15/89 [01:03<05:56,  4.82s/it]\u001b[A\n"," 18%|█▊        | 16/89 [01:08<05:40,  4.67s/it]\u001b[A\n"," 19%|█▉        | 17/89 [01:14<06:10,  5.14s/it]\u001b[A\n"," 20%|██        | 18/89 [01:20<06:26,  5.44s/it]\u001b[A\n"," 21%|██▏       | 19/89 [01:26<06:38,  5.69s/it]\u001b[A\n"," 22%|██▏       | 20/89 [01:30<05:56,  5.17s/it]\u001b[A\n"," 24%|██▎       | 21/89 [01:36<06:10,  5.45s/it]\u001b[A\n"," 25%|██▍       | 22/89 [01:40<05:33,  4.98s/it]\u001b[A\n"," 26%|██▌       | 23/89 [01:44<05:03,  4.59s/it]\u001b[A\n"," 27%|██▋       | 24/89 [01:50<05:26,  5.02s/it]\u001b[A\n"," 28%|██▊       | 25/89 [01:56<05:33,  5.22s/it]\u001b[A\n"," 29%|██▉       | 26/89 [02:01<05:27,  5.20s/it]\u001b[A\n"," 30%|███       | 27/89 [02:06<05:21,  5.18s/it]\u001b[A\n"," 31%|███▏      | 28/89 [02:11<05:12,  5.13s/it]\u001b[A\n"," 33%|███▎      | 29/89 [02:16<05:04,  5.07s/it]\u001b[A\n"," 34%|███▎      | 30/89 [02:21<05:07,  5.20s/it]\u001b[A\n"," 35%|███▍      | 31/89 [02:28<05:23,  5.57s/it]\u001b[A\n"," 36%|███▌      | 32/89 [02:34<05:24,  5.69s/it]\u001b[A\n"," 37%|███▋      | 33/89 [02:40<05:30,  5.91s/it]\u001b[A\n"," 38%|███▊      | 34/89 [02:44<04:48,  5.25s/it]\u001b[A\n"," 39%|███▉      | 35/89 [02:48<04:18,  4.78s/it]\u001b[A\n"," 40%|████      | 36/89 [02:52<04:13,  4.78s/it]\u001b[A\n"," 42%|████▏     | 37/89 [02:57<04:08,  4.78s/it]\u001b[A\n"," 43%|████▎     | 38/89 [03:03<04:21,  5.12s/it]\u001b[A\n"," 44%|████▍     | 39/89 [03:09<04:30,  5.40s/it]\u001b[A\n"," 45%|████▍     | 40/89 [03:15<04:30,  5.52s/it]\u001b[A\n"," 46%|████▌     | 41/89 [03:20<04:19,  5.40s/it]\u001b[A\n"," 47%|████▋     | 42/89 [03:24<03:51,  4.93s/it]\u001b[A\n"," 48%|████▊     | 43/89 [03:30<04:04,  5.33s/it]\u001b[A\n"," 49%|████▉     | 44/89 [03:36<04:11,  5.60s/it]\u001b[A\n"," 51%|█████     | 45/89 [03:39<03:23,  4.62s/it]\u001b[A\n"," 52%|█████▏    | 46/89 [03:44<03:30,  4.89s/it]\u001b[A\n"," 53%|█████▎    | 47/89 [03:49<03:23,  4.85s/it]\u001b[A\n"," 54%|█████▍    | 48/89 [03:55<03:35,  5.25s/it]\u001b[A\n"," 55%|█████▌    | 49/89 [04:01<03:41,  5.55s/it]\u001b[A\n"," 56%|█████▌    | 50/89 [04:08<03:45,  5.77s/it]\u001b[A\n"," 57%|█████▋    | 51/89 [04:13<03:36,  5.69s/it]\u001b[A\n"," 58%|█████▊    | 52/89 [04:17<03:10,  5.15s/it]\u001b[A\n"," 60%|█████▉    | 53/89 [04:20<02:44,  4.57s/it]\u001b[A\n"," 61%|██████    | 54/89 [04:26<02:55,  5.03s/it]\u001b[A\n"," 62%|██████▏   | 55/89 [04:32<03:00,  5.30s/it]\u001b[A\n"," 63%|██████▎   | 56/89 [04:37<02:43,  4.95s/it]\u001b[A\n"," 64%|██████▍   | 57/89 [04:43<02:48,  5.28s/it]\u001b[A\n"," 65%|██████▌   | 58/89 [04:49<02:51,  5.53s/it]\u001b[A\n"," 66%|██████▋   | 59/89 [04:51<02:18,  4.62s/it]\u001b[A\n"," 67%|██████▋   | 60/89 [04:57<02:25,  5.03s/it]\u001b[A\n"," 69%|██████▊   | 61/89 [05:00<02:05,  4.48s/it]\u001b[A\n"," 70%|██████▉   | 62/89 [05:05<02:03,  4.57s/it]\u001b[A\n"," 71%|███████   | 63/89 [05:11<02:11,  5.06s/it]\u001b[A\n"," 72%|███████▏  | 64/89 [05:16<02:03,  4.94s/it]\u001b[A\n"," 73%|███████▎  | 65/89 [05:22<02:04,  5.20s/it]\u001b[A\n"," 74%|███████▍  | 66/89 [05:28<02:07,  5.54s/it]\u001b[A\n"," 75%|███████▌  | 67/89 [05:34<02:05,  5.68s/it]\u001b[A\n"," 76%|███████▋  | 68/89 [05:39<01:53,  5.42s/it]\u001b[A\n"," 78%|███████▊  | 69/89 [05:45<01:52,  5.65s/it]\u001b[A\n"," 79%|███████▊  | 70/89 [05:51<01:46,  5.62s/it]\u001b[A\n"," 80%|███████▉  | 71/89 [05:57<01:42,  5.72s/it]\u001b[A\n"," 81%|████████  | 72/89 [06:01<01:31,  5.40s/it]\u001b[A\n"," 82%|████████▏ | 73/89 [06:05<01:20,  5.03s/it]\u001b[A\n"," 83%|████████▎ | 74/89 [06:10<01:15,  5.02s/it]\u001b[A\n"," 84%|████████▍ | 75/89 [06:16<01:10,  5.06s/it]\u001b[A\n"," 85%|████████▌ | 76/89 [06:20<01:02,  4.84s/it]\u001b[A\n"," 87%|████████▋ | 77/89 [06:25<00:57,  4.76s/it]\u001b[A\n"," 88%|████████▊ | 78/89 [06:31<00:57,  5.24s/it]\u001b[A\n"," 89%|████████▉ | 79/89 [06:36<00:52,  5.29s/it]\u001b[A\n"," 90%|████████▉ | 80/89 [06:42<00:49,  5.46s/it]\u001b[A\n"," 91%|█████████ | 81/89 [06:46<00:39,  4.89s/it]\u001b[A\n"," 92%|█████████▏| 82/89 [06:51<00:34,  4.91s/it]\u001b[A\n"," 93%|█████████▎| 83/89 [06:54<00:27,  4.52s/it]\u001b[A\n"," 94%|█████████▍| 84/89 [06:57<00:20,  4.09s/it]\u001b[A\n"," 96%|█████████▌| 85/89 [07:01<00:15,  3.86s/it]\u001b[A\n"," 97%|█████████▋| 86/89 [07:07<00:13,  4.55s/it]\u001b[A\n"," 98%|█████████▊| 87/89 [07:12<00:09,  4.80s/it]\u001b[A\n"," 99%|█████████▉| 88/89 [07:16<00:04,  4.43s/it]\u001b[A\n","                                                   \n","\u001b[A{'eval_loss': 1035.119873046875, 'eval_rouge1': 35.9447, 'eval_rouge2': 10.9182, 'eval_rougeL': 19.6619, 'eval_rougeLsum': 33.0957, 'eval_gen_len': 238.9, 'epoch': 2.0}\n"," 67%|██████▋   | 1432/2148 [33:17<08:10,  1.46it/s]\n","100%|██████████| 89/89 [07:27<00:00,  4.98s/it]\u001b[A\n"," 67%|██████▋   | 1440/2148 [33:23<2:20:51, 11.94s/it]{'loss': 955.451953125, 'learning_rate': 9.88826815642458e-06, 'epoch': 2.011173184357542}\n","                                                   \n","                                                   \n"," 68%|██████▊   | 1470/2148 [33:44<08:47,  1.29it/s]{'loss': 921.42998046875, 'learning_rate': 9.46927374301676e-06, 'epoch': 2.053072625698324}\n"," 69%|██████▉   | 1480/2148 [33:51<07:54,  1.41it/s]{'loss': 876.8212890625, 'learning_rate': 9.329608938547486e-06, 'epoch': 2.0670391061452515}\n","{'loss': 871.66708984375, 'learning_rate': 9.189944134078213e-06, 'epoch': 2.0810055865921786}\n","                                                   {'loss': 967.15185546875, 'learning_rate': 9.050279329608939e-06, 'epoch': 2.094972067039106}\n"," 70%|███████   | 1510/2148 [34:13<07:57,  1.34it/s]{'loss': 974.6646484375, 'learning_rate': 8.910614525139666e-06, 'epoch': 2.1089385474860336}\n"," 71%|███████   | 1520/2148 [34:20<07:16,  1.44it/s]\n","{'loss': 929.50458984375, 'learning_rate': 8.631284916201118e-06, 'epoch': 2.136871508379888}\n","{'loss': 894.675, 'learning_rate': 8.491620111731843e-06, 'epoch': 2.1508379888268156}\n"," 72%|███████▏  | 1550/2148 [34:42<07:16,  1.37it/s]{'loss': 1219.76298828125, 'learning_rate': 8.35195530726257e-06, 'epoch': 2.164804469273743}\n","                                                   \n","{'loss': 955.76064453125, 'learning_rate': 8.072625698324023e-06, 'epoch': 2.1927374301675977}\n","{'loss': 851.3958984375, 'learning_rate': 7.932960893854749e-06, 'epoch': 2.206703910614525}\n"," 74%|███████▍  | 1590/2148 [35:11<06:12,  1.50it/s]{'loss': 894.20341796875, 'learning_rate': 7.793296089385474e-06, 'epoch': 2.2206703910614527}\n"," 74%|███████▍  | 1600/2148 [35:19<09:33,  1.05s/it]\n","[INFO|trainer.py:1226] 2021-01-05 10:11:55,896 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1600\n","[INFO|configuration_utils.py:289] 2021-01-05 10:11:55,906 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1600/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 10:12:06,579 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1600/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 10:12:10,832 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1000] due to args.save_total_limit\n"," 75%|███████▍  | 1610/2148 [35:42<08:47,  1.02it/s]{'loss': 878.05869140625, 'learning_rate': 7.5139664804469275e-06, 'epoch': 2.2486033519553073}\n"," 75%|███████▌  | 1620/2148 [35:50<07:01,  1.25it/s]{'loss': 1076.73330078125, 'learning_rate': 7.374301675977653e-06, 'epoch': 2.2625698324022347}\n","                                                   \n"," 76%|███████▋  | 1640/2148 [36:05<06:09,  1.38it/s]\n"," 77%|███████▋  | 1650/2148 [36:12<05:55,  1.40it/s]{'loss': 1160.61572265625, 'learning_rate': 6.9553072625698325e-06, 'epoch': 2.304469273743017}\n"," 77%|███████▋  | 1660/2148 [36:19<05:47,  1.40it/s]{'loss': 866.45390625, 'learning_rate': 6.815642458100559e-06, 'epoch': 2.3184357541899443}\n"," 78%|███████▊  | 1670/2148 [36:27<05:54,  1.35it/s]{'loss': 1047.57060546875, 'learning_rate': 6.675977653631285e-06, 'epoch': 2.3324022346368714}\n"," 78%|███████▊  | 1680/2148 [36:34<05:35,  1.39it/s]{'loss': 891.41025390625, 'learning_rate': 6.536312849162011e-06, 'epoch': 2.346368715083799}\n"," 79%|███████▊  | 1690/2148 [36:42<05:50,  1.31it/s]\n"," 79%|███████▉  | 1700/2148 [36:50<07:38,  1.02s/it]{'loss': 891.356640625, 'learning_rate': 6.256983240223464e-06, 'epoch': 2.3743016759776534}\n","{'loss': 870.6537109375, 'learning_rate': 6.1173184357541904e-06, 'epoch': 2.388268156424581}\n","{'loss': 811.488330078125, 'learning_rate': 5.977653631284917e-06, 'epoch': 2.4022346368715084}\n"," 81%|████████  | 1730/2148 [37:11<04:57,  1.41it/s]{'loss': 933.566796875, 'learning_rate': 5.8379888268156425e-06, 'epoch': 2.416201117318436}\n","{'loss': 1016.3642578125, 'learning_rate': 5.698324022346369e-06, 'epoch': 2.430167597765363}\n","                                                   \n","{'loss': 1248.640234375, 'learning_rate': 5.418994413407822e-06, 'epoch': 2.458100558659218}\n"," 82%|████████▏ | 1770/2148 [37:41<04:43,  1.33it/s]{'loss': 759.125, 'learning_rate': 5.2793296089385475e-06, 'epoch': 2.472067039106145}\n","{'loss': 661.626123046875, 'learning_rate': 5.139664804469274e-06, 'epoch': 2.4860335195530725}\n"," 83%|████████▎ | 1790/2148 [37:55<04:03,  1.47it/s]\n"," 84%|████████▍ | 1800/2148 [38:03<06:11,  1.07s/it]{'loss': 946.904296875, 'learning_rate': 4.860335195530726e-06, 'epoch': 2.5139664804469275}\n"," 84%|████████▍ | 1800/2148 [38:03<06:11,  1.07s/it][INFO|trainer.py:1226] 2021-01-05 10:14:39,740 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1800\n","[INFO|configuration_utils.py:289] 2021-01-05 10:14:39,764 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1800/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 10:14:50,314 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1800/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 10:14:50,455 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1200] due to args.save_total_limit\n"," 84%|████████▍ | 1810/2148 [38:21<05:23,  1.05it/s]{'loss': 831.24541015625, 'learning_rate': 4.7206703910614525e-06, 'epoch': 2.527932960893855}\n","{'loss': 1085.86201171875, 'learning_rate': 4.581005586592179e-06, 'epoch': 2.541899441340782}\n","{'loss': 824.9279296875, 'learning_rate': 4.441340782122905e-06, 'epoch': 2.5558659217877095}\n"," 86%|████████▌ | 1840/2148 [38:44<03:39,  1.41it/s]{'loss': 952.009375, 'learning_rate': 4.301675977653631e-06, 'epoch': 2.5698324022346366}\n","{'loss': 954.709375, 'learning_rate': 4.1620111731843575e-06, 'epoch': 2.583798882681564}\n"," 87%|████████▋ | 1860/2148 [38:59<03:30,  1.37it/s]\n","                                                   \n","{'loss': 774.276318359375, 'learning_rate': 3.7430167597765364e-06, 'epoch': 2.6256983240223466}\n","{'loss': 811.401416015625, 'learning_rate': 3.6033519553072625e-06, 'epoch': 2.6396648044692737}\n","{'loss': 934.6541015625, 'learning_rate': 3.463687150837989e-06, 'epoch': 2.653631284916201}\n","                                                   {'loss': 1060.260546875, 'learning_rate': 3.324022346368715e-06, 'epoch': 2.6675977653631286}\n","{'loss': 1071.51376953125, 'learning_rate': 3.1843575418994414e-06, 'epoch': 2.6815642458100557}\n","{'loss': 856.8123046875, 'learning_rate': 3.044692737430168e-06, 'epoch': 2.695530726256983}\n"," 90%|█████████ | 1940/2148 [39:57<02:36,  1.33it/s]\n"," 91%|█████████ | 1950/2148 [40:05<02:16,  1.45it/s]{'loss': 858.85546875, 'learning_rate': 2.7653631284916204e-06, 'epoch': 2.723463687150838}\n","                                                   \n","{'loss': 893.9748046875, 'learning_rate': 2.4860335195530724e-06, 'epoch': 2.7513966480446927}\n"," 92%|█████████▏| 1980/2148 [40:26<02:01,  1.38it/s]{'loss': 868.87138671875, 'learning_rate': 2.346368715083799e-06, 'epoch': 2.7653631284916202}\n","                                                   {'loss': 887.37548828125, 'learning_rate': 2.2067039106145253e-06, 'epoch': 2.7793296089385473}\n","{'loss': 940.7509765625, 'learning_rate': 2.0670391061452514e-06, 'epoch': 2.793296089385475}\n"," 93%|█████████▎| 2000/2148 [40:41<02:24,  1.02it/s][INFO|trainer.py:1226] 2021-01-05 10:17:18,465 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-2000\n","[INFO|configuration_utils.py:289] 2021-01-05 10:17:18,475 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 10:17:28,970 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-2000/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-05 10:17:33,502 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/checkpoint-1400] due to args.save_total_limit\n"," 94%|█████████▎| 2010/2148 [41:04<02:13,  1.03it/s]\n","{'loss': 1030.60517578125, 'learning_rate': 1.7877094972067039e-06, 'epoch': 2.82122905027933}\n"," 95%|█████████▍| 2030/2148 [41:20<01:29,  1.31it/s]{'loss': 1220.04296875, 'learning_rate': 1.6480446927374301e-06, 'epoch': 2.835195530726257}\n"," 95%|█████████▍| 2040/2148 [41:27<01:14,  1.45it/s]\n"," 95%|█████████▌| 2050/2148 [41:34<01:11,  1.37it/s]{'loss': 855.31875, 'learning_rate': 1.3687150837988828e-06, 'epoch': 2.863128491620112}\n","{'loss': 899.3482421875, 'learning_rate': 1.2290502793296089e-06, 'epoch': 2.877094972067039}\n","                                                   \n"," 97%|█████████▋| 2080/2148 [41:56<00:50,  1.35it/s]\n"," 97%|█████████▋| 2090/2148 [42:03<00:39,  1.47it/s]{'loss': 924.22783203125, 'learning_rate': 8.100558659217877e-07, 'epoch': 2.9189944134078214}\n"," 98%|█████████▊| 2100/2148 [42:12<00:53,  1.11s/it]{'loss': 1181.2330078125, 'learning_rate': 6.70391061452514e-07, 'epoch': 2.9329608938547485}\n"," 98%|█████████▊| 2110/2148 [42:19<00:28,  1.32it/s]\n"," 99%|█████████▊| 2120/2148 [42:27<00:21,  1.29it/s]\n"," 99%|█████████▉| 2130/2148 [42:34<00:13,  1.33it/s]{'loss': 858.450390625, 'learning_rate': 2.5139664804469275e-07, 'epoch': 2.9748603351955305}\n","{'loss': 1006.98671875, 'learning_rate': 1.1173184357541899e-07, 'epoch': 2.988826815642458}\n","100%|██████████| 2148/2148 [42:47<00:00,  1.60it/s][INFO|trainer.py:1412] 2021-01-05 10:19:24,359 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-05 10:19:24,359 >>   Num examples = 178\n","[INFO|trainer.py:1414] 2021-01-05 10:19:24,359 >>   Batch size = 2\n","\n","  0%|          | 0/89 [00:00<?, ?it/s]\u001b[A\n","  2%|▏         | 2/89 [00:06<04:25,  3.05s/it]\u001b[A\n","  3%|▎         | 3/89 [00:09<04:35,  3.21s/it]\u001b[A\n","  4%|▍         | 4/89 [00:13<04:48,  3.39s/it]\u001b[A\n","  6%|▌         | 5/89 [00:19<05:49,  4.16s/it]\u001b[A\n","  7%|▋         | 6/89 [00:22<05:28,  3.95s/it]\u001b[A\n","  8%|▊         | 7/89 [00:27<05:35,  4.09s/it]\u001b[A\n","  9%|▉         | 8/89 [00:30<05:19,  3.95s/it]\u001b[A\n"," 10%|█         | 9/89 [00:36<06:03,  4.54s/it]\u001b[A\n"," 11%|█         | 10/89 [00:40<05:41,  4.32s/it]\u001b[A\n"," 12%|█▏        | 11/89 [00:46<06:15,  4.81s/it]\u001b[A\n"," 13%|█▎        | 12/89 [00:50<05:46,  4.49s/it]\u001b[A\n"," 15%|█▍        | 13/89 [00:56<06:16,  4.95s/it]\u001b[A\n"," 16%|█▌        | 14/89 [01:02<06:38,  5.32s/it]\u001b[A\n"," 17%|█▋        | 15/89 [01:08<06:52,  5.57s/it]\u001b[A\n"," 18%|█▊        | 16/89 [01:14<06:59,  5.74s/it]\u001b[A\n"," 19%|█▉        | 17/89 [01:20<06:59,  5.83s/it]\u001b[A\n"," 20%|██        | 18/89 [01:26<06:55,  5.85s/it]\u001b[A\n"," 21%|██▏       | 19/89 [01:33<06:56,  5.96s/it]\u001b[A\n"," 22%|██▏       | 20/89 [01:38<06:43,  5.85s/it]\u001b[A\n"," 24%|██▎       | 21/89 [01:44<06:40,  5.88s/it]\u001b[A\n"," 25%|██▍       | 22/89 [01:50<06:37,  5.93s/it]\u001b[A\n"," 26%|██▌       | 23/89 [01:55<06:16,  5.71s/it]\u001b[A\n"," 27%|██▋       | 24/89 [02:01<06:08,  5.67s/it]\u001b[A\n"," 28%|██▊       | 25/89 [02:05<05:42,  5.35s/it]\u001b[A\n"," 29%|██▉       | 26/89 [02:11<05:37,  5.35s/it]\u001b[A\n"," 30%|███       | 27/89 [02:14<04:59,  4.83s/it]\u001b[A\n"," 31%|███▏      | 28/89 [02:19<04:52,  4.80s/it]\u001b[A\n"," 33%|███▎      | 29/89 [02:23<04:26,  4.45s/it]\u001b[A\n"," 34%|███▎      | 30/89 [02:28<04:38,  4.72s/it]\u001b[A\n"," 35%|███▍      | 31/89 [02:32<04:15,  4.41s/it]\u001b[A\n"," 36%|███▌      | 32/89 [02:38<04:41,  4.95s/it]\u001b[A\n"," 37%|███▋      | 33/89 [02:44<04:57,  5.31s/it]\u001b[A\n"," 38%|███▊      | 34/89 [02:47<04:11,  4.57s/it]\u001b[A\n"," 39%|███▉      | 35/89 [02:52<04:15,  4.73s/it]\u001b[A\n"," 40%|████      | 36/89 [02:56<03:52,  4.38s/it]\u001b[A\n"," 42%|████▏     | 37/89 [03:02<04:12,  4.86s/it]\u001b[A\n"," 43%|████▎     | 38/89 [03:06<03:52,  4.56s/it]\u001b[A\n"," 44%|████▍     | 39/89 [03:10<03:42,  4.45s/it]\u001b[A\n"," 45%|████▍     | 40/89 [03:16<04:03,  4.98s/it]\u001b[A\n"," 46%|████▌     | 41/89 [03:21<03:55,  4.90s/it]\u001b[A\n"," 47%|████▋     | 42/89 [03:26<03:59,  5.09s/it]\u001b[A\n"," 48%|████▊     | 43/89 [03:32<04:05,  5.35s/it]\u001b[A\n"," 49%|████▉     | 44/89 [03:38<04:09,  5.55s/it]\u001b[A\n"," 51%|█████     | 45/89 [03:41<03:34,  4.87s/it]\u001b[A\n"," 52%|█████▏    | 46/89 [03:46<03:25,  4.78s/it]\u001b[A\n"," 53%|█████▎    | 47/89 [03:50<03:11,  4.55s/it]\u001b[A\n"," 54%|█████▍    | 48/89 [03:56<03:24,  4.98s/it]\u001b[A\n"," 55%|█████▌    | 49/89 [04:02<03:29,  5.25s/it]\u001b[A\n"," 56%|█████▌    | 50/89 [04:06<03:08,  4.82s/it]\u001b[A\n"," 57%|█████▋    | 51/89 [04:11<03:06,  4.91s/it]\u001b[A\n"," 58%|█████▊    | 52/89 [04:17<03:11,  5.18s/it]\u001b[A\n"," 60%|█████▉    | 53/89 [04:22<03:10,  5.30s/it]\u001b[A\n"," 61%|██████    | 54/89 [04:26<02:48,  4.81s/it]\u001b[A\n"," 62%|██████▏   | 55/89 [04:32<02:54,  5.13s/it]\u001b[A\n"," 63%|██████▎   | 56/89 [04:35<02:30,  4.57s/it]\u001b[A\n"," 64%|██████▍   | 57/89 [04:39<02:17,  4.29s/it]\u001b[A\n"," 65%|██████▌   | 58/89 [04:45<02:27,  4.76s/it]\u001b[A\n"," 66%|██████▋   | 59/89 [04:47<02:06,  4.21s/it]\u001b[A\n"," 67%|██████▋   | 60/89 [04:54<02:17,  4.76s/it]\u001b[A\n"," 69%|██████▊   | 61/89 [04:59<02:22,  5.10s/it]\u001b[A\n"," 70%|██████▉   | 62/89 [05:04<02:14,  4.98s/it]\u001b[A\n"," 71%|███████   | 63/89 [05:09<02:05,  4.81s/it]\u001b[A\n"," 72%|███████▏  | 64/89 [05:13<01:57,  4.71s/it]\u001b[A\n"," 73%|███████▎  | 65/89 [05:19<02:01,  5.05s/it]\u001b[A\n"," 74%|███████▍  | 66/89 [05:25<02:02,  5.32s/it]\u001b[A\n"," 75%|███████▌  | 67/89 [05:31<02:01,  5.55s/it]\u001b[A\n"," 76%|███████▋  | 68/89 [05:36<01:56,  5.56s/it]\u001b[A\n"," 78%|███████▊  | 69/89 [05:42<01:53,  5.67s/it]\u001b[A\n"," 79%|███████▊  | 70/89 [05:48<01:48,  5.71s/it]\u001b[A\n"," 80%|███████▉  | 71/89 [05:54<01:44,  5.78s/it]\u001b[A\n"," 81%|████████  | 72/89 [06:00<01:39,  5.84s/it]\u001b[A\n"," 82%|████████▏ | 73/89 [06:04<01:25,  5.35s/it]\u001b[A\n"," 83%|████████▎ | 74/89 [06:09<01:19,  5.27s/it]\u001b[A\n"," 84%|████████▍ | 75/89 [06:15<01:16,  5.50s/it]\u001b[A\n"," 85%|████████▌ | 76/89 [06:19<01:04,  4.97s/it]\u001b[A\n"," 87%|████████▋ | 77/89 [06:25<01:03,  5.26s/it]\u001b[A\n"," 88%|████████▊ | 78/89 [06:31<01:00,  5.51s/it]\u001b[A\n"," 89%|████████▉ | 79/89 [06:36<00:51,  5.18s/it]\u001b[A\n"," 90%|████████▉ | 80/89 [06:40<00:44,  4.90s/it]\u001b[A\n"," 91%|█████████ | 81/89 [06:44<00:37,  4.64s/it]\u001b[A\n"," 92%|█████████▏| 82/89 [06:48<00:31,  4.49s/it]\u001b[A\n"," 93%|█████████▎| 83/89 [06:54<00:29,  4.92s/it]\u001b[A\n"," 94%|█████████▍| 84/89 [06:58<00:23,  4.65s/it]\u001b[A\n"," 96%|█████████▌| 85/89 [07:03<00:18,  4.63s/it]\u001b[A\n"," 97%|█████████▋| 86/89 [07:09<00:15,  5.06s/it]\u001b[A\n"," 98%|█████████▊| 87/89 [07:15<00:10,  5.40s/it]\u001b[A\n"," 99%|█████████▉| 88/89 [07:18<00:04,  4.74s/it]\u001b[A\n","                                                   \n","\u001b[A{'eval_loss': 978.48193359375, 'eval_rouge1': 38.6615, 'eval_rouge2': 14.0409, 'eval_rougeL': 22.0011, 'eval_rougeLsum': 35.6463, 'eval_gen_len': 243.5, 'epoch': 3.0}\n","100%|██████████| 2148/2148 [50:20<00:00,  1.60it/s]\n","100%|██████████| 89/89 [07:28<00:00,  4.98s/it]\u001b[A\n","                                               \u001b[A[INFO|trainer.py:862] 2021-01-05 10:26:56,849 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'epoch': 3.0}\n","100%|██████████| 2148/2148 [50:20<00:00,  1.41s/it]\n","[INFO|trainer.py:1226] 2021-01-05 10:26:56,899 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train\n","[INFO|configuration_utils.py:289] 2021-01-05 10:26:57,055 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/config.json\n","[INFO|modeling_utils.py:814] 2021-01-05 10:27:52,460 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train/pytorch_model.bin\n","01/05/2021 10:27:52 - INFO - __main__ -   ***** train metrics *****\n","01/05/2021 10:27:52 - INFO - __main__ -     train_samples_per_second = -0.0\n","01/05/2021 10:27:52 - INFO - __main__ -     train_runtime = 3021.7196\n","01/05/2021 10:27:52 - INFO - __main__ -     train_n_ojbs = -1\n","01/05/2021 10:27:54 - INFO - __main__ -   *** Evaluate ***\n","[INFO|trainer.py:1412] 2021-01-05 10:27:54,414 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-05 10:27:54,414 >>   Num examples = 178\n","[INFO|trainer.py:1414] 2021-01-05 10:27:54,414 >>   Batch size = 2\n","100%|██████████| 89/89 [07:27<00:00,  5.03s/it]\n","01/05/2021 10:35:25 - INFO - __main__ -   ***** val metrics *****\n","01/05/2021 10:35:25 - INFO - __main__ -     val_loss = 978.4819\n","01/05/2021 10:35:25 - INFO - __main__ -     val_rouge1 = 38.7078\n","01/05/2021 10:35:25 - INFO - __main__ -     val_rouge2 = 13.9899\n","01/05/2021 10:35:25 - INFO - __main__ -     val_rougeL = 21.9788\n","01/05/2021 10:35:25 - INFO - __main__ -     val_rougeLsum = 35.7167\n","01/05/2021 10:35:25 - INFO - __main__ -     val_gen_len = 243.5\n","01/05/2021 10:35:25 - INFO - __main__ -     epoch = 3.0\n","01/05/2021 10:35:25 - INFO - __main__ -     val_samples_per_second = -0.002\n","01/05/2021 10:35:25 - INFO - __main__ -     val_runtime = 451.316\n","01/05/2021 10:35:25 - INFO - __main__ -     val_n_ojbs = -1\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 399\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210105_093635-exbr6eju/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210105_093635-exbr6eju/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                         train/loss 1006.98672\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                train/learning_rate 0.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                        train/epoch 3.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _step 2148\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                           _runtime 3531\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                         _timestamp 1609842926\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                          eval/loss 978.48193\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                        eval/rouge1 38.6615\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                        eval/rouge2 14.0409\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                        eval/rougeL 22.0011\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                     eval/rougeLsum 35.6463\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                       eval/gen_len 243.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                   train/total_flos 11147765182562304\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                     train/val_loss 978.48193\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                   train/val_rouge1 38.7078\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                   train/val_rouge2 13.9899\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                   train/val_rougeL 21.9788\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                train/val_rougeLsum 35.7167\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/val_gen_len 243.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▇▆▆▆▇▄▅▇▆▄▄▅▄▄▄▄▄▄▂▄▃▃▄▃▂▂▂▁▄▂▃▂▂▃▁▃▁▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▄▄▆▆▆▆▆▆▆▆▆▇▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▄▄▆▆▆▆▆▆▆▆▆▇▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:             eval/loss █▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rouge1 ▁▂█\n","\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rouge2 ▁▃█\n","\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rougeL ▁▃█\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/rougeLsum ▁▃█\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/gen_len ▁▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train/val_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rouge1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rouge2 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rougeL ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/val_rougeLsum ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train/val_gen_len ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/bart_karger_books_moc_train\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/huggingface/runs/exbr6eju\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ah9sssub5DXX"},"source":["##### Evaluate"]},{"cell_type":"code","metadata":{"id":"rNlNs9bF04Yy"},"source":["source_test_dir = data_dir[:-1] + '/test.source\"'\n","reference_test_dir = data_dir[:-1] + '/test.target\"'\n","\n","save_dir = output_dir[:-1] + '/'+model_name_or_path.replace('/', '?')+'_test_karger_books_moc.txt\"'\n","score_dir = output_dir[:-1] + '/'+model_name_or_path.replace('/', '?')+'_test_karger_books_moc.json\"'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TqLfsjfMEt1n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609843916353,"user_tz":-60,"elapsed":248119,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"f73bb113-c365-4787-aceb-3f4c4a0cc31c"},"source":["!python3 $eval_script \\\n","$output_dir \\\n","$source_test_dir \\\n","$save_dir \\\n","--reference_path $reference_test_dir \\\n","--score_path $score_dir \\\n","--task summarization \\\n","--bs 2 \\\n","--length_penalty $config.LENAGTH_PENALTY \\\n","--no_repeat_ngram_size $config.NO_REPEAT_NGRAM_SIZE \\\n","--num_beams $config.NUM_BEAMS \\\n","--dump-args"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-01-05 10:47:49.980394: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n","parsed the following generate kwargs: {'num_beams': 2}\n","100% 90/90 [03:31<00:00,  2.35s/it]\n","{'rouge1': 37.4339, 'rouge2': 14.2967, 'rougeL': 23.0324, 'rougeLsum': 34.7576, 'n_obs': 179, 'runtime': 211, 'seconds_per_sample': 1.1788, 'num_beams': 2}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uPZ7A-sBVOam"},"source":["### Karger Books Para"]},{"cell_type":"code","metadata":{"id":"vJOYl_g6F1e2","executionInfo":{"status":"ok","timestamp":1610616705613,"user_tz":-60,"elapsed":113928,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}}},"source":["model_name_or_path = 'sshleifer/distilbart-cnn-12-6'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xs_XkbCVOar","executionInfo":{"status":"ok","timestamp":1610616705616,"user_tz":-60,"elapsed":113926,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}}},"source":["data_dir = '\"/content/drive/My Drive/MAGMA: Summarization/datasets/karger_books_para/bart\"'\n","\n","output_dir = '\"/content/drive/My Drive/MAGMA: Summarization/fine-tuning/'+\\\n","    model_name_or_path.replace('/', '?')+'_karger_books_para_train\"'\n","\n","log_dir = output_dir + '/logs'"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"wtLC1O1ZJpU3","colab":{"base_uri":"https://localhost:8080/","height":34,"referenced_widgets":["88734567ccb2435091eeab9f30b2de9b"]},"executionInfo":{"status":"ok","timestamp":1610616711730,"user_tz":-60,"elapsed":120037,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"16632326-7ab5-4ef4-aa65-44efbdf45fb3"},"source":["from transformers import AutoConfig\n","model_config = AutoConfig.from_pretrained(model_name_or_path)\n","model_config.min_length = config.ONE_BULLET_MIN_LEN\n","model_config.max_length = config.ONE_BULLET_MAX_LEN\n","model_config.length_penalty = config.LENGTH_PENALTY\n","model_config.no_repeat_ngram_size = config.NO_REPEAT_NGRAM_SIZE\n","\n","model_config.task_specific_params['summarization']['min_length'] = config.ONE_BULLET_MIN_LEN\n","model_config.task_specific_params['summarization']['max_length'] = config.ONE_BULLET_MAX_LEN\n","model_config.task_specific_params['summarization']['length_penalty'] = config.LENGTH_PENALTY\n","model_config.task_specific_params['summarization']['no_repeat_ngram_size'] = config.NO_REPEAT_NGRAM_SIZE\n","model_config_dir = '\"/content/drive/My Drive/MAGMA: Summarization/fine-tuning/'+\\\n","    model_name_or_path.replace('/', '?')+'_config\"'\n","model_config.save_pretrained(model_config_dir[1:-1])"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88734567ccb2435091eeab9f30b2de9b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1621.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M58yiP1yVOav"},"source":["##### Fine tune"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K66kY1WOVOaw","executionInfo":{"status":"ok","timestamp":1610555308072,"user_tz":-60,"elapsed":2522396,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"11421fe6-93e5-461f-8b97-0615b4c2e7af"},"source":["!python3 $finetune_script \\\n","--model_name_or_path $model_name_or_path \\\n","--config_name $model_config_dir \\\n","--tokenizer_name $model_name_or_path \\\n","--data_dir $data_dir \\\n","--fp16 \\\n","--learning_rate 3e-5 --label_smoothing 0.1 \\\n","--sortish_sampler --freeze_embeds --adafactor \\\n","--task summarization \\\n","--max_source_length 1024 \\\n","--max_target_length $config.ONE_BULLET_MAX_LEN \\\n","--val_max_target_length $config.ONE_BULLET_MAX_LEN \\\n","--test_max_target_length $config.ONE_BULLET_MAX_LEN \\\n","--do_train \\\n","--num_train_epochs 10 \\\n","--logging_steps 10 --save_steps 1000 --save_total_limit 3 \\\n","--per_device_train_batch_size 4 --per_device_eval_batch_size 4 \\\n","--do_eval --evaluation_strategy steps --eval_steps 1000 --eval_beams 2 \\\n","--metric_for_best_model rougeL --greater_is_better True \\\n","--predict_with_generate \\\n","--output_dir $output_dir \\\n","--overwrite_output_dir \\\n","--seed $config.SEED \\\n","--run_name $output_dir \\"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-01-13 15:46:33.581020: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n","01/13/2021 15:46:36 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n","01/13/2021 15:46:36 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jan13_15-46-36_856bff34268f', logging_first_step=False, logging_steps=10, save_steps=1000, save_total_limit=3, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model='rougeL', greater_is_better='True', ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False, label_smoothing=0.1, sortish_sampler=True, predict_with_generate=True, adafactor=True, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear')\n","[INFO|configuration_utils.py:429] 2021-01-13 15:46:36,544 >> loading configuration file /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_config/config.json\n","[INFO|configuration_utils.py:467] 2021-01-13 15:46:36,545 >> Model config BartConfig {\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 6,\n","  \"decoder_start_token_id\": 2,\n","  \"do_blenderbot_90_layernorm\": false,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 150,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 10,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 5,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"replacing_rate\": 0,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"student_decoder_layers\": null,\n","  \"student_encoder_layers\": null,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 1,\n","      \"max_length\": 150,\n","      \"min_length\": 10,\n","      \"no_repeat_ngram_size\": 5,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","[INFO|configuration_utils.py:431] 2021-01-13 15:46:36,564 >> loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.9307b6cec4435559ec6e79d5a210a334b17706465329e138f335649d14f27e78\n","[INFO|configuration_utils.py:467] 2021-01-13 15:46:36,564 >> Model config BartConfig {\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 6,\n","  \"decoder_start_token_id\": 2,\n","  \"do_blenderbot_90_layernorm\": false,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": true,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 56,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"replacing_rate\": 0,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"student_decoder_layers\": null,\n","  \"student_encoder_layers\": null,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","[INFO|tokenization_utils_base.py:1721] 2021-01-13 15:46:36,564 >> Model name 'sshleifer/distilbart-cnn-12-6' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'sshleifer/distilbart-cnn-12-6' is a path, a model identifier, or url to a directory containing tokenizer files.\n","[INFO|tokenization_utils_base.py:1802] 2021-01-13 15:46:36,674 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/9951e68693b9a7c583ae677e9cb53c02715d9bd0311a78706401372653cdea0a.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","[INFO|tokenization_utils_base.py:1802] 2021-01-13 15:46:36,675 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/7588c8d398d659b230a038240cc023f67b6848117d2999f06ab625af7bfc7ec1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1802] 2021-01-13 15:46:36,675 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1802] 2021-01-13 15:46:36,675 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1802] 2021-01-13 15:46:36,675 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1802] 2021-01-13 15:46:36,675 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f5316f64f9716436994a7ad76a354dc20ecb2dd74eb61d278f103a9c8b80291f.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n","[INFO|modeling_utils.py:1024] 2021-01-13 15:46:36,824 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n","[INFO|modeling_utils.py:1140] 2021-01-13 15:46:53,364 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:1149] 2021-01-13 15:46:53,364 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n","01/13/2021 15:46:53 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 1, 'max_length': 150, 'min_length': 10, 'no_repeat_ngram_size': 5, 'num_beams': 4}\n","01/13/2021 15:46:59 - INFO - __main__ -   *** Train ***\n","[INFO|trainer.py:703] 2021-01-13 15:46:59,295 >> ***** Running training *****\n","[INFO|trainer.py:704] 2021-01-13 15:46:59,295 >>   Num examples = 2046\n","[INFO|trainer.py:705] 2021-01-13 15:46:59,295 >>   Num Epochs = 10\n","[INFO|trainer.py:706] 2021-01-13 15:46:59,295 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:707] 2021-01-13 15:46:59,295 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n","[INFO|trainer.py:708] 2021-01-13 15:46:59,295 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:709] 2021-01-13 15:46:59,295 >>   Total optimization steps = 5120\n","/content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train\n","[INFO|integrations.py:371] 2021-01-13 15:46:59,308 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-01-13 15:47:00.573314: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.13\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/huggingface/runs/3pq7vevr\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210113_154659-3pq7vevr\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","  0%|          | 0/5120 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","  0%|          | 9/5120 [00:02<18:16,  4.66it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:506: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n","  0%|          | 10/5120 [00:02<22:49,  3.73it/s]\n","  0%|          | 20/5120 [00:06<34:21,  2.47it/s]{'loss': 534.3111328125, 'learning_rate': 2.9882812500000002e-05, 'epoch': 0.0390625}\n","                                                 {'loss': 572.50146484375, 'learning_rate': 2.982421875e-05, 'epoch': 0.05859375}\n","                                                 \n","                                                 \n","{'loss': 581.776220703125, 'learning_rate': 2.96484375e-05, 'epoch': 0.1171875}\n","{'loss': 677.74326171875, 'learning_rate': 2.958984375e-05, 'epoch': 0.13671875}\n","                                                 {'loss': 466.47900390625, 'learning_rate': 2.953125e-05, 'epoch': 0.15625}\n","  2%|▏         | 90/5120 [00:32<31:09,  2.69it/s]{'loss': 578.668017578125, 'learning_rate': 2.9472656250000002e-05, 'epoch': 0.17578125}\n","  2%|▏         | 100/5120 [00:36<37:20,  2.24it/s]\n","{'loss': 562.623681640625, 'learning_rate': 2.935546875e-05, 'epoch': 0.21484375}\n","  2%|▏         | 120/5120 [00:44<31:15,  2.67it/s]{'loss': 539.428662109375, 'learning_rate': 2.9296875000000002e-05, 'epoch': 0.234375}\n","{'loss': 518.97998046875, 'learning_rate': 2.923828125e-05, 'epoch': 0.25390625}\n","                                                  {'loss': 514.114794921875, 'learning_rate': 2.91796875e-05, 'epoch': 0.2734375}\n","                                                  \n","                                                  \n","  3%|▎         | 170/5120 [01:03<35:03,  2.35it/s]{'loss': 573.9013671875, 'learning_rate': 2.900390625e-05, 'epoch': 0.33203125}\n","{'loss': 618.3265625, 'learning_rate': 2.89453125e-05, 'epoch': 0.3515625}\n","                                                  {'loss': 485.85166015625, 'learning_rate': 2.8886718750000002e-05, 'epoch': 0.37109375}\n","  4%|▍         | 200/5120 [01:15<42:34,  1.93it/s]{'loss': 461.227783203125, 'learning_rate': 2.8828125e-05, 'epoch': 0.390625}\n","  4%|▍         | 210/5120 [01:19<31:31,  2.60it/s]{'loss': 423.422900390625, 'learning_rate': 2.876953125e-05, 'epoch': 0.41015625}\n","  4%|▍         | 220/5120 [01:23<32:15,  2.53it/s]\n","  4%|▍         | 230/5120 [01:27<29:18,  2.78it/s]{'loss': 488.1884765625, 'learning_rate': 2.865234375e-05, 'epoch': 0.44921875}\n","  5%|▍         | 240/5120 [01:31<34:33,  2.35it/s]{'loss': 456.462353515625, 'learning_rate': 2.859375e-05, 'epoch': 0.46875}\n","  5%|▍         | 250/5120 [01:35<31:15,  2.60it/s]{'loss': 599.41240234375, 'learning_rate': 2.853515625e-05, 'epoch': 0.48828125}\n","{'loss': 544.673095703125, 'learning_rate': 2.8476562500000002e-05, 'epoch': 0.5078125}\n","                                                  \n","                                                  \n","                                                  \n","                                                  \n","  6%|▌         | 310/5120 [01:58<30:30,  2.63it/s]\n","                                                  {'loss': 489.595458984375, 'learning_rate': 2.8125e-05, 'epoch': 0.625}\n","  6%|▋         | 330/5120 [02:06<30:02,  2.66it/s]{'loss': 554.8892578125, 'learning_rate': 2.806640625e-05, 'epoch': 0.64453125}\n","{'loss': 575.384619140625, 'learning_rate': 2.80078125e-05, 'epoch': 0.6640625}\n","  7%|▋         | 350/5120 [02:14<29:14,  2.72it/s]\n","{'loss': 538.481689453125, 'learning_rate': 2.7890625000000002e-05, 'epoch': 0.703125}\n","{'loss': 453.1533203125, 'learning_rate': 2.783203125e-05, 'epoch': 0.72265625}\n","  7%|▋         | 380/5120 [02:26<29:52,  2.64it/s]\n","{'loss': 601.09873046875, 'learning_rate': 2.7714843750000002e-05, 'epoch': 0.76171875}\n","                                                  \n","  8%|▊         | 410/5120 [02:37<29:20,  2.68it/s]{'loss': 495.05576171875, 'learning_rate': 2.759765625e-05, 'epoch': 0.80078125}\n","                                                  \n","{'loss': 446.37138671875, 'learning_rate': 2.7480468750000002e-05, 'epoch': 0.83984375}\n","{'loss': 530.330615234375, 'learning_rate': 2.7421875e-05, 'epoch': 0.859375}\n","{'loss': 525.82919921875, 'learning_rate': 2.736328125e-05, 'epoch': 0.87890625}\n","{'loss': 472.428466796875, 'learning_rate': 2.7304687500000002e-05, 'epoch': 0.8984375}\n","{'loss': 578.53388671875, 'learning_rate': 2.724609375e-05, 'epoch': 0.91796875}\n","                                                  \n","{'loss': 501.469970703125, 'learning_rate': 2.712890625e-05, 'epoch': 0.95703125}\n"," 10%|▉         | 500/5120 [03:13<37:07,  2.07it/s]\n","                                                  {'loss': 503.589453125, 'learning_rate': 2.701171875e-05, 'epoch': 0.99609375}\n","{'loss': 414.7068359375, 'learning_rate': 2.6953125e-05, 'epoch': 1.015625}\n","                                                  \n","{'loss': 411.94921875, 'learning_rate': 2.68359375e-05, 'epoch': 1.0546875}\n"," 11%|█         | 550/5120 [03:32<28:29,  2.67it/s]{'loss': 418.356298828125, 'learning_rate': 2.677734375e-05, 'epoch': 1.07421875}\n"," 11%|█         | 560/5120 [03:36<28:00,  2.71it/s]{'loss': 446.763818359375, 'learning_rate': 2.6718750000000002e-05, 'epoch': 1.09375}\n"," 11%|█         | 570/5120 [03:39<25:32,  2.97it/s]{'loss': 424.091748046875, 'learning_rate': 2.666015625e-05, 'epoch': 1.11328125}\n","{'loss': 417.8630859375, 'learning_rate': 2.66015625e-05, 'epoch': 1.1328125}\n","{'loss': 446.083984375, 'learning_rate': 2.654296875e-05, 'epoch': 1.15234375}\n","                                                  \n","{'loss': 377.046923828125, 'learning_rate': 2.642578125e-05, 'epoch': 1.19140625}\n","                                                  \n","{'loss': 422.6318359375, 'learning_rate': 2.6308593750000002e-05, 'epoch': 1.23046875}\n","{'loss': 424.27109375, 'learning_rate': 2.625e-05, 'epoch': 1.25}\n"," 13%|█▎        | 650/5120 [04:10<26:56,  2.77it/s]{'loss': 464.16904296875, 'learning_rate': 2.619140625e-05, 'epoch': 1.26953125}\n"," 13%|█▎        | 660/5120 [04:14<29:50,  2.49it/s]{'loss': 473.835400390625, 'learning_rate': 2.61328125e-05, 'epoch': 1.2890625}\n"," 13%|█▎        | 670/5120 [04:18<32:33,  2.28it/s]{'loss': 440.253515625, 'learning_rate': 2.607421875e-05, 'epoch': 1.30859375}\n"," 13%|█▎        | 680/5120 [04:22<27:55,  2.65it/s]{'loss': 427.96240234375, 'learning_rate': 2.6015625e-05, 'epoch': 1.328125}\n"," 13%|█▎        | 690/5120 [04:26<30:50,  2.39it/s]{'loss': 455.83291015625, 'learning_rate': 2.595703125e-05, 'epoch': 1.34765625}\n"," 14%|█▎        | 700/5120 [04:30<34:32,  2.13it/s]{'loss': 462.933740234375, 'learning_rate': 2.5898437500000002e-05, 'epoch': 1.3671875}\n"," 14%|█▍        | 710/5120 [04:34<24:54,  2.95it/s]{'loss': 399.699755859375, 'learning_rate': 2.583984375e-05, 'epoch': 1.38671875}\n"," 14%|█▍        | 720/5120 [04:38<26:41,  2.75it/s]{'loss': 493.972998046875, 'learning_rate': 2.578125e-05, 'epoch': 1.40625}\n"," 14%|█▍        | 730/5120 [04:41<30:05,  2.43it/s]{'loss': 508.777685546875, 'learning_rate': 2.5722656250000002e-05, 'epoch': 1.42578125}\n","{'loss': 529.47861328125, 'learning_rate': 2.56640625e-05, 'epoch': 1.4453125}\n"," 15%|█▍        | 750/5120 [04:49<26:10,  2.78it/s]\n"," 15%|█▍        | 760/5120 [04:53<28:45,  2.53it/s]{'loss': 492.04609375, 'learning_rate': 2.5546875e-05, 'epoch': 1.484375}\n","{'loss': 450.648876953125, 'learning_rate': 2.548828125e-05, 'epoch': 1.50390625}\n"," 15%|█▌        | 780/5120 [05:00<23:42,  3.05it/s]{'loss': 376.23095703125, 'learning_rate': 2.54296875e-05, 'epoch': 1.5234375}\n","                                                  {'loss': 461.6560546875, 'learning_rate': 2.537109375e-05, 'epoch': 1.54296875}\n"," 16%|█▌        | 800/5120 [05:08<35:10,  2.05it/s]\n","{'loss': 449.8970703125, 'learning_rate': 2.525390625e-05, 'epoch': 1.58203125}\n"," 16%|█▌        | 820/5120 [05:16<28:06,  2.55it/s]{'loss': 462.022216796875, 'learning_rate': 2.51953125e-05, 'epoch': 1.6015625}\n"," 16%|█▌        | 830/5120 [05:21<32:52,  2.18it/s]{'loss': 612.90263671875, 'learning_rate': 2.513671875e-05, 'epoch': 1.62109375}\n","{'loss': 451.10634765625, 'learning_rate': 2.5078125e-05, 'epoch': 1.640625}\n","{'loss': 535.747119140625, 'learning_rate': 2.501953125e-05, 'epoch': 1.66015625}\n","                                                  \n","{'loss': 431.568603515625, 'learning_rate': 2.4902343750000002e-05, 'epoch': 1.69921875}\n","                                                  \n","                                                  \n"," 18%|█▊        | 900/5120 [05:47<31:32,  2.23it/s]{'loss': 392.8036865234375, 'learning_rate': 2.4726562500000002e-05, 'epoch': 1.7578125}\n"," 18%|█▊        | 910/5120 [05:51<27:03,  2.59it/s]{'loss': 489.491943359375, 'learning_rate': 2.466796875e-05, 'epoch': 1.77734375}\n","                                                  {'loss': 520.41240234375, 'learning_rate': 2.4609375e-05, 'epoch': 1.796875}\n","                                                  \n"," 18%|█▊        | 940/5120 [06:04<24:51,  2.80it/s]{'loss': 457.93173828125, 'learning_rate': 2.44921875e-05, 'epoch': 1.8359375}\n","{'loss': 395.2188720703125, 'learning_rate': 2.443359375e-05, 'epoch': 1.85546875}\n"," 19%|█▉        | 960/5120 [06:12<30:05,  2.30it/s]{'loss': 547.123974609375, 'learning_rate': 2.4375e-05, 'epoch': 1.875}\n"," 19%|█▉        | 970/5120 [06:16<28:15,  2.45it/s]{'loss': 499.803271484375, 'learning_rate': 2.4316406250000002e-05, 'epoch': 1.89453125}\n","                                                  \n","                                                  {'loss': 430.345361328125, 'learning_rate': 2.419921875e-05, 'epoch': 1.93359375}\n"," 20%|█▉        | 1000/5120 [06:28<34:08,  2.01it/s][INFO|trainer.py:1412] 2021-01-13 15:53:31,037 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-13 15:53:31,037 >>   Num examples = 266\n","[INFO|trainer.py:1414] 2021-01-13 15:53:31,037 >>   Batch size = 4\n","\n","\n","  0%|          | 0/67 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 2/67 [00:01<00:34,  1.88it/s]\u001b[A\n","  4%|▍         | 3/67 [00:01<00:41,  1.53it/s]\u001b[A\n","  6%|▌         | 4/67 [00:02<00:43,  1.45it/s]\u001b[A\n","  7%|▋         | 5/67 [00:03<00:52,  1.18it/s]\u001b[A\n","  9%|▉         | 6/67 [00:05<01:08,  1.12s/it]\u001b[A\n"," 10%|█         | 7/67 [00:06<01:05,  1.09s/it]\u001b[A\n"," 12%|█▏        | 8/67 [00:07<00:59,  1.00s/it]\u001b[A\n"," 13%|█▎        | 9/67 [00:08<00:53,  1.08it/s]\u001b[A\n"," 15%|█▍        | 10/67 [00:09<00:54,  1.05it/s]\u001b[A\n"," 16%|█▋        | 11/67 [00:10<00:53,  1.05it/s]\u001b[A\n"," 18%|█▊        | 12/67 [00:11<00:52,  1.06it/s]\u001b[A\n"," 19%|█▉        | 13/67 [00:12<01:00,  1.12s/it]\u001b[A\n"," 21%|██        | 14/67 [00:14<01:03,  1.21s/it]\u001b[A\n"," 22%|██▏       | 15/67 [00:15<01:07,  1.30s/it]\u001b[A\n"," 24%|██▍       | 16/67 [00:16<01:04,  1.26s/it]\u001b[A\n"," 25%|██▌       | 17/67 [00:17<00:57,  1.14s/it]\u001b[A\n"," 27%|██▋       | 18/67 [00:18<00:48,  1.01it/s]\u001b[A\n"," 28%|██▊       | 19/67 [00:19<00:44,  1.09it/s]\u001b[A\n"," 30%|██▉       | 20/67 [00:20<00:45,  1.03it/s]\u001b[A\n"," 31%|███▏      | 21/67 [00:21<00:43,  1.06it/s]\u001b[A\n"," 33%|███▎      | 22/67 [00:21<00:40,  1.11it/s]\u001b[A\n"," 34%|███▍      | 23/67 [00:22<00:40,  1.10it/s]\u001b[A\n"," 36%|███▌      | 24/67 [00:23<00:34,  1.24it/s]\u001b[A\n"," 37%|███▋      | 25/67 [00:24<00:34,  1.20it/s]\u001b[A\n"," 39%|███▉      | 26/67 [00:26<00:47,  1.16s/it]\u001b[A\n"," 40%|████      | 27/67 [00:26<00:41,  1.03s/it]\u001b[A\n"," 42%|████▏     | 28/67 [00:27<00:38,  1.01it/s]\u001b[A\n"," 43%|████▎     | 29/67 [00:28<00:38,  1.00s/it]\u001b[A\n"," 45%|████▍     | 30/67 [00:29<00:35,  1.03it/s]\u001b[A\n"," 46%|████▋     | 31/67 [00:30<00:35,  1.01it/s]\u001b[A\n"," 48%|████▊     | 32/67 [00:31<00:35,  1.00s/it]\u001b[A\n"," 49%|████▉     | 33/67 [00:32<00:33,  1.03it/s]\u001b[A\n"," 51%|█████     | 34/67 [00:34<00:35,  1.08s/it]\u001b[A\n"," 52%|█████▏    | 35/67 [00:35<00:35,  1.10s/it]\u001b[A\n"," 54%|█████▎    | 36/67 [00:35<00:31,  1.01s/it]\u001b[A\n"," 55%|█████▌    | 37/67 [00:37<00:31,  1.04s/it]\u001b[A\n"," 57%|█████▋    | 38/67 [00:38<00:30,  1.04s/it]\u001b[A\n"," 58%|█████▊    | 39/67 [00:38<00:26,  1.07it/s]\u001b[A\n"," 60%|█████▉    | 40/67 [00:40<00:29,  1.10s/it]\u001b[A\n"," 61%|██████    | 41/67 [00:41<00:26,  1.02s/it]\u001b[A\n"," 63%|██████▎   | 42/67 [00:41<00:22,  1.10it/s]\u001b[A\n"," 64%|██████▍   | 43/67 [00:43<00:25,  1.08s/it]\u001b[A\n"," 66%|██████▌   | 44/67 [00:43<00:21,  1.09it/s]\u001b[A\n"," 67%|██████▋   | 45/67 [00:45<00:26,  1.20s/it]\u001b[A\n"," 69%|██████▊   | 46/67 [00:46<00:23,  1.13s/it]\u001b[A\n"," 70%|███████   | 47/67 [00:47<00:21,  1.06s/it]\u001b[A\n"," 72%|███████▏  | 48/67 [00:48<00:18,  1.03it/s]\u001b[A\n"," 73%|███████▎  | 49/67 [00:49<00:19,  1.08s/it]\u001b[A\n"," 75%|███████▍  | 50/67 [00:50<00:18,  1.06s/it]\u001b[A\n"," 76%|███████▌  | 51/67 [00:51<00:17,  1.10s/it]\u001b[A\n"," 78%|███████▊  | 52/67 [00:52<00:14,  1.00it/s]\u001b[A\n"," 79%|███████▉  | 53/67 [00:53<00:12,  1.09it/s]\u001b[A\n"," 81%|████████  | 54/67 [00:53<00:10,  1.22it/s]\u001b[A\n"," 82%|████████▏ | 55/67 [00:54<00:08,  1.33it/s]\u001b[A\n"," 84%|████████▎ | 56/67 [00:55<00:10,  1.05it/s]\u001b[A\n"," 85%|████████▌ | 57/67 [00:57<00:10,  1.06s/it]\u001b[A\n"," 87%|████████▋ | 58/67 [00:58<00:10,  1.21s/it]\u001b[A\n"," 88%|████████▊ | 59/67 [00:59<00:08,  1.10s/it]\u001b[A\n"," 90%|████████▉ | 60/67 [01:00<00:06,  1.03it/s]\u001b[A\n"," 91%|█████████ | 61/67 [01:01<00:05,  1.09it/s]\u001b[A\n"," 93%|█████████▎| 62/67 [01:02<00:04,  1.06it/s]\u001b[A\n"," 94%|█████████▍| 63/67 [01:03<00:03,  1.07it/s]\u001b[A\n"," 96%|█████████▌| 64/67 [01:04<00:03,  1.00s/it]\u001b[A\n"," 97%|█████████▋| 65/67 [01:05<00:02,  1.11s/it]\u001b[A\n"," 99%|█████████▊| 66/67 [01:06<00:01,  1.01s/it]\u001b[A\n","                                                   \n"," 20%|█▉        | 1000/5120 [07:37<34:08,  2.01it/s]\n","100%|██████████| 67/67 [01:07<00:00,  1.13it/s]\u001b[A\n","                                               \u001b[A{'eval_loss': 536.7622680664062, 'eval_rouge1': 32.6571, 'eval_rouge2': 14.9015, 'eval_rougeL': 26.7681, 'eval_rougeLsum': 27.7186, 'eval_gen_len': 35.8, 'epoch': 1.953125}\n","[INFO|trainer.py:1226] 2021-01-13 15:54:40,484 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-1000\n","[INFO|configuration_utils.py:289] 2021-01-13 15:54:40,493 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:814] 2021-01-13 15:54:49,798 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-1000/pytorch_model.bin\n","{'loss': 455.197216796875, 'learning_rate': 2.408203125e-05, 'epoch': 1.97265625}\n"," 20%|█▉        | 1020/5120 [07:54<29:27,  2.32it/s]{'loss': 451.1169921875, 'learning_rate': 2.40234375e-05, 'epoch': 1.9921875}\n","                                                   {'loss': 393.06201171875, 'learning_rate': 2.396484375e-05, 'epoch': 2.01171875}\n"," 20%|██        | 1040/5120 [08:03<29:21,  2.32it/s]{'loss': 422.968994140625, 'learning_rate': 2.3906250000000002e-05, 'epoch': 2.03125}\n","{'loss': 395.59443359375, 'learning_rate': 2.384765625e-05, 'epoch': 2.05078125}\n","{'loss': 389.3944091796875, 'learning_rate': 2.37890625e-05, 'epoch': 2.0703125}\n","                                                   \n"," 21%|██        | 1080/5120 [08:19<25:04,  2.68it/s]{'loss': 397.0306884765625, 'learning_rate': 2.3671875e-05, 'epoch': 2.109375}\n"," 21%|██▏       | 1090/5120 [08:24<44:02,  1.53it/s]{'loss': 422.5958984375, 'learning_rate': 2.361328125e-05, 'epoch': 2.12890625}\n"," 21%|██▏       | 1100/5120 [08:29<33:35,  1.99it/s]{'loss': 363.7034912109375, 'learning_rate': 2.35546875e-05, 'epoch': 2.1484375}\n"," 22%|██▏       | 1110/5120 [08:32<23:57,  2.79it/s]{'loss': 344.085400390625, 'learning_rate': 2.349609375e-05, 'epoch': 2.16796875}\n"," 22%|██▏       | 1120/5120 [08:36<24:26,  2.73it/s]{'loss': 374.651416015625, 'learning_rate': 2.34375e-05, 'epoch': 2.1875}\n"," 22%|██▏       | 1130/5120 [08:40<28:48,  2.31it/s]\n"," 22%|██▏       | 1140/5120 [08:44<23:30,  2.82it/s]{'loss': 360.42509765625, 'learning_rate': 2.3320312500000002e-05, 'epoch': 2.2265625}\n"," 22%|██▏       | 1150/5120 [08:48<23:51,  2.77it/s]{'loss': 376.337255859375, 'learning_rate': 2.326171875e-05, 'epoch': 2.24609375}\n"," 23%|██▎       | 1160/5120 [08:51<24:34,  2.69it/s]\n"," 23%|██▎       | 1170/5120 [08:55<24:07,  2.73it/s]{'loss': 383.8984619140625, 'learning_rate': 2.3144531250000002e-05, 'epoch': 2.28515625}\n","{'loss': 368.1463623046875, 'learning_rate': 2.30859375e-05, 'epoch': 2.3046875}\n"," 23%|██▎       | 1190/5120 [09:03<27:44,  2.36it/s]{'loss': 372.379638671875, 'learning_rate': 2.302734375e-05, 'epoch': 2.32421875}\n","{'loss': 393.23818359375, 'learning_rate': 2.296875e-05, 'epoch': 2.34375}\n"," 24%|██▎       | 1210/5120 [09:11<23:07,  2.82it/s]{'loss': 383.299072265625, 'learning_rate': 2.291015625e-05, 'epoch': 2.36328125}\n","                                                   \n"," 24%|██▍       | 1230/5120 [09:19<27:26,  2.36it/s]{'loss': 420.47216796875, 'learning_rate': 2.279296875e-05, 'epoch': 2.40234375}\n"," 24%|██▍       | 1240/5120 [09:23<25:14,  2.56it/s]{'loss': 336.5530029296875, 'learning_rate': 2.2734375000000002e-05, 'epoch': 2.421875}\n"," 24%|██▍       | 1250/5120 [09:27<26:06,  2.47it/s]{'loss': 332.9434326171875, 'learning_rate': 2.267578125e-05, 'epoch': 2.44140625}\n"," 25%|██▍       | 1260/5120 [09:30<23:07,  2.78it/s]{'loss': 412.69765625, 'learning_rate': 2.26171875e-05, 'epoch': 2.4609375}\n"," 25%|██▍       | 1270/5120 [09:34<25:56,  2.47it/s]{'loss': 339.901806640625, 'learning_rate': 2.255859375e-05, 'epoch': 2.48046875}\n"," 25%|██▌       | 1280/5120 [09:39<33:22,  1.92it/s]\n"," 25%|██▌       | 1290/5120 [09:42<25:08,  2.54it/s]{'loss': 412.99287109375, 'learning_rate': 2.244140625e-05, 'epoch': 2.51953125}\n"," 25%|██▌       | 1300/5120 [09:47<32:36,  1.95it/s]{'loss': 456.983447265625, 'learning_rate': 2.23828125e-05, 'epoch': 2.5390625}\n","{'loss': 356.8503662109375, 'learning_rate': 2.2324218750000002e-05, 'epoch': 2.55859375}\n","                                                   \n","                                                   \n","                                                   \n","                                                   \n","{'loss': 365.683984375, 'learning_rate': 2.203125e-05, 'epoch': 2.65625}\n","{'loss': 440.962744140625, 'learning_rate': 2.197265625e-05, 'epoch': 2.67578125}\n","{'loss': 362.89677734375, 'learning_rate': 2.19140625e-05, 'epoch': 2.6953125}\n"," 27%|██▋       | 1390/5120 [10:21<30:06,  2.06it/s]{'loss': 435.821435546875, 'learning_rate': 2.185546875e-05, 'epoch': 2.71484375}\n","{'loss': 461.74326171875, 'learning_rate': 2.1796875e-05, 'epoch': 2.734375}\n"," 28%|██▊       | 1410/5120 [10:29<23:20,  2.65it/s]{'loss': 377.876220703125, 'learning_rate': 2.1738281250000002e-05, 'epoch': 2.75390625}\n","{'loss': 420.4216796875, 'learning_rate': 2.16796875e-05, 'epoch': 2.7734375}\n","                                                   \n","{'loss': 401.033349609375, 'learning_rate': 2.15625e-05, 'epoch': 2.8125}\n"," 28%|██▊       | 1450/5120 [10:44<22:53,  2.67it/s]\n","{'loss': 374.9667724609375, 'learning_rate': 2.14453125e-05, 'epoch': 2.8515625}\n"," 29%|██▊       | 1470/5120 [10:52<23:46,  2.56it/s]{'loss': 363.991259765625, 'learning_rate': 2.138671875e-05, 'epoch': 2.87109375}\n","{'loss': 356.6601806640625, 'learning_rate': 2.1328125000000002e-05, 'epoch': 2.890625}\n","{'loss': 452.916943359375, 'learning_rate': 2.126953125e-05, 'epoch': 2.91015625}\n","{'loss': 361.74345703125, 'learning_rate': 2.12109375e-05, 'epoch': 2.9296875}\n"," 29%|██▉       | 1510/5120 [11:08<23:04,  2.61it/s]{'loss': 457.8619140625, 'learning_rate': 2.1152343750000002e-05, 'epoch': 2.94921875}\n","{'loss': 410.294482421875, 'learning_rate': 2.109375e-05, 'epoch': 2.96875}\n","{'loss': 382.7613037109375, 'learning_rate': 2.103515625e-05, 'epoch': 2.98828125}\n","{'loss': 359.771044921875, 'learning_rate': 2.09765625e-05, 'epoch': 3.0078125}\n","{'loss': 340.6985107421875, 'learning_rate': 2.091796875e-05, 'epoch': 3.02734375}\n","{'loss': 327.004296875, 'learning_rate': 2.0859375e-05, 'epoch': 3.046875}\n"," 31%|███       | 1570/5120 [11:31<22:39,  2.61it/s]\n","{'loss': 335.4514404296875, 'learning_rate': 2.0742187500000002e-05, 'epoch': 3.0859375}\n","                                                   \n"," 31%|███▏      | 1600/5120 [11:43<29:25,  1.99it/s]{'loss': 345.37021484375, 'learning_rate': 2.0625e-05, 'epoch': 3.125}\n"," 31%|███▏      | 1610/5120 [11:47<22:40,  2.58it/s]{'loss': 377.60400390625, 'learning_rate': 2.056640625e-05, 'epoch': 3.14453125}\n","{'loss': 339.835107421875, 'learning_rate': 2.05078125e-05, 'epoch': 3.1640625}\n","                                                   {'loss': 338.1076171875, 'learning_rate': 2.044921875e-05, 'epoch': 3.18359375}\n"," 32%|███▏      | 1640/5120 [11:59<22:03,  2.63it/s]{'loss': 340.15361328125, 'learning_rate': 2.0390625e-05, 'epoch': 3.203125}\n","                                                   \n","{'loss': 384.9115234375, 'learning_rate': 2.02734375e-05, 'epoch': 3.2421875}\n"," 33%|███▎      | 1670/5120 [12:10<20:39,  2.78it/s]\n","{'loss': 378.732861328125, 'learning_rate': 2.0156250000000002e-05, 'epoch': 3.28125}\n","{'loss': 361.619873046875, 'learning_rate': 2.009765625e-05, 'epoch': 3.30078125}\n","{'loss': 317.918603515625, 'learning_rate': 2.00390625e-05, 'epoch': 3.3203125}\n"," 33%|███▎      | 1710/5120 [12:27<22:03,  2.58it/s]{'loss': 366.3254150390625, 'learning_rate': 1.998046875e-05, 'epoch': 3.33984375}\n"," 34%|███▎      | 1720/5120 [12:31<20:28,  2.77it/s]\n"," 34%|███▍      | 1730/5120 [12:34<21:04,  2.68it/s]{'loss': 316.72529296875, 'learning_rate': 1.986328125e-05, 'epoch': 3.37890625}\n","{'loss': 324.7845703125, 'learning_rate': 1.98046875e-05, 'epoch': 3.3984375}\n"," 34%|███▍      | 1750/5120 [12:42<22:16,  2.52it/s]{'loss': 359.9656494140625, 'learning_rate': 1.9746093750000002e-05, 'epoch': 3.41796875}\n"," 34%|███▍      | 1760/5120 [12:46<21:30,  2.60it/s]\n"," 35%|███▍      | 1770/5120 [12:50<24:21,  2.29it/s]\n","                                                   \n"," 35%|███▍      | 1790/5120 [12:59<23:57,  2.32it/s]\n","{'loss': 351.135693359375, 'learning_rate': 1.9453125e-05, 'epoch': 3.515625}\n"," 35%|███▌      | 1810/5120 [13:06<20:06,  2.74it/s]{'loss': 328.7241943359375, 'learning_rate': 1.939453125e-05, 'epoch': 3.53515625}\n"," 36%|███▌      | 1820/5120 [13:10<21:25,  2.57it/s]{'loss': 340.489013671875, 'learning_rate': 1.93359375e-05, 'epoch': 3.5546875}\n"," 36%|███▌      | 1830/5120 [13:14<20:24,  2.69it/s]{'loss': 294.6648193359375, 'learning_rate': 1.927734375e-05, 'epoch': 3.57421875}\n","                                                   \n","{'loss': 337.1810546875, 'learning_rate': 1.9160156250000002e-05, 'epoch': 3.61328125}\n","{'loss': 362.678271484375, 'learning_rate': 1.91015625e-05, 'epoch': 3.6328125}\n"," 37%|███▋      | 1870/5120 [13:30<20:47,  2.61it/s]{'loss': 336.679931640625, 'learning_rate': 1.904296875e-05, 'epoch': 3.65234375}\n","{'loss': 296.819140625, 'learning_rate': 1.8984375e-05, 'epoch': 3.671875}\n","                                                   \n"," 37%|███▋      | 1900/5120 [13:42<28:52,  1.86it/s]{'loss': 375.5504638671875, 'learning_rate': 1.88671875e-05, 'epoch': 3.7109375}\n"," 37%|███▋      | 1910/5120 [13:46<22:41,  2.36it/s]{'loss': 365.11181640625, 'learning_rate': 1.880859375e-05, 'epoch': 3.73046875}\n"," 38%|███▊      | 1920/5120 [13:50<22:34,  2.36it/s]\n"," 38%|███▊      | 1930/5120 [13:54<20:59,  2.53it/s]\n"," 38%|███▊      | 1940/5120 [13:58<18:23,  2.88it/s]{'loss': 372.22666015625, 'learning_rate': 1.86328125e-05, 'epoch': 3.7890625}\n","{'loss': 411.6421875, 'learning_rate': 1.8574218750000002e-05, 'epoch': 3.80859375}\n"," 38%|███▊      | 1960/5120 [14:06<17:39,  2.98it/s]{'loss': 309.273583984375, 'learning_rate': 1.8515625e-05, 'epoch': 3.828125}\n","{'loss': 332.2284912109375, 'learning_rate': 1.845703125e-05, 'epoch': 3.84765625}\n"," 39%|███▊      | 1980/5120 [14:13<20:15,  2.58it/s]\n","{'loss': 307.8968505859375, 'learning_rate': 1.833984375e-05, 'epoch': 3.88671875}\n"," 39%|███▉      | 2000/5120 [14:21<27:22,  1.90it/s][INFO|trainer.py:1412] 2021-01-13 16:01:24,767 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-13 16:01:24,767 >>   Num examples = 266\n","[INFO|trainer.py:1414] 2021-01-13 16:01:24,767 >>   Batch size = 4\n","\n","\n","  0%|          | 0/67 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 2/67 [00:00<00:32,  2.03it/s]\u001b[A\n","  4%|▍         | 3/67 [00:01<00:40,  1.59it/s]\u001b[A\n","  6%|▌         | 4/67 [00:02<00:42,  1.47it/s]\u001b[A\n","  7%|▋         | 5/67 [00:03<00:50,  1.22it/s]\u001b[A\n","  9%|▉         | 6/67 [00:05<01:07,  1.10s/it]\u001b[A\n"," 10%|█         | 7/67 [00:06<01:06,  1.11s/it]\u001b[A\n"," 12%|█▏        | 8/67 [00:07<01:01,  1.04s/it]\u001b[A\n"," 13%|█▎        | 9/67 [00:08<00:53,  1.08it/s]\u001b[A\n"," 15%|█▍        | 10/67 [00:10<01:07,  1.18s/it]\u001b[A\n"," 16%|█▋        | 11/67 [00:11<01:04,  1.15s/it]\u001b[A\n"," 18%|█▊        | 12/67 [00:12<01:01,  1.12s/it]\u001b[A\n"," 19%|█▉        | 13/67 [00:13<01:10,  1.30s/it]\u001b[A\n"," 21%|██        | 14/67 [00:15<01:13,  1.39s/it]\u001b[A\n"," 22%|██▏       | 15/67 [00:16<01:11,  1.38s/it]\u001b[A\n"," 24%|██▍       | 16/67 [00:17<01:03,  1.24s/it]\u001b[A\n"," 25%|██▌       | 17/67 [00:18<00:50,  1.01s/it]\u001b[A\n"," 27%|██▋       | 18/67 [00:18<00:45,  1.08it/s]\u001b[A\n"," 28%|██▊       | 19/67 [00:20<00:52,  1.08s/it]\u001b[A\n"," 30%|██▉       | 20/67 [00:21<00:47,  1.01s/it]\u001b[A\n"," 31%|███▏      | 21/67 [00:22<00:44,  1.03it/s]\u001b[A\n"," 33%|███▎      | 22/67 [00:23<00:43,  1.02it/s]\u001b[A\n"," 34%|███▍      | 23/67 [00:23<00:37,  1.17it/s]\u001b[A\n"," 36%|███▌      | 24/67 [00:24<00:35,  1.22it/s]\u001b[A\n"," 37%|███▋      | 25/67 [00:25<00:32,  1.30it/s]\u001b[A\n"," 39%|███▉      | 26/67 [00:26<00:41,  1.02s/it]\u001b[A\n"," 40%|████      | 27/67 [00:27<00:36,  1.10it/s]\u001b[A\n"," 42%|████▏     | 28/67 [00:28<00:35,  1.08it/s]\u001b[A\n"," 43%|████▎     | 29/67 [00:29<00:37,  1.00it/s]\u001b[A\n"," 45%|████▍     | 30/67 [00:30<00:35,  1.05it/s]\u001b[A\n"," 46%|████▋     | 31/67 [00:31<00:33,  1.07it/s]\u001b[A\n"," 48%|████▊     | 32/67 [00:32<00:33,  1.06it/s]\u001b[A\n"," 49%|████▉     | 33/67 [00:33<00:32,  1.05it/s]\u001b[A\n"," 51%|█████     | 34/67 [00:34<00:33,  1.02s/it]\u001b[A\n"," 52%|█████▏    | 35/67 [00:35<00:31,  1.02it/s]\u001b[A\n"," 54%|█████▎    | 36/67 [00:36<00:29,  1.07it/s]\u001b[A\n"," 55%|█████▌    | 37/67 [00:37<00:33,  1.11s/it]\u001b[A\n"," 57%|█████▋    | 38/67 [00:38<00:29,  1.01s/it]\u001b[A\n"," 58%|█████▊    | 39/67 [00:39<00:26,  1.07it/s]\u001b[A\n"," 60%|█████▉    | 40/67 [00:40<00:32,  1.21s/it]\u001b[A\n"," 61%|██████    | 41/67 [00:41<00:28,  1.11s/it]\u001b[A\n"," 63%|██████▎   | 42/67 [00:42<00:23,  1.05it/s]\u001b[A\n"," 64%|██████▍   | 43/67 [00:44<00:27,  1.14s/it]\u001b[A\n"," 66%|██████▌   | 44/67 [00:44<00:23,  1.01s/it]\u001b[A\n"," 67%|██████▋   | 45/67 [00:46<00:25,  1.17s/it]\u001b[A\n"," 69%|██████▊   | 46/67 [00:47<00:23,  1.11s/it]\u001b[A\n"," 70%|███████   | 47/67 [00:48<00:21,  1.08s/it]\u001b[A\n"," 72%|███████▏  | 48/67 [00:49<00:19,  1.02s/it]\u001b[A\n"," 73%|███████▎  | 49/67 [00:50<00:19,  1.07s/it]\u001b[A\n"," 75%|███████▍  | 50/67 [00:51<00:19,  1.12s/it]\u001b[A\n"," 76%|███████▌  | 51/67 [00:52<00:17,  1.09s/it]\u001b[A\n"," 78%|███████▊  | 52/67 [00:53<00:14,  1.03it/s]\u001b[A\n"," 79%|███████▉  | 53/67 [00:54<00:12,  1.10it/s]\u001b[A\n"," 81%|████████  | 54/67 [00:54<00:10,  1.20it/s]\u001b[A\n"," 82%|████████▏ | 55/67 [00:55<00:09,  1.20it/s]\u001b[A\n"," 84%|████████▎ | 56/67 [00:56<00:10,  1.05it/s]\u001b[A\n"," 85%|████████▌ | 57/67 [00:58<00:11,  1.18s/it]\u001b[A\n"," 87%|████████▋ | 58/67 [01:00<00:12,  1.38s/it]\u001b[A\n"," 88%|████████▊ | 59/67 [01:01<00:09,  1.25s/it]\u001b[A\n"," 90%|████████▉ | 60/67 [01:01<00:07,  1.04s/it]\u001b[A\n"," 91%|█████████ | 61/67 [01:03<00:06,  1.14s/it]\u001b[A\n"," 93%|█████████▎| 62/67 [01:04<00:05,  1.16s/it]\u001b[A\n"," 94%|█████████▍| 63/67 [01:05<00:04,  1.09s/it]\u001b[A\n"," 96%|█████████▌| 64/67 [01:06<00:03,  1.02s/it]\u001b[A\n"," 97%|█████████▋| 65/67 [01:08<00:02,  1.36s/it]\u001b[A\n"," 99%|█████████▊| 66/67 [01:09<00:01,  1.27s/it]\u001b[A\n","                                                   \n","{'eval_loss': 558.7825317382812, 'eval_rouge1': 32.5835, 'eval_rouge2': 15.0989, 'eval_rougeL': 26.5244, 'eval_rougeLsum': 27.1085, 'eval_gen_len': 36.8, 'epoch': 3.90625}\n"," 39%|███▉      | 2000/5120 [15:34<27:22,  1.90it/s]\n","100%|██████████| 67/67 [01:10<00:00,  1.03s/it]\u001b[A\n","                                               \u001b[A[INFO|trainer.py:1226] 2021-01-13 16:02:37,881 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-2000\n","[INFO|configuration_utils.py:289] 2021-01-13 16:02:37,888 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:814] 2021-01-13 16:02:45,247 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-2000/pytorch_model.bin\n","                                                     \n"," 39%|███▉      | 2020/5120 [15:50<26:58,  1.91it/s]{'loss': 380.5328857421875, 'learning_rate': 1.8164062500000002e-05, 'epoch': 3.9453125}\n"," 40%|███▉      | 2030/5120 [15:55<19:29,  2.64it/s]{'loss': 318.232763671875, 'learning_rate': 1.810546875e-05, 'epoch': 3.96484375}\n"," 40%|███▉      | 2040/5120 [15:59<24:30,  2.09it/s]\n"," 40%|████      | 2050/5120 [16:04<21:08,  2.42it/s]{'loss': 301.572216796875, 'learning_rate': 1.798828125e-05, 'epoch': 4.00390625}\n","{'loss': 322.978369140625, 'learning_rate': 1.79296875e-05, 'epoch': 4.0234375}\n","{'loss': 362.1740234375, 'learning_rate': 1.787109375e-05, 'epoch': 4.04296875}\n","                                                   \n"," 41%|████      | 2090/5120 [16:18<17:19,  2.91it/s]{'loss': 286.3010009765625, 'learning_rate': 1.775390625e-05, 'epoch': 4.08203125}\n"," 41%|████      | 2100/5120 [16:23<23:53,  2.11it/s]\n"," 41%|████      | 2110/5120 [16:27<23:24,  2.14it/s]{'loss': 313.1720458984375, 'learning_rate': 1.763671875e-05, 'epoch': 4.12109375}\n"," 41%|████▏     | 2120/5120 [16:31<17:33,  2.85it/s]{'loss': 330.2454833984375, 'learning_rate': 1.7578125000000002e-05, 'epoch': 4.140625}\n"," 42%|████▏     | 2130/5120 [16:35<17:25,  2.86it/s]\n"," 42%|████▏     | 2140/5120 [16:39<18:10,  2.73it/s]\n"," 42%|████▏     | 2150/5120 [16:42<17:51,  2.77it/s]{'loss': 342.6125, 'learning_rate': 1.740234375e-05, 'epoch': 4.19921875}\n"," 42%|████▏     | 2160/5120 [16:47<22:13,  2.22it/s]{'loss': 287.8828125, 'learning_rate': 1.734375e-05, 'epoch': 4.21875}\n","{'loss': 336.959423828125, 'learning_rate': 1.728515625e-05, 'epoch': 4.23828125}\n","{'loss': 336.89765625, 'learning_rate': 1.72265625e-05, 'epoch': 4.2578125}\n"," 43%|████▎     | 2190/5120 [16:58<16:52,  2.89it/s]\n"," 43%|████▎     | 2200/5120 [17:02<23:54,  2.04it/s]{'loss': 284.273046875, 'learning_rate': 1.7109375e-05, 'epoch': 4.296875}\n"," 43%|████▎     | 2210/5120 [17:06<20:12,  2.40it/s]{'loss': 353.6642578125, 'learning_rate': 1.705078125e-05, 'epoch': 4.31640625}\n","{'loss': 375.30478515625, 'learning_rate': 1.69921875e-05, 'epoch': 4.3359375}\n","{'loss': 367.31025390625, 'learning_rate': 1.693359375e-05, 'epoch': 4.35546875}\n","{'loss': 332.928466796875, 'learning_rate': 1.6875e-05, 'epoch': 4.375}\n"," 44%|████▍     | 2250/5120 [17:23<17:54,  2.67it/s]{'loss': 303.9752197265625, 'learning_rate': 1.681640625e-05, 'epoch': 4.39453125}\n"," 44%|████▍     | 2260/5120 [17:27<16:31,  2.88it/s]{'loss': 292.0725341796875, 'learning_rate': 1.67578125e-05, 'epoch': 4.4140625}\n"," 44%|████▍     | 2270/5120 [17:31<17:26,  2.72it/s]\n"," 45%|████▍     | 2280/5120 [17:34<17:48,  2.66it/s]{'loss': 316.3568359375, 'learning_rate': 1.6640625e-05, 'epoch': 4.453125}\n"," 45%|████▍     | 2290/5120 [17:38<17:24,  2.71it/s]{'loss': 290.0806640625, 'learning_rate': 1.6582031250000002e-05, 'epoch': 4.47265625}\n"," 45%|████▍     | 2300/5120 [17:43<25:29,  1.84it/s]\n","                                                   \n"," 45%|████▌     | 2320/5120 [17:51<19:01,  2.45it/s]{'loss': 322.137841796875, 'learning_rate': 1.640625e-05, 'epoch': 4.53125}\n"," 46%|████▌     | 2330/5120 [17:54<17:36,  2.64it/s]{'loss': 299.268701171875, 'learning_rate': 1.634765625e-05, 'epoch': 4.55078125}\n"," 46%|████▌     | 2340/5120 [17:58<18:38,  2.49it/s]{'loss': 283.20537109375, 'learning_rate': 1.62890625e-05, 'epoch': 4.5703125}\n"," 46%|████▌     | 2350/5120 [18:02<18:17,  2.52it/s]\n","                                                   \n"," 46%|████▋     | 2370/5120 [18:10<17:11,  2.67it/s]\n","                                                   {'loss': 344.495361328125, 'learning_rate': 1.60546875e-05, 'epoch': 4.6484375}\n","{'loss': 378.1505126953125, 'learning_rate': 1.599609375e-05, 'epoch': 4.66796875}\n"," 47%|████▋     | 2400/5120 [18:23<22:08,  2.05it/s]\n"," 47%|████▋     | 2410/5120 [18:27<20:24,  2.21it/s]\n","{'loss': 300.4457275390625, 'learning_rate': 1.58203125e-05, 'epoch': 4.7265625}\n"," 47%|████▋     | 2430/5120 [18:35<17:13,  2.60it/s]{'loss': 289.324462890625, 'learning_rate': 1.576171875e-05, 'epoch': 4.74609375}\n"," 48%|████▊     | 2440/5120 [18:39<16:26,  2.72it/s]{'loss': 298.4052490234375, 'learning_rate': 1.5703125e-05, 'epoch': 4.765625}\n"," 48%|████▊     | 2450/5120 [18:43<18:06,  2.46it/s]{'loss': 303.5906005859375, 'learning_rate': 1.564453125e-05, 'epoch': 4.78515625}\n","{'loss': 301.3013427734375, 'learning_rate': 1.5585937500000002e-05, 'epoch': 4.8046875}\n","                                                   {'loss': 311.5985595703125, 'learning_rate': 1.552734375e-05, 'epoch': 4.82421875}\n"," 48%|████▊     | 2480/5120 [18:55<15:58,  2.76it/s]{'loss': 269.1014404296875, 'learning_rate': 1.546875e-05, 'epoch': 4.84375}\n"," 49%|████▊     | 2490/5120 [18:59<17:32,  2.50it/s]{'loss': 330.5791015625, 'learning_rate': 1.541015625e-05, 'epoch': 4.86328125}\n","                                                   \n","{'loss': 321.638623046875, 'learning_rate': 1.529296875e-05, 'epoch': 4.90234375}\n"," 49%|████▉     | 2520/5120 [19:11<16:13,  2.67it/s]{'loss': 297.0454345703125, 'learning_rate': 1.5234375000000001e-05, 'epoch': 4.921875}\n","                                                   {'loss': 332.9388671875, 'learning_rate': 1.517578125e-05, 'epoch': 4.94140625}\n","                                                   \n","{'loss': 272.376123046875, 'learning_rate': 1.505859375e-05, 'epoch': 4.98046875}\n"," 50%|█████     | 2560/5120 [19:26<14:23,  2.96it/s]{'loss': 298.4180419921875, 'learning_rate': 1.5e-05, 'epoch': 5.0}\n"," 50%|█████     | 2569/5120 [19:29<16:03,  2.65it/s]{'loss': 299.9702880859375, 'learning_rate': 1.4941406250000001e-05, 'epoch': 5.01953125}\n","{'loss': 286.5856689453125, 'learning_rate': 1.48828125e-05, 'epoch': 5.0390625}\n","{'loss': 309.1790771484375, 'learning_rate': 1.482421875e-05, 'epoch': 5.05859375}\n","                                                   \n","{'loss': 292.40498046875, 'learning_rate': 1.470703125e-05, 'epoch': 5.09765625}\n"," 51%|█████     | 2620/5120 [19:49<16:16,  2.56it/s]\n","{'loss': 279.921484375, 'learning_rate': 1.458984375e-05, 'epoch': 5.13671875}\n","                                                   \n","{'loss': 304.1515869140625, 'learning_rate': 1.447265625e-05, 'epoch': 5.17578125}\n"," 52%|█████▏    | 2660/5120 [20:05<17:14,  2.38it/s]{'loss': 307.93427734375, 'learning_rate': 1.44140625e-05, 'epoch': 5.1953125}\n"," 52%|█████▏    | 2670/5120 [20:09<18:10,  2.25it/s]{'loss': 296.9010986328125, 'learning_rate': 1.435546875e-05, 'epoch': 5.21484375}\n","{'loss': 256.701611328125, 'learning_rate': 1.4296875e-05, 'epoch': 5.234375}\n","{'loss': 296.2774169921875, 'learning_rate': 1.4238281250000001e-05, 'epoch': 5.25390625}\n","{'loss': 261.0813720703125, 'learning_rate': 1.41796875e-05, 'epoch': 5.2734375}\n","                                                   \n"," 53%|█████▎    | 2720/5120 [20:28<15:36,  2.56it/s]{'loss': 339.7791015625, 'learning_rate': 1.40625e-05, 'epoch': 5.3125}\n","{'loss': 296.79873046875, 'learning_rate': 1.400390625e-05, 'epoch': 5.33203125}\n"," 54%|█████▎    | 2740/5120 [20:36<16:44,  2.37it/s]{'loss': 313.3123779296875, 'learning_rate': 1.3945312500000001e-05, 'epoch': 5.3515625}\n"," 54%|█████▎    | 2750/5120 [20:40<14:30,  2.72it/s]\n","{'loss': 330.19814453125, 'learning_rate': 1.3828125e-05, 'epoch': 5.390625}\n","                                                   \n"," 54%|█████▍    | 2780/5120 [20:52<15:36,  2.50it/s]{'loss': 271.348193359375, 'learning_rate': 1.37109375e-05, 'epoch': 5.4296875}\n","{'loss': 320.9727783203125, 'learning_rate': 1.3652343750000001e-05, 'epoch': 5.44921875}\n"," 55%|█████▍    | 2800/5120 [21:00<18:22,  2.10it/s]{'loss': 294.0260009765625, 'learning_rate': 1.359375e-05, 'epoch': 5.46875}\n","{'loss': 281.0314697265625, 'learning_rate': 1.353515625e-05, 'epoch': 5.48828125}\n"," 55%|█████▌    | 2820/5120 [21:08<13:12,  2.90it/s]{'loss': 285.5546875, 'learning_rate': 1.34765625e-05, 'epoch': 5.5078125}\n","{'loss': 355.0394287109375, 'learning_rate': 1.341796875e-05, 'epoch': 5.52734375}\n","{'loss': 267.450390625, 'learning_rate': 1.3359375000000001e-05, 'epoch': 5.546875}\n"," 56%|█████▌    | 2850/5120 [21:20<14:22,  2.63it/s]{'loss': 322.084619140625, 'learning_rate': 1.330078125e-05, 'epoch': 5.56640625}\n","{'loss': 277.7777099609375, 'learning_rate': 1.3242187500000001e-05, 'epoch': 5.5859375}\n","{'loss': 314.5709228515625, 'learning_rate': 1.318359375e-05, 'epoch': 5.60546875}\n"," 56%|█████▋    | 2880/5120 [21:31<13:39,  2.73it/s]\n"," 56%|█████▋    | 2889/5120 [21:35<15:33,  2.39it/s]{'loss': 303.0074462890625, 'learning_rate': 1.306640625e-05, 'epoch': 5.64453125}\n","{'loss': 326.9336181640625, 'learning_rate': 1.30078125e-05, 'epoch': 5.6640625}\n"," 57%|█████▋    | 2910/5120 [21:43<15:00,  2.46it/s]{'loss': 270.0774658203125, 'learning_rate': 1.2949218750000001e-05, 'epoch': 5.68359375}\n"," 57%|█████▋    | 2920/5120 [21:47<14:17,  2.57it/s]\n"," 57%|█████▋    | 2930/5120 [21:51<13:08,  2.78it/s]{'loss': 262.4771240234375, 'learning_rate': 1.283203125e-05, 'epoch': 5.72265625}\n"," 57%|█████▋    | 2940/5120 [21:55<13:59,  2.60it/s]{'loss': 293.1346435546875, 'learning_rate': 1.27734375e-05, 'epoch': 5.7421875}\n","{'loss': 288.2627197265625, 'learning_rate': 1.271484375e-05, 'epoch': 5.76171875}\n","{'loss': 326.654443359375, 'learning_rate': 1.2656250000000001e-05, 'epoch': 5.78125}\n","                                                   \n"," 58%|█████▊    | 2980/5120 [22:10<12:51,  2.77it/s]{'loss': 298.205615234375, 'learning_rate': 1.25390625e-05, 'epoch': 5.8203125}\n","                                                   \n"," 59%|█████▊    | 3000/5120 [22:17<15:49,  2.23it/s][INFO|trainer.py:1412] 2021-01-13 16:09:20,805 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-13 16:09:20,806 >>   Num examples = 266\n","[INFO|trainer.py:1414] 2021-01-13 16:09:20,809 >>   Batch size = 4\n","\n","\n","  0%|          | 0/67 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 2/67 [00:00<00:28,  2.28it/s]\u001b[A\n","  4%|▍         | 3/67 [00:02<00:47,  1.35it/s]\u001b[A\n","  6%|▌         | 4/67 [00:03<00:45,  1.38it/s]\u001b[A\n","  7%|▋         | 5/67 [00:04<00:57,  1.09it/s]\u001b[A\n","  9%|▉         | 6/67 [00:06<01:24,  1.39s/it]\u001b[A\n"," 10%|█         | 7/67 [00:08<01:19,  1.33s/it]\u001b[A\n"," 12%|█▏        | 8/67 [00:09<01:13,  1.25s/it]\u001b[A\n"," 13%|█▎        | 9/67 [00:10<01:10,  1.22s/it]\u001b[A\n"," 15%|█▍        | 10/67 [00:11<01:09,  1.22s/it]\u001b[A\n"," 16%|█▋        | 11/67 [00:12<01:05,  1.17s/it]\u001b[A\n"," 18%|█▊        | 12/67 [00:13<00:59,  1.08s/it]\u001b[A\n"," 19%|█▉        | 13/67 [00:14<01:02,  1.15s/it]\u001b[A\n"," 21%|██        | 14/67 [00:16<01:04,  1.22s/it]\u001b[A\n"," 22%|██▏       | 15/67 [00:17<01:06,  1.28s/it]\u001b[A\n"," 24%|██▍       | 16/67 [00:18<00:58,  1.14s/it]\u001b[A\n"," 25%|██▌       | 17/67 [00:19<00:49,  1.00it/s]\u001b[A\n"," 27%|██▋       | 18/67 [00:19<00:44,  1.11it/s]\u001b[A\n"," 28%|██▊       | 19/67 [00:20<00:44,  1.08it/s]\u001b[A\n"," 30%|██▉       | 20/67 [00:21<00:43,  1.08it/s]\u001b[A\n"," 31%|███▏      | 21/67 [00:22<00:41,  1.12it/s]\u001b[A\n"," 33%|███▎      | 22/67 [00:23<00:39,  1.15it/s]\u001b[A\n"," 34%|███▍      | 23/67 [00:24<00:38,  1.15it/s]\u001b[A\n"," 36%|███▌      | 24/67 [00:24<00:36,  1.19it/s]\u001b[A\n"," 37%|███▋      | 25/67 [00:25<00:34,  1.22it/s]\u001b[A\n"," 39%|███▉      | 26/67 [00:26<00:39,  1.03it/s]\u001b[A\n"," 40%|████      | 27/67 [00:27<00:34,  1.15it/s]\u001b[A\n"," 42%|████▏     | 28/67 [00:28<00:34,  1.14it/s]\u001b[A\n"," 43%|████▎     | 29/67 [00:30<00:43,  1.14s/it]\u001b[A\n"," 45%|████▍     | 30/67 [00:30<00:36,  1.01it/s]\u001b[A\n"," 46%|████▋     | 31/67 [00:31<00:35,  1.01it/s]\u001b[A\n"," 48%|████▊     | 32/67 [00:32<00:34,  1.02it/s]\u001b[A\n"," 49%|████▉     | 33/67 [00:33<00:32,  1.05it/s]\u001b[A\n"," 51%|█████     | 34/67 [00:34<00:31,  1.04it/s]\u001b[A\n"," 52%|█████▏    | 35/67 [00:35<00:29,  1.07it/s]\u001b[A\n"," 54%|█████▎    | 36/67 [00:36<00:28,  1.09it/s]\u001b[A\n"," 55%|█████▌    | 37/67 [00:37<00:32,  1.08s/it]\u001b[A\n"," 57%|█████▋    | 38/67 [00:38<00:28,  1.01it/s]\u001b[A\n"," 58%|█████▊    | 39/67 [00:39<00:25,  1.12it/s]\u001b[A\n"," 60%|█████▉    | 40/67 [00:41<00:30,  1.13s/it]\u001b[A\n"," 61%|██████    | 41/67 [00:41<00:27,  1.05s/it]\u001b[A\n"," 63%|██████▎   | 42/67 [00:42<00:22,  1.12it/s]\u001b[A\n"," 64%|██████▍   | 43/67 [00:44<00:27,  1.16s/it]\u001b[A\n"," 66%|██████▌   | 44/67 [00:44<00:24,  1.05s/it]\u001b[A\n"," 67%|██████▋   | 45/67 [00:46<00:25,  1.14s/it]\u001b[A\n"," 69%|██████▊   | 46/67 [00:47<00:23,  1.11s/it]\u001b[A\n"," 70%|███████   | 47/67 [00:48<00:21,  1.08s/it]\u001b[A\n"," 72%|███████▏  | 48/67 [00:49<00:19,  1.01s/it]\u001b[A\n"," 73%|███████▎  | 49/67 [00:50<00:22,  1.23s/it]\u001b[A\n"," 75%|███████▍  | 50/67 [00:52<00:21,  1.28s/it]\u001b[A\n"," 76%|███████▌  | 51/67 [00:53<00:19,  1.25s/it]\u001b[A\n"," 78%|███████▊  | 52/67 [00:54<00:16,  1.10s/it]\u001b[A\n"," 79%|███████▉  | 53/67 [00:55<00:14,  1.05s/it]\u001b[A\n"," 81%|████████  | 54/67 [00:56<00:12,  1.01it/s]\u001b[A\n"," 82%|████████▏ | 55/67 [00:56<00:11,  1.06it/s]\u001b[A\n"," 84%|████████▎ | 56/67 [00:58<00:11,  1.05s/it]\u001b[A\n"," 85%|████████▌ | 57/67 [00:59<00:11,  1.13s/it]\u001b[A\n"," 87%|████████▋ | 58/67 [01:00<00:10,  1.13s/it]\u001b[A\n"," 88%|████████▊ | 59/67 [01:01<00:09,  1.13s/it]\u001b[A\n"," 90%|████████▉ | 60/67 [01:02<00:06,  1.04it/s]\u001b[A\n"," 91%|█████████ | 61/67 [01:03<00:05,  1.11it/s]\u001b[A\n"," 93%|█████████▎| 62/67 [01:04<00:04,  1.02it/s]\u001b[A\n"," 94%|█████████▍| 63/67 [01:05<00:03,  1.05it/s]\u001b[A\n"," 96%|█████████▌| 64/67 [01:05<00:02,  1.15it/s]\u001b[A\n"," 97%|█████████▋| 65/67 [01:07<00:02,  1.14s/it]\u001b[A\n"," 99%|█████████▊| 66/67 [01:08<00:01,  1.06s/it]\u001b[A\n","                                                   \n","{'eval_loss': 589.0091552734375, 'eval_rouge1': 32.1612, 'eval_rouge2': 13.877, 'eval_rougeL': 25.9094, 'eval_rougeLsum': 26.4614, 'eval_gen_len': 38.5, 'epoch': 5.859375}\n"," 59%|█████▊    | 3000/5120 [23:30<15:49,  2.23it/s]\n","100%|██████████| 67/67 [01:09<00:00,  1.14it/s]\u001b[A\n","                                               \u001b[A[INFO|trainer.py:1226] 2021-01-13 16:10:33,017 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-3000\n","[INFO|configuration_utils.py:289] 2021-01-13 16:10:33,024 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:814] 2021-01-13 16:10:39,975 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-3000/pytorch_model.bin\n"," 59%|█████▉    | 3010/5120 [23:40<46:06,  1.31s/it]{'loss': 277.22509765625, 'learning_rate': 1.2363281250000001e-05, 'epoch': 5.87890625}\n"," 59%|█████▉    | 3020/5120 [23:44<13:47,  2.54it/s]{'loss': 298.820361328125, 'learning_rate': 1.23046875e-05, 'epoch': 5.8984375}\n"," 59%|█████▉    | 3030/5120 [23:48<15:43,  2.22it/s]{'loss': 297.18701171875, 'learning_rate': 1.224609375e-05, 'epoch': 5.91796875}\n"," 59%|█████▉    | 3040/5120 [23:52<14:31,  2.39it/s]{'loss': 323.9154052734375, 'learning_rate': 1.21875e-05, 'epoch': 5.9375}\n"," 60%|█████▉    | 3050/5120 [23:57<14:42,  2.35it/s]{'loss': 262.9142822265625, 'learning_rate': 1.212890625e-05, 'epoch': 5.95703125}\n","                                                   \n","{'loss': 270.157275390625, 'learning_rate': 1.201171875e-05, 'epoch': 5.99609375}\n"," 60%|██████    | 3080/5120 [24:09<13:36,  2.50it/s]{'loss': 276.3843994140625, 'learning_rate': 1.1953125000000001e-05, 'epoch': 6.015625}\n"," 60%|██████    | 3090/5120 [24:13<14:21,  2.36it/s]{'loss': 299.0829833984375, 'learning_rate': 1.189453125e-05, 'epoch': 6.03515625}\n"," 61%|██████    | 3100/5120 [24:17<15:35,  2.16it/s]{'loss': 266.4299072265625, 'learning_rate': 1.18359375e-05, 'epoch': 6.0546875}\n","{'loss': 235.7068359375, 'learning_rate': 1.177734375e-05, 'epoch': 6.07421875}\n"," 61%|██████    | 3120/5120 [24:25<13:08,  2.54it/s]{'loss': 234.8366943359375, 'learning_rate': 1.171875e-05, 'epoch': 6.09375}\n"," 61%|██████    | 3130/5120 [24:29<13:32,  2.45it/s]{'loss': 288.545751953125, 'learning_rate': 1.1660156250000001e-05, 'epoch': 6.11328125}\n","                                                   \n","{'loss': 295.833203125, 'learning_rate': 1.154296875e-05, 'epoch': 6.15234375}\n"," 62%|██████▏   | 3160/5120 [24:41<12:31,  2.61it/s]\n","{'loss': 261.589697265625, 'learning_rate': 1.142578125e-05, 'epoch': 6.19140625}\n","                                                   {'loss': 268.8537841796875, 'learning_rate': 1.1367187500000001e-05, 'epoch': 6.2109375}\n"," 62%|██████▏   | 3190/5120 [24:52<12:55,  2.49it/s]\n","                                                   \n"," 63%|██████▎   | 3210/5120 [25:00<10:57,  2.91it/s]{'loss': 245.4281982421875, 'learning_rate': 1.119140625e-05, 'epoch': 6.26953125}\n"," 63%|██████▎   | 3220/5120 [25:03<11:25,  2.77it/s]{'loss': 257.0566650390625, 'learning_rate': 1.11328125e-05, 'epoch': 6.2890625}\n"," 63%|██████▎   | 3230/5120 [25:07<11:47,  2.67it/s]{'loss': 294.456005859375, 'learning_rate': 1.1074218750000001e-05, 'epoch': 6.30859375}\n","{'loss': 249.8479736328125, 'learning_rate': 1.1015625e-05, 'epoch': 6.328125}\n"," 63%|██████▎   | 3250/5120 [25:14<11:08,  2.80it/s]{'loss': 262.554541015625, 'learning_rate': 1.095703125e-05, 'epoch': 6.34765625}\n"," 64%|██████▎   | 3260/5120 [25:18<12:26,  2.49it/s]{'loss': 343.2723876953125, 'learning_rate': 1.08984375e-05, 'epoch': 6.3671875}\n"," 64%|██████▍   | 3270/5120 [25:22<11:58,  2.57it/s]{'loss': 300.8560546875, 'learning_rate': 1.083984375e-05, 'epoch': 6.38671875}\n","                                                   \n","{'loss': 239.431494140625, 'learning_rate': 1.072265625e-05, 'epoch': 6.42578125}\n","                                                   \n","{'loss': 283.2430908203125, 'learning_rate': 1.060546875e-05, 'epoch': 6.46484375}\n","{'loss': 257.2052490234375, 'learning_rate': 1.0546875e-05, 'epoch': 6.484375}\n","{'loss': 276.4732666015625, 'learning_rate': 1.048828125e-05, 'epoch': 6.50390625}\n","{'loss': 286.3289306640625, 'learning_rate': 1.04296875e-05, 'epoch': 6.5234375}\n","{'loss': 304.4281005859375, 'learning_rate': 1.0371093750000001e-05, 'epoch': 6.54296875}\n","                                                   {'loss': 227.2767822265625, 'learning_rate': 1.03125e-05, 'epoch': 6.5625}\n"," 66%|██████▌   | 3370/5120 [26:00<12:11,  2.39it/s]\n"," 66%|██████▌   | 3380/5120 [26:04<11:30,  2.52it/s]{'loss': 345.4634765625, 'learning_rate': 1.01953125e-05, 'epoch': 6.6015625}\n"," 66%|██████▌   | 3390/5120 [26:08<09:32,  3.02it/s]\n"," 66%|██████▋   | 3400/5120 [26:12<15:18,  1.87it/s]\n"," 67%|██████▋   | 3410/5120 [26:16<12:48,  2.22it/s]{'loss': 261.3174560546875, 'learning_rate': 1.001953125e-05, 'epoch': 6.66015625}\n"," 67%|██████▋   | 3420/5120 [26:20<11:17,  2.51it/s]{'loss': 273.527978515625, 'learning_rate': 9.9609375e-06, 'epoch': 6.6796875}\n","                                                   \n","{'loss': 267.0091796875, 'learning_rate': 9.84375e-06, 'epoch': 6.71875}\n"," 67%|██████▋   | 3450/5120 [26:32<10:30,  2.65it/s]\n"," 68%|██████▊   | 3460/5120 [26:36<11:36,  2.38it/s]{'loss': 275.335302734375, 'learning_rate': 9.7265625e-06, 'epoch': 6.7578125}\n","                                                   {'loss': 268.948486328125, 'learning_rate': 9.66796875e-06, 'epoch': 6.77734375}\n","{'loss': 261.73564453125, 'learning_rate': 9.609375e-06, 'epoch': 6.796875}\n","                                                   \n"," 68%|██████▊   | 3500/5120 [26:51<12:31,  2.16it/s]\n","{'loss': 270.2991455078125, 'learning_rate': 9.43359375e-06, 'epoch': 6.85546875}\n","{'loss': 248.53896484375, 'learning_rate': 9.375000000000001e-06, 'epoch': 6.875}\n","{'loss': 282.072314453125, 'learning_rate': 9.31640625e-06, 'epoch': 6.89453125}\n"," 69%|██████▉   | 3540/5120 [27:06<09:59,  2.63it/s]{'loss': 270.7455078125, 'learning_rate': 9.2578125e-06, 'epoch': 6.9140625}\n","                                                   \n"," 70%|██████▉   | 3560/5120 [27:13<09:00,  2.89it/s]\n","                                                   {'loss': 301.6590087890625, 'learning_rate': 9.082031250000001e-06, 'epoch': 6.97265625}\n"," 70%|██████▉   | 3580/5120 [27:21<09:23,  2.73it/s]{'loss': 295.332861328125, 'learning_rate': 9.0234375e-06, 'epoch': 6.9921875}\n"," 70%|███████   | 3590/5120 [27:24<08:27,  3.01it/s]\n"," 70%|███████   | 3600/5120 [27:28<12:54,  1.96it/s]\n"," 71%|███████   | 3610/5120 [27:32<09:43,  2.59it/s]{'loss': 300.90224609375, 'learning_rate': 8.84765625e-06, 'epoch': 7.05078125}\n"," 71%|███████   | 3620/5120 [27:37<15:28,  1.62it/s]\n"," 71%|███████   | 3630/5120 [27:41<09:40,  2.57it/s]{'loss': 311.4925048828125, 'learning_rate': 8.73046875e-06, 'epoch': 7.08984375}\n"," 71%|███████   | 3640/5120 [27:45<08:35,  2.87it/s]{'loss': 273.951025390625, 'learning_rate': 8.671875e-06, 'epoch': 7.109375}\n","{'loss': 264.994970703125, 'learning_rate': 8.61328125e-06, 'epoch': 7.12890625}\n","                                                   {'loss': 251.948779296875, 'learning_rate': 8.5546875e-06, 'epoch': 7.1484375}\n"," 72%|███████▏  | 3670/5120 [27:56<08:44,  2.77it/s]{'loss': 255.768408203125, 'learning_rate': 8.49609375e-06, 'epoch': 7.16796875}\n","{'loss': 263.7571533203125, 'learning_rate': 8.4375e-06, 'epoch': 7.1875}\n"," 72%|███████▏  | 3690/5120 [28:04<08:31,  2.80it/s]\n","{'loss': 264.7352294921875, 'learning_rate': 8.3203125e-06, 'epoch': 7.2265625}\n"," 72%|███████▏  | 3710/5120 [28:12<08:25,  2.79it/s]{'loss': 257.552490234375, 'learning_rate': 8.26171875e-06, 'epoch': 7.24609375}\n","{'loss': 240.7489990234375, 'learning_rate': 8.203125e-06, 'epoch': 7.265625}\n","{'loss': 218.1004150390625, 'learning_rate': 8.14453125e-06, 'epoch': 7.28515625}\n","{'loss': 256.070556640625, 'learning_rate': 8.085937500000001e-06, 'epoch': 7.3046875}\n","                                                   \n","                                                   \n","                                                   \n","                                                   {'loss': 251.8556884765625, 'learning_rate': 7.8515625e-06, 'epoch': 7.3828125}\n"," 74%|███████▍  | 3790/5120 [28:41<09:28,  2.34it/s]{'loss': 249.6400634765625, 'learning_rate': 7.792968750000001e-06, 'epoch': 7.40234375}\n","{'loss': 255.1410400390625, 'learning_rate': 7.734375e-06, 'epoch': 7.421875}\n"," 74%|███████▍  | 3810/5120 [28:49<07:43,  2.82it/s]{'loss': 244.4318359375, 'learning_rate': 7.67578125e-06, 'epoch': 7.44140625}\n"," 75%|███████▍  | 3820/5120 [28:53<08:09,  2.66it/s]{'loss': 283.863330078125, 'learning_rate': 7.6171875000000005e-06, 'epoch': 7.4609375}\n","{'loss': 258.471728515625, 'learning_rate': 7.55859375e-06, 'epoch': 7.48046875}\n","{'loss': 250.3172607421875, 'learning_rate': 7.5e-06, 'epoch': 7.5}\n"," 75%|███████▌  | 3850/5120 [29:04<08:40,  2.44it/s]\n","                                                   {'loss': 257.63798828125, 'learning_rate': 7.3828125e-06, 'epoch': 7.5390625}\n","{'loss': 281.3369140625, 'learning_rate': 7.3242187500000006e-06, 'epoch': 7.55859375}\n"," 76%|███████▌  | 3880/5120 [29:16<07:39,  2.70it/s]{'loss': 294.3148681640625, 'learning_rate': 7.265625e-06, 'epoch': 7.578125}\n"," 76%|███████▌  | 3890/5120 [29:19<07:39,  2.68it/s]{'loss': 220.8043701171875, 'learning_rate': 7.20703125e-06, 'epoch': 7.59765625}\n","{'loss': 273.3595458984375, 'learning_rate': 7.1484375e-06, 'epoch': 7.6171875}\n"," 76%|███████▋  | 3910/5120 [29:27<07:36,  2.65it/s]{'loss': 243.6197021484375, 'learning_rate': 7.08984375e-06, 'epoch': 7.63671875}\n","{'loss': 275.356591796875, 'learning_rate': 7.03125e-06, 'epoch': 7.65625}\n","                                                   \n","{'loss': 280.196044921875, 'learning_rate': 6.9140625e-06, 'epoch': 7.6953125}\n","{'loss': 248.9279296875, 'learning_rate': 6.85546875e-06, 'epoch': 7.71484375}\n","                                                   \n","                                                   \n"," 78%|███████▊  | 3980/5120 [29:54<06:58,  2.73it/s]\n","                                                   \n"," 78%|███████▊  | 4000/5120 [30:02<09:01,  2.07it/s][INFO|trainer.py:1412] 2021-01-13 16:17:05,144 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-13 16:17:05,144 >>   Num examples = 266\n","[INFO|trainer.py:1414] 2021-01-13 16:17:05,144 >>   Batch size = 4\n","\n","\n","  0%|          | 0/67 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 2/67 [00:00<00:28,  2.31it/s]\u001b[A\n","  4%|▍         | 3/67 [00:02<00:46,  1.39it/s]\u001b[A\n","  6%|▌         | 4/67 [00:03<00:47,  1.33it/s]\u001b[A\n","  7%|▋         | 5/67 [00:05<01:10,  1.14s/it]\u001b[A\n","  9%|▉         | 6/67 [00:07<01:33,  1.53s/it]\u001b[A\n"," 10%|█         | 7/67 [00:08<01:22,  1.37s/it]\u001b[A\n"," 12%|█▏        | 8/67 [00:09<01:11,  1.22s/it]\u001b[A\n"," 13%|█▎        | 9/67 [00:10<01:06,  1.15s/it]\u001b[A\n"," 15%|█▍        | 10/67 [00:11<01:09,  1.22s/it]\u001b[A\n"," 16%|█▋        | 11/67 [00:12<01:05,  1.17s/it]\u001b[A\n"," 18%|█▊        | 12/67 [00:13<00:59,  1.08s/it]\u001b[A\n"," 19%|█▉        | 13/67 [00:15<01:06,  1.23s/it]\u001b[A\n"," 21%|██        | 14/67 [00:16<01:04,  1.21s/it]\u001b[A\n"," 22%|██▏       | 15/67 [00:17<01:05,  1.25s/it]\u001b[A\n"," 24%|██▍       | 16/67 [00:19<01:04,  1.26s/it]\u001b[A\n"," 25%|██▌       | 17/67 [00:20<01:00,  1.21s/it]\u001b[A\n"," 27%|██▋       | 18/67 [00:20<00:52,  1.08s/it]\u001b[A\n"," 28%|██▊       | 19/67 [00:21<00:50,  1.05s/it]\u001b[A\n"," 30%|██▉       | 20/67 [00:22<00:45,  1.03it/s]\u001b[A\n"," 31%|███▏      | 21/67 [00:23<00:45,  1.02it/s]\u001b[A\n"," 33%|███▎      | 22/67 [00:25<00:48,  1.08s/it]\u001b[A\n"," 34%|███▍      | 23/67 [00:25<00:42,  1.04it/s]\u001b[A\n"," 36%|███▌      | 24/67 [00:26<00:36,  1.18it/s]\u001b[A\n"," 37%|███▋      | 25/67 [00:27<00:37,  1.12it/s]\u001b[A\n"," 39%|███▉      | 26/67 [00:28<00:44,  1.08s/it]\u001b[A\n"," 40%|████      | 27/67 [00:29<00:40,  1.02s/it]\u001b[A\n"," 42%|████▏     | 28/67 [00:30<00:39,  1.00s/it]\u001b[A\n"," 43%|████▎     | 29/67 [00:31<00:40,  1.06s/it]\u001b[A\n"," 45%|████▍     | 30/67 [00:32<00:36,  1.02it/s]\u001b[A\n"," 46%|████▋     | 31/67 [00:33<00:35,  1.02it/s]\u001b[A\n"," 48%|████▊     | 32/67 [00:34<00:34,  1.03it/s]\u001b[A\n"," 49%|████▉     | 33/67 [00:36<00:37,  1.11s/it]\u001b[A\n"," 51%|█████     | 34/67 [00:37<00:38,  1.17s/it]\u001b[A\n"," 52%|█████▏    | 35/67 [00:39<00:43,  1.35s/it]\u001b[A\n"," 54%|█████▎    | 36/67 [00:39<00:37,  1.21s/it]\u001b[A\n"," 55%|█████▌    | 37/67 [00:41<00:38,  1.28s/it]\u001b[A\n"," 57%|█████▋    | 38/67 [00:42<00:37,  1.28s/it]\u001b[A\n"," 58%|█████▊    | 39/67 [00:43<00:31,  1.12s/it]\u001b[A\n"," 60%|█████▉    | 40/67 [00:44<00:29,  1.11s/it]\u001b[A\n"," 61%|██████    | 41/67 [00:45<00:27,  1.04s/it]\u001b[A\n"," 63%|██████▎   | 42/67 [00:46<00:22,  1.09it/s]\u001b[A\n"," 64%|██████▍   | 43/67 [00:47<00:23,  1.01it/s]\u001b[A\n"," 66%|██████▌   | 44/67 [00:47<00:19,  1.15it/s]\u001b[A\n"," 67%|██████▋   | 45/67 [00:49<00:24,  1.11s/it]\u001b[A\n"," 69%|██████▊   | 46/67 [00:50<00:22,  1.05s/it]\u001b[A\n"," 70%|███████   | 47/67 [00:51<00:20,  1.02s/it]\u001b[A\n"," 72%|███████▏  | 48/67 [00:52<00:20,  1.05s/it]\u001b[A\n"," 73%|███████▎  | 49/67 [00:54<00:24,  1.34s/it]\u001b[A\n"," 75%|███████▍  | 50/67 [00:55<00:23,  1.37s/it]\u001b[A\n"," 76%|███████▌  | 51/67 [00:57<00:21,  1.33s/it]\u001b[A\n"," 78%|███████▊  | 52/67 [00:57<00:17,  1.16s/it]\u001b[A\n"," 79%|███████▉  | 53/67 [00:59<00:16,  1.18s/it]\u001b[A\n"," 81%|████████  | 54/67 [00:59<00:13,  1.04s/it]\u001b[A\n"," 82%|████████▏ | 55/67 [01:00<00:11,  1.03it/s]\u001b[A\n"," 84%|████████▎ | 56/67 [01:02<00:11,  1.08s/it]\u001b[A\n"," 85%|████████▌ | 57/67 [01:03<00:10,  1.06s/it]\u001b[A\n"," 87%|████████▋ | 58/67 [01:04<00:09,  1.09s/it]\u001b[A\n"," 88%|████████▊ | 59/67 [01:05<00:08,  1.11s/it]\u001b[A\n"," 90%|████████▉ | 60/67 [01:05<00:06,  1.04it/s]\u001b[A\n"," 91%|█████████ | 61/67 [01:06<00:05,  1.09it/s]\u001b[A\n"," 93%|█████████▎| 62/67 [01:07<00:04,  1.03it/s]\u001b[A\n"," 94%|█████████▍| 63/67 [01:08<00:03,  1.05it/s]\u001b[A\n"," 96%|█████████▌| 64/67 [01:09<00:02,  1.07it/s]\u001b[A\n"," 97%|█████████▋| 65/67 [01:12<00:02,  1.39s/it]\u001b[A\n"," 99%|█████████▊| 66/67 [01:12<00:01,  1.20s/it]\u001b[A\n","                                                   \n","\u001b[A{'eval_loss': 603.61376953125, 'eval_rouge1': 32.8823, 'eval_rouge2': 14.3036, 'eval_rougeL': 26.2614, 'eval_rougeLsum': 27.0731, 'eval_gen_len': 41.1, 'epoch': 7.8125}\n"," 78%|███████▊  | 4000/5120 [31:18<09:01,  2.07it/s]\n","100%|██████████| 67/67 [01:14<00:00,  1.05s/it]\u001b[A\n","                                               \u001b[A[INFO|trainer.py:1226] 2021-01-13 16:18:21,823 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-4000\n","[INFO|configuration_utils.py:289] 2021-01-13 16:18:21,830 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:814] 2021-01-13 16:18:28,821 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-4000/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-13 16:18:28,945 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-4000] due to args.save_total_limit\n"," 78%|███████▊  | 4010/5120 [31:29<25:34,  1.38s/it]{'loss': 254.439501953125, 'learning_rate': 6.50390625e-06, 'epoch': 7.83203125}\n"," 79%|███████▊  | 4020/5120 [31:33<08:11,  2.24it/s]{'loss': 249.0356689453125, 'learning_rate': 6.4453125e-06, 'epoch': 7.8515625}\n"," 79%|███████▊  | 4030/5120 [31:37<07:49,  2.32it/s]\n","{'loss': 244.483447265625, 'learning_rate': 6.3281250000000005e-06, 'epoch': 7.890625}\n","{'loss': 263.8673583984375, 'learning_rate': 6.26953125e-06, 'epoch': 7.91015625}\n"," 79%|███████▉  | 4060/5120 [31:50<06:58,  2.53it/s]{'loss': 314.275634765625, 'learning_rate': 6.2109375e-06, 'epoch': 7.9296875}\n","{'loss': 268.9635986328125, 'learning_rate': 6.15234375e-06, 'epoch': 7.94921875}\n"," 80%|███████▉  | 4080/5120 [31:58<06:24,  2.71it/s]\n","                                                   \n"," 80%|████████  | 4100/5120 [32:06<08:03,  2.11it/s]{'loss': 235.37392578125, 'learning_rate': 5.9765625000000004e-06, 'epoch': 8.0078125}\n"," 80%|████████  | 4110/5120 [32:10<06:46,  2.48it/s]{'loss': 261.684423828125, 'learning_rate': 5.91796875e-06, 'epoch': 8.02734375}\n","                                                   {'loss': 249.8090087890625, 'learning_rate': 5.859375e-06, 'epoch': 8.046875}\n"," 81%|████████  | 4130/5120 [32:18<06:25,  2.57it/s]{'loss': 266.3074462890625, 'learning_rate': 5.80078125e-06, 'epoch': 8.06640625}\n","{'loss': 288.5031982421875, 'learning_rate': 5.7421875e-06, 'epoch': 8.0859375}\n"," 81%|████████  | 4150/5120 [32:26<06:52,  2.35it/s]{'loss': 286.51181640625, 'learning_rate': 5.6835937500000005e-06, 'epoch': 8.10546875}\n","                                                   \n","{'loss': 304.0958984375, 'learning_rate': 5.56640625e-06, 'epoch': 8.14453125}\n"," 82%|████████▏ | 4180/5120 [32:37<05:09,  3.03it/s]{'loss': 248.00634765625, 'learning_rate': 5.5078125e-06, 'epoch': 8.1640625}\n"," 82%|████████▏ | 4190/5120 [32:41<05:16,  2.93it/s]{'loss': 232.946630859375, 'learning_rate': 5.44921875e-06, 'epoch': 8.18359375}\n","                                                   {'loss': 279.51904296875, 'learning_rate': 5.390625e-06, 'epoch': 8.203125}\n"," 82%|████████▏ | 4210/5120 [32:49<06:23,  2.37it/s]{'loss': 244.1521484375, 'learning_rate': 5.3320312500000004e-06, 'epoch': 8.22265625}\n"," 82%|████████▏ | 4220/5120 [32:53<04:52,  3.08it/s]{'loss': 227.0260986328125, 'learning_rate': 5.2734375e-06, 'epoch': 8.2421875}\n","                                                   {'loss': 232.2361572265625, 'learning_rate': 5.21484375e-06, 'epoch': 8.26171875}\n","{'loss': 295.70732421875, 'learning_rate': 5.15625e-06, 'epoch': 8.28125}\n","{'loss': 243.9069091796875, 'learning_rate': 5.09765625e-06, 'epoch': 8.30078125}\n"," 83%|████████▎ | 4260/5120 [33:08<05:23,  2.66it/s]{'loss': 257.3701416015625, 'learning_rate': 5.0390625000000005e-06, 'epoch': 8.3203125}\n"," 83%|████████▎ | 4270/5120 [33:12<05:40,  2.50it/s]{'loss': 262.2930908203125, 'learning_rate': 4.98046875e-06, 'epoch': 8.33984375}\n"," 84%|████████▎ | 4280/5120 [33:16<05:46,  2.43it/s]{'loss': 239.87890625, 'learning_rate': 4.921875e-06, 'epoch': 8.359375}\n"," 84%|████████▍ | 4290/5120 [33:19<04:38,  2.98it/s]{'loss': 253.5322021484375, 'learning_rate': 4.86328125e-06, 'epoch': 8.37890625}\n"," 84%|████████▍ | 4300/5120 [33:23<06:28,  2.11it/s]{'loss': 277.65322265625, 'learning_rate': 4.8046875e-06, 'epoch': 8.3984375}\n","                                                   \n"," 84%|████████▍ | 4320/5120 [33:31<04:54,  2.71it/s]{'loss': 261.628271484375, 'learning_rate': 4.6875000000000004e-06, 'epoch': 8.4375}\n"," 85%|████████▍ | 4330/5120 [33:34<04:21,  3.02it/s]{'loss': 239.9655029296875, 'learning_rate': 4.62890625e-06, 'epoch': 8.45703125}\n"," 85%|████████▍ | 4340/5120 [33:38<05:18,  2.45it/s]{'loss': 262.3738525390625, 'learning_rate': 4.5703125e-06, 'epoch': 8.4765625}\n"," 85%|████████▍ | 4350/5120 [33:42<05:25,  2.37it/s]{'loss': 261.626708984375, 'learning_rate': 4.51171875e-06, 'epoch': 8.49609375}\n","{'loss': 230.9654052734375, 'learning_rate': 4.453125e-06, 'epoch': 8.515625}\n","                                                   \n","                                                   \n"," 86%|████████▌ | 4390/5120 [33:58<05:37,  2.16it/s]{'loss': 311.483642578125, 'learning_rate': 4.27734375e-06, 'epoch': 8.57421875}\n"," 86%|████████▌ | 4400/5120 [34:02<05:49,  2.06it/s]\n"," 86%|████████▌ | 4410/5120 [34:06<04:49,  2.45it/s]{'loss': 269.147607421875, 'learning_rate': 4.16015625e-06, 'epoch': 8.61328125}\n"," 86%|████████▋ | 4420/5120 [34:10<03:59,  2.92it/s]{'loss': 267.850634765625, 'learning_rate': 4.1015625e-06, 'epoch': 8.6328125}\n","                                                   \n"," 87%|████████▋ | 4440/5120 [34:17<03:55,  2.89it/s]{'loss': 233.3083984375, 'learning_rate': 3.984375e-06, 'epoch': 8.671875}\n"," 87%|████████▋ | 4450/5120 [34:21<03:50,  2.91it/s]{'loss': 239.8341796875, 'learning_rate': 3.92578125e-06, 'epoch': 8.69140625}\n","{'loss': 244.930517578125, 'learning_rate': 3.8671875e-06, 'epoch': 8.7109375}\n"," 87%|████████▋ | 4470/5120 [34:28<04:14,  2.55it/s]{'loss': 261.104736328125, 'learning_rate': 3.8085937500000002e-06, 'epoch': 8.73046875}\n","{'loss': 272.527099609375, 'learning_rate': 3.75e-06, 'epoch': 8.75}\n","                                                   {'loss': 271.4335205078125, 'learning_rate': 3.69140625e-06, 'epoch': 8.76953125}\n"," 88%|████████▊ | 4500/5120 [34:40<05:04,  2.03it/s]\n"," 88%|████████▊ | 4510/5120 [34:44<04:03,  2.51it/s]\n"," 88%|████████▊ | 4520/5120 [34:47<03:18,  3.03it/s]\n"," 88%|████████▊ | 4530/5120 [34:51<03:48,  2.59it/s]{'loss': 238.50517578125, 'learning_rate': 3.45703125e-06, 'epoch': 8.84765625}\n"," 89%|████████▊ | 4540/5120 [34:54<03:28,  2.78it/s]{'loss': 246.83037109375, 'learning_rate': 3.3984375e-06, 'epoch': 8.8671875}\n","{'loss': 208.3982177734375, 'learning_rate': 3.3398437500000003e-06, 'epoch': 8.88671875}\n","{'loss': 250.239501953125, 'learning_rate': 3.28125e-06, 'epoch': 8.90625}\n","                                                   \n","                                                   {'loss': 281.008447265625, 'learning_rate': 3.1640625000000003e-06, 'epoch': 8.9453125}\n"," 90%|████████▉ | 4590/5120 [35:13<03:22,  2.61it/s]{'loss': 232.7110107421875, 'learning_rate': 3.10546875e-06, 'epoch': 8.96484375}\n"," 90%|████████▉ | 4600/5120 [35:17<04:25,  1.96it/s]{'loss': 245.8653076171875, 'learning_rate': 3.046875e-06, 'epoch': 8.984375}\n","                                                   \n","{'loss': 225.388916015625, 'learning_rate': 2.9296875e-06, 'epoch': 9.0234375}\n"," 90%|█████████ | 4630/5120 [35:29<03:08,  2.59it/s]\n","{'loss': 250.4541259765625, 'learning_rate': 2.8125e-06, 'epoch': 9.0625}\n"," 91%|█████████ | 4650/5120 [35:36<02:49,  2.78it/s]{'loss': 246.7146728515625, 'learning_rate': 2.75390625e-06, 'epoch': 9.08203125}\n","{'loss': 277.820458984375, 'learning_rate': 2.6953125e-06, 'epoch': 9.1015625}\n"," 91%|█████████ | 4670/5120 [35:43<02:56,  2.55it/s]{'loss': 234.55263671875, 'learning_rate': 2.63671875e-06, 'epoch': 9.12109375}\n"," 91%|█████████▏| 4680/5120 [35:47<02:42,  2.71it/s]{'loss': 246.97978515625, 'learning_rate': 2.578125e-06, 'epoch': 9.140625}\n"," 92%|█████████▏| 4690/5120 [35:51<02:36,  2.75it/s]{'loss': 232.3405517578125, 'learning_rate': 2.5195312500000003e-06, 'epoch': 9.16015625}\n","{'loss': 235.5179443359375, 'learning_rate': 2.4609375e-06, 'epoch': 9.1796875}\n","                                                   \n"," 92%|█████████▏| 4720/5120 [36:02<02:39,  2.51it/s]{'loss': 290.0740234375, 'learning_rate': 2.3437500000000002e-06, 'epoch': 9.21875}\n"," 92%|█████████▏| 4730/5120 [36:06<02:23,  2.71it/s]{'loss': 266.69951171875, 'learning_rate': 2.28515625e-06, 'epoch': 9.23828125}\n"," 93%|█████████▎| 4740/5120 [36:10<02:12,  2.86it/s]{'loss': 243.182958984375, 'learning_rate': 2.2265625e-06, 'epoch': 9.2578125}\n"," 93%|█████████▎| 4750/5120 [36:14<02:26,  2.52it/s]{'loss': 231.802685546875, 'learning_rate': 2.16796875e-06, 'epoch': 9.27734375}\n","{'loss': 213.414404296875, 'learning_rate': 2.109375e-06, 'epoch': 9.296875}\n"," 93%|█████████▎| 4770/5120 [36:21<02:01,  2.87it/s]\n","                                                   \n","{'loss': 238.402880859375, 'learning_rate': 1.93359375e-06, 'epoch': 9.35546875}\n","{'loss': 255.386572265625, 'learning_rate': 1.875e-06, 'epoch': 9.375}\n"," 94%|█████████▍| 4810/5120 [36:36<01:42,  3.04it/s]{'loss': 209.396435546875, 'learning_rate': 1.81640625e-06, 'epoch': 9.39453125}\n"," 94%|█████████▍| 4820/5120 [36:41<01:58,  2.53it/s]{'loss': 306.7337158203125, 'learning_rate': 1.7578125e-06, 'epoch': 9.4140625}\n"," 94%|█████████▍| 4830/5120 [36:44<01:45,  2.74it/s]{'loss': 228.140869140625, 'learning_rate': 1.69921875e-06, 'epoch': 9.43359375}\n"," 95%|█████████▍| 4840/5120 [36:48<01:44,  2.69it/s]{'loss': 245.381298828125, 'learning_rate': 1.640625e-06, 'epoch': 9.453125}\n"," 95%|█████████▍| 4850/5120 [36:52<01:40,  2.68it/s]{'loss': 285.7904296875, 'learning_rate': 1.5820312500000001e-06, 'epoch': 9.47265625}\n","{'loss': 241.3634033203125, 'learning_rate': 1.5234375e-06, 'epoch': 9.4921875}\n","{'loss': 240.1320556640625, 'learning_rate': 1.46484375e-06, 'epoch': 9.51171875}\n","                                                   {'loss': 257.31953125, 'learning_rate': 1.40625e-06, 'epoch': 9.53125}\n"," 96%|█████████▌| 4890/5120 [37:07<01:21,  2.83it/s]{'loss': 219.1105224609375, 'learning_rate': 1.34765625e-06, 'epoch': 9.55078125}\n"," 96%|█████████▌| 4900/5120 [37:11<01:42,  2.16it/s]{'loss': 247.1271484375, 'learning_rate': 1.2890625e-06, 'epoch': 9.5703125}\n","{'loss': 267.2373291015625, 'learning_rate': 1.23046875e-06, 'epoch': 9.58984375}\n"," 96%|█████████▌| 4920/5120 [37:19<01:24,  2.38it/s]\n"," 96%|█████████▋| 4930/5120 [37:23<01:21,  2.34it/s]{'loss': 298.66708984375, 'learning_rate': 1.11328125e-06, 'epoch': 9.62890625}\n","{'loss': 238.954833984375, 'learning_rate': 1.0546875e-06, 'epoch': 9.6484375}\n"," 97%|█████████▋| 4950/5120 [37:30<01:01,  2.76it/s]{'loss': 244.569677734375, 'learning_rate': 9.9609375e-07, 'epoch': 9.66796875}\n"," 97%|█████████▋| 4960/5120 [37:34<01:02,  2.56it/s]{'loss': 269.912158203125, 'learning_rate': 9.375e-07, 'epoch': 9.6875}\n"," 97%|█████████▋| 4970/5120 [37:38<00:54,  2.73it/s]{'loss': 248.5887451171875, 'learning_rate': 8.7890625e-07, 'epoch': 9.70703125}\n"," 97%|█████████▋| 4980/5120 [37:41<00:53,  2.61it/s]{'loss': 261.3606689453125, 'learning_rate': 8.203125e-07, 'epoch': 9.7265625}\n","{'loss': 241.5871337890625, 'learning_rate': 7.6171875e-07, 'epoch': 9.74609375}\n"," 98%|█████████▊| 5000/5120 [37:49<00:55,  2.16it/s][INFO|trainer.py:1412] 2021-01-13 16:24:52,346 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-13 16:24:52,346 >>   Num examples = 266\n","[INFO|trainer.py:1414] 2021-01-13 16:24:52,346 >>   Batch size = 4\n","\n","\n","  0%|          | 0/67 [00:00<?, ?it/s]\u001b[A\n","  3%|▎         | 2/67 [00:00<00:30,  2.15it/s]\u001b[A\n","  4%|▍         | 3/67 [00:02<00:50,  1.26it/s]\u001b[A\n","  6%|▌         | 4/67 [00:03<00:49,  1.28it/s]\u001b[A\n","  7%|▋         | 5/67 [00:04<00:57,  1.08it/s]\u001b[A\n","  9%|▉         | 6/67 [00:06<01:08,  1.12s/it]\u001b[A\n"," 10%|█         | 7/67 [00:07<01:06,  1.10s/it]\u001b[A\n"," 12%|█▏        | 8/67 [00:08<01:09,  1.18s/it]\u001b[A\n"," 13%|█▎        | 9/67 [00:09<01:05,  1.13s/it]\u001b[A\n"," 15%|█▍        | 10/67 [00:10<01:07,  1.19s/it]\u001b[A\n"," 16%|█▋        | 11/67 [00:11<01:04,  1.15s/it]\u001b[A\n"," 18%|█▊        | 12/67 [00:12<01:00,  1.10s/it]\u001b[A\n"," 19%|█▉        | 13/67 [00:14<01:06,  1.23s/it]\u001b[A\n"," 21%|██        | 14/67 [00:15<01:10,  1.33s/it]\u001b[A\n"," 22%|██▏       | 15/67 [00:17<01:05,  1.26s/it]\u001b[A\n"," 24%|██▍       | 16/67 [00:17<00:57,  1.13s/it]\u001b[A\n"," 25%|██▌       | 17/67 [00:18<00:51,  1.03s/it]\u001b[A\n"," 27%|██▋       | 18/67 [00:19<00:47,  1.04it/s]\u001b[A\n"," 28%|██▊       | 19/67 [00:20<00:48,  1.01s/it]\u001b[A\n"," 30%|██▉       | 20/67 [00:21<00:45,  1.03it/s]\u001b[A\n"," 31%|███▏      | 21/67 [00:22<00:46,  1.01s/it]\u001b[A\n"," 33%|███▎      | 22/67 [00:24<00:51,  1.15s/it]\u001b[A\n"," 34%|███▍      | 23/67 [00:24<00:44,  1.01s/it]\u001b[A\n"," 36%|███▌      | 24/67 [00:25<00:37,  1.14it/s]\u001b[A\n"," 37%|███▋      | 25/67 [00:26<00:36,  1.17it/s]\u001b[A\n"," 39%|███▉      | 26/67 [00:27<00:42,  1.03s/it]\u001b[A\n"," 40%|████      | 27/67 [00:28<00:40,  1.01s/it]\u001b[A\n"," 42%|████▏     | 28/67 [00:29<00:38,  1.01it/s]\u001b[A\n"," 43%|████▎     | 29/67 [00:30<00:37,  1.01it/s]\u001b[A\n"," 45%|████▍     | 30/67 [00:31<00:35,  1.05it/s]\u001b[A\n"," 46%|████▋     | 31/67 [00:32<00:34,  1.04it/s]\u001b[A\n"," 48%|████▊     | 32/67 [00:33<00:33,  1.05it/s]\u001b[A\n"," 49%|████▉     | 33/67 [00:34<00:37,  1.09s/it]\u001b[A\n"," 51%|█████     | 34/67 [00:36<00:44,  1.36s/it]\u001b[A\n"," 52%|█████▏    | 35/67 [00:37<00:40,  1.26s/it]\u001b[A\n"," 54%|█████▎    | 36/67 [00:38<00:37,  1.20s/it]\u001b[A\n"," 55%|█████▌    | 37/67 [00:40<00:40,  1.34s/it]\u001b[A\n"," 57%|█████▋    | 38/67 [00:41<00:35,  1.23s/it]\u001b[A\n"," 58%|█████▊    | 39/67 [00:42<00:30,  1.08s/it]\u001b[A\n"," 60%|█████▉    | 40/67 [00:44<00:40,  1.49s/it]\u001b[A\n"," 61%|██████    | 41/67 [00:45<00:34,  1.32s/it]\u001b[A\n"," 63%|██████▎   | 42/67 [00:46<00:27,  1.10s/it]\u001b[A\n"," 64%|██████▍   | 43/67 [00:47<00:27,  1.13s/it]\u001b[A\n"," 66%|██████▌   | 44/67 [00:47<00:21,  1.08it/s]\u001b[A\n"," 67%|██████▋   | 45/67 [00:49<00:25,  1.18s/it]\u001b[A\n"," 69%|██████▊   | 46/67 [00:50<00:24,  1.17s/it]\u001b[A\n"," 70%|███████   | 47/67 [00:52<00:25,  1.27s/it]\u001b[A\n"," 72%|███████▏  | 48/67 [00:53<00:21,  1.15s/it]\u001b[A\n"," 73%|███████▎  | 49/67 [00:54<00:23,  1.30s/it]\u001b[A\n"," 75%|███████▍  | 50/67 [00:55<00:21,  1.24s/it]\u001b[A\n"," 76%|███████▌  | 51/67 [00:57<00:19,  1.25s/it]\u001b[A\n"," 78%|███████▊  | 52/67 [00:57<00:16,  1.10s/it]\u001b[A\n"," 79%|███████▉  | 53/67 [00:58<00:15,  1.11s/it]\u001b[A\n"," 81%|████████  | 54/67 [00:59<00:14,  1.08s/it]\u001b[A\n"," 82%|████████▏ | 55/67 [01:00<00:11,  1.06it/s]\u001b[A\n"," 84%|████████▎ | 56/67 [01:01<00:11,  1.06s/it]\u001b[A\n"," 85%|████████▌ | 57/67 [01:03<00:11,  1.14s/it]\u001b[A\n"," 87%|████████▋ | 58/67 [01:04<00:11,  1.25s/it]\u001b[A\n"," 88%|████████▊ | 59/67 [01:05<00:09,  1.20s/it]\u001b[A\n"," 90%|████████▉ | 60/67 [01:06<00:07,  1.01s/it]\u001b[A\n"," 91%|█████████ | 61/67 [01:07<00:05,  1.03it/s]\u001b[A\n"," 93%|█████████▎| 62/67 [01:08<00:05,  1.00s/it]\u001b[A\n"," 94%|█████████▍| 63/67 [01:09<00:03,  1.04it/s]\u001b[A\n"," 96%|█████████▌| 64/67 [01:10<00:02,  1.08it/s]\u001b[A\n"," 97%|█████████▋| 65/67 [01:11<00:02,  1.01s/it]\u001b[A\n"," 99%|█████████▊| 66/67 [01:12<00:00,  1.05it/s]\u001b[A\n","                                                   \n"," 98%|█████████▊| 5000/5120 [39:05<00:55,  2.16it/s]\n","100%|██████████| 67/67 [01:13<00:00,  1.17it/s]\u001b[A\n","                                               \u001b[A\n","[INFO|trainer.py:1226] 2021-01-13 16:26:08,055 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-5000\n","[INFO|configuration_utils.py:289] 2021-01-13 16:26:08,063 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:814] 2021-01-13 16:26:14,323 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-5000/pytorch_model.bin\n","[INFO|trainer.py:1285] 2021-01-13 16:26:14,428 >> Deleting older checkpoint [/content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-5000] due to args.save_total_limit\n","{'loss': 235.861376953125, 'learning_rate': 6.4453125e-07, 'epoch': 9.78515625}\n","                                                   \n","{'loss': 282.5458984375, 'learning_rate': 5.2734375e-07, 'epoch': 9.82421875}\n"," 98%|█████████▊| 5040/5120 [39:28<00:35,  2.27it/s]{'loss': 261.556005859375, 'learning_rate': 4.6875e-07, 'epoch': 9.84375}\n"," 99%|█████████▊| 5050/5120 [39:32<00:26,  2.66it/s]{'loss': 224.3278076171875, 'learning_rate': 4.1015625e-07, 'epoch': 9.86328125}\n","{'loss': 248.7773681640625, 'learning_rate': 3.515625e-07, 'epoch': 9.8828125}\n","{'loss': 236.4655517578125, 'learning_rate': 2.9296875000000003e-07, 'epoch': 9.90234375}\n"," 99%|█████████▉| 5080/5120 [39:43<00:15,  2.53it/s]\n"," 99%|█████████▉| 5089/5120 [39:46<00:11,  2.67it/s]{'loss': 246.687841796875, 'learning_rate': 1.7578125e-07, 'epoch': 9.94140625}\n","100%|█████████▉| 5100/5120 [39:51<00:10,  1.93it/s]{'loss': 267.0839599609375, 'learning_rate': 1.171875e-07, 'epoch': 9.9609375}\n","{'loss': 240.50107421875, 'learning_rate': 5.859375e-08, 'epoch': 9.98046875}\n","100%|██████████| 5120/5120 [39:59<00:00,  2.98it/s][INFO|trainer.py:862] 2021-01-13 16:27:02,367 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\n","100%|██████████| 5120/5120 [39:59<00:00,  2.98it/s]\n","100%|██████████| 5120/5120 [39:59<00:00,  2.13it/s]\n","[INFO|trainer.py:1226] 2021-01-13 16:27:02,383 >> Saving model checkpoint to /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train\n","[INFO|configuration_utils.py:289] 2021-01-13 16:27:02,390 >> Configuration saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/config.json\n","[INFO|modeling_utils.py:814] 2021-01-13 16:27:08,880 >> Model weights saved in /content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/pytorch_model.bin\n","01/13/2021 16:27:08 - INFO - __main__ -   ***** train metrics *****\n","01/13/2021 16:27:08 - INFO - __main__ -     train_samples_per_second = -0.0\n","01/13/2021 16:27:08 - INFO - __main__ -     train_runtime = 2403.0932\n","01/13/2021 16:27:08 - INFO - __main__ -     train_n_ojbs = -1\n","01/13/2021 16:27:09 - INFO - __main__ -   *** Evaluate ***\n","[INFO|trainer.py:1412] 2021-01-13 16:27:09,022 >> ***** Running Evaluation *****\n","[INFO|trainer.py:1413] 2021-01-13 16:27:09,022 >>   Num examples = 266\n","[INFO|trainer.py:1414] 2021-01-13 16:27:09,022 >>   Batch size = 4\n","100%|██████████| 67/67 [01:10<00:00,  1.05s/it]\n","01/13/2021 16:28:21 - INFO - __main__ -   ***** val metrics *****\n","01/13/2021 16:28:21 - INFO - __main__ -     val_loss = 609.9545\n","01/13/2021 16:28:21 - INFO - __main__ -     val_rouge1 = 32.6288\n","01/13/2021 16:28:21 - INFO - __main__ -     val_rouge2 = 14.3641\n","01/13/2021 16:28:21 - INFO - __main__ -     val_rougeL = 26.148\n","01/13/2021 16:28:21 - INFO - __main__ -     val_rougeLsum = 26.8574\n","01/13/2021 16:28:21 - INFO - __main__ -     val_gen_len = 38.9\n","01/13/2021 16:28:21 - INFO - __main__ -     epoch = 10.0\n","01/13/2021 16:28:21 - INFO - __main__ -     val_samples_per_second = -0.014\n","01/13/2021 16:28:21 - INFO - __main__ -     val_runtime = 72.6978\n","01/13/2021 16:28:21 - INFO - __main__ -     val_n_ojbs = -1\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1635\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210113_154659-3pq7vevr/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210113_154659-3pq7vevr/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                        train/loss 213.21042\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                               train/learning_rate 0.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                       train/epoch 10.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                             _step 5120\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                          _runtime 2482\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                        _timestamp 1610555301\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                         eval/loss 610.2326\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                       eval/rouge1 32.8164\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                       eval/rouge2 14.0652\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                       eval/rougeL 26.1155\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                    eval/rougeLsum 26.9397\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                      eval/gen_len 40.7\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/total_flos 12493064479334400\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                    train/val_loss 609.95453\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/val_rouge1 32.6288\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/val_rouge2 14.3641\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/val_rougeL 26.148\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                               train/val_rougeLsum 26.8574\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                 train/val_gen_len 38.9\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▆▇▇▇▆▇▆█▅▅▄▅▄▅▅▃▂▄▄▂▂▄▂▃▁▂▂▂▂▂▂▂▁▂▂▂▃▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:             eval/loss ▁▃▆▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rouge1 ▆▅▁█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rouge2 ▇█▁▃▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rougeL █▆▁▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:        eval/rougeLsum █▅▁▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          eval/gen_len ▁▂▅█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/total_flos ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train/val_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rouge1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rouge2 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rougeL ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   train/val_rougeLsum ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train/val_gen_len ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/huggingface/runs/3pq7vevr\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aeSSMVZwVOa1"},"source":["##### Evaluate"]},{"cell_type":"code","metadata":{"id":"C2YaHOesVOa4"},"source":["source_test_dir = data_dir[:-1] + '/test.source\"'\n","reference_test_dir = data_dir[:-1] + '/test.target\"'\n","\n","save_dir = output_dir[:-1] + '/'+model_name_or_path.replace('/', '?')+'_test_karger_books_para.txt\"'\n","score_dir = output_dir[:-1] + '/'+model_name_or_path.replace('/', '?')+'_test_karger_books_para.json\"'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AgY3gO94VOa9","executionInfo":{"status":"ok","timestamp":1610471442216,"user_tz":-60,"elapsed":1399,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"8436601c-f530-405f-e6c6-c025a7cf5b71"},"source":["!python3 $eval_script \\\n","$output_dir \\\n","$source_test_dir \\\n","$save_dir \\\n","--reference_path $reference_test_dir \\\n","--score_path $score_dir \\\n","--task summarization \\\n","--bs 2 \\\n","--length_penalty $config.LENAGTH_PENALTY \\\n","--no_repeat_ngram_size $config.NO_REPEAT_NGRAM_SIZE \\\n","--num_beams $config.NUM_BEAMS \\\n","--dump-args"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Unknown option: --\n","usage: python3 [option] ... [-c cmd | -m mod | file | -] [arg] ...\n","Try `python -h' for more information.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1bbRUTtNDHth"},"source":[""],"execution_count":null,"outputs":[]}]}