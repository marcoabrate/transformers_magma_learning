{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "magma_dir = '/home/ubuntu/magma/'\n",
    "bucket_dir = '/home/ubuntu/s3/'\n",
    "transformers_dir = '/home/ubuntu/transformers/'\n",
    "cache_dir = bucket_dir+'.cache/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EddY1WDNsKlS"
   },
   "source": [
    "## **Fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2d6M41X9AKBi"
   },
   "outputs": [],
   "source": [
    "finetune_script = '\"'+transformers_dir+'examples/seq2seq/finetune_trainer.py\"'\n",
    "eval_script = '\"'+transformers_dir+'examples/seq2seq/run_eval.py\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "82WSp6khIcua",
    "outputId": "6b5776d2-758b-455f-f3f5-6ff21f1b0a9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=hps_bart_para_wordembed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, magma_dir)\n",
    "import config\n",
    "\n",
    "import torch\n",
    "torch.manual_seed = config.SEED\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "project_name = 'hps_bart_para_wordembed'\n",
    "%env WANDB_PROJECT=$project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPZ7A-sBVOam"
   },
   "source": [
    "### HP search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vJOYl_g6F1e2"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = 'sshleifer/distilbart-cnn-12-6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4xs_XkbCVOar"
   },
   "outputs": [],
   "source": [
    "data_dir = '\"'+bucket_dir+'datasets/karger_books_para_wordembed/bart/st/\"'\n",
    "\n",
    "output_dir = '\"'+bucket_dir+'fine-tuning/hps_bart_para_wordembed\"'\n",
    "\n",
    "log_dir = output_dir + '/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_search_checkpoint_dir = output_dir + 'ray_checkpoints'\n",
    "\n",
    "hp_search_log_dir = output_dir + 'ray_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "4dca0f8d944943a7879b3de0a2f8347a",
      "c8804e5babea4223b192bca1a9111fac",
      "bb421fdbad584767bff8da4c882fdfbf",
      "46539ad5e91e4af9860cab29a8200b53",
      "0cf5224ab8cb40198d84fa37cfcc68d5",
      "521e472af5e24843a467a768c042d6c6",
      "dad549b11e194e53af76c965e3f482a2",
      "d3095d48314a49d2b81e066c2c3fd389"
     ]
    },
    "id": "wtLC1O1ZJpU3",
    "outputId": "317a9e46-1c7b-4fba-c55a-eef2f13d243e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "model_config = AutoConfig.from_pretrained(model_name_or_path, use_cache=False)\n",
    "model_config.min_length = config.ONE_BULLET_MIN_LEN\n",
    "model_config.max_length = config.ONE_BULLET_MAX_LEN\n",
    "\n",
    "model_config.task_specific_params['summarization']['min_length'] = config.ONE_BULLET_MIN_LEN\n",
    "model_config.task_specific_params['summarization']['max_length'] = config.ONE_BULLET_MAX_LEN\n",
    "model_config_dir = '\"'+bucket_dir+'fine-tuning/'+\\\n",
    "    model_name_or_path.replace('/', '?')+'_config\"'\n",
    "model_config.save_pretrained(model_config_dir[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n--logging_steps 20 --logging_first_step --evaluation_strategy steps --eval_steps 50 --eval_beams 3 '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "--logging_steps 20 --logging_first_step \\\n",
    "--evaluation_strategy steps --eval_steps 50 --eval_beams 3 \\\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K66kY1WOVOaw",
    "outputId": "c50158bf-7522-42c0-d9e7-164da464ba26",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:20:12 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "02/10/2021 14:20:12 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/home/ubuntu/s3/fine-tuning/hps_bart_para_wordembed', overwrite_output_dir=True, do_train=False, do_eval=None, do_predict=False, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=10, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/Feb10_14-20-12_ip-172-31-39-35', logging_first_step=True, logging_steps=1, save_steps=50, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=3, dataloader_num_workers=0, past_index=-1, run_name='/home/ubuntu/s3/fine-tuning/hps_bart_para_wordembed', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.1, adafactor=False, group_by_length=False, report_to=['tensorboard', 'wandb'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, label_smoothing=0.0, sortish_sampler=True, predict_with_generate=True, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='constant')\n",
      "[INFO|configuration_utils.py:449] 2021-02-10 14:20:12,866 >> loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /home/ubuntu/s3/.cache/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.9307b6cec4435559ec6e79d5a210a334b17706465329e138f335649d14f27e78\n",
      "[INFO|configuration_utils.py:485] 2021-02-10 14:20:12,867 >> Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:449] 2021-02-10 14:20:13,167 >> loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /home/ubuntu/s3/.cache/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.9307b6cec4435559ec6e79d5a210a334b17706465329e138f335649d14f27e78\n",
      "[INFO|configuration_utils.py:485] 2021-02-10 14:20:13,168 >> Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1688] 2021-02-10 14:20:13,168 >> Model name 'sshleifer/distilbart-cnn-12-6' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'sshleifer/distilbart-cnn-12-6' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 14:20:14,900 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/vocab.json from cache at /home/ubuntu/s3/.cache/9951e68693b9a7c583ae677e9cb53c02715d9bd0311a78706401372653cdea0a.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 14:20:14,901 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/merges.txt from cache at /home/ubuntu/s3/.cache/7588c8d398d659b230a038240cc023f67b6848117d2999f06ab625af7bfc7ec1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 14:20:14,901 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 14:20:14,901 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 14:20:14,901 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 14:20:14,901 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer_config.json from cache at /home/ubuntu/s3/.cache/f5316f64f9716436994a7ad76a354dc20ecb2dd74eb61d278f103a9c8b80291f.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n",
      "[INFO|modeling_utils.py:1027] 2021-02-10 14:20:15,666 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1143] 2021-02-10 14:20:44,153 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-10 14:20:44,153 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/10/2021 14:20:44 - INFO - utils -   setting model.config to task specific params for summarization:\n",
      " {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "02/10/2021 14:20:44 - INFO - utils -   note: command line args may override some of these\n",
      "[INFO|modeling_utils.py:1027] 2021-02-10 14:20:45,299 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1143] 2021-02-10 14:21:09,834 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-10 14:21:09,835 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/10/2021 14:21:09 - INFO - utils -   setting model.config to task specific params for summarization:\n",
      " {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "02/10/2021 14:21:09 - INFO - utils -   note: command line args may override some of these\n",
      "[INFO|trainer.py:309] 2021-02-10 14:21:15,253 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:348] 2021-02-10 14:21:15,253 >> Using amp fp16 backend\n",
      "02/10/2021 14:21:15 - INFO - __main__ -   ** Hyperparameter Search **\n",
      "2021-02-10 14:21:16,467\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://172.31.39.35:8265\u001b[39m\u001b[22m\n",
      "2021-02-10 14:21:21,309\tWARNING function_runner.py:540 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "[INFO|modeling_utils.py:1027] 2021-02-10 14:21:25,004 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1143] 2021-02-10 14:21:50,156 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-10 14:21:50,157 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/10/2021 14:21:50 - INFO - utils -   setting model.config to task specific params for summarization:\n",
      " {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "02/10/2021 14:21:50 - INFO - utils -   note: command line args may override some of these\n",
      "[INFO|trainer.py:837] 2021-02-10 14:21:50,486 >> ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-10 14:21:50,486 >>   Num examples = 50\n",
      "[INFO|trainer.py:839] 2021-02-10 14:21:50,486 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:840] 2021-02-10 14:21:50,486 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:841] 2021-02-10 14:21:50,486 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "[INFO|trainer.py:842] 2021-02-10 14:21:50,486 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:843] 2021-02-10 14:21:50,486 >>   Total optimization steps = 10\n",
      "[INFO|integrations.py:565] 2021-02-10 14:21:51,961 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/hps_bart_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hps_bart_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hps_bart_para_wordembed/runs/p0ugogiw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed/_inner_40086_00000_0_gradient_accumulation_steps=2,learning_rate=1e-05_2021-02-10_14-21-22/wandb/run-20210210_142154-p0ugogiw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      " 10%|█         | 1/10 [00:02<00:25,  2.79s/it]{'loss': 5.5252, 'learning_rate': 1e-05, 'epoch': 0.04}\n",
      " 20%|██        | 2/10 [00:03<00:16,  2.02s/it]h': 0.08}\n",
      " 30%|███       | 3/10 [00:03<00:10,  1.51s/it]h': 0.12}\n",
      "[INFO|trainer.py:1612] 2021-02-10 14:22:23,628 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1613] 2021-02-10 14:22:23,630 >>   Num examples = 10\n",
      "[INFO|trainer.py:1614] 2021-02-10 14:22:23,630 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:03,  2.46it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  1.92it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.55it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.51it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:04<00:02,  1.42it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:04<00:02,  1.39it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:05<00:01,  1.34it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  1.32it/s]\u001b[A\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 68.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 115.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 121.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 122.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 106.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 97.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 117.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.61it/s]\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collected 58 word types from a corpus of 76 raw words and 2 sentences\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 58 unique words (100% of original 58, drops 0)\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 76 word corpus (100% of original 76, drops 0)\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 58 items\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 58 most-common words\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   downsampling leaves estimated 22 word corpus (29.1% of prior 76)\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   estimated required memory for 58 words and 100 dimensions: 75400 bytes\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 58 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 76 raw words (24 effective words) took 0.0s, 15787 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 76 raw words (23 effective words) took 0.0s, 70980 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 76 raw words (23 effective words) took 0.0s, 45699 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 76 raw words (26 effective words) took 0.0s, 54496 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 76 raw words (22 effective words) took 0.0s, 46104 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training on a 380 raw words (118 effective words) took 0.0s, 12758 effective words/s\r\n",
      "02/10/2021 14:22:32 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collected 50 word types from a corpus of 79 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 50 unique words (100% of original 50, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 79 word corpus (100% of original 79, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 50 items\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 50 most-common words\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   downsampling leaves estimated 21 word corpus (26.7% of prior 79)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   estimated required memory for 50 words and 100 dimensions: 65000 bytes\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 50 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 79 raw words (26 effective words) took 0.0s, 50420 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 79 raw words (19 effective words) took 0.0s, 34121 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 79 raw words (17 effective words) took 0.0s, 29871 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 79 raw words (27 effective words) took 0.0s, 51147 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 79 raw words (18 effective words) took 0.0s, 37470 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training on a 395 raw words (107 effective words) took 0.0s, 12219 effective words/s\r\n",
      "02/10/2021 14:22:32 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collected 39 word types from a corpus of 69 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 39 unique words (100% of original 39, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 69 word corpus (100% of original 69, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 39 items\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 39 most-common words\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   downsampling leaves estimated 15 word corpus (22.7% of prior 69)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   estimated required memory for 39 words and 100 dimensions: 50700 bytes\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 39 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 69 raw words (19 effective words) took 0.0s, 40355 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 69 raw words (13 effective words) took 0.0s, 25567 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 69 raw words (15 effective words) took 0.0s, 45798 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 69 raw words (15 effective words) took 0.0s, 53739 effective words/s\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 69 raw words (16 effective words) took 0.0s, 51118 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training on a 345 raw words (78 effective words) took 0.0s, 10058 effective words/s\r\n",
      "02/10/2021 14:22:32 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collected 88 word types from a corpus of 134 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 88 unique words (100% of original 88, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 134 word corpus (100% of original 134, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 88 items\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 88 most-common words\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   downsampling leaves estimated 49 word corpus (37.1% of prior 134)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   estimated required memory for 88 words and 100 dimensions: 114400 bytes\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 88 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 134 raw words (57 effective words) took 0.0s, 104056 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 134 raw words (43 effective words) took 0.0s, 79686 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 134 raw words (35 effective words) took 0.0s, 70644 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 134 raw words (52 effective words) took 0.0s, 68733 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 134 raw words (57 effective words) took 0.0s, 91047 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training on a 670 raw words (244 effective words) took 0.0s, 28936 effective words/s\r\n",
      "02/10/2021 14:22:32 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collected 31 word types from a corpus of 51 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 31 unique words (100% of original 31, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 51 word corpus (100% of original 51, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 31 items\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 31 most-common words\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   downsampling leaves estimated 10 word corpus (20.1% of prior 51)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   estimated required memory for 31 words and 100 dimensions: 40300 bytes\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 31 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 51 raw words (12 effective words) took 0.0s, 24244 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 51 raw words (8 effective words) took 0.0s, 18033 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 51 raw words (9 effective words) took 0.0s, 15215 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 51 raw words (6 effective words) took 0.0s, 15282 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 51 raw words (11 effective words) took 0.0s, 17083 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training on a 255 raw words (46 effective words) took 0.0s, 6042 effective words/s\r\n",
      "02/10/2021 14:22:32 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collected 60 word types from a corpus of 97 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 60 unique words (100% of original 60, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 97 word corpus (100% of original 97, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 60 items\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 60 most-common words\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   downsampling leaves estimated 28 word corpus (29.5% of prior 97)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   estimated required memory for 60 words and 100 dimensions: 78000 bytes\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   resetting layer weights\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 60 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 97 raw words (30 effective words) took 0.0s, 65642 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 97 raw words (35 effective words) took 0.0s, 38745 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 97 raw words (36 effective words) took 0.0s, 77894 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 97 raw words (31 effective words) took 0.0s, 58927 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 97 raw words (25 effective words) took 0.0s, 40656 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training on a 485 raw words (157 effective words) took 0.0s, 18933 effective words/s\r\n",
      "02/10/2021 14:22:32 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collected 38 word types from a corpus of 55 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 38 unique words (100% of original 38, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 55 word corpus (100% of original 55, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 38 items\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 38 most-common words\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   downsampling leaves estimated 12 word corpus (22.8% of prior 55)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   estimated required memory for 38 words and 100 dimensions: 49400 bytes\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 38 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 55 raw words (16 effective words) took 0.0s, 30890 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 55 raw words (14 effective words) took 0.0s, 27149 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 55 raw words (10 effective words) took 0.0s, 22658 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 55 raw words (15 effective words) took 0.0s, 59794 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 55 raw words (11 effective words) took 0.0s, 25117 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training on a 275 raw words (66 effective words) took 0.0s, 8662 effective words/s\r\n",
      "02/10/2021 14:22:32 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collected 48 word types from a corpus of 64 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 48 unique words (100% of original 48, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 64 word corpus (100% of original 64, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 48 items\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 48 most-common words\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   downsampling leaves estimated 16 word corpus (26.1% of prior 64)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   estimated required memory for 48 words and 100 dimensions: 62400 bytes\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 48 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 64 raw words (19 effective words) took 0.0s, 36577 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 64 raw words (12 effective words) took 0.0s, 23414 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 64 raw words (15 effective words) took 0.0s, 31010 effective words/s\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 64 raw words (21 effective words) took 0.0s, 28707 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 64 raw words (13 effective words) took 0.0s, 22835 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training on a 320 raw words (80 effective words) took 0.0s, 9549 effective words/s\r\n",
      "02/10/2021 14:22:32 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collected 42 word types from a corpus of 53 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 42 unique words (100% of original 42, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 53 word corpus (100% of original 53, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 42 items\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 42 most-common words\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   downsampling leaves estimated 12 word corpus (24.3% of prior 53)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   estimated required memory for 42 words and 100 dimensions: 54600 bytes\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 42 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 53 raw words (14 effective words) took 0.0s, 30728 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 53 raw words (15 effective words) took 0.0s, 30303 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 53 raw words (15 effective words) took 0.0s, 46794 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 53 raw words (6 effective words) took 0.0s, 16932 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 53 raw words (11 effective words) took 0.0s, 19762 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training on a 265 raw words (61 effective words) took 0.0s, 7864 effective words/s\r\n",
      "02/10/2021 14:22:32 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   collected 50 word types from a corpus of 65 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 50 unique words (100% of original 50, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 65 word corpus (100% of original 65, drops 0)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 50 items\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 50 most-common words\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   downsampling leaves estimated 17 word corpus (26.8% of prior 65)\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   estimated required memory for 50 words and 100 dimensions: 65000 bytes\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 50 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 65 raw words (22 effective words) took 0.0s, 48471 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 65 raw words (21 effective words) took 0.0s, 32669 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 65 raw words (18 effective words) took 0.0s, 37641 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 65 raw words (17 effective words) took 0.0s, 32237 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 65 raw words (16 effective words) took 0.0s, 36688 effective words/s\r\n",
      "02/10/2021 14:22:32 - INFO - gensim.models.base_any2vec -   training on a 325 raw words (94 effective words) took 0.0s, 12749 effective words/s\r\n",
      "02/10/2021 14:22:32 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "\r",
      "                                              {'eval_loss': 2.980358600616455, 'eval_rouge1_precision': 0.2332, 'eval_rouge1_recall': 0.4466, 'eval_rouge1_fmeasure': 0.2901, 'eval_rouge2_precision': 0.1056, 'eval_rouge2_recall': 0.2259, 'eval_rouge2_fmeasure': 0.1376, 'eval_rougeL_precision': 0.1816, 'eval_rougeL_recall': 0.3705, 'eval_rougeL_fmeasure': 0.2315, 'eval_rougeLsum_precision': 0.1943, 'eval_rougeLsum_recall': 0.3763, 'eval_rougeLsum_fmeasure': 0.244, 'eval_gen_len': 71.3, 'eval_sentence_distilroberta_cosine': 65.88486433029175, 'eval_w2v_cosine': 44.872283935546875, 'eval_runtime': 8.725, 'eval_samples_per_second': 1.146, 'epoch': 0.12}\r\n",
      "\r",
      "\r\n",
      "\r",
      "                                               \r",
      "\u001b[A\r",
      " 30%|███       | 3/10 [00:12<00:10,  1.51s/it]\r\n",
      "\r",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\u001b[A\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               \u001b[A2021-02-10 14:22:33,332\tWARNING util.py:142 -- The `start_trial` operation took 70.673 s, which may be a performance bottleneck.\n",
      "== Status ==\n",
      "Memory usage on this node: 10.1/15.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 1/1 GPUs, 0.0/5.96 GiB heap, 0.0/2.05 GiB objects (0/1.0 accelerator_type:T4)\n",
      "Result logdir: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed\n",
      "Number of trials: 1/4 (1 RUNNING)\n",
      "+--------------------+----------+-------+-------------------------------+-----------------+\n",
      "| Trial name         | status   | loc   |   gradient_accumulation_steps |   learning_rate |\n",
      "|--------------------+----------+-------+-------------------------------+-----------------|\n",
      "| _inner_40086_00000 | RUNNING  |       |                             2 |           1e-05 |\n",
      "+--------------------+----------+-------+-------------------------------+-----------------+\n",
      "\n",
      "\n",
      "2021-02-10 14:22:33,817\tWARNING ray_trial_executor.py:481 -- Over the last 60 seconds, the Tune event loop has been backlogged processing new results. Consider increasing your period of result reporting to improve performance.\n",
      "Result for _inner_40086_00000:\n",
      "  date: 2021-02-10_14-22-32\n",
      "  done: false\n",
      "  epoch: 0.12\n",
      "  eval_gen_len: 71.3\n",
      "  eval_loss: 2.980358600616455\n",
      "  eval_rouge1_fmeasure: 0.2901\n",
      "  eval_rouge1_precision: 0.2332\n",
      "  eval_rouge1_recall: 0.4466\n",
      "  eval_rouge2_fmeasure: 0.1376\n",
      "  eval_rouge2_precision: 0.1056\n",
      "  eval_rouge2_recall: 0.2259\n",
      "  eval_rougeL_fmeasure: 0.2315\n",
      "  eval_rougeL_precision: 0.1816\n",
      "  eval_rougeL_recall: 0.3705\n",
      "  eval_rougeLsum_fmeasure: 0.244\n",
      "  eval_rougeLsum_precision: 0.1943\n",
      "  eval_rougeLsum_recall: 0.3763\n",
      "  eval_runtime: 8.725\n",
      "  eval_samples_per_second: 1.146\n",
      "  eval_sentence_distilroberta_cosine: 65.88486433029175\n",
      "  eval_w2v_cosine: 44.872283935546875\n",
      "  experiment_id: 1fc12aed009c4b01ba5a2cdec6699755\n",
      "  hostname: ip-172-31-39-35\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.31.39.35\n",
      "  objective: 123.66534826583862\n",
      "  pid: 3880\n",
      "  time_since_restore: 69.51111340522766\n",
      "  time_this_iter_s: 69.51111340522766\n",
      "  time_total_s: 69.51111340522766\n",
      "  timestamp: 1612966952\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '40086_00000'\n",
      "  \n",
      " 40%|████      | 4/10 [00:13<00:25,  4.21s/it]h': 0.16}\n",
      " 50%|█████     | 5/10 [00:14<00:15,  3.03s/it]{'loss': 2.4457, 'learning_rate': 1e-05, 'epoch': 0.2}\n",
      "                                              {'loss': 3.7772, 'learning_rate': 1e-05, 'epoch': 0.24}\n",
      " 60%|██████    | 6/10 [00:14<00:08,  2.19s/it][INFO|trainer.py:1612] 2021-02-10 14:22:34,646 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1613] 2021-02-10 14:22:34,646 >>   Num examples = 10\n",
      "[INFO|trainer.py:1614] 2021-02-10 14:22:34,646 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:02,  2.67it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  2.06it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.73it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.57it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.45it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:04<00:02,  1.31it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:05<00:01,  1.35it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  1.38it/s]\u001b[A\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 118.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 143.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 123.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.53it/s]\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collected 58 word types from a corpus of 76 raw words and 2 sentences\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 58 unique words (100% of original 58, drops 0)\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 76 word corpus (100% of original 76, drops 0)\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 58 items\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 58 most-common words\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   downsampling leaves estimated 22 word corpus (29.1% of prior 76)\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   estimated required memory for 58 words and 100 dimensions: 75400 bytes\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 58 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 76 raw words (24 effective words) took 0.0s, 47367 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 76 raw words (23 effective words) took 0.0s, 34719 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 76 raw words (23 effective words) took 0.0s, 31218 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 76 raw words (26 effective words) took 0.0s, 53908 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 76 raw words (22 effective words) took 0.0s, 43803 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training on a 380 raw words (118 effective words) took 0.0s, 13844 effective words/s\n",
      "02/10/2021 14:22:42 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collected 50 word types from a corpus of 72 raw words and 2 sentences\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 50 unique words (100% of original 50, drops 0)\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 72 word corpus (100% of original 72, drops 0)\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 50 items\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 50 most-common words\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   downsampling leaves estimated 19 word corpus (26.7% of prior 72)\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   estimated required memory for 50 words and 100 dimensions: 65000 bytes\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 50 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 72 raw words (24 effective words) took 0.0s, 48843 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 72 raw words (23 effective words) took 0.0s, 45784 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 72 raw words (21 effective words) took 0.0s, 33711 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 72 raw words (14 effective words) took 0.0s, 18862 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 72 raw words (22 effective words) took 0.0s, 47391 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training on a 360 raw words (104 effective words) took 0.0s, 13111 effective words/s\n",
      "02/10/2021 14:22:42 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collected 40 word types from a corpus of 70 raw words and 2 sentences\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 40 unique words (100% of original 40, drops 0)\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 70 word corpus (100% of original 70, drops 0)\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 40 items\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 40 most-common words\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   downsampling leaves estimated 16 word corpus (23.1% of prior 70)\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   estimated required memory for 40 words and 100 dimensions: 52000 bytes\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 40 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 70 raw words (19 effective words) took 0.0s, 39388 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 70 raw words (18 effective words) took 0.0s, 33369 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 70 raw words (14 effective words) took 0.0s, 29861 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 70 raw words (16 effective words) took 0.0s, 26278 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 70 raw words (15 effective words) took 0.0s, 32458 effective words/s\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training on a 350 raw words (82 effective words) took 0.0s, 10577 effective words/s\n",
      "02/10/2021 14:22:42 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collected 80 word types from a corpus of 123 raw words and 2 sentences\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 80 unique words (100% of original 80, drops 0)\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 123 word corpus (100% of original 123, drops 0)\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 80 items\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 80 most-common words\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   downsampling leaves estimated 43 word corpus (35.0% of prior 123)\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   estimated required memory for 80 words and 100 dimensions: 104000 bytes\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 80 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 123 raw words (44 effective words) took 0.0s, 226422 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 123 raw words (41 effective words) took 0.0s, 53075 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 123 raw words (37 effective words) took 0.0s, 71013 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 123 raw words (54 effective words) took 0.0s, 88606 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 123 raw words (44 effective words) took 0.0s, 111680 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training on a 615 raw words (220 effective words) took 0.0s, 21685 effective words/s\r\n",
      "02/10/2021 14:22:42 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collected 35 word types from a corpus of 56 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 35 unique words (100% of original 35, drops 0)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 56 word corpus (100% of original 56, drops 0)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 35 items\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 35 most-common words\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   downsampling leaves estimated 12 word corpus (21.5% of prior 56)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   estimated required memory for 35 words and 100 dimensions: 45500 bytes\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 35 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 56 raw words (14 effective words) took 0.0s, 28823 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 56 raw words (13 effective words) took 0.0s, 27189 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 56 raw words (10 effective words) took 0.0s, 41938 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 56 raw words (11 effective words) took 0.0s, 10789 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 56 raw words (12 effective words) took 0.0s, 28342 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training on a 280 raw words (60 effective words) took 0.0s, 4400 effective words/s\r\n",
      "02/10/2021 14:22:42 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collected 56 word types from a corpus of 98 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 56 unique words (100% of original 56, drops 0)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 98 word corpus (100% of original 98, drops 0)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 56 items\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 56 most-common words\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   downsampling leaves estimated 27 word corpus (28.2% of prior 98)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   estimated required memory for 56 words and 100 dimensions: 72800 bytes\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   resetting layer weights\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 56 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 98 raw words (27 effective words) took 0.0s, 53161 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 98 raw words (23 effective words) took 0.0s, 38504 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 98 raw words (29 effective words) took 0.0s, 55348 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 98 raw words (33 effective words) took 0.0s, 53096 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 98 raw words (26 effective words) took 0.0s, 57635 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training on a 490 raw words (138 effective words) took 0.0s, 11822 effective words/s\r\n",
      "02/10/2021 14:22:42 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collected 47 word types from a corpus of 70 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 47 unique words (100% of original 47, drops 0)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 70 word corpus (100% of original 70, drops 0)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 47 items\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 47 most-common words\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   downsampling leaves estimated 18 word corpus (25.8% of prior 70)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   estimated required memory for 47 words and 100 dimensions: 61100 bytes\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 47 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 70 raw words (24 effective words) took 0.0s, 50543 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 70 raw words (20 effective words) took 0.0s, 34037 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 70 raw words (17 effective words) took 0.0s, 36350 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 70 raw words (18 effective words) took 0.0s, 30055 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 70 raw words (24 effective words) took 0.0s, 53015 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training on a 350 raw words (103 effective words) took 0.0s, 9225 effective words/s\r\n",
      "02/10/2021 14:22:42 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collected 40 word types from a corpus of 55 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 40 unique words (100% of original 40, drops 0)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 55 word corpus (100% of original 55, drops 0)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 40 items\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 40 most-common words\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   downsampling leaves estimated 12 word corpus (23.5% of prior 55)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   estimated required memory for 40 words and 100 dimensions: 52000 bytes\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 40 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 55 raw words (16 effective words) took 0.0s, 34230 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 55 raw words (16 effective words) took 0.0s, 33050 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 55 raw words (13 effective words) took 0.0s, 17490 effective words/s\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 55 raw words (10 effective words) took 0.0s, 20220 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 55 raw words (15 effective words) took 0.0s, 31731 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training on a 275 raw words (70 effective words) took 0.0s, 7557 effective words/s\r\n",
      "02/10/2021 14:22:42 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collected 34 word types from a corpus of 44 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 34 unique words (100% of original 34, drops 0)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 44 word corpus (100% of original 44, drops 0)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 34 items\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 34 most-common words\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   downsampling leaves estimated 9 word corpus (21.5% of prior 44)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   estimated required memory for 34 words and 100 dimensions: 44200 bytes\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 34 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 44 raw words (10 effective words) took 0.0s, 20932 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 44 raw words (9 effective words) took 0.0s, 39629 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 44 raw words (4 effective words) took 0.0s, 5903 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 44 raw words (7 effective words) took 0.0s, 10293 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 44 raw words (6 effective words) took 0.0s, 11934 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training on a 220 raw words (36 effective words) took 0.0s, 4562 effective words/s\r\n",
      "02/10/2021 14:22:42 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   collected 43 word types from a corpus of 56 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 43 unique words (100% of original 43, drops 0)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 56 word corpus (100% of original 56, drops 0)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 43 items\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 43 most-common words\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   downsampling leaves estimated 13 word corpus (24.5% of prior 56)\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   estimated required memory for 43 words and 100 dimensions: 55900 bytes\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 43 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 56 raw words (16 effective words) took 0.0s, 35086 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 56 raw words (18 effective words) took 0.0s, 33528 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 56 raw words (13 effective words) took 0.0s, 40201 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 56 raw words (10 effective words) took 0.0s, 22089 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 56 raw words (17 effective words) took 0.0s, 37759 effective words/s\r\n",
      "02/10/2021 14:22:42 - INFO - gensim.models.base_any2vec -   training on a 280 raw words (74 effective words) took 0.0s, 9798 effective words/s\r\n",
      "02/10/2021 14:22:42 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "\r",
      "                                              {'eval_loss': 2.7685515880584717, 'eval_rouge1_precision': 0.2446, 'eval_rouge1_recall': 0.4476, 'eval_rouge1_fmeasure': 0.2927, 'eval_rouge2_precision': 0.1144, 'eval_rouge2_recall': 0.2371, 'eval_rouge2_fmeasure': 0.1462, 'eval_rougeL_precision': 0.1884, 'eval_rougeL_recall': 0.3552, 'eval_rougeL_fmeasure': 0.2273, 'eval_rougeLsum_precision': 0.2067, 'eval_rougeLsum_recall': 0.3892, 'eval_rougeLsum_fmeasure': 0.2506, 'eval_gen_len': 68.6, 'eval_sentence_distilroberta_cosine': 65.70080518722534, 'eval_w2v_cosine': 44.099801778793335, 'eval_runtime': 8.1411, 'eval_samples_per_second': 1.228, 'epoch': 0.24}\r",
      "\r\n",
      "\r",
      "                                               \r",
      "\u001b[A\r",
      " 60%|██████    | 6/10 [00:22<00:08,  2.19s/it]\r\n",
      "\r",
      "100%|██████████| 10/10 [00:07<00:00,  1.40it/s]\u001b[A\r\n",
      "\r",
      "                                               \u001b[A\r\n",
      "2021-02-10 14:22:42,789\tWARNING util.py:142 -- The `process_trial` operation took 8.971 s, which may be a performance bottleneck.\r\n",
      "== Status ==\r\n",
      "Memory usage on this node: 10.1/15.3 GiB\r\n",
      "Using FIFO scheduling algorithm.\r\n",
      "Resources requested: 3/4 CPUs, 1/1 GPUs, 0.0/5.96 GiB heap, 0.0/2.05 GiB objects (0/1.0 accelerator_type:T4)\r\n",
      "Result logdir: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed\r\n",
      "Number of trials: 2/4 (1 PENDING, 1 RUNNING)\r\n",
      "+--------------------+----------+-------------------+-------------------------------+-----------------+-------------+\r\n",
      "| Trial name         | status   | loc               |   gradient_accumulation_steps |   learning_rate |   objective |\r\n",
      "|--------------------+----------+-------------------+-------------------------------+-----------------+-------------|\r\n",
      "| _inner_40086_00000 | RUNNING  | 172.31.39.35:3880 |                             2 |           1e-05 |     123.665 |\r\n",
      "| _inner_40086_00001 | PENDING  |                   |                             4 |           1e-05 |             |\r\n",
      "+--------------------+----------+-------------------+-------------------------------+-----------------+-------------+\r\n",
      "\r\n",
      "\r\n",
      "Result for _inner_40086_00000:\r\n",
      "  date: 2021-02-10_14-22-42\r\n",
      "  done: false\r\n",
      "  epoch: 0.24\r\n",
      "  eval_gen_len: 68.6\r\n",
      "  eval_loss: 2.7685515880584717\r\n",
      "  eval_rouge1_fmeasure: 0.2927\r\n",
      "  eval_rouge1_precision: 0.2446\r\n",
      "  eval_rouge1_recall: 0.4476\r\n",
      "  eval_rouge2_fmeasure: 0.1462\r\n",
      "  eval_rouge2_precision: 0.1144\r\n",
      "  eval_rouge2_recall: 0.2371\r\n",
      "  eval_rougeL_fmeasure: 0.2273\r\n",
      "  eval_rougeL_precision: 0.1884\r\n",
      "  eval_rougeL_recall: 0.3552\r\n",
      "  eval_rougeLsum_fmeasure: 0.2506\r\n",
      "  eval_rougeLsum_precision: 0.2067\r\n",
      "  eval_rougeLsum_recall: 0.3892\r\n",
      "  eval_runtime: 8.1411\r\n",
      "  eval_samples_per_second: 1.228\r\n",
      "  eval_sentence_distilroberta_cosine: 65.70080518722534\r\n",
      "  eval_w2v_cosine: 44.099801778793335\r\n",
      "  experiment_id: 1fc12aed009c4b01ba5a2cdec6699755\r\n",
      "  hostname: ip-172-31-39-35\r\n",
      "  iterations_since_restore: 2\r\n",
      "  node_ip: 172.31.39.35\r\n",
      "  objective: 122.26970696601867\r\n",
      "  pid: 3880\r\n",
      "  time_since_restore: 79.94510984420776\r\n",
      "  time_this_iter_s: 10.433996438980103\r\n",
      "  time_total_s: 79.94510984420776\r\n",
      "  timestamp: 1612966962\r\n",
      "  timesteps_since_restore: 0\r\n",
      "  training_iteration: 2\r\n",
      "  trial_id: '40086_00000'\r\n",
      "  \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:22<00:12,  4.06s/it]{'loss': 1.575, 'learning_rate': 1e-05, 'epoch': 0.28}\n",
      " 80%|████████  | 8/10 [00:23<00:05,  2.93s/it]{'loss': 3.7893, 'learning_rate': 1e-05, 'epoch': 0.32}\n",
      " 90%|█████████ | 9/10 [00:23<00:02,  2.14s/it][INFO|trainer.py:1612] 2021-02-10 14:22:43,678 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1613] 2021-02-10 14:22:43,678 >>   Num examples = 10\n",
      "[INFO|trainer.py:1614] 2021-02-10 14:22:43,678 >>   Batch size = 1\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:03,  2.54it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  1.89it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:04,  1.49it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.42it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:04<00:03,  1.28it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:05<00:02,  1.26it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:06<00:01,  1.19it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:07<00:00,  1.14it/s]\u001b[A\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 123.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 124.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 120.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 120.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.90it/s]\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   collected 60 word types from a corpus of 79 raw words and 2 sentences\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 60 unique words (100% of original 60, drops 0)\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 79 word corpus (100% of original 79, drops 0)\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 60 items\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 60 most-common words\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   downsampling leaves estimated 23 word corpus (29.7% of prior 79)\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   estimated required memory for 60 words and 100 dimensions: 78000 bytes\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 60 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 79 raw words (27 effective words) took 0.0s, 51349 effective words/s\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 79 raw words (19 effective words) took 0.0s, 34026 effective words/s\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 79 raw words (21 effective words) took 0.0s, 43411 effective words/s\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 79 raw words (21 effective words) took 0.0s, 31566 effective words/s\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 79 raw words (28 effective words) took 0.0s, 62032 effective words/s\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   training on a 395 raw words (116 effective words) took 0.0s, 14082 effective words/s\n",
      "02/10/2021 14:22:52 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   collected 55 word types from a corpus of 71 raw words and 2 sentences\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 55 unique words (100% of original 55, drops 0)\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 71 word corpus (100% of original 71, drops 0)\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 55 items\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 55 most-common words\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   downsampling leaves estimated 20 word corpus (28.5% of prior 71)\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   estimated required memory for 55 words and 100 dimensions: 71500 bytes\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 55 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 71 raw words (22 effective words) took 0.0s, 43543 effective words/s\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 71 raw words (25 effective words) took 0.0s, 65290 effective words/s\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 71 raw words (13 effective words) took 0.0s, 23434 effective words/s\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 71 raw words (25 effective words) took 0.0s, 127556 effective words/s\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 71 raw words (16 effective words) took 0.0s, 32558 effective words/s\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   training on a 355 raw words (101 effective words) took 0.0s, 13254 effective words/s\n",
      "02/10/2021 14:22:52 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   collected 45 word types from a corpus of 77 raw words and 2 sentences\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 45 unique words (100% of original 45, drops 0)\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 77 word corpus (100% of original 77, drops 0)\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 45 items\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 45 most-common words\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   downsampling leaves estimated 19 word corpus (24.8% of prior 77)\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   estimated required memory for 45 words and 100 dimensions: 58500 bytes\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 45 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 77 raw words (20 effective words) took 0.0s, 44366 effective words/s\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 77 raw words (18 effective words) took 0.0s, 37434 effective words/s\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 77 raw words (17 effective words) took 0.0s, 31650 effective words/s\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 77 raw words (13 effective words) took 0.0s, 30039 effective words/s\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 77 raw words (18 effective words) took 0.0s, 54602 effective words/s\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   training on a 385 raw words (86 effective words) took 0.0s, 10215 effective words/s\r\n",
      "02/10/2021 14:22:52 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   collected 90 word types from a corpus of 140 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 90 unique words (100% of original 90, drops 0)\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 140 word corpus (100% of original 140, drops 0)\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 90 items\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 90 most-common words\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   downsampling leaves estimated 52 word corpus (37.4% of prior 140)\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   estimated required memory for 90 words and 100 dimensions: 117000 bytes\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 90 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 140 raw words (58 effective words) took 0.0s, 104358 effective words/s\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 140 raw words (64 effective words) took 0.0s, 59426 effective words/s\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 140 raw words (48 effective words) took 0.0s, 93340 effective words/s\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 140 raw words (58 effective words) took 0.0s, 71644 effective words/s\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 140 raw words (51 effective words) took 0.0s, 97065 effective words/s\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   training on a 700 raw words (279 effective words) took 0.0s, 31740 effective words/s\r\n",
      "02/10/2021 14:22:52 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   collected 35 word types from a corpus of 56 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 35 unique words (100% of original 35, drops 0)\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 56 word corpus (100% of original 56, drops 0)\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 35 items\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 35 most-common words\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   downsampling leaves estimated 12 word corpus (21.5% of prior 56)\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   estimated required memory for 35 words and 100 dimensions: 45500 bytes\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 35 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 56 raw words (14 effective words) took 0.0s, 30841 effective words/s\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 56 raw words (13 effective words) took 0.0s, 51273 effective words/s\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 56 raw words (10 effective words) took 0.0s, 17232 effective words/s\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 56 raw words (11 effective words) took 0.0s, 48937 effective words/s\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 56 raw words (12 effective words) took 0.0s, 19056 effective words/s\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   training on a 280 raw words (60 effective words) took 0.0s, 7775 effective words/s\r\n",
      "02/10/2021 14:22:52 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   collected 60 word types from a corpus of 110 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 60 unique words (100% of original 60, drops 0)\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 110 word corpus (100% of original 110, drops 0)\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 60 items\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 60 most-common words\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   downsampling leaves estimated 32 word corpus (29.2% of prior 110)\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.base_any2vec -   estimated required memory for 60 words and 100 dimensions: 78000 bytes\r\n",
      "02/10/2021 14:22:52 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 60 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 110 raw words (33 effective words) took 0.0s, 67571 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 110 raw words (32 effective words) took 0.0s, 71700 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 110 raw words (31 effective words) took 0.0s, 34914 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 110 raw words (34 effective words) took 0.0s, 54414 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 110 raw words (39 effective words) took 0.0s, 45620 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   training on a 550 raw words (169 effective words) took 0.0s, 20320 effective words/s\r\n",
      "02/10/2021 14:22:53 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   collected 38 word types from a corpus of 55 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 38 unique words (100% of original 38, drops 0)\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 55 word corpus (100% of original 55, drops 0)\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 38 items\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 38 most-common words\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   downsampling leaves estimated 12 word corpus (22.8% of prior 55)\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   estimated required memory for 38 words and 100 dimensions: 49400 bytes\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 38 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 55 raw words (16 effective words) took 0.0s, 48986 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 55 raw words (14 effective words) took 0.0s, 35149 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 55 raw words (10 effective words) took 0.0s, 32161 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 55 raw words (15 effective words) took 0.0s, 45545 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 55 raw words (11 effective words) took 0.0s, 19960 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   training on a 275 raw words (66 effective words) took 0.0s, 8378 effective words/s\r\n",
      "02/10/2021 14:22:53 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   collected 51 word types from a corpus of 73 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 51 unique words (100% of original 51, drops 0)\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 73 word corpus (100% of original 73, drops 0)\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 51 items\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 51 most-common words\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   downsampling leaves estimated 19 word corpus (27.0% of prior 73)\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   estimated required memory for 51 words and 100 dimensions: 66300 bytes\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   resetting layer weights\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 51 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 73 raw words (26 effective words) took 0.0s, 51846 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 73 raw words (15 effective words) took 0.0s, 20959 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 73 raw words (23 effective words) took 0.0s, 48858 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 73 raw words (20 effective words) took 0.0s, 43024 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 73 raw words (24 effective words) took 0.0s, 45100 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   training on a 365 raw words (108 effective words) took 0.0s, 13388 effective words/s\r\n",
      "02/10/2021 14:22:53 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   collected 44 word types from a corpus of 62 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 44 unique words (100% of original 44, drops 0)\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 62 word corpus (100% of original 62, drops 0)\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 44 items\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 44 most-common words\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   downsampling leaves estimated 15 word corpus (24.9% of prior 62)\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   estimated required memory for 44 words and 100 dimensions: 57200 bytes\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 44 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 62 raw words (20 effective words) took 0.0s, 42391 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 62 raw words (12 effective words) took 0.0s, 32515 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 62 raw words (17 effective words) took 0.0s, 29554 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 62 raw words (16 effective words) took 0.0s, 32205 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 62 raw words (19 effective words) took 0.0s, 39936 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   training on a 310 raw words (84 effective words) took 0.0s, 10653 effective words/s\r\n",
      "02/10/2021 14:22:53 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   collected 53 word types from a corpus of 74 raw words and 2 sentences\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 53 unique words (100% of original 53, drops 0)\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 74 word corpus (100% of original 74, drops 0)\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 53 items\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 53 most-common words\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   downsampling leaves estimated 20 word corpus (27.6% of prior 74)\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   estimated required memory for 53 words and 100 dimensions: 68900 bytes\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 53 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 74 raw words (26 effective words) took 0.0s, 53516 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 74 raw words (16 effective words) took 0.0s, 35339 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 74 raw words (17 effective words) took 0.0s, 31944 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 74 raw words (19 effective words) took 0.0s, 27521 effective words/s\r\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 74 raw words (26 effective words) took 0.0s, 54230 effective words/s\n",
      "02/10/2021 14:22:53 - INFO - gensim.models.base_any2vec -   training on a 370 raw words (104 effective words) took 0.0s, 13224 effective words/s\n",
      "02/10/2021 14:22:53 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "                                              _precision': 0.2227, 'eval_rouge1_recall': 0.4666, 'eval_rouge1_fmeasure': 0.2856, 'eval_rouge2_precision': 0.095, 'eval_rouge2_recall': 0.2125, 'eval_rouge2_fmeasure': 0.1265, 'eval_rougeL_precision': 0.1772, 'eval_rougeL_recall': 0.3811, 'eval_rougeL_fmeasure': 0.2277, 'eval_rougeLsum_precision': 0.2007, 'eval_rougeLsum_recall': 0.421, 'eval_rougeLsum_fmeasure': 0.2559, 'eval_gen_len': 78.9, 'eval_sentence_distilroberta_cosine': 67.44509935379028, 'eval_w2v_cosine': 44.70181465148926, 'eval_runtime': 9.4122, 'eval_samples_per_second': 1.062, 'epoch': 0.36}\n",
      " 90%|█████████ | 9/10 [00:32<00:02,  2.14s/it] \n",
      "100%|██████████| 10/10 [00:08<00:00,  1.11it/s]\u001b[A\n",
      "                                               \u001b[A\n",
      "2021-02-10 14:22:53,094\tWARNING util.py:142 -- The `process_trial` operation took 10.304 s, which may be a performance bottleneck.\n",
      "== Status ==\n",
      "Memory usage on this node: 10.1/15.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 1/1 GPUs, 0.0/5.96 GiB heap, 0.0/2.05 GiB objects (0/1.0 accelerator_type:T4)\n",
      "Result logdir: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed\n",
      "Number of trials: 2/4 (1 PENDING, 1 RUNNING)\n",
      "+--------------------+----------+-------------------+-------------------------------+-----------------+-------------+\n",
      "| Trial name         | status   | loc               |   gradient_accumulation_steps |   learning_rate |   objective |\n",
      "|--------------------+----------+-------------------+-------------------------------+-----------------+-------------|\n",
      "| _inner_40086_00000 | RUNNING  | 172.31.39.35:3880 |                             2 |           1e-05 |      122.27 |\n",
      "| _inner_40086_00001 | PENDING  |                   |                             4 |           1e-05 |             |\n",
      "+--------------------+----------+-------------------+-------------------------------+-----------------+-------------+\n",
      "\n",
      "\n",
      "Result for _inner_40086_00000:\n",
      "  date: 2021-02-10_14-22-53\n",
      "  done: false\n",
      "  epoch: 0.36\n",
      "  eval_gen_len: 78.9\n",
      "  eval_loss: 2.5243899822235107\n",
      "  eval_rouge1_fmeasure: 0.2856\n",
      "  eval_rouge1_precision: 0.2227\n",
      "  eval_rouge1_recall: 0.4666\n",
      "  eval_rouge2_fmeasure: 0.1265\n",
      "  eval_rouge2_precision: 0.095\n",
      "  eval_rouge2_recall: 0.2125\n",
      "  eval_rougeL_fmeasure: 0.2277\n",
      "  eval_rougeL_precision: 0.1772\n",
      "  eval_rougeL_recall: 0.3811\n",
      "  eval_rougeLsum_fmeasure: 0.2559\n",
      "  eval_rougeLsum_precision: 0.2007\n",
      "  eval_rougeLsum_recall: 0.421\n",
      "  eval_runtime: 9.4122\n",
      "  eval_samples_per_second: 1.062\n",
      "  eval_sentence_distilroberta_cosine: 67.44509935379028\n",
      "  eval_w2v_cosine: 44.70181465148926\n",
      "  experiment_id: 1fc12aed009c4b01ba5a2cdec6699755\n",
      "  hostname: ip-172-31-39-35\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.31.39.35\n",
      "  objective: 125.69361400527954\n",
      "  pid: 3880\n",
      "  time_since_restore: 90.24960160255432\n",
      "  time_this_iter_s: 10.304491758346558\n",
      "  time_total_s: 90.24960160255432\n",
      "  timestamp: 1612966973\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: '40086_00000'\n",
      "  \n",
      "100%|██████████| 10/10 [00:33<00:00,  4.50s/it][INFO|trainer.py:1007] 2021-02-10 14:22:53,681 >> 0.4}\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:33<00:00,  4.50s/it]\n",
      "{'train_runtime': 63.1947, 'train_samples_per_second': 0.158, 'epoch': 0.4}\n",
      "100%|██████████| 10/10 [00:33<00:00,  3.34s/it]\n",
      "Result for _inner_40086_00000:\n",
      "  date: 2021-02-10_14-22-53\n",
      "  done: true\n",
      "  epoch: 0.36\n",
      "  eval_gen_len: 78.9\n",
      "  eval_loss: 2.5243899822235107\n",
      "  eval_rouge1_fmeasure: 0.2856\n",
      "  eval_rouge1_precision: 0.2227\n",
      "  eval_rouge1_recall: 0.4666\n",
      "  eval_rouge2_fmeasure: 0.1265\n",
      "  eval_rouge2_precision: 0.095\n",
      "  eval_rouge2_recall: 0.2125\n",
      "  eval_rougeL_fmeasure: 0.2277\n",
      "  eval_rougeL_precision: 0.1772\n",
      "  eval_rougeL_recall: 0.3811\n",
      "  eval_rougeLsum_fmeasure: 0.2559\n",
      "  eval_rougeLsum_precision: 0.2007\n",
      "  eval_rougeLsum_recall: 0.421\n",
      "  eval_runtime: 9.4122\n",
      "  eval_samples_per_second: 1.062\n",
      "  eval_sentence_distilroberta_cosine: 67.44509935379028\n",
      "  eval_w2v_cosine: 44.70181465148926\n",
      "  experiment_id: 1fc12aed009c4b01ba5a2cdec6699755\n",
      "  experiment_tag: 0_gradient_accumulation_steps=2,learning_rate=1e-05\n",
      "  hostname: ip-172-31-39-35\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.31.39.35\n",
      "  objective: 125.69361400527954\n",
      "  pid: 3880\n",
      "  time_since_restore: 90.24960160255432\n",
      "  time_this_iter_s: 10.304491758346558\n",
      "  time_total_s: 90.24960160255432\n",
      "  timestamp: 1612966973\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: '40086_00000'\n",
      "  \n",
      "[INFO|modeling_utils.py:1027] 2021-02-10 14:22:55,817 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1143] 2021-02-10 14:23:20,793 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-10 14:23:20,793 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/10/2021 14:23:20 - INFO - utils -   setting model.config to task specific params for summarization:\n",
      " {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "02/10/2021 14:23:20 - INFO - utils -   note: command line args may override some of these\n",
      "[INFO|trainer.py:837] 2021-02-10 14:23:21,125 >> ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-10 14:23:21,126 >>   Num examples = 50\n",
      "[INFO|trainer.py:839] 2021-02-10 14:23:21,126 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:840] 2021-02-10 14:23:21,126 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:841] 2021-02-10 14:23:21,126 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:842] 2021-02-10 14:23:21,126 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:843] 2021-02-10 14:23:21,126 >>   Total optimization steps = 10\n",
      "[INFO|integrations.py:565] 2021-02-10 14:23:21,130 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4459\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed/_inner_40086_00000_0_gradient_accumulation_steps=2,learning_rate=1e-05_2021-02-10_14-21-22/wandb/run-20210210_142154-p0ugogiw/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed/_inner_40086_00000_0_gradient_accumulation_steps=2,learning_rate=1e-05_2021-02-10_14-21-22/wandb/run-20210210_142154-p0ugogiw/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 3.2889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 56\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612966973\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 2.52439\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 0.2227\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 0.4666\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 0.2856\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 0.095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 0.2125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 0.1265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 0.1772\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 0.3811\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 0.2277\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 0.2007\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 0.421\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 0.2559\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 78.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 67.4451\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/w2v_cosine 44.70181\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/runtime 9.4122\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/samples_per_second 1.062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/train_runtime 63.1947\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_samples_per_second 0.158\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos 7667700019200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss ▆█▃▄▂▄▁▄▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁▄▄▄▆▆▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁▄▄▄▆▆▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss █▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision ▄█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall ▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure ▅█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision ▅█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall ▅█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure ▅█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision ▄█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall ▅▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure █▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision ▁█▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall ▁▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure ▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len ▃▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine ▂▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/w2v_cosine █▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/runtime ▄▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/samples_per_second ▅█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/home/ubuntu/s3/fine-tuning/hps_bart_para_wordembed\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hps_bart_para_wordembed/runs/p0ugogiw\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/hps_bart_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hps_bart_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hps_bart_para_wordembed/runs/1iptf7lr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed/_inner_40086_00000_0_gradient_accumulation_steps=2,learning_rate=1e-05_2021-02-10_14-21-22/wandb/run-20210210_142321-1iptf7lr\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "                                              {'loss': 6.2597, 'learning_rate': 1e-05, 'epoch': 0.08}\n",
      "                                              {'loss': 3.8359, 'learning_rate': 1e-05, 'epoch': 0.16}\n",
      "                                              {'loss': 3.2647, 'learning_rate': 1e-05, 'epoch': 0.24}\n",
      " 30%|███       | 3/10 [00:02<00:06,  1.11it/s][INFO|trainer.py:1612] 2021-02-10 14:23:52,006 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1613] 2021-02-10 14:23:52,006 >>   Num examples = 10\n",
      "[INFO|trainer.py:1614] 2021-02-10 14:23:52,006 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:03,  2.45it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  1.92it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.53it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.47it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:04<00:02,  1.39it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:05<00:02,  1.26it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:05<00:01,  1.26it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  1.26it/s]\u001b[A\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 118.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 120.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 117.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 123.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 111.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 105.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.85it/s]\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collected 58 word types from a corpus of 76 raw words and 2 sentences\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 58 unique words (100% of original 58, drops 0)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 76 word corpus (100% of original 76, drops 0)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 58 items\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 58 most-common words\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   downsampling leaves estimated 22 word corpus (29.1% of prior 76)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   estimated required memory for 58 words and 100 dimensions: 75400 bytes\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 58 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 76 raw words (24 effective words) took 0.0s, 52175 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 76 raw words (23 effective words) took 0.0s, 40038 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 76 raw words (23 effective words) took 0.0s, 38584 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 76 raw words (26 effective words) took 0.0s, 51482 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 76 raw words (22 effective words) took 0.0s, 43510 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training on a 380 raw words (118 effective words) took 0.0s, 13532 effective words/s\n",
      "02/10/2021 14:24:00 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collected 50 word types from a corpus of 79 raw words and 2 sentences\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 50 unique words (100% of original 50, drops 0)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 79 word corpus (100% of original 79, drops 0)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 50 items\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 50 most-common words\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   downsampling leaves estimated 21 word corpus (26.7% of prior 79)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   estimated required memory for 50 words and 100 dimensions: 65000 bytes\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 50 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 79 raw words (26 effective words) took 0.0s, 50449 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 79 raw words (19 effective words) took 0.0s, 21576 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 79 raw words (17 effective words) took 0.0s, 46915 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 79 raw words (27 effective words) took 0.0s, 55055 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 79 raw words (18 effective words) took 0.0s, 39583 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training on a 395 raw words (107 effective words) took 0.0s, 13074 effective words/s\n",
      "02/10/2021 14:24:00 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collected 39 word types from a corpus of 69 raw words and 2 sentences\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 39 unique words (100% of original 39, drops 0)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 69 word corpus (100% of original 69, drops 0)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 39 items\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 39 most-common words\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   downsampling leaves estimated 15 word corpus (22.7% of prior 69)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   estimated required memory for 39 words and 100 dimensions: 50700 bytes\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 39 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 69 raw words (19 effective words) took 0.0s, 42682 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 69 raw words (13 effective words) took 0.0s, 19917 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 69 raw words (15 effective words) took 0.0s, 30198 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 69 raw words (15 effective words) took 0.0s, 35389 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 69 raw words (16 effective words) took 0.0s, 44860 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training on a 345 raw words (78 effective words) took 0.0s, 10065 effective words/s\n",
      "02/10/2021 14:24:00 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collected 88 word types from a corpus of 135 raw words and 2 sentences\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 88 unique words (100% of original 88, drops 0)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 135 word corpus (100% of original 135, drops 0)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 88 items\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 88 most-common words\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   downsampling leaves estimated 50 word corpus (37.1% of prior 135)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   estimated required memory for 88 words and 100 dimensions: 114400 bytes\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 88 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 135 raw words (51 effective words) took 0.0s, 87843 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 135 raw words (53 effective words) took 0.0s, 69420 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 135 raw words (45 effective words) took 0.0s, 89440 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 135 raw words (42 effective words) took 0.0s, 82869 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 135 raw words (55 effective words) took 0.0s, 94033 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training on a 675 raw words (246 effective words) took 0.0s, 29988 effective words/s\r\n",
      "02/10/2021 14:24:00 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collected 34 word types from a corpus of 55 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 34 unique words (100% of original 34, drops 0)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 55 word corpus (100% of original 55, drops 0)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 34 items\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 34 most-common words\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   downsampling leaves estimated 11 word corpus (21.2% of prior 55)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   estimated required memory for 34 words and 100 dimensions: 44200 bytes\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 34 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 55 raw words (14 effective words) took 0.0s, 30932 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 55 raw words (13 effective words) took 0.0s, 39211 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 55 raw words (7 effective words) took 0.0s, 15745 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 55 raw words (8 effective words) took 0.0s, 29527 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 55 raw words (17 effective words) took 0.0s, 37211 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training on a 275 raw words (59 effective words) took 0.0s, 7802 effective words/s\r\n",
      "02/10/2021 14:24:00 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collected 56 word types from a corpus of 98 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 56 unique words (100% of original 56, drops 0)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 98 word corpus (100% of original 98, drops 0)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 56 items\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 56 most-common words\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   downsampling leaves estimated 27 word corpus (28.2% of prior 98)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   estimated required memory for 56 words and 100 dimensions: 72800 bytes\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 56 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 98 raw words (27 effective words) took 0.0s, 59180 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 98 raw words (23 effective words) took 0.0s, 45970 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 98 raw words (29 effective words) took 0.0s, 83485 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 98 raw words (33 effective words) took 0.0s, 69671 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 98 raw words (26 effective words) took 0.0s, 31670 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training on a 490 raw words (138 effective words) took 0.0s, 17698 effective words/s\r\n",
      "02/10/2021 14:24:00 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collected 47 word types from a corpus of 73 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 47 unique words (100% of original 47, drops 0)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 73 word corpus (100% of original 73, drops 0)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 47 items\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 47 most-common words\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   downsampling leaves estimated 18 word corpus (25.7% of prior 73)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   estimated required memory for 47 words and 100 dimensions: 61100 bytes\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 47 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 73 raw words (23 effective words) took 0.0s, 45361 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 73 raw words (24 effective words) took 0.0s, 51940 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 73 raw words (11 effective words) took 0.0s, 24377 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 73 raw words (15 effective words) took 0.0s, 23891 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 73 raw words (24 effective words) took 0.0s, 47876 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training on a 365 raw words (97 effective words) took 0.0s, 12517 effective words/s\r\n",
      "02/10/2021 14:24:00 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collected 48 word types from a corpus of 64 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 48 unique words (100% of original 48, drops 0)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 64 word corpus (100% of original 64, drops 0)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 48 items\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 48 most-common words\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   downsampling leaves estimated 16 word corpus (26.1% of prior 64)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   estimated required memory for 48 words and 100 dimensions: 62400 bytes\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 48 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 64 raw words (19 effective words) took 0.0s, 39403 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 64 raw words (12 effective words) took 0.0s, 12664 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 64 raw words (15 effective words) took 0.0s, 22330 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 64 raw words (21 effective words) took 0.0s, 45693 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 64 raw words (13 effective words) took 0.0s, 27208 effective words/s\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training on a 320 raw words (80 effective words) took 0.0s, 9859 effective words/s\r\n",
      "02/10/2021 14:24:00 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collected 42 word types from a corpus of 53 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 42 unique words (100% of original 42, drops 0)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 53 word corpus (100% of original 53, drops 0)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 42 items\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 42 most-common words\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   downsampling leaves estimated 12 word corpus (24.3% of prior 53)\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   estimated required memory for 42 words and 100 dimensions: 54600 bytes\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 42 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 53 raw words (14 effective words) took 0.0s, 28055 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 53 raw words (15 effective words) took 0.0s, 32357 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 53 raw words (15 effective words) took 0.0s, 37022 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 53 raw words (6 effective words) took 0.0s, 8707 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 53 raw words (11 effective words) took 0.0s, 21073 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training on a 265 raw words (61 effective words) took 0.0s, 7198 effective words/s\n",
      "02/10/2021 14:24:00 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   collected 50 word types from a corpus of 65 raw words and 2 sentences\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 50 unique words (100% of original 50, drops 0)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 65 word corpus (100% of original 65, drops 0)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 50 items\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 50 most-common words\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   downsampling leaves estimated 17 word corpus (26.8% of prior 65)\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   estimated required memory for 50 words and 100 dimensions: 65000 bytes\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 50 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 65 raw words (22 effective words) took 0.0s, 48072 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 65 raw words (21 effective words) took 0.0s, 33957 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 65 raw words (18 effective words) took 0.0s, 39146 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 65 raw words (17 effective words) took 0.0s, 36282 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 65 raw words (16 effective words) took 0.0s, 34979 effective words/s\n",
      "02/10/2021 14:24:00 - INFO - gensim.models.base_any2vec -   training on a 325 raw words (94 effective words) took 0.0s, 11644 effective words/s\n",
      "02/10/2021 14:24:00 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "                                              precision': 0.2409, 'eval_rouge1_recall': 0.4749, 'eval_rouge1_fmeasure': 0.2991, 'eval_rouge2_precision': 0.1163, 'eval_rouge2_recall': 0.2479, 'eval_rouge2_fmeasure': 0.1499, 'eval_rougeL_precision': 0.1933, 'eval_rougeL_recall': 0.3986, 'eval_rougeL_fmeasure': 0.2448, 'eval_rougeLsum_precision': 0.2097, 'eval_rougeLsum_recall': 0.4148, 'eval_rougeLsum_fmeasure': 0.2607, 'eval_gen_len': 74.8, 'eval_sentence_distilroberta_cosine': 66.96356534957886, 'eval_w2v_cosine': 46.831315755844116, 'eval_runtime': 8.7383, 'eval_samples_per_second': 1.144, 'epoch': 0.24}\n",
      "\n",
      " 30%|███       | 3/10 [00:10<00:06,  1.11it/s] \n",
      "100%|██████████| 10/10 [00:08<00:00,  1.26it/s]\u001b[A\n",
      "                                               \u001b[A2021-02-10 14:24:01,556\tWARNING util.py:142 -- The `start_trial` operation took 67.645 s, which may be a performance bottleneck.\n",
      "== Status ==\n",
      "Memory usage on this node: 10.6/15.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 1/1 GPUs, 0.0/5.96 GiB heap, 0.0/2.05 GiB objects (0/1.0 accelerator_type:T4)\n",
      "Result logdir: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed\n",
      "Number of trials: 2/4 (1 RUNNING, 1 TERMINATED)\n",
      "+--------------------+------------+-------+-------------------------------+-----------------+-------------+\n",
      "| Trial name         | status     | loc   |   gradient_accumulation_steps |   learning_rate |   objective |\n",
      "|--------------------+------------+-------+-------------------------------+-----------------+-------------|\n",
      "| _inner_40086_00001 | RUNNING    |       |                             4 |           1e-05 |             |\n",
      "| _inner_40086_00000 | TERMINATED |       |                             2 |           1e-05 |     125.694 |\n",
      "+--------------------+------------+-------+-------------------------------+-----------------+-------------+\n",
      "\n",
      "\n",
      "2021-02-10 14:24:02,044\tWARNING ray_trial_executor.py:481 -- Over the last 60 seconds, the Tune event loop has been backlogged processing new results. Consider increasing your period of result reporting to improve performance.\n",
      "Result for _inner_40086_00001:\n",
      "  date: 2021-02-10_14-24-00\n",
      "  done: false\n",
      "  epoch: 0.24\n",
      "  eval_gen_len: 74.8\n",
      "  eval_loss: 2.953345775604248\n",
      "  eval_rouge1_fmeasure: 0.2991\n",
      "  eval_rouge1_precision: 0.2409\n",
      "  eval_rouge1_recall: 0.4749\n",
      "  eval_rouge2_fmeasure: 0.1499\n",
      "  eval_rouge2_precision: 0.1163\n",
      "  eval_rouge2_recall: 0.2479\n",
      "  eval_rougeL_fmeasure: 0.2448\n",
      "  eval_rougeL_precision: 0.1933\n",
      "  eval_rougeL_recall: 0.3986\n",
      "  eval_rougeLsum_fmeasure: 0.2607\n",
      "  eval_rougeLsum_precision: 0.2097\n",
      "  eval_rougeLsum_recall: 0.4148\n",
      "  eval_runtime: 8.7383\n",
      "  eval_samples_per_second: 1.144\n",
      "  eval_sentence_distilroberta_cosine: 66.96356534957886\n",
      "  eval_w2v_cosine: 46.831315755844116\n",
      "  experiment_id: 97f732f367354fb59c02384bff762af3\n",
      "  hostname: ip-172-31-39-35\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.31.39.35\n",
      "  objective: 126.92808110542298\n",
      "  pid: 3880\n",
      "  time_since_restore: 66.68461489677429\n",
      "  time_this_iter_s: 66.68461489677429\n",
      "  time_total_s: 66.68461489677429\n",
      "  timestamp: 1612967040\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '40086_00001'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              {'loss': 2.9003, 'learning_rate': 1e-05, 'epoch': 0.32}\n",
      "                                              {'loss': 4.246, 'learning_rate': 1e-05, 'epoch': 0.4}\n",
      "                                              {'loss': 3.075, 'learning_rate': 1e-05, 'epoch': 0.48}\n",
      " 60%|██████    | 6/10 [00:13<00:08,  2.08s/it][INFO|trainer.py:1612] 2021-02-10 14:24:03,416 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1613] 2021-02-10 14:24:03,416 >>   Num examples = 10\n",
      "[INFO|trainer.py:1614] 2021-02-10 14:24:03,416 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:02,  2.73it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  2.11it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.65it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.58it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.51it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:04<00:02,  1.45it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:05<00:01,  1.36it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  1.31it/s]\u001b[A\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 113.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 113.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 123.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 143.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.61it/s]\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collected 64 word types from a corpus of 85 raw words and 2 sentences\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 64 unique words (100% of original 64, drops 0)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 85 word corpus (100% of original 85, drops 0)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 64 items\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 64 most-common words\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   downsampling leaves estimated 26 word corpus (30.8% of prior 85)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   estimated required memory for 64 words and 100 dimensions: 83200 bytes\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 64 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 85 raw words (30 effective words) took 0.0s, 53725 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 85 raw words (30 effective words) took 0.0s, 50499 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 85 raw words (32 effective words) took 0.0s, 48807 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 85 raw words (28 effective words) took 0.0s, 118472 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 85 raw words (24 effective words) took 0.0s, 51407 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training on a 425 raw words (144 effective words) took 0.0s, 17157 effective words/s\n",
      "02/10/2021 14:24:11 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collected 48 word types from a corpus of 62 raw words and 2 sentences\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 48 unique words (100% of original 48, drops 0)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 62 word corpus (100% of original 62, drops 0)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 48 items\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 48 most-common words\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   downsampling leaves estimated 16 word corpus (26.3% of prior 62)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   estimated required memory for 48 words and 100 dimensions: 62400 bytes\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 48 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 62 raw words (20 effective words) took 0.0s, 45013 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 62 raw words (13 effective words) took 0.0s, 39222 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 62 raw words (14 effective words) took 0.0s, 29811 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 62 raw words (20 effective words) took 0.0s, 25302 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 62 raw words (17 effective words) took 0.0s, 38554 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training on a 310 raw words (84 effective words) took 0.0s, 10754 effective words/s\n",
      "02/10/2021 14:24:11 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collected 39 word types from a corpus of 69 raw words and 2 sentences\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 39 unique words (100% of original 39, drops 0)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 69 word corpus (100% of original 69, drops 0)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 39 items\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 39 most-common words\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   downsampling leaves estimated 15 word corpus (22.7% of prior 69)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   estimated required memory for 39 words and 100 dimensions: 50700 bytes\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 39 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 69 raw words (19 effective words) took 0.0s, 52336 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 69 raw words (13 effective words) took 0.0s, 18507 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 69 raw words (15 effective words) took 0.0s, 44223 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 69 raw words (15 effective words) took 0.0s, 29284 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 69 raw words (16 effective words) took 0.0s, 33264 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training on a 345 raw words (78 effective words) took 0.0s, 10012 effective words/s\n",
      "02/10/2021 14:24:11 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collected 86 word types from a corpus of 133 raw words and 2 sentences\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 86 unique words (100% of original 86, drops 0)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 133 word corpus (100% of original 133, drops 0)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 86 items\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 86 most-common words\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   downsampling leaves estimated 48 word corpus (36.5% of prior 133)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   estimated required memory for 86 words and 100 dimensions: 111800 bytes\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 86 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 133 raw words (55 effective words) took 0.0s, 87258 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 133 raw words (41 effective words) took 0.0s, 106135 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 133 raw words (55 effective words) took 0.0s, 88532 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 133 raw words (42 effective words) took 0.0s, 81945 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 133 raw words (45 effective words) took 0.0s, 81204 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training on a 665 raw words (238 effective words) took 0.0s, 23586 effective words/s\r\n",
      "02/10/2021 14:24:11 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collected 33 word types from a corpus of 50 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 33 unique words (100% of original 33, drops 0)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 50 word corpus (100% of original 50, drops 0)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 33 items\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 33 most-common words\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   downsampling leaves estimated 10 word corpus (20.9% of prior 50)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   estimated required memory for 33 words and 100 dimensions: 42900 bytes\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 33 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 50 raw words (12 effective words) took 0.0s, 26571 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 50 raw words (9 effective words) took 0.0s, 17906 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 50 raw words (5 effective words) took 0.0s, 8862 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 50 raw words (10 effective words) took 0.0s, 38652 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 50 raw words (10 effective words) took 0.0s, 21939 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training on a 250 raw words (46 effective words) took 0.0s, 5931 effective words/s\r\n",
      "02/10/2021 14:24:11 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collected 39 word types from a corpus of 99 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 39 unique words (100% of original 39, drops 0)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 99 word corpus (100% of original 99, drops 0)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 39 items\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 39 most-common words\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   downsampling leaves estimated 22 word corpus (22.9% of prior 99)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   estimated required memory for 39 words and 100 dimensions: 50700 bytes\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 39 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 99 raw words (26 effective words) took 0.0s, 54625 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 99 raw words (24 effective words) took 0.0s, 53368 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 99 raw words (22 effective words) took 0.0s, 46719 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 99 raw words (27 effective words) took 0.0s, 52370 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 99 raw words (26 effective words) took 0.0s, 51175 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training on a 495 raw words (125 effective words) took 0.0s, 15941 effective words/s\r\n",
      "02/10/2021 14:24:11 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collected 43 word types from a corpus of 60 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 43 unique words (100% of original 43, drops 0)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 60 word corpus (100% of original 60, drops 0)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 43 items\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 43 most-common words\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   downsampling leaves estimated 14 word corpus (24.5% of prior 60)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   estimated required memory for 43 words and 100 dimensions: 55900 bytes\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   resetting layer weights\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 43 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 60 raw words (18 effective words) took 0.0s, 41142 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 60 raw words (16 effective words) took 0.0s, 21324 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 60 raw words (15 effective words) took 0.0s, 30549 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 60 raw words (15 effective words) took 0.0s, 26588 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 60 raw words (16 effective words) took 0.0s, 38669 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training on a 300 raw words (80 effective words) took 0.0s, 9294 effective words/s\r\n",
      "02/10/2021 14:24:11 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collected 48 word types from a corpus of 68 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 48 unique words (100% of original 48, drops 0)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 68 word corpus (100% of original 68, drops 0)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 48 items\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 48 most-common words\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   downsampling leaves estimated 17 word corpus (26.1% of prior 68)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   estimated required memory for 48 words and 100 dimensions: 62400 bytes\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 48 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 68 raw words (20 effective words) took 0.0s, 36541 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 68 raw words (16 effective words) took 0.0s, 31513 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 68 raw words (16 effective words) took 0.0s, 37370 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 68 raw words (20 effective words) took 0.0s, 42799 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 68 raw words (17 effective words) took 0.0s, 32657 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training on a 340 raw words (89 effective words) took 0.0s, 11288 effective words/s\r\n",
      "02/10/2021 14:24:11 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collected 42 word types from a corpus of 57 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 42 unique words (100% of original 42, drops 0)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 57 word corpus (100% of original 57, drops 0)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 42 items\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 42 most-common words\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   downsampling leaves estimated 13 word corpus (24.3% of prior 57)\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   estimated required memory for 42 words and 100 dimensions: 54600 bytes\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 42 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 57 raw words (15 effective words) took 0.0s, 34090 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 57 raw words (12 effective words) took 0.0s, 17688 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 57 raw words (11 effective words) took 0.0s, 16621 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 57 raw words (14 effective words) took 0.0s, 29904 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 57 raw words (12 effective words) took 0.0s, 36343 effective words/s\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training on a 285 raw words (64 effective words) took 0.0s, 8161 effective words/s\r\n",
      "02/10/2021 14:24:11 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   collected 50 word types from a corpus of 69 raw words and 2 sentences\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 50 unique words (100% of original 50, drops 0)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 69 word corpus (100% of original 69, drops 0)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 50 items\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 50 most-common words\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   downsampling leaves estimated 18 word corpus (26.7% of prior 69)\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   estimated required memory for 50 words and 100 dimensions: 65000 bytes\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 50 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 69 raw words (21 effective words) took 0.0s, 46967 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 69 raw words (16 effective words) took 0.0s, 33853 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 69 raw words (16 effective words) took 0.0s, 27727 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 69 raw words (14 effective words) took 0.0s, 46480 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 69 raw words (16 effective words) took 0.0s, 23059 effective words/s\n",
      "02/10/2021 14:24:11 - INFO - gensim.models.base_any2vec -   training on a 345 raw words (83 effective words) took 0.0s, 10759 effective words/s\n",
      "02/10/2021 14:24:11 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "                                              {'eval_loss': 2.611384391784668, 'eval_rouge1_precision': 0.2807, 'eval_rouge1_recall': 0.5127, 'eval_rouge1_fmeasure': 0.3407, 'eval_rouge2_precision': 0.1523, 'eval_rouge2_recall': 0.2775, 'eval_rouge2_fmeasure': 0.1888, 'eval_rougeL_precision': 0.2226, 'eval_rougeL_recall': 0.4109, 'eval_rougeL_fmeasure': 0.2742, 'eval_rougeLsum_precision': 0.2496, 'eval_rougeLsum_recall': 0.4643, 'eval_rougeLsum_fmeasure': 0.3059, 'eval_gen_len': 71.2, 'eval_sentence_distilroberta_cosine': 68.75938177108765, 'eval_w2v_cosine': 47.66811430454254, 'eval_runtime': 8.3634, 'eval_samples_per_second': 1.196, 'epoch': 0.48}\n",
      "\n",
      " 60%|██████    | 6/10 [00:22<00:08,  2.08s/it] \n",
      "100%|██████████| 10/10 [00:07<00:00,  1.29it/s]\u001b[A\n",
      "                                               \u001b[A2021-02-10 14:24:11,781\tWARNING util.py:142 -- The `process_trial` operation took 9.737 s, which may be a performance bottleneck.\n",
      "== Status ==\n",
      "Memory usage on this node: 10.6/15.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 1/1 GPUs, 0.0/5.96 GiB heap, 0.0/2.05 GiB objects (0/1.0 accelerator_type:T4)\n",
      "Result logdir: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed\n",
      "Number of trials: 3/4 (1 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+--------------------+------------+-------------------+-------------------------------+-----------------+-------------+\n",
      "| Trial name         | status     | loc               |   gradient_accumulation_steps |   learning_rate |   objective |\n",
      "|--------------------+------------+-------------------+-------------------------------+-----------------+-------------|\n",
      "| _inner_40086_00001 | RUNNING    | 172.31.39.35:3880 |                             4 |           1e-05 |     126.928 |\n",
      "| _inner_40086_00002 | PENDING    |                   |                             2 |           5e-05 |             |\n",
      "| _inner_40086_00000 | TERMINATED |                   |                             2 |           1e-05 |     125.694 |\n",
      "+--------------------+------------+-------------------+-------------------------------+-----------------+-------------+\n",
      "\n",
      "\n",
      "Result for _inner_40086_00001:\n",
      "  date: 2021-02-10_14-24-11\n",
      "  done: false\n",
      "  epoch: 0.48\n",
      "  eval_gen_len: 71.2\n",
      "  eval_loss: 2.611384391784668\n",
      "  eval_rouge1_fmeasure: 0.3407\n",
      "  eval_rouge1_precision: 0.2807\n",
      "  eval_rouge1_recall: 0.5127\n",
      "  eval_rouge2_fmeasure: 0.1888\n",
      "  eval_rouge2_precision: 0.1523\n",
      "  eval_rouge2_recall: 0.2775\n",
      "  eval_rougeL_fmeasure: 0.2742\n",
      "  eval_rougeL_precision: 0.2226\n",
      "  eval_rougeL_recall: 0.4109\n",
      "  eval_rougeLsum_fmeasure: 0.3059\n",
      "  eval_rougeLsum_precision: 0.2496\n",
      "  eval_rougeLsum_recall: 0.4643\n",
      "  eval_runtime: 8.3634\n",
      "  eval_samples_per_second: 1.196\n",
      "  eval_sentence_distilroberta_cosine: 68.75938177108765\n",
      "  eval_w2v_cosine: 47.66811430454254\n",
      "  experiment_id: 97f732f367354fb59c02384bff762af3\n",
      "  hostname: ip-172-31-39-35\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.31.39.35\n",
      "  objective: 129.66709607563018\n",
      "  pid: 3880\n",
      "  time_since_restore: 77.71921110153198\n",
      "  time_this_iter_s: 11.03459620475769\n",
      "  time_total_s: 77.71921110153198\n",
      "  timestamp: 1612967051\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: '40086_00001'\n",
      "  \n",
      "                                              {'loss': 4.0621, 'learning_rate': 1e-05, 'epoch': 0.56}\n",
      "                                              {'loss': 3.1664, 'learning_rate': 1e-05, 'epoch': 0.64}\n",
      "                                              {'loss': 2.7878, 'learning_rate': 1e-05, 'epoch': 0.72}\n",
      " 90%|█████████ | 9/10 [00:23<00:02,  2.27s/it][INFO|trainer.py:1612] 2021-02-10 14:24:13,388 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1613] 2021-02-10 14:24:13,388 >>   Num examples = 10\n",
      "[INFO|trainer.py:1614] 2021-02-10 14:24:13,388 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:03,  2.52it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  1.87it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.58it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.51it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.47it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:04<00:02,  1.37it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:05<00:01,  1.30it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  1.25it/s]\u001b[A\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 144.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 144.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 144.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.94it/s]\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   collected 64 word types from a corpus of 85 raw words and 2 sentences\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 64 unique words (100% of original 64, drops 0)\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 85 word corpus (100% of original 85, drops 0)\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 64 items\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 64 most-common words\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   downsampling leaves estimated 26 word corpus (30.8% of prior 85)\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   estimated required memory for 64 words and 100 dimensions: 83200 bytes\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 64 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 85 raw words (30 effective words) took 0.0s, 60264 effective words/s\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 85 raw words (30 effective words) took 0.0s, 32827 effective words/s\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 85 raw words (32 effective words) took 0.0s, 41780 effective words/s\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 85 raw words (28 effective words) took 0.0s, 62453 effective words/s\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 85 raw words (24 effective words) took 0.0s, 46908 effective words/s\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   training on a 425 raw words (144 effective words) took 0.0s, 17026 effective words/s\n",
      "02/10/2021 14:24:21 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   collected 55 word types from a corpus of 71 raw words and 2 sentences\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 55 unique words (100% of original 55, drops 0)\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 71 word corpus (100% of original 71, drops 0)\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 55 items\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 55 most-common words\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   downsampling leaves estimated 20 word corpus (28.5% of prior 71)\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   estimated required memory for 55 words and 100 dimensions: 71500 bytes\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 55 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 71 raw words (22 effective words) took 0.0s, 49150 effective words/s\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 71 raw words (25 effective words) took 0.0s, 38100 effective words/s\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 71 raw words (13 effective words) took 0.0s, 29167 effective words/s\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 71 raw words (25 effective words) took 0.0s, 109758 effective words/s\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 71 raw words (16 effective words) took 0.0s, 6744 effective words/s\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   training on a 355 raw words (101 effective words) took 0.0s, 8144 effective words/s\r\n",
      "02/10/2021 14:24:21 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   collected 43 word types from a corpus of 71 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 43 unique words (100% of original 43, drops 0)\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 71 word corpus (100% of original 71, drops 0)\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 43 items\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 43 most-common words\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   downsampling leaves estimated 17 word corpus (24.2% of prior 71)\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   estimated required memory for 43 words and 100 dimensions: 55900 bytes\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 43 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 71 raw words (24 effective words) took 0.0s, 50032 effective words/s\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 71 raw words (18 effective words) took 0.0s, 38067 effective words/s\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 71 raw words (20 effective words) took 0.0s, 37764 effective words/s\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 71 raw words (18 effective words) took 0.0s, 37251 effective words/s\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 71 raw words (10 effective words) took 0.0s, 28887 effective words/s\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   training on a 355 raw words (90 effective words) took 0.0s, 10689 effective words/s\r\n",
      "02/10/2021 14:24:21 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   collected 79 word types from a corpus of 134 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 79 unique words (100% of original 79, drops 0)\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 134 word corpus (100% of original 134, drops 0)\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 79 items\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 79 most-common words\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   downsampling leaves estimated 46 word corpus (34.6% of prior 134)\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   estimated required memory for 79 words and 100 dimensions: 102700 bytes\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 79 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 134 raw words (52 effective words) took 0.0s, 107014 effective words/s\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:21 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 134 raw words (57 effective words) took 0.0s, 65028 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 134 raw words (40 effective words) took 0.0s, 67278 effective words/s\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 134 raw words (46 effective words) took 0.0s, 69157 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 134 raw words (40 effective words) took 0.0s, 81875 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   training on a 670 raw words (235 effective words) took 0.0s, 18249 effective words/s\r\n",
      "02/10/2021 14:24:22 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   collected 31 word types from a corpus of 51 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 31 unique words (100% of original 31, drops 0)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 51 word corpus (100% of original 51, drops 0)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 31 items\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 31 most-common words\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   downsampling leaves estimated 10 word corpus (20.1% of prior 51)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   estimated required memory for 31 words and 100 dimensions: 40300 bytes\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 31 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 51 raw words (12 effective words) took 0.0s, 26317 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 51 raw words (8 effective words) took 0.0s, 15745 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 51 raw words (9 effective words) took 0.0s, 14910 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 51 raw words (6 effective words) took 0.0s, 13513 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 51 raw words (11 effective words) took 0.0s, 22366 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   training on a 255 raw words (46 effective words) took 0.0s, 4186 effective words/s\r\n",
      "02/10/2021 14:24:22 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   collected 40 word types from a corpus of 101 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 40 unique words (100% of original 40, drops 0)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 101 word corpus (100% of original 101, drops 0)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 40 items\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 40 most-common words\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   downsampling leaves estimated 23 word corpus (23.2% of prior 101)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   estimated required memory for 40 words and 100 dimensions: 52000 bytes\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 40 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 101 raw words (28 effective words) took 0.0s, 73582 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 101 raw words (21 effective words) took 0.0s, 42073 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 101 raw words (22 effective words) took 0.0s, 28251 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 101 raw words (23 effective words) took 0.0s, 38837 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 101 raw words (22 effective words) took 0.0s, 26939 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   training on a 505 raw words (116 effective words) took 0.0s, 14706 effective words/s\r\n",
      "02/10/2021 14:24:22 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   collected 44 word types from a corpus of 66 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 44 unique words (100% of original 44, drops 0)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 66 word corpus (100% of original 66, drops 0)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 44 items\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 44 most-common words\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   downsampling leaves estimated 16 word corpus (24.7% of prior 66)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   estimated required memory for 44 words and 100 dimensions: 57200 bytes\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   resetting layer weights\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 44 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 66 raw words (21 effective words) took 0.0s, 46399 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 66 raw words (12 effective words) took 0.0s, 22983 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 66 raw words (19 effective words) took 0.0s, 26002 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 66 raw words (20 effective words) took 0.0s, 28820 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 66 raw words (22 effective words) took 0.0s, 30563 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   training on a 330 raw words (94 effective words) took 0.0s, 11412 effective words/s\r\n",
      "02/10/2021 14:24:22 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   collected 51 word types from a corpus of 70 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 51 unique words (100% of original 51, drops 0)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 70 word corpus (100% of original 70, drops 0)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 51 items\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 51 most-common words\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   downsampling leaves estimated 18 word corpus (27.0% of prior 70)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   estimated required memory for 51 words and 100 dimensions: 66300 bytes\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 51 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 70 raw words (24 effective words) took 0.0s, 45296 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 70 raw words (18 effective words) took 0.0s, 24440 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 70 raw words (22 effective words) took 0.0s, 62359 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 70 raw words (22 effective words) took 0.0s, 31473 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 70 raw words (18 effective words) took 0.0s, 37509 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   training on a 350 raw words (104 effective words) took 0.0s, 10850 effective words/s\r\n",
      "02/10/2021 14:24:22 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   collected 44 word types from a corpus of 59 raw words and 2 sentences\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 44 unique words (100% of original 44, drops 0)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 59 word corpus (100% of original 59, drops 0)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 44 items\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 44 most-common words\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   downsampling leaves estimated 14 word corpus (25.0% of prior 59)\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   estimated required memory for 44 words and 100 dimensions: 57200 bytes\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 44 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 59 raw words (17 effective words) took 0.0s, 35785 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 59 raw words (17 effective words) took 0.0s, 33846 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 59 raw words (9 effective words) took 0.0s, 19109 effective words/s\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 59 raw words (14 effective words) took 0.0s, 30796 effective words/s\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 59 raw words (19 effective words) took 0.0s, 27262 effective words/s\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   training on a 295 raw words (76 effective words) took 0.0s, 9478 effective words/s\n",
      "02/10/2021 14:24:22 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   collected 53 word types from a corpus of 71 raw words and 2 sentences\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 53 unique words (100% of original 53, drops 0)\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 71 word corpus (100% of original 71, drops 0)\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 53 items\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 53 most-common words\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   downsampling leaves estimated 19 word corpus (27.6% of prior 71)\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   estimated required memory for 53 words and 100 dimensions: 68900 bytes\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 53 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 71 raw words (25 effective words) took 0.0s, 52116 effective words/s\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 71 raw words (21 effective words) took 0.0s, 37358 effective words/s\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 71 raw words (16 effective words) took 0.0s, 32909 effective words/s\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 71 raw words (21 effective words) took 0.0s, 44011 effective words/s\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 71 raw words (24 effective words) took 0.0s, 33908 effective words/s\n",
      "02/10/2021 14:24:22 - INFO - gensim.models.base_any2vec -   training on a 355 raw words (107 effective words) took 0.0s, 13612 effective words/s\n",
      "02/10/2021 14:24:22 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "                                              {'eval_loss': 2.380706787109375, 'eval_rouge1_precision': 0.2728, 'eval_rouge1_recall': 0.5053, 'eval_rouge1_fmeasure': 0.3296, 'eval_rouge2_precision': 0.1563, 'eval_rouge2_recall': 0.2676, 'eval_rouge2_fmeasure': 0.1825, 'eval_rougeL_precision': 0.231, 'eval_rougeL_recall': 0.4178, 'eval_rougeL_fmeasure': 0.2781, 'eval_rougeLsum_precision': 0.2509, 'eval_rougeLsum_recall': 0.4577, 'eval_rougeLsum_fmeasure': 0.3, 'eval_gen_len': 75.2, 'eval_sentence_distilroberta_cosine': 69.51743364334106, 'eval_w2v_cosine': 48.50805401802063, 'eval_runtime': 8.7371, 'eval_samples_per_second': 1.145, 'epoch': 0.72}\n",
      "\n",
      " 90%|█████████ | 9/10 [00:32<00:02,  2.27s/it] \n",
      "100%|██████████| 10/10 [00:08<00:00,  1.22it/s]\u001b[A\n",
      "                                               \u001b[A2021-02-10 14:24:22,127\tWARNING util.py:142 -- The `process_trial` operation took 10.071 s, which may be a performance bottleneck.\n",
      "== Status ==\n",
      "Memory usage on this node: 10.6/15.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 1/1 GPUs, 0.0/5.96 GiB heap, 0.0/2.05 GiB objects (0/1.0 accelerator_type:T4)\n",
      "Result logdir: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed\n",
      "Number of trials: 3/4 (1 PENDING, 1 RUNNING, 1 TERMINATED)\n",
      "+--------------------+------------+-------------------+-------------------------------+-----------------+-------------+\n",
      "| Trial name         | status     | loc               |   gradient_accumulation_steps |   learning_rate |   objective |\n",
      "|--------------------+------------+-------------------+-------------------------------+-----------------+-------------|\n",
      "| _inner_40086_00001 | RUNNING    | 172.31.39.35:3880 |                             4 |           1e-05 |     129.667 |\n",
      "| _inner_40086_00002 | PENDING    |                   |                             2 |           5e-05 |             |\n",
      "| _inner_40086_00000 | TERMINATED |                   |                             2 |           1e-05 |     125.694 |\n",
      "+--------------------+------------+-------------------+-------------------------------+-----------------+-------------+\n",
      "\n",
      "\n",
      "Result for _inner_40086_00001:\n",
      "  date: 2021-02-10_14-24-22\n",
      "  done: false\n",
      "  epoch: 0.72\n",
      "  eval_gen_len: 75.2\n",
      "  eval_loss: 2.380706787109375\n",
      "  eval_rouge1_fmeasure: 0.3296\n",
      "  eval_rouge1_precision: 0.2728\n",
      "  eval_rouge1_recall: 0.5053\n",
      "  eval_rouge2_fmeasure: 0.1825\n",
      "  eval_rouge2_precision: 0.1563\n",
      "  eval_rouge2_recall: 0.2676\n",
      "  eval_rougeL_fmeasure: 0.2781\n",
      "  eval_rougeL_precision: 0.231\n",
      "  eval_rougeL_recall: 0.4178\n",
      "  eval_rougeLsum_fmeasure: 0.3\n",
      "  eval_rougeLsum_precision: 0.2509\n",
      "  eval_rougeLsum_recall: 0.4577\n",
      "  eval_runtime: 8.7371\n",
      "  eval_samples_per_second: 1.145\n",
      "  eval_sentence_distilroberta_cosine: 69.51743364334106\n",
      "  eval_w2v_cosine: 48.50805401802063\n",
      "  experiment_id: 97f732f367354fb59c02384bff762af3\n",
      "  hostname: ip-172-31-39-35\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.31.39.35\n",
      "  objective: 131.5571876613617\n",
      "  pid: 3880\n",
      "  time_since_restore: 88.06511664390564\n",
      "  time_this_iter_s: 10.345905542373657\n",
      "  time_total_s: 88.06511664390564\n",
      "  timestamp: 1612967062\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: '40086_00001'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               {'loss': 3.4555, 'learning_rate': 1e-05, 'epoch': 0.8}\n",
      "{'train_runtime': 61.7303, 'train_samples_per_second': 0.162, 'epoch': 0.8}\n",
      "100%|██████████| 10/10 [00:33<00:00,  4.43s/it][INFO|trainer.py:1007] 2021-02-10 14:24:22,856 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:33<00:00,  3.31s/it]\n",
      "Result for _inner_40086_00001:\n",
      "  date: 2021-02-10_14-24-22\n",
      "  done: true\n",
      "  epoch: 0.72\n",
      "  eval_gen_len: 75.2\n",
      "  eval_loss: 2.380706787109375\n",
      "  eval_rouge1_fmeasure: 0.3296\n",
      "  eval_rouge1_precision: 0.2728\n",
      "  eval_rouge1_recall: 0.5053\n",
      "  eval_rouge2_fmeasure: 0.1825\n",
      "  eval_rouge2_precision: 0.1563\n",
      "  eval_rouge2_recall: 0.2676\n",
      "  eval_rougeL_fmeasure: 0.2781\n",
      "  eval_rougeL_precision: 0.231\n",
      "  eval_rougeL_recall: 0.4178\n",
      "  eval_rougeLsum_fmeasure: 0.3\n",
      "  eval_rougeLsum_precision: 0.2509\n",
      "  eval_rougeLsum_recall: 0.4577\n",
      "  eval_runtime: 8.7371\n",
      "  eval_samples_per_second: 1.145\n",
      "  eval_sentence_distilroberta_cosine: 69.51743364334106\n",
      "  eval_w2v_cosine: 48.50805401802063\n",
      "  experiment_id: 97f732f367354fb59c02384bff762af3\n",
      "  experiment_tag: 1_gradient_accumulation_steps=4,learning_rate=1e-05\n",
      "  hostname: ip-172-31-39-35\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.31.39.35\n",
      "  objective: 131.5571876613617\n",
      "  pid: 3880\n",
      "  time_since_restore: 88.06511664390564\n",
      "  time_this_iter_s: 10.345905542373657\n",
      "  time_total_s: 88.06511664390564\n",
      "  timestamp: 1612967062\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: '40086_00001'\n",
      "  \n",
      "[INFO|modeling_utils.py:1027] 2021-02-10 14:24:24,979 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1143] 2021-02-10 14:24:45,129 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-10 14:24:45,129 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/10/2021 14:24:45 - INFO - utils -   setting model.config to task specific params for summarization:\n",
      " {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "02/10/2021 14:24:45 - INFO - utils -   note: command line args may override some of these\n",
      "[INFO|trainer.py:837] 2021-02-10 14:24:45,446 >> ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-10 14:24:45,446 >>   Num examples = 50\n",
      "[INFO|trainer.py:839] 2021-02-10 14:24:45,446 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:840] 2021-02-10 14:24:45,446 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:841] 2021-02-10 14:24:45,446 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "[INFO|trainer.py:842] 2021-02-10 14:24:45,446 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:843] 2021-02-10 14:24:45,446 >>   Total optimization steps = 10\n",
      "[INFO|integrations.py:565] 2021-02-10 14:24:45,452 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 5322\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed/_inner_40086_00000_0_gradient_accumulation_steps=2,learning_rate=1e-05_2021-02-10_14-21-22/wandb/run-20210210_142321-1iptf7lr/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed/_inner_40086_00000_0_gradient_accumulation_steps=2,learning_rate=1e-05_2021-02-10_14-21-22/wandb/run-20210210_142321-1iptf7lr/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 3.4555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612967062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 2.38071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 0.2728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 0.5053\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 0.3296\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 0.1563\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 0.2676\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 0.1825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 0.231\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 0.4178\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 0.2781\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 0.2509\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 0.4577\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 75.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 69.51743\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/w2v_cosine 48.50805\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/runtime 8.7371\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/samples_per_second 1.145\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/train_runtime 61.7303\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_samples_per_second 0.162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos 14352878592000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss █▃▂▁▄▂▄▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁▃▃▄▆▆▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁▃▃▄▆▆▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss █▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision ▁█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall ▁█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure ▁█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision ▁▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall ▁█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure ▁█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision ▁▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall ▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure ▁▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision ▁██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall ▁█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure ▁█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len ▇▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine ▁▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/w2v_cosine ▁▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/runtime █▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/samples_per_second ▁█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/home/ubuntu/s3/fine-tuning/hps_bart_para_wordembed\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hps_bart_para_wordembed/runs/1iptf7lr\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/hps_bart_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hps_bart_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hps_bart_para_wordembed/runs/3b3ncyu5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed/_inner_40086_00000_0_gradient_accumulation_steps=2,learning_rate=1e-05_2021-02-10_14-21-22/wandb/run-20210210_142445-3b3ncyu5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      " 10%|█         | 1/10 [00:01<00:11,  1.27s/it]{'loss': 5.5252, 'learning_rate': 5e-05, 'epoch': 0.04}\n",
      " 20%|██        | 2/10 [00:01<00:07,  1.04it/s]{'loss': 6.9941, 'learning_rate': 5e-05, 'epoch': 0.08}\n",
      "                                              {'loss': 3.3514, 'learning_rate': 5e-05, 'epoch': 0.12}\n",
      " 30%|███       | 3/10 [00:01<00:05,  1.30it/s][INFO|trainer.py:1612] 2021-02-10 14:25:14,700 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1613] 2021-02-10 14:25:14,701 >>   Num examples = 10\n",
      "[INFO|trainer.py:1614] 2021-02-10 14:25:14,701 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:02,  2.72it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  2.09it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.69it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.60it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.53it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:04<00:02,  1.34it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:05<00:01,  1.31it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  1.29it/s]\u001b[A\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 114.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 121.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 121.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.32it/s]\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   collected 58 word types from a corpus of 76 raw words and 2 sentences\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 58 unique words (100% of original 58, drops 0)\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 76 word corpus (100% of original 76, drops 0)\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 58 items\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 58 most-common words\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   downsampling leaves estimated 22 word corpus (29.1% of prior 76)\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   estimated required memory for 58 words and 100 dimensions: 75400 bytes\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 58 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 76 raw words (24 effective words) took 0.0s, 43513 effective words/s\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 76 raw words (23 effective words) took 0.0s, 26747 effective words/s\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 76 raw words (23 effective words) took 0.0s, 48715 effective words/s\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 76 raw words (26 effective words) took 0.0s, 51206 effective words/s\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 76 raw words (22 effective words) took 0.0s, 33931 effective words/s\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   training on a 380 raw words (118 effective words) took 0.0s, 14121 effective words/s\n",
      "02/10/2021 14:25:22 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   collected 48 word types from a corpus of 62 raw words and 2 sentences\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 48 unique words (100% of original 48, drops 0)\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 62 word corpus (100% of original 62, drops 0)\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 48 items\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 48 most-common words\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   downsampling leaves estimated 16 word corpus (26.3% of prior 62)\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   estimated required memory for 48 words and 100 dimensions: 62400 bytes\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 48 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 62 raw words (20 effective words) took 0.0s, 37585 effective words/s\n",
      "02/10/2021 14:25:22 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 62 raw words (13 effective words) took 0.0s, 23405 effective words/s\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 62 raw words (14 effective words) took 0.0s, 29706 effective words/s\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 62 raw words (20 effective words) took 0.0s, 29585 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 62 raw words (17 effective words) took 0.0s, 34398 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training on a 310 raw words (84 effective words) took 0.0s, 8002 effective words/s\r\n",
      "02/10/2021 14:25:23 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collected 39 word types from a corpus of 69 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 39 unique words (100% of original 39, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 69 word corpus (100% of original 69, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 39 items\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 39 most-common words\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   downsampling leaves estimated 15 word corpus (22.7% of prior 69)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   estimated required memory for 39 words and 100 dimensions: 50700 bytes\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 39 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 69 raw words (19 effective words) took 0.0s, 41693 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 69 raw words (13 effective words) took 0.0s, 30183 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 69 raw words (15 effective words) took 0.0s, 33098 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 69 raw words (15 effective words) took 0.0s, 45906 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 69 raw words (16 effective words) took 0.0s, 23064 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training on a 345 raw words (78 effective words) took 0.0s, 9876 effective words/s\r\n",
      "02/10/2021 14:25:23 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collected 74 word types from a corpus of 128 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 74 unique words (100% of original 74, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 128 word corpus (100% of original 128, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 74 items\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 74 most-common words\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   downsampling leaves estimated 42 word corpus (33.2% of prior 128)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   estimated required memory for 74 words and 100 dimensions: 96200 bytes\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 74 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 128 raw words (46 effective words) took 0.0s, 92647 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 128 raw words (46 effective words) took 0.0s, 87863 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 128 raw words (44 effective words) took 0.0s, 61870 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 128 raw words (41 effective words) took 0.0s, 78670 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 128 raw words (43 effective words) took 0.0s, 55770 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training on a 640 raw words (220 effective words) took 0.0s, 28089 effective words/s\r\n",
      "02/10/2021 14:25:23 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collected 31 word types from a corpus of 49 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 31 unique words (100% of original 31, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 49 word corpus (100% of original 49, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 31 items\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 31 most-common words\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   downsampling leaves estimated 9 word corpus (20.1% of prior 49)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   estimated required memory for 31 words and 100 dimensions: 40300 bytes\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   resetting layer weights\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 31 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 49 raw words (15 effective words) took 0.0s, 31189 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 49 raw words (6 effective words) took 0.0s, 9519 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 49 raw words (13 effective words) took 0.0s, 35432 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 49 raw words (11 effective words) took 0.0s, 23152 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 49 raw words (6 effective words) took 0.0s, 21008 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training on a 245 raw words (51 effective words) took 0.0s, 6529 effective words/s\r\n",
      "02/10/2021 14:25:23 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collected 39 word types from a corpus of 97 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 39 unique words (100% of original 39, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 97 word corpus (100% of original 97, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 39 items\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 39 most-common words\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   downsampling leaves estimated 22 word corpus (22.9% of prior 97)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   estimated required memory for 39 words and 100 dimensions: 50700 bytes\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 39 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 97 raw words (23 effective words) took 0.0s, 51460 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 97 raw words (20 effective words) took 0.0s, 23836 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 97 raw words (24 effective words) took 0.0s, 54140 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 97 raw words (23 effective words) took 0.0s, 49129 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 97 raw words (22 effective words) took 0.0s, 45056 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training on a 485 raw words (112 effective words) took 0.0s, 14442 effective words/s\r\n",
      "02/10/2021 14:25:23 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collected 47 word types from a corpus of 73 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 47 unique words (100% of original 47, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 73 word corpus (100% of original 73, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 47 items\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 47 most-common words\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   downsampling leaves estimated 18 word corpus (25.7% of prior 73)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   estimated required memory for 47 words and 100 dimensions: 61100 bytes\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 47 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 73 raw words (23 effective words) took 0.0s, 51370 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 73 raw words (24 effective words) took 0.0s, 28962 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 73 raw words (11 effective words) took 0.0s, 23835 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 73 raw words (15 effective words) took 0.0s, 29251 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 73 raw words (24 effective words) took 0.0s, 38628 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training on a 365 raw words (97 effective words) took 0.0s, 12230 effective words/s\r\n",
      "02/10/2021 14:25:23 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collected 47 word types from a corpus of 65 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 47 unique words (100% of original 47, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 65 word corpus (100% of original 65, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 47 items\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 47 most-common words\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   downsampling leaves estimated 16 word corpus (25.7% of prior 65)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   estimated required memory for 47 words and 100 dimensions: 61100 bytes\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 47 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 65 raw words (18 effective words) took 0.0s, 39698 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 65 raw words (18 effective words) took 0.0s, 40602 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 65 raw words (17 effective words) took 0.0s, 36218 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 65 raw words (16 effective words) took 0.0s, 21744 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 65 raw words (14 effective words) took 0.0s, 22338 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training on a 325 raw words (83 effective words) took 0.0s, 10666 effective words/s\r\n",
      "02/10/2021 14:25:23 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collected 40 word types from a corpus of 54 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 40 unique words (100% of original 40, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 54 word corpus (100% of original 54, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 40 items\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 40 most-common words\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   downsampling leaves estimated 12 word corpus (23.6% of prior 54)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   estimated required memory for 40 words and 100 dimensions: 52000 bytes\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 40 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 54 raw words (14 effective words) took 0.0s, 31667 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 54 raw words (13 effective words) took 0.0s, 29472 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 54 raw words (9 effective words) took 0.0s, 24408 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 54 raw words (12 effective words) took 0.0s, 18683 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 54 raw words (11 effective words) took 0.0s, 29013 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training on a 270 raw words (59 effective words) took 0.0s, 7613 effective words/s\r\n",
      "02/10/2021 14:25:23 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   collected 48 word types from a corpus of 66 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 48 unique words (100% of original 48, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 66 word corpus (100% of original 66, drops 0)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 48 items\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 48 most-common words\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   downsampling leaves estimated 17 word corpus (26.1% of prior 66)\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   estimated required memory for 48 words and 100 dimensions: 62400 bytes\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 48 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 66 raw words (19 effective words) took 0.0s, 43467 effective words/s\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 66 raw words (16 effective words) took 0.0s, 20769 effective words/s\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 66 raw words (18 effective words) took 0.0s, 36915 effective words/s\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 66 raw words (14 effective words) took 0.0s, 26600 effective words/s\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 66 raw words (16 effective words) took 0.0s, 25039 effective words/s\n",
      "02/10/2021 14:25:23 - INFO - gensim.models.base_any2vec -   training on a 330 raw words (83 effective words) took 0.0s, 9704 effective words/s\n",
      "02/10/2021 14:25:23 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "                                              \n",
      "\u001b[A{'eval_loss': 2.6240768432617188, 'eval_rouge1_precision': 0.3115, 'eval_rouge1_recall': 0.5324, 'eval_rouge1_fmeasure': 0.367, 'eval_rouge2_precision': 0.1761, 'eval_rouge2_recall': 0.2781, 'eval_rouge2_fmeasure': 0.2017, 'eval_rougeL_precision': 0.256, 'eval_rougeL_recall': 0.4308, 'eval_rougeL_fmeasure': 0.2965, 'eval_rougeLsum_precision': 0.2817, 'eval_rougeLsum_recall': 0.4838, 'eval_rougeLsum_fmeasure': 0.3325, 'eval_gen_len': 70.8, 'eval_sentence_distilroberta_cosine': 69.83027458190918, 'eval_w2v_cosine': 50.02332925796509, 'eval_runtime': 8.4604, 'eval_samples_per_second': 1.182, 'epoch': 0.12}\n",
      " 30%|███       | 3/10 [00:10<00:05,  1.30it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.27it/s]\u001b[A\n",
      "                                               \u001b[A2021-02-10 14:25:23,996\tWARNING util.py:142 -- The `start_trial` operation took 60.940 s, which may be a performance bottleneck.\n",
      "2021-02-10 14:25:25,760\tWARNING util.py:142 -- The `experiment_checkpoint` operation took 1.764 s, which may be a performance bottleneck.\n",
      "== Status ==\n",
      "Memory usage on this node: 10.6/15.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 1/1 GPUs, 0.0/5.96 GiB heap, 0.0/2.05 GiB objects (0/1.0 accelerator_type:T4)\n",
      "Result logdir: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed\n",
      "Number of trials: 3/4 (1 RUNNING, 2 TERMINATED)\n",
      "+--------------------+------------+-------+-------------------------------+-----------------+-------------+\n",
      "| Trial name         | status     | loc   |   gradient_accumulation_steps |   learning_rate |   objective |\n",
      "|--------------------+------------+-------+-------------------------------+-----------------+-------------|\n",
      "| _inner_40086_00002 | RUNNING    |       |                             2 |           5e-05 |             |\n",
      "| _inner_40086_00000 | TERMINATED |       |                             2 |           1e-05 |     125.694 |\n",
      "| _inner_40086_00001 | TERMINATED |       |                             4 |           1e-05 |     131.557 |\n",
      "+--------------------+------------+-------+-------------------------------+-----------------+-------------+\n",
      "\n",
      "\n",
      "2021-02-10 14:25:25,765\tWARNING ray_trial_executor.py:481 -- Over the last 60 seconds, the Tune event loop has been backlogged processing new results. Consider increasing your period of result reporting to improve performance.\n",
      "Result for _inner_40086_00002:\n",
      "  date: 2021-02-10_14-25-23\n",
      "  done: false\n",
      "  epoch: 0.12\n",
      "  eval_gen_len: 70.8\n",
      "  eval_loss: 2.6240768432617188\n",
      "  eval_rouge1_fmeasure: 0.367\n",
      "  eval_rouge1_precision: 0.3115\n",
      "  eval_rouge1_recall: 0.5324\n",
      "  eval_rouge2_fmeasure: 0.2017\n",
      "  eval_rouge2_precision: 0.1761\n",
      "  eval_rouge2_recall: 0.2781\n",
      "  eval_rougeL_fmeasure: 0.2965\n",
      "  eval_rougeL_precision: 0.256\n",
      "  eval_rougeL_recall: 0.4308\n",
      "  eval_rougeLsum_fmeasure: 0.3325\n",
      "  eval_rougeLsum_precision: 0.2817\n",
      "  eval_rougeLsum_recall: 0.4838\n",
      "  eval_runtime: 8.4604\n",
      "  eval_samples_per_second: 1.182\n",
      "  eval_sentence_distilroberta_cosine: 69.83027458190918\n",
      "  eval_w2v_cosine: 50.02332925796509\n",
      "  experiment_id: f1cc038fa0b04f7dba01792b6f107133\n",
      "  hostname: ip-172-31-39-35\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.31.39.35\n",
      "  objective: 133.44410383987426\n",
      "  pid: 3880\n",
      "  time_since_restore: 59.94325065612793\n",
      "  time_this_iter_s: 59.94325065612793\n",
      "  time_total_s: 59.94325065612793\n",
      "  timestamp: 1612967123\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '40086_00002'\n",
      "  \n",
      "                                              {'loss': 3.8236, 'learning_rate': 5e-05, 'epoch': 0.16}\n",
      "                                              {'loss': 2.0088, 'learning_rate': 5e-05, 'epoch': 0.2}\n",
      "                                              {'loss': 3.4905, 'learning_rate': 5e-05, 'epoch': 0.24}\n",
      " 60%|██████    | 6/10 [00:13<00:08,  2.09s/it][INFO|trainer.py:1612] 2021-02-10 14:25:26,674 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1613] 2021-02-10 14:25:26,674 >>   Num examples = 10\n",
      "[INFO|trainer.py:1614] 2021-02-10 14:25:26,675 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:03,  2.54it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:04,  1.71it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:04,  1.49it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.43it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:04<00:02,  1.37it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:04<00:02,  1.41it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:05<00:01,  1.39it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  1.37it/s]\u001b[A\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.93it/s]\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   collected 53 word types from a corpus of 77 raw words and 2 sentences\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 53 unique words (100% of original 53, drops 0)\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 77 word corpus (100% of original 77, drops 0)\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 53 items\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 53 most-common words\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   downsampling leaves estimated 21 word corpus (27.5% of prior 77)\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   estimated required memory for 53 words and 100 dimensions: 68900 bytes\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 53 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 77 raw words (28 effective words) took 0.0s, 56744 effective words/s\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 77 raw words (20 effective words) took 0.0s, 44170 effective words/s\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 77 raw words (25 effective words) took 0.0s, 51791 effective words/s\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 77 raw words (25 effective words) took 0.0s, 46076 effective words/s\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 77 raw words (17 effective words) took 0.0s, 51945 effective words/s\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   training on a 385 raw words (115 effective words) took 0.0s, 13805 effective words/s\n",
      "02/10/2021 14:25:34 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   collected 48 word types from a corpus of 62 raw words and 2 sentences\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 48 unique words (100% of original 48, drops 0)\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 62 word corpus (100% of original 62, drops 0)\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 48 items\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 48 most-common words\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   downsampling leaves estimated 16 word corpus (26.3% of prior 62)\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   estimated required memory for 48 words and 100 dimensions: 62400 bytes\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 48 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 62 raw words (20 effective words) took 0.0s, 42752 effective words/s\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 62 raw words (13 effective words) took 0.0s, 27305 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 62 raw words (14 effective words) took 0.0s, 53248 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 62 raw words (20 effective words) took 0.0s, 45759 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 62 raw words (17 effective words) took 0.0s, 32693 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   training on a 310 raw words (84 effective words) took 0.0s, 9466 effective words/s\r\n",
      "02/10/2021 14:25:34 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   collected 48 word types from a corpus of 86 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 48 unique words (100% of original 48, drops 0)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 86 word corpus (100% of original 86, drops 0)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 48 items\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 48 most-common words\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   downsampling leaves estimated 22 word corpus (25.8% of prior 86)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   estimated required memory for 48 words and 100 dimensions: 62400 bytes\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 48 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 86 raw words (26 effective words) took 0.0s, 54893 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 86 raw words (16 effective words) took 0.0s, 26786 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 86 raw words (19 effective words) took 0.0s, 35079 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 86 raw words (29 effective words) took 0.0s, 39179 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 86 raw words (26 effective words) took 0.0s, 52870 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   training on a 430 raw words (116 effective words) took 0.0s, 13912 effective words/s\r\n",
      "02/10/2021 14:25:34 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   collected 81 word types from a corpus of 132 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 81 unique words (100% of original 81, drops 0)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 132 word corpus (100% of original 132, drops 0)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 81 items\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 81 most-common words\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   downsampling leaves estimated 45 word corpus (34.7% of prior 132)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   estimated required memory for 81 words and 100 dimensions: 105300 bytes\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 81 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 132 raw words (47 effective words) took 0.0s, 97107 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 132 raw words (37 effective words) took 0.0s, 73272 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 132 raw words (42 effective words) took 0.0s, 72197 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 132 raw words (52 effective words) took 0.0s, 74006 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 132 raw words (49 effective words) took 0.0s, 84990 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   training on a 660 raw words (227 effective words) took 0.0s, 27828 effective words/s\r\n",
      "02/10/2021 14:25:34 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   collected 35 word types from a corpus of 55 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 35 unique words (100% of original 35, drops 0)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 55 word corpus (100% of original 55, drops 0)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 35 items\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 35 most-common words\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   downsampling leaves estimated 11 word corpus (21.7% of prior 55)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   estimated required memory for 35 words and 100 dimensions: 45500 bytes\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 35 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 55 raw words (17 effective words) took 0.0s, 32912 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 55 raw words (13 effective words) took 0.0s, 25370 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 55 raw words (13 effective words) took 0.0s, 37132 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 55 raw words (11 effective words) took 0.0s, 21485 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 55 raw words (11 effective words) took 0.0s, 22288 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   training on a 275 raw words (65 effective words) took 0.0s, 8200 effective words/s\r\n",
      "02/10/2021 14:25:34 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   collected 49 word types from a corpus of 100 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 49 unique words (100% of original 49, drops 0)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 100 word corpus (100% of original 100, drops 0)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 49 items\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 49 most-common words\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   downsampling leaves estimated 25 word corpus (25.8% of prior 100)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   estimated required memory for 49 words and 100 dimensions: 63700 bytes\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 49 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 100 raw words (32 effective words) took 0.0s, 49132 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 100 raw words (25 effective words) took 0.0s, 48052 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 100 raw words (33 effective words) took 0.0s, 67768 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 100 raw words (24 effective words) took 0.0s, 31748 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 100 raw words (28 effective words) took 0.0s, 37362 effective words/s\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   training on a 500 raw words (142 effective words) took 0.0s, 15240 effective words/s\r\n",
      "02/10/2021 14:25:34 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   collected 35 word types from a corpus of 48 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 35 unique words (100% of original 35, drops 0)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 48 word corpus (100% of original 48, drops 0)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 35 items\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 35 most-common words\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   downsampling leaves estimated 10 word corpus (21.8% of prior 48)\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.base_any2vec -   estimated required memory for 35 words and 100 dimensions: 45500 bytes\r\n",
      "02/10/2021 14:25:34 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 35 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 48 raw words (13 effective words) took 0.0s, 28828 effective words/s\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 48 raw words (11 effective words) took 0.0s, 13091 effective words/s\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 48 raw words (10 effective words) took 0.0s, 21584 effective words/s\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 48 raw words (9 effective words) took 0.0s, 31646 effective words/s\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 48 raw words (9 effective words) took 0.0s, 15783 effective words/s\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   training on a 240 raw words (52 effective words) took 0.0s, 5549 effective words/s\r\n",
      "02/10/2021 14:25:35 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   collected 46 word types from a corpus of 73 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 46 unique words (100% of original 46, drops 0)\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 73 word corpus (100% of original 73, drops 0)\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 46 items\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 46 most-common words\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   downsampling leaves estimated 18 word corpus (25.1% of prior 73)\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   estimated required memory for 46 words and 100 dimensions: 59800 bytes\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 46 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 73 raw words (21 effective words) took 0.0s, 46921 effective words/s\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 73 raw words (16 effective words) took 0.0s, 22202 effective words/s\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 73 raw words (18 effective words) took 0.0s, 42510 effective words/s\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 73 raw words (21 effective words) took 0.0s, 30515 effective words/s\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 73 raw words (21 effective words) took 0.0s, 48034 effective words/s\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   training on a 365 raw words (97 effective words) took 0.0s, 12696 effective words/s\r\n",
      "02/10/2021 14:25:35 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   collected 41 word types from a corpus of 62 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 41 unique words (100% of original 41, drops 0)\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 62 word corpus (100% of original 62, drops 0)\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 41 items\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 41 most-common words\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   downsampling leaves estimated 14 word corpus (23.6% of prior 62)\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   estimated required memory for 41 words and 100 dimensions: 53300 bytes\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 41 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 62 raw words (16 effective words) took 0.0s, 34840 effective words/s\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 62 raw words (15 effective words) took 0.0s, 35281 effective words/s\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 62 raw words (12 effective words) took 0.0s, 23023 effective words/s\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 62 raw words (11 effective words) took 0.0s, 33386 effective words/s\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 62 raw words (12 effective words) took 0.0s, 26460 effective words/s\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   training on a 310 raw words (66 effective words) took 0.0s, 8490 effective words/s\r\n",
      "02/10/2021 14:25:35 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   collected 48 word types from a corpus of 74 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 48 unique words (100% of original 48, drops 0)\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 74 word corpus (100% of original 74, drops 0)\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 48 items\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 48 most-common words\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   downsampling leaves estimated 19 word corpus (25.7% of prior 74)\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   estimated required memory for 48 words and 100 dimensions: 62400 bytes\r\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.word2vec -   resetting layer weights\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 48 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 74 raw words (23 effective words) took 0.0s, 47654 effective words/s\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 74 raw words (21 effective words) took 0.0s, 30362 effective words/s\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 74 raw words (19 effective words) took 0.0s, 36499 effective words/s\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 74 raw words (22 effective words) took 0.0s, 43579 effective words/s\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 74 raw words (18 effective words) took 0.0s, 40303 effective words/s\n",
      "02/10/2021 14:25:35 - INFO - gensim.models.base_any2vec -   training on a 370 raw words (103 effective words) took 0.0s, 12758 effective words/s\n",
      "02/10/2021 14:25:35 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "                                              {'eval_loss': 2.4498701095581055, 'eval_rouge1_precision': 0.2557, 'eval_rouge1_recall': 0.4771, 'eval_rouge1_fmeasure': 0.3101, 'eval_rouge2_precision': 0.1149, 'eval_rouge2_recall': 0.227, 'eval_rouge2_fmeasure': 0.1464, 'eval_rougeL_precision': 0.2034, 'eval_rougeL_recall': 0.3864, 'eval_rougeL_fmeasure': 0.2514, 'eval_rougeLsum_precision': 0.2165, 'eval_rougeLsum_recall': 0.416, 'eval_rougeLsum_fmeasure': 0.2676, 'eval_gen_len': 70.6, 'eval_sentence_distilroberta_cosine': 65.0467574596405, 'eval_w2v_cosine': 47.96280264854431, 'eval_runtime': 8.3979, 'eval_samples_per_second': 1.191, 'epoch': 0.24}\n",
      "\u001b[A                                            \n",
      " 60%|██████    | 6/10 [00:22<00:08,  2.09s/it]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.36it/s]\u001b[A\n",
      "                                               \u001b[A2021-02-10 14:25:35,074\tWARNING util.py:142 -- The `process_trial` operation took 9.310 s, which may be a performance bottleneck.\n",
      "== Status ==\n",
      "Memory usage on this node: 10.6/15.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 1/1 GPUs, 0.0/5.96 GiB heap, 0.0/2.05 GiB objects (0/1.0 accelerator_type:T4)\n",
      "Result logdir: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed\n",
      "Number of trials: 4/4 (1 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+--------------------+------------+-------------------+-------------------------------+-----------------+-------------+\n",
      "| Trial name         | status     | loc               |   gradient_accumulation_steps |   learning_rate |   objective |\n",
      "|--------------------+------------+-------------------+-------------------------------+-----------------+-------------|\n",
      "| _inner_40086_00002 | RUNNING    | 172.31.39.35:3880 |                             2 |           5e-05 |     133.444 |\n",
      "| _inner_40086_00003 | PENDING    |                   |                             4 |           5e-05 |             |\n",
      "| _inner_40086_00000 | TERMINATED |                   |                             2 |           1e-05 |     125.694 |\n",
      "| _inner_40086_00001 | TERMINATED |                   |                             4 |           1e-05 |     131.557 |\n",
      "+--------------------+------------+-------------------+-------------------------------+-----------------+-------------+\n",
      "\n",
      "\n",
      "Result for _inner_40086_00002:\n",
      "  date: 2021-02-10_14-25-35\n",
      "  done: false\n",
      "  epoch: 0.24\n",
      "  eval_gen_len: 70.6\n",
      "  eval_loss: 2.4498701095581055\n",
      "  eval_rouge1_fmeasure: 0.3101\n",
      "  eval_rouge1_precision: 0.2557\n",
      "  eval_rouge1_recall: 0.4771\n",
      "  eval_rouge2_fmeasure: 0.1464\n",
      "  eval_rouge2_precision: 0.1149\n",
      "  eval_rouge2_recall: 0.227\n",
      "  eval_rougeL_fmeasure: 0.2514\n",
      "  eval_rougeL_precision: 0.2034\n",
      "  eval_rougeL_recall: 0.3864\n",
      "  eval_rougeLsum_fmeasure: 0.2676\n",
      "  eval_rougeLsum_precision: 0.2165\n",
      "  eval_rougeLsum_recall: 0.416\n",
      "  eval_runtime: 8.3979\n",
      "  eval_samples_per_second: 1.191\n",
      "  eval_sentence_distilroberta_cosine: 65.0467574596405\n",
      "  eval_w2v_cosine: 47.96280264854431\n",
      "  experiment_id: f1cc038fa0b04f7dba01792b6f107133\n",
      "  hostname: ip-172-31-39-35\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.31.39.35\n",
      "  objective: 125.87096010818482\n",
      "  pid: 3880\n",
      "  time_since_restore: 71.85478448867798\n",
      "  time_this_iter_s: 11.911533832550049\n",
      "  time_total_s: 71.85478448867798\n",
      "  timestamp: 1612967135\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: '40086_00002'\n",
      "  \n",
      "                                              {'loss': 1.2309, 'learning_rate': 5e-05, 'epoch': 0.28}\n",
      " 80%|████████  | 8/10 [00:23<00:06,  3.00s/it]{'loss': 3.6911, 'learning_rate': 5e-05, 'epoch': 0.32}\n",
      "                                              {'loss': 4.6439, 'learning_rate': 5e-05, 'epoch': 0.36}\n",
      " 90%|█████████ | 9/10 [00:23<00:02,  2.19s/it][INFO|trainer.py:1612] 2021-02-10 14:25:36,284 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1613] 2021-02-10 14:25:36,284 >>   Num examples = 10\n",
      "[INFO|trainer.py:1614] 2021-02-10 14:25:36,284 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:02,  2.87it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  2.18it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.76it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.59it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.54it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:04<00:02,  1.48it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:05<00:01,  1.48it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:05<00:00,  1.47it/s]\u001b[A\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 113.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 97.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 118.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 143.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 143.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.91it/s]\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   collected 66 word types from a corpus of 88 raw words and 2 sentences\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 66 unique words (100% of original 66, drops 0)\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 88 word corpus (100% of original 88, drops 0)\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 66 items\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 66 most-common words\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   downsampling leaves estimated 27 word corpus (31.3% of prior 88)\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   estimated required memory for 66 words and 100 dimensions: 85800 bytes\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 66 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 88 raw words (30 effective words) took 0.0s, 60322 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 88 raw words (30 effective words) took 0.0s, 60790 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 88 raw words (29 effective words) took 0.0s, 25830 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 88 raw words (25 effective words) took 0.0s, 46542 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 88 raw words (32 effective words) took 0.0s, 68938 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   training on a 440 raw words (146 effective words) took 0.0s, 16965 effective words/s\n",
      "02/10/2021 14:25:43 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   collected 48 word types from a corpus of 62 raw words and 2 sentences\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 48 unique words (100% of original 48, drops 0)\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 62 word corpus (100% of original 62, drops 0)\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 48 items\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 48 most-common words\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   downsampling leaves estimated 16 word corpus (26.3% of prior 62)\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   estimated required memory for 48 words and 100 dimensions: 62400 bytes\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 48 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 62 raw words (20 effective words) took 0.0s, 41245 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 62 raw words (13 effective words) took 0.0s, 18568 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 62 raw words (14 effective words) took 0.0s, 41320 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 62 raw words (20 effective words) took 0.0s, 42072 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 62 raw words (17 effective words) took 0.0s, 33095 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   training on a 310 raw words (84 effective words) took 0.0s, 9789 effective words/s\n",
      "02/10/2021 14:25:43 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   collected 27 word types from a corpus of 59 raw words and 2 sentences\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 27 unique words (100% of original 27, drops 0)\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 59 word corpus (100% of original 59, drops 0)\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 27 items\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 27 most-common words\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   downsampling leaves estimated 10 word corpus (18.3% of prior 59)\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   estimated required memory for 27 words and 100 dimensions: 35100 bytes\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 27 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 59 raw words (12 effective words) took 0.0s, 26617 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 59 raw words (11 effective words) took 0.0s, 39021 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 59 raw words (15 effective words) took 0.0s, 12643 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 59 raw words (6 effective words) took 0.0s, 11828 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 59 raw words (12 effective words) took 0.0s, 17486 effective words/s\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   training on a 295 raw words (56 effective words) took 0.0s, 6733 effective words/s\n",
      "02/10/2021 14:25:43 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   collected 74 word types from a corpus of 113 raw words and 2 sentences\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 74 unique words (100% of original 74, drops 0)\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 113 word corpus (100% of original 113, drops 0)\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 74 items\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 74 most-common words\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   downsampling leaves estimated 37 word corpus (33.3% of prior 113)\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.base_any2vec -   estimated required memory for 74 words and 100 dimensions: 96200 bytes\n",
      "02/10/2021 14:25:43 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 74 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 113 raw words (42 effective words) took 0.0s, 77970 effective words/s\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 113 raw words (34 effective words) took 0.0s, 64944 effective words/s\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 113 raw words (37 effective words) took 0.0s, 73839 effective words/s\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 113 raw words (38 effective words) took 0.0s, 70142 effective words/s\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 113 raw words (36 effective words) took 0.0s, 73492 effective words/s\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   training on a 565 raw words (187 effective words) took 0.0s, 23278 effective words/s\n",
      "02/10/2021 14:25:44 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   collected 35 word types from a corpus of 56 raw words and 2 sentences\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 35 unique words (100% of original 35, drops 0)\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 56 word corpus (100% of original 56, drops 0)\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 35 items\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 35 most-common words\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   downsampling leaves estimated 12 word corpus (21.5% of prior 56)\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   estimated required memory for 35 words and 100 dimensions: 45500 bytes\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 35 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 56 raw words (14 effective words) took 0.0s, 29450 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 56 raw words (13 effective words) took 0.0s, 14541 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 56 raw words (10 effective words) took 0.0s, 13897 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 56 raw words (11 effective words) took 0.0s, 29178 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 56 raw words (12 effective words) took 0.0s, 23160 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   training on a 280 raw words (60 effective words) took 0.0s, 7413 effective words/s\r\n",
      "02/10/2021 14:25:44 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   collected 37 word types from a corpus of 92 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 37 unique words (100% of original 37, drops 0)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 92 word corpus (100% of original 92, drops 0)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 37 items\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 37 most-common words\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   downsampling leaves estimated 20 word corpus (22.2% of prior 92)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   estimated required memory for 37 words and 100 dimensions: 48100 bytes\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 37 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 92 raw words (24 effective words) took 0.0s, 51452 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 92 raw words (26 effective words) took 0.0s, 38421 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 92 raw words (21 effective words) took 0.0s, 47116 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 92 raw words (27 effective words) took 0.0s, 51267 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 92 raw words (16 effective words) took 0.0s, 33432 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   training on a 460 raw words (114 effective words) took 0.0s, 14521 effective words/s\r\n",
      "02/10/2021 14:25:44 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   collected 34 word types from a corpus of 49 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 34 unique words (100% of original 34, drops 0)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 49 word corpus (100% of original 49, drops 0)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 34 items\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 34 most-common words\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   downsampling leaves estimated 10 word corpus (21.3% of prior 49)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   estimated required memory for 34 words and 100 dimensions: 44200 bytes\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 34 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 49 raw words (15 effective words) took 0.0s, 26988 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 49 raw words (12 effective words) took 0.0s, 23785 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 49 raw words (6 effective words) took 0.0s, 12476 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 49 raw words (8 effective words) took 0.0s, 11928 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 49 raw words (9 effective words) took 0.0s, 15977 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   training on a 245 raw words (50 effective words) took 0.0s, 6453 effective words/s\r\n",
      "02/10/2021 14:25:44 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   collected 47 word types from a corpus of 63 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 47 unique words (100% of original 47, drops 0)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 63 word corpus (100% of original 63, drops 0)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 47 items\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 47 most-common words\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   downsampling leaves estimated 16 word corpus (25.6% of prior 63)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   estimated required memory for 47 words and 100 dimensions: 61100 bytes\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   resetting layer weights\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 47 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 63 raw words (23 effective words) took 0.0s, 46555 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 63 raw words (21 effective words) took 0.0s, 29790 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 63 raw words (13 effective words) took 0.0s, 51623 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 63 raw words (12 effective words) took 0.0s, 17681 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 63 raw words (14 effective words) took 0.0s, 26722 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   training on a 315 raw words (83 effective words) took 0.0s, 10471 effective words/s\r\n",
      "02/10/2021 14:25:44 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   collected 42 word types from a corpus of 52 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 42 unique words (100% of original 42, drops 0)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 52 word corpus (100% of original 52, drops 0)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 42 items\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 42 most-common words\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   downsampling leaves estimated 12 word corpus (24.4% of prior 52)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   estimated required memory for 42 words and 100 dimensions: 54600 bytes\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 42 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 52 raw words (15 effective words) took 0.0s, 31503 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 52 raw words (12 effective words) took 0.0s, 38871 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 52 raw words (8 effective words) took 0.0s, 23192 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 52 raw words (12 effective words) took 0.0s, 62785 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 52 raw words (9 effective words) took 0.0s, 18486 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   training on a 260 raw words (56 effective words) took 0.0s, 7324 effective words/s\r\n",
      "02/10/2021 14:25:44 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   collecting all words and their counts\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   collected 48 word types from a corpus of 64 raw words and 2 sentences\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 48 unique words (100% of original 48, drops 0)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 64 word corpus (100% of original 64, drops 0)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 48 items\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 48 most-common words\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   downsampling leaves estimated 16 word corpus (26.1% of prior 64)\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   estimated required memory for 48 words and 100 dimensions: 62400 bytes\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.word2vec -   resetting layer weights\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 48 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 64 raw words (22 effective words) took 0.0s, 47285 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 64 raw words (20 effective words) took 0.0s, 53577 effective words/s\r\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 64 raw words (15 effective words) took 0.0s, 30025 effective words/s\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 64 raw words (13 effective words) took 0.0s, 31503 effective words/s\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 64 raw words (24 effective words) took 0.0s, 29018 effective words/s\n",
      "02/10/2021 14:25:44 - INFO - gensim.models.base_any2vec -   training on a 320 raw words (94 effective words) took 0.0s, 11113 effective words/s\n",
      "02/10/2021 14:25:44 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "                                              {'eval_loss': 2.360079050064087, 'eval_rouge1_precision': 0.3089, 'eval_rouge1_recall': 0.4718, 'eval_rouge1_fmeasure': 0.3456, 'eval_rouge2_precision': 0.1685, 'eval_rouge2_recall': 0.2618, 'eval_rouge2_fmeasure': 0.1891, 'eval_rougeL_precision': 0.2556, 'eval_rougeL_recall': 0.4049, 'eval_rougeL_fmeasure': 0.2916, 'eval_rougeLsum_precision': 0.2733, 'eval_rougeLsum_recall': 0.424, 'eval_rougeLsum_fmeasure': 0.3083, 'eval_gen_len': 63.4, 'eval_sentence_distilroberta_cosine': 69.57169771194458, 'eval_w2v_cosine': 47.86504805088043, 'eval_runtime': 7.8484, 'eval_samples_per_second': 1.274, 'epoch': 0.36}\n",
      "\n",
      " 90%|█████████ | 9/10 [00:31<00:02,  2.19s/it] \n",
      "100%|██████████| 10/10 [00:07<00:00,  1.47it/s]\u001b[A\n",
      "                                               \u001b[A2021-02-10 14:25:44,134\tWARNING util.py:142 -- The `process_trial` operation took 8.737 s, which may be a performance bottleneck.\n",
      "== Status ==\n",
      "Memory usage on this node: 10.6/15.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 1/1 GPUs, 0.0/5.96 GiB heap, 0.0/2.05 GiB objects (0/1.0 accelerator_type:T4)\n",
      "Result logdir: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed\n",
      "Number of trials: 4/4 (1 PENDING, 1 RUNNING, 2 TERMINATED)\n",
      "+--------------------+------------+-------------------+-------------------------------+-----------------+-------------+\n",
      "| Trial name         | status     | loc               |   gradient_accumulation_steps |   learning_rate |   objective |\n",
      "|--------------------+------------+-------------------+-------------------------------+-----------------+-------------|\n",
      "| _inner_40086_00002 | RUNNING    | 172.31.39.35:3880 |                             2 |           5e-05 |     125.871 |\n",
      "| _inner_40086_00003 | PENDING    |                   |                             4 |           5e-05 |             |\n",
      "| _inner_40086_00000 | TERMINATED |                   |                             2 |           1e-05 |     125.694 |\n",
      "| _inner_40086_00001 | TERMINATED |                   |                             4 |           1e-05 |     131.557 |\n",
      "+--------------------+------------+-------------------+-------------------------------+-----------------+-------------+\n",
      "\n",
      "\n",
      "Result for _inner_40086_00002:\n",
      "  date: 2021-02-10_14-25-44\n",
      "  done: false\n",
      "  epoch: 0.36\n",
      "  eval_gen_len: 63.4\n",
      "  eval_loss: 2.360079050064087\n",
      "  eval_rouge1_fmeasure: 0.3456\n",
      "  eval_rouge1_precision: 0.3089\n",
      "  eval_rouge1_recall: 0.4718\n",
      "  eval_rouge2_fmeasure: 0.1891\n",
      "  eval_rouge2_precision: 0.1685\n",
      "  eval_rouge2_recall: 0.2618\n",
      "  eval_rougeL_fmeasure: 0.2916\n",
      "  eval_rougeL_precision: 0.2556\n",
      "  eval_rougeL_recall: 0.4049\n",
      "  eval_rougeLsum_fmeasure: 0.3083\n",
      "  eval_rougeLsum_precision: 0.2733\n",
      "  eval_rougeLsum_recall: 0.424\n",
      "  eval_runtime: 7.8484\n",
      "  eval_samples_per_second: 1.274\n",
      "  eval_sentence_distilroberta_cosine: 69.57169771194458\n",
      "  eval_w2v_cosine: 47.86504805088043\n",
      "  experiment_id: f1cc038fa0b04f7dba01792b6f107133\n",
      "  hostname: ip-172-31-39-35\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.31.39.35\n",
      "  objective: 130.262545762825\n",
      "  pid: 3880\n",
      "  time_since_restore: 80.91458296775818\n",
      "  time_this_iter_s: 9.0597984790802\n",
      "  time_total_s: 80.91458296775818\n",
      "  timestamp: 1612967144\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: '40086_00002'\n",
      "  \n",
      "100%|██████████| 10/10 [00:31<00:00,  3.98s/it]{'loss': 2.9814, 'learning_rate': 5e-05, 'epoch': 0.4}\n",
      "{'train_runtime': 58.9993, 'train_samples_per_second': 0.169, 'epoch': 0.4}\n",
      "100%|██████████| 10/10 [00:31<00:00,  3.98s/it][INFO|trainer.py:1007] 2021-02-10 14:25:44,445 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:31<00:00,  3.16s/it]\n",
      "Result for _inner_40086_00002:\n",
      "  date: 2021-02-10_14-25-44\n",
      "  done: true\n",
      "  epoch: 0.36\n",
      "  eval_gen_len: 63.4\n",
      "  eval_loss: 2.360079050064087\n",
      "  eval_rouge1_fmeasure: 0.3456\n",
      "  eval_rouge1_precision: 0.3089\n",
      "  eval_rouge1_recall: 0.4718\n",
      "  eval_rouge2_fmeasure: 0.1891\n",
      "  eval_rouge2_precision: 0.1685\n",
      "  eval_rouge2_recall: 0.2618\n",
      "  eval_rougeL_fmeasure: 0.2916\n",
      "  eval_rougeL_precision: 0.2556\n",
      "  eval_rougeL_recall: 0.4049\n",
      "  eval_rougeLsum_fmeasure: 0.3083\n",
      "  eval_rougeLsum_precision: 0.2733\n",
      "  eval_rougeLsum_recall: 0.424\n",
      "  eval_runtime: 7.8484\n",
      "  eval_samples_per_second: 1.274\n",
      "  eval_sentence_distilroberta_cosine: 69.57169771194458\n",
      "  eval_w2v_cosine: 47.86504805088043\n",
      "  experiment_id: f1cc038fa0b04f7dba01792b6f107133\n",
      "  experiment_tag: 2_gradient_accumulation_steps=2,learning_rate=5e-05\n",
      "  hostname: ip-172-31-39-35\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.31.39.35\n",
      "  objective: 130.262545762825\n",
      "  pid: 3880\n",
      "  time_since_restore: 80.91458296775818\n",
      "  time_this_iter_s: 9.0597984790802\n",
      "  time_total_s: 80.91458296775818\n",
      "  timestamp: 1612967144\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 3\n",
      "  trial_id: '40086_00002'\n",
      "  \n",
      "[INFO|modeling_utils.py:1027] 2021-02-10 14:25:46,680 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1143] 2021-02-10 14:26:05,526 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-10 14:26:05,526 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/10/2021 14:26:05 - INFO - utils -   setting model.config to task specific params for summarization:\n",
      " {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "02/10/2021 14:26:05 - INFO - utils -   note: command line args may override some of these\n",
      "[INFO|trainer.py:837] 2021-02-10 14:26:05,849 >> ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-10 14:26:05,850 >>   Num examples = 50\n",
      "[INFO|trainer.py:839] 2021-02-10 14:26:05,850 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:840] 2021-02-10 14:26:05,850 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:841] 2021-02-10 14:26:05,850 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:842] 2021-02-10 14:26:05,850 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:843] 2021-02-10 14:26:05,850 >>   Total optimization steps = 10\n",
      "[INFO|integrations.py:565] 2021-02-10 14:26:05,855 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 6148\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed/_inner_40086_00000_0_gradient_accumulation_steps=2,learning_rate=1e-05_2021-02-10_14-21-22/wandb/run-20210210_142445-3b3ncyu5/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed/_inner_40086_00000_0_gradient_accumulation_steps=2,learning_rate=1e-05_2021-02-10_14-21-22/wandb/run-20210210_142445-3b3ncyu5/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 2.9814\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612967144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 2.36008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 0.3089\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 0.4718\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 0.3456\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 0.1685\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 0.2618\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 0.1891\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 0.2556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 0.4049\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 0.2916\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 0.2733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 0.424\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 0.3083\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 63.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 69.5717\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/w2v_cosine 47.86505\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/runtime 7.8484\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/samples_per_second 1.274\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/train_runtime 58.9993\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_samples_per_second 0.169\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos 7667700019200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss ▆█▄▄▂▄▁▄▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁▄▄▄▆▆▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁▄▄▄▆▆▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss █▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision █▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall █▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure █▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision █▁▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall █▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure █▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision █▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall █▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure █▁▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision █▁▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall █▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure █▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len ██▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine █▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/w2v_cosine █▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/runtime █▇▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/samples_per_second ▁▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/home/ubuntu/s3/fine-tuning/hps_bart_para_wordembed\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hps_bart_para_wordembed/runs/3b3ncyu5\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/hps_bart_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hps_bart_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hps_bart_para_wordembed/runs/23iega4g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed/_inner_40086_00000_0_gradient_accumulation_steps=2,learning_rate=1e-05_2021-02-10_14-21-22/wandb/run-20210210_142605-23iega4g\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "                                              {'loss': 6.2597, 'learning_rate': 5e-05, 'epoch': 0.08}\n",
      " 20%|██        | 2/10 [00:01<00:09,  1.19s/it]{'loss': 3.8359, 'learning_rate': 5e-05, 'epoch': 0.16}\n",
      " 20%|██        | 2/10 [00:01<00:09,  1.19s/it]2021-02-10 14:26:33,981\tERROR function_runner.py:254 -- Runner Thread raised error.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
      "    self._status_reporter.get_checkpoint())\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 650, in _inner\n",
      "    inner(config, checkpoint_dir=None)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 644, in inner\n",
      "    fn(config, **fn_kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/integrations.py\", line 179, in _objective\n",
      "    local_trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/trainer.py\", line 940, in train\n",
      "    tr_loss += self.training_step(model, inputs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/trainer.py\", line 1314, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/ubuntu/transformers/examples/seq2seq/seq2seq_trainer.py\", line 180, in compute_loss\n",
      "    loss, _ = self._compute_loss(model, inputs, labels)\n",
      "  File \"/home/ubuntu/transformers/examples/seq2seq/seq2seq_trainer.py\", line 166, in _compute_loss\n",
      "    logits = model(**inputs, use_cache=False)[0]\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 1295, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 1157, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 796, in forward\n",
      "    output_attentions=output_attentions,\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 318, in forward\n",
      "    hidden_states = self.fc2(hidden_states)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 93, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/functional.py\", line 1692, in linear\n",
      "    output = input.matmul(weight.t())\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.70 GiB already allocated; 7.75 MiB free; 13.83 GiB reserved in total by PyTorch)\n",
      "Exception in thread Thread-2001:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 267, in run\n",
      "    raise e\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
      "    self._status_reporter.get_checkpoint())\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 650, in _inner\n",
      "    inner(config, checkpoint_dir=None)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 644, in inner\n",
      "    fn(config, **fn_kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/integrations.py\", line 179, in _objective\n",
      "    local_trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/trainer.py\", line 940, in train\n",
      "    tr_loss += self.training_step(model, inputs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/trainer.py\", line 1314, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/ubuntu/transformers/examples/seq2seq/seq2seq_trainer.py\", line 180, in compute_loss\n",
      "    loss, _ = self._compute_loss(model, inputs, labels)\n",
      "  File \"/home/ubuntu/transformers/examples/seq2seq/seq2seq_trainer.py\", line 166, in _compute_loss\n",
      "    logits = model(**inputs, use_cache=False)[0]\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 1295, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 1157, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 796, in forward\n",
      "    output_attentions=output_attentions,\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 318, in forward\n",
      "    hidden_states = self.fc2(hidden_states)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 93, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/functional.py\", line 1692, in linear\n",
      "    output = input.matmul(weight.t())\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.70 GiB already allocated; 7.75 MiB free; 13.83 GiB reserved in total by PyTorch)\n",
      "\n",
      " 20%|██        | 2/10 [00:02<00:08,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-10 14:26:35,669\tWARNING util.py:142 -- The `start_trial` operation took 51.032 s, which may be a performance bottleneck.\n",
      "== Status ==\n",
      "Memory usage on this node: 11.3/15.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 1/1 GPUs, 0.0/5.96 GiB heap, 0.0/2.05 GiB objects (0/1.0 accelerator_type:T4)\n",
      "Result logdir: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed\n",
      "Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)\n",
      "+--------------------+------------+-------+-------------------------------+-----------------+-------------+\n",
      "| Trial name         | status     | loc   |   gradient_accumulation_steps |   learning_rate |   objective |\n",
      "|--------------------+------------+-------+-------------------------------+-----------------+-------------|\n",
      "| _inner_40086_00003 | RUNNING    |       |                             4 |           5e-05 |             |\n",
      "| _inner_40086_00000 | TERMINATED |       |                             2 |           1e-05 |     125.694 |\n",
      "| _inner_40086_00001 | TERMINATED |       |                             4 |           1e-05 |     131.557 |\n",
      "| _inner_40086_00002 | TERMINATED |       |                             2 |           5e-05 |     130.263 |\n",
      "+--------------------+------------+-------+-------------------------------+-----------------+-------------+\n",
      "\n",
      "\n",
      "2021-02-10 14:26:36,002\tWARNING ray_trial_executor.py:481 -- Over the last 60 seconds, the Tune event loop has been backlogged processing new results. Consider increasing your period of result reporting to improve performance.\n",
      "2021-02-10 14:26:36,003\tERROR trial_runner.py:607 -- Trial _inner_40086_00003: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 519, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 497, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/worker.py\", line 1379, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3880, ip=172.31.39.35)\n",
      "  File \"python/ray/_raylet.pyx\", line 463, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 415, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/trainable.py\", line 183, in train\n",
      "    result = self.step()\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 366, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n",
      "    .format(err_tb_str)))\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3880, ip=172.31.39.35)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
      "    self._status_reporter.get_checkpoint())\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 650, in _inner\n",
      "    inner(config, checkpoint_dir=None)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/function_runner.py\", line 644, in inner\n",
      "    fn(config, **fn_kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/integrations.py\", line 179, in _objective\n",
      "    local_trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/trainer.py\", line 940, in train\n",
      "    tr_loss += self.training_step(model, inputs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/trainer.py\", line 1314, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/ubuntu/transformers/examples/seq2seq/seq2seq_trainer.py\", line 180, in compute_loss\n",
      "    loss, _ = self._compute_loss(model, inputs, labels)\n",
      "  File \"/home/ubuntu/transformers/examples/seq2seq/seq2seq_trainer.py\", line 166, in _compute_loss\n",
      "    logits = model(**inputs, use_cache=False)[0]\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 1295, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 1157, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 796, in forward\n",
      "    output_attentions=output_attentions,\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input,Result for _inner_40086_00003:\n",
      "  {}\n",
      "   **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 318, in forward\n",
      "    hidden_states = self.fc2(hidden_states)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 93, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/functional.py\", line 1692, in linear\n",
      "    output = input.matmul(weight.t())\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.70 GiB already allocated; 7.75 MiB free; 13.83 GiB reserved in total by PyTorch)\n",
      "\n",
      "== Status ==\n",
      "Memory usage on this node: 11.3/15.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/5.96 GiB heap, 0.0/2.05 GiB objects (0/1.0 accelerator_type:T4)\n",
      "Result logdir: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed\n",
      "Number of trials: 4/4 (1 ERROR, 3 TERMINATED)\n",
      "+--------------------+------------+-------+-------------------------------+-----------------+-------------+\n",
      "| Trial name         | status     | loc   |   gradient_accumulation_steps |   learning_rate |   objective |\n",
      "|--------------------+------------+-------+-------------------------------+-----------------+-------------|\n",
      "| _inner_40086_00000 | TERMINATED |       |                             2 |           1e-05 |     125.694 |\n",
      "| _inner_40086_00001 | TERMINATED |       |                             4 |           1e-05 |     131.557 |\n",
      "| _inner_40086_00002 | TERMINATED |       |                             2 |           5e-05 |     130.263 |\n",
      "| _inner_40086_00003 | ERROR      |       |                             4 |           5e-05 |             |\n",
      "+--------------------+------------+-------+-------------------------------+-----------------+-------------+\n",
      "Number of errored trials: 1\n",
      "+--------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name         |   # failures | error file                                                                                                                                                                               |\n",
      "|--------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| _inner_40086_00003 |            1 | /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed/_inner_40086_00003_3_gradient_accumulation_steps=4,learning_rate=5e-05_2021-02-10_14-25-44/error.txt |\n",
      "+--------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/transformers/examples/seq2seq/finetune_trainer.py\", line 450, in <module>\n",
      "    main()\n",
      "  File \"/home/ubuntu/transformers/examples/seq2seq/finetune_trainer.py\", line 363, in main\n",
      "    resources_per_trial = {'cpu': 3, 'gpu': 1},\n",
      "  File \"/home/ubuntu/transformers/src/transformers/trainer.py\", line 1254, in hyperparameter_search\n",
      "    **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/integrations.py\", line 244, in run_hp_search_ray\n",
      "    **kwargs,\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/ray/tune/tune.py\", line 442, in run\n",
      "    raise TuneError(\"Trials did not complete\", incomplete_trials)\n",
      "ray.tune.error.TuneError: ('Trials did not complete', [_inner_40086_00003])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 6977\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program failed with code 1.  Press ctrl-c to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed/_inner_40086_00000_0_gradient_accumulation_steps=2,learning_rate=1e-05_2021-02-10_14-21-22/wandb/run-20210210_142605-23iega4g/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/ubuntu/s3/fine-tuning/hps_bart_para_wordembedray_logs/hps_bart_para_wordembed/_inner_40086_00000_0_gradient_accumulation_steps=2,learning_rate=1e-05_2021-02-10_14-21-22/wandb/run-20210210_142605-23iega4g/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 3.8359\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/learning_rate 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/epoch 0.16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1612967196\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/learning_rate ▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/epoch ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/home/ubuntu/s3/fine-tuning/hps_bart_para_wordembed\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hps_bart_para_wordembed/runs/23iega4g\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 $finetune_script \\\n",
    "--model_name_or_path $model_name_or_path \\\n",
    "--config_name $model_config_dir \\\n",
    "--tokenizer_name $model_name_or_path \\\n",
    "--cache_dir $cache_dir \\\n",
    "--data_dir $data_dir \\\n",
    "--freeze_embeds \\\n",
    "--hp_search \\\n",
    "--hp_search_checkpoint_dir $hp_search_checkpoint_dir --hp_search_local_mode --hp_search_db_host '0.0.0.0' \\\n",
    "--hp_search_name $project_name --hp_search_log_dir $hp_search_log_dir \\\n",
    "--lr_scheduler constant \\\n",
    "--warmup_steps 0 \\\n",
    "--fp16 \\\n",
    "--label_smoothing_factor 0.1 \\\n",
    "--sortish_sampler \\\n",
    "--task summarization \\\n",
    "--max_target_length $config.ONE_BULLET_MAX_LEN \\\n",
    "--val_max_target_length $config.ONE_BULLET_MAX_LEN \\\n",
    "--n_train 50 --n_val 10 \\\n",
    "--max_steps 10 \\\n",
    "--logging_steps 1 --logging_first_step \\\n",
    "--per_device_train_batch_size 1 --per_device_eval_batch_size 1 \\\n",
    "--evaluation_strategy steps --eval_steps 3 --eval_beams 3 \\\n",
    "--predict_with_generate \\\n",
    "--save_steps 50 \\\n",
    "--output_dir $output_dir \\\n",
    "--overwrite_output_dir \\\n",
    "--seed $config.SEED \\\n",
    "--run_name $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66ynxjmYEB5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LevExsI7oNF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "L5sXxqeNCtkN"
   ],
   "name": "bart_hyperparameters_search.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0cf5224ab8cb40198d84fa37cfcc68d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "46539ad5e91e4af9860cab29a8200b53": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3095d48314a49d2b81e066c2c3fd389",
      "placeholder": "​",
      "style": "IPY_MODEL_dad549b11e194e53af76c965e3f482a2",
      "value": " 1.52k/1.52k [00:00&lt;00:00, 50.8kB/s]"
     }
    },
    "4dca0f8d944943a7879b3de0a2f8347a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb421fdbad584767bff8da4c882fdfbf",
       "IPY_MODEL_46539ad5e91e4af9860cab29a8200b53"
      ],
      "layout": "IPY_MODEL_c8804e5babea4223b192bca1a9111fac"
     }
    },
    "521e472af5e24843a467a768c042d6c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb421fdbad584767bff8da4c882fdfbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_521e472af5e24843a467a768c042d6c6",
      "max": 1525,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0cf5224ab8cb40198d84fa37cfcc68d5",
      "value": 1525
     }
    },
    "c8804e5babea4223b192bca1a9111fac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3095d48314a49d2b81e066c2c3fd389": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dad549b11e194e53af76c965e3f482a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
