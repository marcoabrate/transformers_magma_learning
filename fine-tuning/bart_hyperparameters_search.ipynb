{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "magma_dir = '/home/ubuntu/magma/'\n",
    "bucket_dir = '/home/ubuntu/s3/'\n",
    "transformers_dir = '/home/ubuntu/transformers/'\n",
    "cache_dir = bucket_dir+'.cache/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EddY1WDNsKlS"
   },
   "source": [
    "## **Fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2d6M41X9AKBi"
   },
   "outputs": [],
   "source": [
    "finetune_script = '\"'+transformers_dir+'examples/seq2seq/finetune_trainer.py\"'\n",
    "eval_script = '\"'+transformers_dir+'examples/seq2seq/run_eval.py\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "82WSp6khIcua",
    "outputId": "6b5776d2-758b-455f-f3f5-6ff21f1b0a9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=hp_search_para_wordembed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, magma_dir)\n",
    "import config\n",
    "\n",
    "import torch\n",
    "torch.manual_seed = config.SEED\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "project_name = 'hp_search_para_wordembed'\n",
    "%env WANDB_PROJECT=$project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPZ7A-sBVOam"
   },
   "source": [
    "### HP search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vJOYl_g6F1e2"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = 'sshleifer/distilbart-cnn-12-6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4xs_XkbCVOar"
   },
   "outputs": [],
   "source": [
    "data_dir = '\"'+bucket_dir+'datasets/karger_books_para_wordembed/bart/st/\"'\n",
    "\n",
    "output_dir = '\"'+bucket_dir+'fine-tuning/hyperparameters_test\"'\n",
    "\n",
    "log_dir = output_dir + '/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "4dca0f8d944943a7879b3de0a2f8347a",
      "c8804e5babea4223b192bca1a9111fac",
      "bb421fdbad584767bff8da4c882fdfbf",
      "46539ad5e91e4af9860cab29a8200b53",
      "0cf5224ab8cb40198d84fa37cfcc68d5",
      "521e472af5e24843a467a768c042d6c6",
      "dad549b11e194e53af76c965e3f482a2",
      "d3095d48314a49d2b81e066c2c3fd389"
     ]
    },
    "id": "wtLC1O1ZJpU3",
    "outputId": "317a9e46-1c7b-4fba-c55a-eef2f13d243e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "model_config = AutoConfig.from_pretrained(model_name_or_path, use_cache=False)\n",
    "model_config.min_length = config.ONE_BULLET_MIN_LEN\n",
    "model_config.max_length = config.ONE_BULLET_MAX_LEN\n",
    "model_config.length_penalty = config.LENGTH_PENALTY\n",
    "model_config.no_repeat_ngram_size = config.NO_REPEAT_NGRAM_SIZE\n",
    "\n",
    "model_config.task_specific_params['summarization']['min_length'] = config.ONE_BULLET_MIN_LEN\n",
    "model_config.task_specific_params['summarization']['max_length'] = config.ONE_BULLET_MAX_LEN\n",
    "model_config.task_specific_params['summarization']['length_penalty'] = config.LENGTH_PENALTY\n",
    "model_config.task_specific_params['summarization']['no_repeat_ngram_size'] = config.NO_REPEAT_NGRAM_SIZE\n",
    "model_config_dir = '\"'+bucket_dir+'fine-tuning/'+\\\n",
    "    model_name_or_path.replace('/', '?')+'_config\"'\n",
    "model_config.save_pretrained(model_config_dir[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K66kY1WOVOaw",
    "outputId": "c50158bf-7522-42c0-d9e7-164da464ba26",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 306M/306M [00:03<00:00, 96.2MB/s]\n",
      "02/01/2021 15:18:21 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "02/01/2021 15:18:21 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/home/ubuntu/s3/fine-tuning/hyperparameters_test', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Feb01_15-18-21_ip-172-31-39-35', logging_first_step=True, logging_steps=1, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=3, dataloader_num_workers=0, past_index=-1, run_name='/home/ubuntu/s3/fine-tuning/hyperparameters_test', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False, label_smoothing=0.0, sortish_sampler=True, predict_with_generate=True, adafactor=False, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear')\n",
      "[INFO|configuration_utils.py:431] 2021-02-01 15:18:22,170 >> loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /home/ubuntu/s3/.cache/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.9307b6cec4435559ec6e79d5a210a334b17706465329e138f335649d14f27e78\n",
      "[INFO|configuration_utils.py:467] 2021-02-01 15:18:22,171 >> Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:431] 2021-02-01 15:18:22,471 >> loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /home/ubuntu/s3/.cache/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.9307b6cec4435559ec6e79d5a210a334b17706465329e138f335649d14f27e78\n",
      "[INFO|configuration_utils.py:467] 2021-02-01 15:18:22,471 >> Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1721] 2021-02-01 15:18:22,472 >> Model name 'sshleifer/distilbart-cnn-12-6' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'sshleifer/distilbart-cnn-12-6' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-02-01 15:18:24,157 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/vocab.json from cache at /home/ubuntu/s3/.cache/9951e68693b9a7c583ae677e9cb53c02715d9bd0311a78706401372653cdea0a.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-02-01 15:18:24,157 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/merges.txt from cache at /home/ubuntu/s3/.cache/7588c8d398d659b230a038240cc023f67b6848117d2999f06ab625af7bfc7ec1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-02-01 15:18:24,157 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-02-01 15:18:24,157 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-02-01 15:18:24,157 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-02-01 15:18:24,157 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer_config.json from cache at /home/ubuntu/s3/.cache/f5316f64f9716436994a7ad76a354dc20ecb2dd74eb61d278f103a9c8b80291f.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n",
      "[INFO|modeling_utils.py:1024] 2021-02-01 15:18:24,770 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1140] 2021-02-01 15:18:56,420 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-02-01 15:18:56,420 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/01/2021 15:18:56 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "[INFO|modeling_utils.py:1024] 2021-02-01 15:18:56,971 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1140] 2021-02-01 15:19:21,168 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-02-01 15:19:21,168 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/01/2021 15:19:21 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "02/01/2021 15:19:26 - INFO - __main__ -   *** Hyperparameters Search ***\n",
      "\u001b[32m[I 2021-02-01 15:19:26,481]\u001b[0m A new study created in memory with name: no-name-11fe241f-0c29-46a4-ac00-5c9b04cc46db\u001b[0m\n",
      "[INFO|trainer.py:557] 2021-02-01 15:19:26,482 >> Trial:\n",
      "[INFO|modeling_utils.py:1024] 2021-02-01 15:19:26,763 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1140] 2021-02-01 15:19:44,973 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-02-01 15:19:44,974 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/01/2021 15:19:44 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "[INFO|trainer.py:703] 2021-02-01 15:19:45,300 >> ***** Running training *****\n",
      "[INFO|trainer.py:704] 2021-02-01 15:19:45,300 >>   Num examples = 50\n",
      "[INFO|trainer.py:705] 2021-02-01 15:19:45,300 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:706] 2021-02-01 15:19:45,300 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:707] 2021-02-01 15:19:45,300 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:708] 2021-02-01 15:19:45,300 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:709] 2021-02-01 15:19:45,300 >>   Total optimization steps = 6\n",
      "/home/ubuntu/s3/fine-tuning/hyperparameters_test\n",
      "[INFO|integrations.py:371] 2021-02-01 15:19:45,304 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/3ruexhqv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/magma/fine-tuning/wandb/run-20210201_151945-3ruexhqv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      " 17%|█▋        | 1/6 [00:01<00:09,  1.86s/it]{'loss': 5.04779052734375, 'learning_rate': 0.0005737944375412901, 'epoch': 0.16}\n",
      "                                             {'loss': 3.188976764678955, 'learning_rate': 0.00045903555003303203, 'epoch': 0.32}\n",
      " 50%|█████     | 3/6 [00:02<00:02,  1.28it/s]{'loss': 15.534832954406738, 'learning_rate': 0.00034427666252477405, 'epoch': 0.48}\n",
      " 50%|█████     | 3/6 [00:02<00:02,  1.28it/s][INFO|trainer.py:1412] 2021-02-01 15:19:49,629 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:19:49,630 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:19:49,631 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:00,  3.04it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.11it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.71it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.63it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.85it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.75it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.10it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.14it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.08it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.02it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.79it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 127.42it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.17it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 119.83it/s]\n",
      "                                             \n",
      " 50%|█████     | 3/6 [00:06<00:02,  1.28it/s]uge1_precision': 22.71, 'eval_rouge1_recall': 9.629999999999999, 'eval_rouge1_fmeasure': 13.350000000000001, 'eval_rouge2_precision': 0.0, 'eval_rouge2_recall': 0.0, 'eval_rouge2_fmeasure': 0.0, 'eval_rougeL_precision': 16.17, 'eval_rougeL_recall': 6.79, 'eval_rougeL_fmeasure': 9.44, 'eval_rougeLsum_precision': 17.84, 'eval_rougeLsum_recall': 7.3, 'eval_rougeLsum_fmeasure': 10.23, 'eval_gen_len': 58.6, 'eval_sentence_distilroberta_cosine': 18.80435049533844, 'epoch': 0.48}\n",
      "\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.63it/s]\u001b[A\n",
      " 67%|██████▋   | 4/6 [00:07<00:04,  2.22s/it] 0.00022951777501651602, 'epoch': 0.64}\n",
      " 83%|████████▎ | 5/6 [00:07<00:01,  1.58s/it]{'loss': 13.849506378173828, 'learning_rate': 0.00011475888750825801, 'epoch': 0.8}\n",
      "100%|██████████| 6/6 [00:08<00:00,  1.21s/it][INFO|trainer.py:1412] 2021-02-01 15:19:55,006 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:19:55,006 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:19:55,006 >>   Batch size = 1\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:00,  3.00it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.07it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.68it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.60it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 119.13it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.89it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.56it/s]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.77it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.71it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.75it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.22it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.65it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.17it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.10it/s]\n",
      "                                             \n",
      "100%|██████████| 6/6 [00:11<00:00,  1.21s/it]ge1_precision': 15.620000000000001, 'eval_rouge1_recall': 15.010000000000002, 'eval_rouge1_fmeasure': 15.15, 'eval_rouge2_precision': 3.08, 'eval_rouge2_recall': 3.3300000000000005, 'eval_rouge2_fmeasure': 3.2, 'eval_rougeL_precision': 15.21, 'eval_rougeL_recall': 14.46, 'eval_rougeL_fmeasure': 14.71, 'eval_rougeLsum_precision': 14.57, 'eval_rougeLsum_recall': 14.29, 'eval_rougeLsum_fmeasure': 14.37, 'eval_gen_len': 59.8, 'eval_sentence_distilroberta_cosine': 25.606048107147217, 'epoch': 0.96}\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.60it/s]\u001b[A\n",
      "                                             \u001b[A[INFO|trainer.py:862] 2021-02-01 15:19:58,765 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 6/6 [00:11<00:00,  1.99s/it]\n",
      "\n",
      "{'epoch': 0.96}\n",
      "\u001b[32m[I 2021-02-01 15:19:58,766]\u001b[0m Trial 0 finished with value: 168.60604810714722 and parameters: {'learning_rate': 0.0006885533250495481, 'gradient_accumulation_steps': 8}. Best is trial 0 with value: 168.60604810714722.\u001b[0m\n",
      "[INFO|trainer.py:557] 2021-02-01 15:19:58,766 >> Trial:\n",
      "[INFO|modeling_utils.py:1024] 2021-02-01 15:19:59,050 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1140] 2021-02-01 15:20:22,570 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-02-01 15:20:22,571 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/01/2021 15:20:22 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "[INFO|trainer.py:703] 2021-02-01 15:20:22,887 >> ***** Running training *****\n",
      "[INFO|trainer.py:704] 2021-02-01 15:20:22,887 >>   Num examples = 50\n",
      "[INFO|trainer.py:705] 2021-02-01 15:20:22,887 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:706] 2021-02-01 15:20:22,887 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:707] 2021-02-01 15:20:22,887 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:708] 2021-02-01 15:20:22,887 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:709] 2021-02-01 15:20:22,887 >>   Total optimization steps = 12\n",
      "/home/ubuntu/s3/fine-tuning/hyperparameters_test[INFO|integrations.py:371] 2021-02-01 15:20:22,891 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 7176\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_151945-3ruexhqv/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_151945-3ruexhqv/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 6.54555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 37\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612192822\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 3.9924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 15.62\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 15.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 15.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 3.08\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 3.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 3.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 15.21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 14.46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 14.71\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 14.57\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 14.29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 14.37\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 59.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 25.60605\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos 16767021772800\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss ▂▁█▇▇▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate █▇▅▄▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▂▄▅▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▂▄▅▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁▂▂▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁▂▂▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/3ruexhqv\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/2era7tue\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152022-2era7tue\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "                                              {'loss': 6.2596588134765625, 'learning_rate': 7.554055358338701e-05, 'epoch': 0.08}\n",
      "                                              {'loss': 3.1888999938964844, 'learning_rate': 6.867323053035183e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              {'loss': 2.6997389793395996, 'learning_rate': 6.180590747731665e-05, 'epoch': 0.24}\n",
      " 25%|██▌       | 3/12 [00:00<00:02,  3.65it/s][INFO|trainer.py:1412] 2021-02-01 15:20:27,917 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:20:27,917 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:20:27,917 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.94it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.06it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.68it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.60it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 111.74it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 117.12it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.52it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.92it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.26it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.66it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.05it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.19it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 122.62it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.68it/s]{'eval_loss': 1.7531497478485107, 'eval_rouge1_precision': 30.049999999999997, 'eval_rouge1_recall': 52.26, 'eval_rouge1_fmeasure': 36.84, 'eval_rouge2_precision': 13.5, 'eval_rouge2_recall': 26.66, 'eval_rouge2_fmeasure': 17.09, 'eval_rougeL_precision': 22.56, 'eval_rougeL_recall': 41.07, 'eval_rougeL_fmeasure': 28.43, 'eval_rougeLsum_precision': 22.3, 'eval_rougeLsum_recall': 40.27, 'eval_rougeLsum_fmeasure': 27.82, 'eval_gen_len': 59.6, 'eval_sentence_distilroberta_cosine': 66.52716994285583, 'epoch': 0.24}\n",
      "\n",
      "                                              \n",
      " 25%|██▌       | 3/12 [00:04<00:02,  3.65it/s]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.60it/s]\u001b[A\n",
      "                                              {'loss': 2.481719970703125, 'learning_rate': 5.4938584424281465e-05, 'epoch': 0.32}\n",
      "                                              {'loss': 3.7724080085754395, 'learning_rate': 4.807126137124628e-05, 'epoch': 0.4}\n",
      "                                              {'loss': 2.7987046241760254, 'learning_rate': 4.12039383182111e-05, 'epoch': 0.48}\n",
      " 50%|█████     | 6/12 [00:05<00:05,  1.11it/s][INFO|trainer.py:1412] 2021-02-01 15:20:32,496 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:20:32,496 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:20:32,497 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.96it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.06it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.67it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.62it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 118.61it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.94it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.25it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.01it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.58it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.91it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.11it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.31it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.72it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.64it/s]{'eval_loss': 1.644972562789917, 'eval_rouge1_precision': 36.68, 'eval_rouge1_recall': 60.47, 'eval_rouge1_fmeasure': 44.47, 'eval_rouge2_precision': 21.54, 'eval_rouge2_recall': 37.84, 'eval_rouge2_fmeasure': 26.38, 'eval_rougeL_precision': 29.98, 'eval_rougeL_recall': 51.07000000000001, 'eval_rougeL_fmeasure': 36.71, 'eval_rougeLsum_precision': 32.269999999999996, 'eval_rougeLsum_recall': 54.15, 'eval_rougeLsum_fmeasure': 39.45, 'eval_gen_len': 59.2, 'eval_sentence_distilroberta_cosine': 71.16144895553589, 'epoch': 0.48}\n",
      "\n",
      "                                              \n",
      " 50%|█████     | 6/12 [00:09<00:05,  1.11it/s]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.62it/s]\u001b[A\n",
      "                                              {'loss': 3.6891942024230957, 'learning_rate': 3.4336615265175916e-05, 'epoch': 0.56}\n",
      "                                              {'loss': 2.8805317878723145, 'learning_rate': 2.7469292212140733e-05, 'epoch': 0.64}\n",
      "                                              {'loss': 2.2684073448181152, 'learning_rate': 2.060196915910555e-05, 'epoch': 0.72}\n",
      " 75%|███████▌  | 9/12 [00:09<00:03,  1.04s/it][INFO|trainer.py:1412] 2021-02-01 15:20:37,067 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:20:37,067 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:20:37,067 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.93it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.05it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.67it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.63it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.82it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.00it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.51it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.87it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.09it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.66it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.78it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.54it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.10it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.77it/s]{'eval_loss': 1.6453415155410767, 'eval_rouge1_precision': 30.769999999999996, 'eval_rouge1_recall': 51.64, 'eval_rouge1_fmeasure': 37.72, 'eval_rouge2_precision': 18.15, 'eval_rouge2_recall': 32.59, 'eval_rouge2_fmeasure': 22.54, 'eval_rougeL_precision': 25.580000000000002, 'eval_rougeL_recall': 44.22, 'eval_rougeL_fmeasure': 31.319999999999997, 'eval_rougeLsum_precision': 27.46, 'eval_rougeLsum_recall': 46.239999999999995, 'eval_rougeLsum_fmeasure': 33.67, 'eval_gen_len': 59.2, 'eval_sentence_distilroberta_cosine': 69.15130019187927, 'epoch': 0.72}\n",
      "                                              \n",
      "\u001b[A                                          \n",
      " 75%|███████▌  | 9/12 [00:13<00:03,  1.04s/it]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.63it/s]\u001b[A\n",
      "                                               {'loss': 3.2824485301971436, 'learning_rate': 1.3734646106070366e-05, 'epoch': 0.8}\n",
      "                                               {'loss': 2.705660581588745, 'learning_rate': 6.867323053035183e-06, 'epoch': 0.88}\n",
      "                                               {'loss': 3.002878189086914, 'learning_rate': 0.0, 'epoch': 0.96}\n",
      "100%|██████████| 12/12 [00:14<00:00,  1.08s/it][INFO|trainer.py:1412] 2021-02-01 15:20:41,601 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:20:41,601 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:20:41,601 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.93it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.04it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.66it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.59it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 117.22it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.27it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 127.32it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.61it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.05it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.92it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.77it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.92it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.93it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.82it/s]val_loss': 1.6183569431304932, 'eval_rouge1_precision': 31.53, 'eval_rouge1_recall': 52.6, 'eval_rouge1_fmeasure': 38.15, 'eval_rouge2_precision': 16.21, 'eval_rouge2_recall': 28.549999999999997, 'eval_rouge2_fmeasure': 19.900000000000002, 'eval_rougeL_precision': 24.8, 'eval_rougeL_recall': 41.83, 'eval_rougeL_fmeasure': 30.0, 'eval_rougeLsum_precision': 27.41, 'eval_rougeLsum_recall': 47.22, 'eval_rougeLsum_fmeasure': 33.040000000000006, 'eval_gen_len': 59.8, 'eval_sentence_distilroberta_cosine': 68.32696199417114, 'epoch': 0.96}\n",
      "\n",
      "                                               \n",
      "100%|██████████| 12/12 [00:18<00:00,  1.08s/it]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.59it/s]\u001b[A\n",
      "                                             \u001b[A[INFO|trainer.py:862] 2021-02-01 15:20:45,405 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "                                               {'epoch': 0.96}\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.53s/it]\n",
      "\u001b[32m[I 2021-02-01 15:20:45,406]\u001b[0m Trial 1 finished with value: 459.5669619941712 and parameters: {'learning_rate': 8.24078766364222e-05, 'gradient_accumulation_steps': 4}. Best is trial 1 with value: 459.5669619941712.\u001b[0m\n",
      "[INFO|trainer.py:557] 2021-02-01 15:20:45,407 >> Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1024] 2021-02-01 15:20:45,689 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1140] 2021-02-01 15:21:04,764 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-02-01 15:21:04,764 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/01/2021 15:21:04 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "[INFO|trainer.py:703] 2021-02-01 15:21:05,106 >> ***** Running training *****\n",
      "/home/ubuntu/s3/fine-tuning/hyperparameters_test[INFO|trainer.py:704] 2021-02-01 15:21:05,106 >>   Num examples = 50\n",
      "[INFO|trainer.py:705] 2021-02-01 15:21:05,106 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:706] 2021-02-01 15:21:05,106 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:707] 2021-02-01 15:21:05,106 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:708] 2021-02-01 15:21:05,106 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:709] 2021-02-01 15:21:05,106 >>   Total optimization steps = 12\n",
      "\n",
      "[INFO|integrations.py:371] 2021-02-01 15:21:05,111 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 7337\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152022-2era7tue/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152022-2era7tue/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 3.00288\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 39\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612192865\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 1.61836\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 31.53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 52.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 38.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 16.21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 28.55\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 24.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 41.83\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 30.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 27.41\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 47.22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 33.04\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 59.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 68.32696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos 16767021772800\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss █▃▂▁▄▂▃▂▁▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate █▇▇▆▅▅▄▄▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▂▂▃▄▄▅▅▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▂▂▃▄▄▅▅▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁▂▂▂▃▃▃▄▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁▂▂▂▃▃▃▄▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss █▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision ▁█▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall ▁█▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure ▁█▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision ▁█▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall ▁█▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure ▁█▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision ▁█▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall ▁█▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure ▁█▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision ▁█▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall ▁█▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure ▁█▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len ▆▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine ▁█▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/2era7tue\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/1nwfgbwk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152105-1nwfgbwk\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "                                              {'loss': 6.2596588134765625, 'learning_rate': 0.00027209267155403323, 'epoch': 0.08}\n",
      "                                              {'loss': 4.213378429412842, 'learning_rate': 0.0002473569741400302, 'epoch': 0.16}\n",
      "                                              {'loss': 2.855748414993286, 'learning_rate': 0.00022262127672602718, 'epoch': 0.24}\n",
      " 25%|██▌       | 3/12 [00:00<00:02,  3.61it/s][INFO|trainer.py:1412] 2021-02-01 15:21:10,160 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:21:10,160 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:21:10,160 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.98it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.06it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.67it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.60it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 108.35it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.13it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.76it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.58it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.04it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.62it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.36it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.72it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.24it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.35it/s]{'eval_loss': 2.023538112640381, 'eval_rouge1_precision': 27.29, 'eval_rouge1_recall': 45.37, 'eval_rouge1_fmeasure': 32.99, 'eval_rouge2_precision': 9.8, 'eval_rouge2_recall': 18.82, 'eval_rouge2_fmeasure': 12.509999999999998, 'eval_rougeL_precision': 20.26, 'eval_rougeL_recall': 35.04, 'eval_rougeL_fmeasure': 25.069999999999997, 'eval_rougeLsum_precision': 20.13, 'eval_rougeLsum_recall': 34.93, 'eval_rougeLsum_fmeasure': 24.67, 'eval_gen_len': 59.4, 'eval_sentence_distilroberta_cosine': 62.01319098472595, 'epoch': 0.24}\n",
      "\n",
      "\n",
      "                                              \n",
      " 25%|██▌       | 3/12 [00:04<00:02,  3.61it/s][A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              {'loss': 2.7174179553985596, 'learning_rate': 0.00019788557931202415, 'epoch': 0.32}\n",
      "                                              {'loss': 4.240965843200684, 'learning_rate': 0.00017314988189802115, 'epoch': 0.4}\n",
      "                                              {'loss': 3.256470203399658, 'learning_rate': 0.00014841418448401813, 'epoch': 0.48}\n",
      " 50%|█████     | 6/12 [00:05<00:05,  1.11it/s][INFO|trainer.py:1412] 2021-02-01 15:21:14,747 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:21:14,747 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:21:14,747 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.94it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.06it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.67it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.60it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 115.56it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.29it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.77it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.30it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.15it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.64it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.67it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.21it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.80it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 117.98it/s]{'eval_loss': 2.092001438140869, 'eval_rouge1_precision': 32.45, 'eval_rouge1_recall': 50.86000000000001, 'eval_rouge1_fmeasure': 38.31, 'eval_rouge2_precision': 18.02, 'eval_rouge2_recall': 29.28, 'eval_rouge2_fmeasure': 21.23, 'eval_rougeL_precision': 26.97, 'eval_rougeL_recall': 41.61, 'eval_rougeL_fmeasure': 31.46, 'eval_rougeLsum_precision': 27.639999999999997, 'eval_rougeLsum_recall': 43.91, 'eval_rougeLsum_fmeasure': 32.57, 'eval_gen_len': 60.0, 'eval_sentence_distilroberta_cosine': 71.3562548160553, 'epoch': 0.48}\n",
      "\n",
      "                                              \n",
      "\n",
      " 50%|█████     | 6/12 [00:09<00:05,  1.11it/s][A\n",
      "                                              {'loss': 4.211101531982422, 'learning_rate': 0.0001236784870700151, 'epoch': 0.56}\n",
      "                                              {'loss': 3.507113456726074, 'learning_rate': 9.894278965601208e-05, 'epoch': 0.64}\n",
      "                                              {'loss': 2.612485885620117, 'learning_rate': 7.420709224200906e-05, 'epoch': 0.72}\n",
      " 75%|███████▌  | 9/12 [00:10<00:03,  1.04s/it][INFO|trainer.py:1412] 2021-02-01 15:21:19,320 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:21:19,320 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:21:19,320 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.95it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.05it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.67it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.60it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 114.63it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.61it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.87it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.49it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.08it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.96it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.32it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.67it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.24it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.50it/s]val_loss': 2.1006810665130615, 'eval_rouge1_precision': 25.19, 'eval_rouge1_recall': 45.540000000000006, 'eval_rouge1_fmeasure': 31.419999999999998, 'eval_rouge2_precision': 10.549999999999999, 'eval_rouge2_recall': 22.58, 'eval_rouge2_fmeasure': 13.76, 'eval_rougeL_precision': 20.21, 'eval_rougeL_recall': 37.55, 'eval_rougeL_fmeasure': 25.31, 'eval_rougeLsum_precision': 19.830000000000002, 'eval_rougeLsum_recall': 37.1, 'eval_rougeLsum_fmeasure': 25.03, 'eval_gen_len': 59.6, 'eval_sentence_distilroberta_cosine': 64.2305076122284, 'epoch': 0.72}\n",
      "\n",
      "\n",
      "                                              \n",
      " 75%|███████▌  | 9/12 [00:13<00:03,  1.04s/it][A\n",
      "                                               {'loss': 3.616454601287842, 'learning_rate': 4.947139482800604e-05, 'epoch': 0.8}\n",
      "                                               {'loss': 2.899123430252075, 'learning_rate': 2.473569741400302e-05, 'epoch': 0.88}\n",
      "                                               {'loss': 3.3588316440582275, 'learning_rate': 0.0, 'epoch': 0.96}\n",
      "100%|██████████| 12/12 [00:14<00:00,  1.09s/it][INFO|trainer.py:1412] 2021-02-01 15:21:23,879 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:21:23,879 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:21:23,879 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.95it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.05it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.67it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.60it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.09it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.49it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.61it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.06it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.98it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.10it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.58it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.26it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.68it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.88it/s]val_loss': 2.038618564605713, 'eval_rouge1_precision': 26.38, 'eval_rouge1_recall': 47.4, 'eval_rouge1_fmeasure': 32.87, 'eval_rouge2_precision': 8.36, 'eval_rouge2_recall': 19.869999999999997, 'eval_rouge2_fmeasure': 11.35, 'eval_rougeL_precision': 19.78, 'eval_rougeL_recall': 37.25, 'eval_rougeL_fmeasure': 24.959999999999997, 'eval_rougeLsum_precision': 20.71, 'eval_rougeLsum_recall': 39.269999999999996, 'eval_rougeLsum_fmeasure': 26.21, 'eval_gen_len': 60.0, 'eval_sentence_distilroberta_cosine': 66.19983315467834, 'epoch': 0.96}\n",
      "\n",
      "{'epoch': 0.96}\n",
      "                                               \n",
      "100%|██████████| 12/12 [00:18<00:00,  1.09s/it]A\n",
      "                                             \u001b[A[INFO|trainer.py:862] 2021-02-01 15:21:27,663 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.53s/it]\n",
      "\u001b[32m[I 2021-02-01 15:21:27,665]\u001b[0m Trial 2 finished with value: 380.6098331546783 and parameters: {'learning_rate': 0.00029682836896803626, 'gradient_accumulation_steps': 4}. Best is trial 1 with value: 459.5669619941712.\u001b[0m\n",
      "[INFO|trainer.py:557] 2021-02-01 15:21:27,665 >> Trial:\n",
      "[INFO|modeling_utils.py:1024] 2021-02-01 15:21:27,948 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1140] 2021-02-01 15:21:46,333 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-02-01 15:21:46,333 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/01/2021 15:21:46 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "[INFO|trainer.py:703] 2021-02-01 15:21:46,660 >> ***** Running training *****\n",
      "/home/ubuntu/s3/fine-tuning/hyperparameters_test\n",
      "[INFO|trainer.py:704] 2021-02-01 15:21:46,660 >>   Num examples = 50\n",
      "[INFO|trainer.py:705] 2021-02-01 15:21:46,660 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:706] 2021-02-01 15:21:46,660 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:707] 2021-02-01 15:21:46,660 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:708] 2021-02-01 15:21:46,660 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:709] 2021-02-01 15:21:46,660 >>   Total optimization steps = 6\n",
      "[INFO|integrations.py:371] 2021-02-01 15:21:46,664 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 7489\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152105-1nwfgbwk/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152105-1nwfgbwk/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 3.35883\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612192906\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 2.03862\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 26.38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 47.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 32.87\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 8.36\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 19.87\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 11.35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 19.78\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 37.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 24.96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 20.71\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 39.27\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 26.21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 60.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 66.19983\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos 16767021772800\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss █▄▁▁▄▂▄▃▁▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate █▇▇▆▅▅▄▄▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▂▂▃▄▄▅▅▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▂▂▃▄▄▅▅▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁▂▂▂▃▃▃▄▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁▂▂▂▃▃▃▄▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss ▁▇█▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision ▃█▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall ▁█▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure ▃█▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision ▂█▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall ▁█▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure ▂█▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision ▁█▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall ▁█▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure ▁█▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision ▁█▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall ▁█▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure ▁█▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len ▁█▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine ▁█▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/1nwfgbwk\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/erljly4u\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152146-erljly4u\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "                                             {'loss': 5.04779052734375, 'learning_rate': 0.0010383715776193423, 'epoch': 0.16}\n",
      "                                             {'loss': 38.79265213012695, 'learning_rate': 0.0008306972620954738, 'epoch': 0.32}\n",
      "                                             {'loss': 10.319368362426758, 'learning_rate': 0.0006230229465716054, 'epoch': 0.48}\n",
      " 50%|█████     | 3/6 [00:01<00:01,  1.96it/s][INFO|trainer.py:1412] 2021-02-01 15:21:52,546 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:21:52,546 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:21:52,546 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:00,  3.03it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.13it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.73it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.64it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 122.65it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.31it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 127.25it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.12it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.23it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.93it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.96it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.74it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.62it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.65it/s]val_loss': 12.230620384216309, 'eval_rouge1_precision': 8.4, 'eval_rouge1_recall': 6.1, 'eval_rouge1_fmeasure': 6.950000000000001, 'eval_rouge2_precision': 0.0, 'eval_rouge2_recall': 0.0, 'eval_rouge2_fmeasure': 0.0, 'eval_rougeL_precision': 7.51, 'eval_rougeL_recall': 5.48, 'eval_rougeL_fmeasure': 6.1899999999999995, 'eval_rougeLsum_precision': 8.36, 'eval_rougeLsum_recall': 6.1, 'eval_rougeLsum_fmeasure': 6.909999999999999, 'eval_gen_len': 57.0, 'eval_sentence_distilroberta_cosine': 8.324646949768066, 'epoch': 0.48}\n",
      "\n",
      "\n",
      "                                             \n",
      " 50%|█████     | 3/6 [00:05<00:01,  1.96it/s]\u001b[A\n",
      "                                             {'loss': 11.261039733886719, 'learning_rate': 0.0004153486310477369, 'epoch': 0.64}\n",
      "                                             {'loss': 9.09743595123291, 'learning_rate': 0.00020767431552386846, 'epoch': 0.8}\n",
      "                                             {'loss': 8.56303596496582, 'learning_rate': 0.0, 'epoch': 0.96}\n",
      "100%|██████████| 6/6 [00:06<00:00,  1.09s/it][INFO|trainer.py:1412] 2021-02-01 15:21:57,633 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:21:57,633 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:21:57,633 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.89it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.03it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.64it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.57it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 119.53it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.29it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.65it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.53it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.59it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.36it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.31it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.09it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.47it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.82it/s]{'eval_loss': 8.270854949951172, 'eval_rouge1_precision': 7.91, 'eval_rouge1_recall': 12.659999999999998, 'eval_rouge1_fmeasure': 9.46, 'eval_rouge2_precision': 1.11, 'eval_rouge2_recall': 1.71, 'eval_rouge2_fmeasure': 1.34, 'eval_rougeL_precision': 7.01, 'eval_rougeL_recall': 11.559999999999999, 'eval_rougeL_fmeasure': 8.42, 'eval_rougeLsum_precision': 7.489999999999999, 'eval_rougeLsum_recall': 12.07, 'eval_rougeLsum_fmeasure': 8.94, 'eval_gen_len': 60.0, 'eval_sentence_distilroberta_cosine': 13.663984835147858, 'epoch': 0.96}\n",
      "\n",
      "\n",
      "                                             \n",
      "{'epoch': 0.96}\n",
      "100%|██████████| 6/6 [00:10<00:00,  1.09s/it]\u001b[A\n",
      "                                             \u001b[A[INFO|trainer.py:862] 2021-02-01 15:22:01,473 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 6/6 [00:10<00:00,  1.75s/it]\n",
      "\u001b[32m[I 2021-02-01 15:22:01,475]\u001b[0m Trial 3 finished with value: 103.34398483514786 and parameters: {'learning_rate': 0.0012460458931432107, 'gradient_accumulation_steps': 8}. Best is trial 1 with value: 459.5669619941712.\u001b[0m\n",
      "[INFO|trainer.py:557] 2021-02-01 15:22:01,475 >> Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1024] 2021-02-01 15:22:01,756 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1140] 2021-02-01 15:22:20,212 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-02-01 15:22:20,212 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/01/2021 15:22:20 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "[INFO|trainer.py:703] 2021-02-01 15:22:20,542 >> ***** Running training *****\n",
      "/home/ubuntu/s3/fine-tuning/hyperparameters_test\n",
      "[INFO|trainer.py:704] 2021-02-01 15:22:20,542 >>   Num examples = 50\n",
      "[INFO|trainer.py:705] 2021-02-01 15:22:20,542 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:706] 2021-02-01 15:22:20,542 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:707] 2021-02-01 15:22:20,542 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:708] 2021-02-01 15:22:20,542 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:709] 2021-02-01 15:22:20,542 >>   Total optimization steps = 12\n",
      "[INFO|integrations.py:371] 2021-02-01 15:22:20,547 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 7683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152146-erljly4u/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152146-erljly4u/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 8.56304\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612192940\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 8.27085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 7.91\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 12.66\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 9.46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 1.11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 1.71\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 1.34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 7.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 11.56\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 8.42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 7.49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 12.07\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 8.94\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 60.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 13.66398\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos 16767021772800\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss ▁█▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate █▇▅▄▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▂▄▅▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▂▄▅▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁▂▂▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁▂▂▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/erljly4u\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/2v95q0vm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152220-2v95q0vm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "                                              {'loss': 6.2596588134765625, 'learning_rate': 0.0001544731084453181, 'epoch': 0.08}\n",
      "                                              {'loss': 3.242954969406128, 'learning_rate': 0.00014043009858665282, 'epoch': 0.16}\n",
      "                                              {'loss': 2.695523500442505, 'learning_rate': 0.00012638708872798752, 'epoch': 0.24}\n",
      " 25%|██▌       | 3/12 [00:00<00:02,  3.53it/s][INFO|trainer.py:1412] 2021-02-01 15:22:25,720 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:22:25,720 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:22:25,720 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.95it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.07it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.67it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.59it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 115.05it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.30it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.56it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.72it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.59it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.43it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.77it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.23it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.91it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 120.86it/s]{'eval_loss': 1.8011051416397095, 'eval_rouge1_precision': 29.4, 'eval_rouge1_recall': 51.57000000000001, 'eval_rouge1_fmeasure': 35.980000000000004, 'eval_rouge2_precision': 11.700000000000001, 'eval_rouge2_recall': 24.75, 'eval_rouge2_fmeasure': 15.509999999999998, 'eval_rougeL_precision': 21.7, 'eval_rougeL_recall': 40.56, 'eval_rougeL_fmeasure': 27.58, 'eval_rougeLsum_precision': 22.6, 'eval_rougeLsum_recall': 41.27, 'eval_rougeLsum_fmeasure': 28.050000000000004, 'eval_gen_len': 58.4, 'eval_sentence_distilroberta_cosine': 61.9293212890625, 'epoch': 0.24}\n",
      "\n",
      "                                              \n",
      " 25%|██▌       | 3/12 [00:04<00:02,  3.53it/s]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.59it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              {'loss': 2.505849838256836, 'learning_rate': 0.00011234407886932224, 'epoch': 0.32}\n",
      "                                              {'loss': 3.909991979598999, 'learning_rate': 9.830106901065698e-05, 'epoch': 0.4}\n",
      "                                              {'loss': 3.0809574127197266, 'learning_rate': 8.425805915199169e-05, 'epoch': 0.48}\n",
      " 50%|█████     | 6/12 [00:05<00:05,  1.10it/s][INFO|trainer.py:1412] 2021-02-01 15:22:30,329 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:22:30,329 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:22:30,329 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.95it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.05it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.67it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.59it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 120.26it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.96it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.81it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.34it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.21it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.17it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.04it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.70it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.06it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.74it/s]{'eval_loss': 1.7239596843719482, 'eval_rouge1_precision': 36.25, 'eval_rouge1_recall': 59.78, 'eval_rouge1_fmeasure': 44.06, 'eval_rouge2_precision': 20.34, 'eval_rouge2_recall': 35.5, 'eval_rouge2_fmeasure': 24.98, 'eval_rougeL_precision': 28.32, 'eval_rougeL_recall': 47.3, 'eval_rougeL_fmeasure': 34.52, 'eval_rougeLsum_precision': 31.630000000000003, 'eval_rougeLsum_recall': 52.72, 'eval_rougeLsum_fmeasure': 38.25, 'eval_gen_len': 59.6, 'eval_sentence_distilroberta_cosine': 69.65660452842712, 'epoch': 0.48}\n",
      "                                              \n",
      "\u001b[A                                          \n",
      " 50%|█████     | 6/12 [00:09<00:05,  1.10it/s]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.59it/s]\u001b[A\n",
      "                                              {'loss': 3.870163917541504, 'learning_rate': 7.021504929332641e-05, 'epoch': 0.56}\n",
      "                                              {'loss': 3.135366439819336, 'learning_rate': 5.617203943466112e-05, 'epoch': 0.64}\n",
      "                                              {'loss': 2.326892614364624, 'learning_rate': 4.2129029575995843e-05, 'epoch': 0.72}\n",
      " 75%|███████▌  | 9/12 [00:10<00:03,  1.05s/it][INFO|trainer.py:1412] 2021-02-01 15:22:34,936 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:22:34,936 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:22:34,936 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.96it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.06it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.67it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.60it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 118.99it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 127.90it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.38it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.59it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.59it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.30it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.33it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.44it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.36it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.13it/s]{'eval_loss': 1.768291711807251, 'eval_rouge1_precision': 29.93, 'eval_rouge1_recall': 52.33, 'eval_rouge1_fmeasure': 37.74, 'eval_rouge2_precision': 14.32, 'eval_rouge2_recall': 27.36, 'eval_rouge2_fmeasure': 18.43, 'eval_rougeL_precision': 24.79, 'eval_rougeL_recall': 42.83, 'eval_rougeL_fmeasure': 30.669999999999998, 'eval_rougeLsum_precision': 26.41, 'eval_rougeLsum_recall': 46.77, 'eval_rougeLsum_fmeasure': 32.97, 'eval_gen_len': 60.0, 'eval_sentence_distilroberta_cosine': 66.59511923789978, 'epoch': 0.72}\n",
      "\n",
      "                                              \n",
      " 75%|███████▌  | 9/12 [00:13<00:03,  1.05s/it]\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.60it/s]\u001b[A\n",
      "                                               {'loss': 3.4484453201293945, 'learning_rate': 2.808601971733056e-05, 'epoch': 0.8}\n",
      "                                               {'loss': 2.781693458557129, 'learning_rate': 1.404300985866528e-05, 'epoch': 0.88}\n",
      "                                               {'loss': 3.1299686431884766, 'learning_rate': 0.0, 'epoch': 0.96}\n",
      "100%|██████████| 12/12 [00:14<00:00,  1.09s/it][INFO|trainer.py:1412] 2021-02-01 15:22:39,509 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:22:39,509 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:22:39,509 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.95it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.06it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.67it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.60it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.51it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.29it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.87it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 115.91it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.84it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.35it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.81it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.77it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.94it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.45it/s]{'eval_loss': 1.7359977960586548, 'eval_rouge1_precision': 36.730000000000004, 'eval_rouge1_recall': 59.78, 'eval_rouge1_fmeasure': 44.26, 'eval_rouge2_precision': 22.040000000000003, 'eval_rouge2_recall': 37.53, 'eval_rouge2_fmeasure': 26.68, 'eval_rougeL_precision': 31.94, 'eval_rougeL_recall': 52.910000000000004, 'eval_rougeL_fmeasure': 38.51, 'eval_rougeLsum_precision': 32.0, 'eval_rougeLsum_recall': 53.349999999999994, 'eval_rougeLsum_fmeasure': 38.67, 'eval_gen_len': 59.8, 'eval_sentence_distilroberta_cosine': 72.51580953598022, 'epoch': 0.96}\n",
      "\n",
      "                                               \n",
      "{'epoch': 0.96}\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.09s/it]A\n",
      "                                             \u001b[A[INFO|trainer.py:862] 2021-02-01 15:22:43,294 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "                                               \n",
      "100%|██████████| 12/12 [00:18<00:00,  1.54s/it]\n",
      "\u001b[32m[I 2021-02-01 15:22:43,295]\u001b[0m Trial 4 finished with value: 546.9158095359803 and parameters: {'learning_rate': 0.00016851611830398337, 'gradient_accumulation_steps': 4}. Best is trial 4 with value: 546.9158095359803.\u001b[0m\n",
      "[INFO|trainer.py:557] 2021-02-01 15:22:43,296 >> Trial:\n",
      "[INFO|modeling_utils.py:1024] 2021-02-01 15:22:43,581 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1140] 2021-02-01 15:23:02,192 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-02-01 15:23:02,213 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/01/2021 15:23:02 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "[INFO|trainer.py:703] 2021-02-01 15:23:02,539 >> ***** Running training *****\n",
      "[INFO|trainer.py:704] 2021-02-01 15:23:02,539 >>   Num examples = 50\n",
      "[INFO|trainer.py:705] 2021-02-01 15:23:02,539 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:706] 2021-02-01 15:23:02,539 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:707] 2021-02-01 15:23:02,539 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:708] 2021-02-01 15:23:02,539 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:709] 2021-02-01 15:23:02,539 >>   Total optimization steps = 6\n",
      "/home/ubuntu/s3/fine-tuning/hyperparameters_test\n",
      "[INFO|integrations.py:371] 2021-02-01 15:23:02,544 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 7838\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152220-2v95q0vm/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152220-2v95q0vm/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 3.12997\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 39\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612192982\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 1.736\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 36.73\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 59.78\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 44.26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 22.04\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 37.53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 26.68\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 31.94\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 52.91\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 38.51\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 32.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 53.35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 38.67\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 59.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 72.51581\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos 16767021772800\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss █▃▂▁▄▂▄▂▁▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate █▇▇▆▅▅▄▄▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▂▂▃▄▄▅▅▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▂▂▃▄▄▅▅▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁▂▂▂▃▃▃▃▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁▂▂▂▃▃▃▃▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss █▁▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision ▁█▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall ▁█▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure ▁█▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision ▁▇▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall ▁▇▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure ▁▇▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision ▁▆▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall ▁▅▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure ▁▅▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision ▁█▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall ▁█▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure ▁█▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len ▁▆█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine ▁▆▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/2v95q0vm\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/2qz0av63\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152302-2qz0av63\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "                                             {'loss': 5.04779052734375, 'learning_rate': 0.00258466197114603, 'epoch': 0.16}\n",
      "                                             {'loss': 68.90023803710938, 'learning_rate': 0.002067729576916824, 'epoch': 0.32}\n",
      "                                             {'loss': 48.246826171875, 'learning_rate': 0.001550797182687618, 'epoch': 0.48}\n",
      " 50%|█████     | 3/6 [00:01<00:01,  2.02it/s][INFO|trainer.py:1412] 2021-02-01 15:23:08,380 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:23:08,381 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:23:08,381 >>   Batch size = 1\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.80it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:01,  1.99it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.61it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.53it/s]\u001b[A\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 114.61it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.25it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 127.01it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.58it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.89it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.22it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.16it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.60it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.30it/s]\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.12it/s]{'eval_loss': 15.517797470092773, 'eval_rouge1_precision': 0.8, 'eval_rouge1_recall': 0.59, 'eval_rouge1_fmeasure': 0.6799999999999999, 'eval_rouge2_precision': 0.0, 'eval_rouge2_recall': 0.0, 'eval_rouge2_fmeasure': 0.0, 'eval_rougeL_precision': 0.8, 'eval_rougeL_recall': 0.59, 'eval_rougeL_fmeasure': 0.6799999999999999, 'eval_rougeLsum_precision': 0.8, 'eval_rougeLsum_recall': 0.59, 'eval_rougeLsum_fmeasure': 0.6799999999999999, 'eval_gen_len': 60.0, 'eval_sentence_distilroberta_cosine': 9.136281907558441, 'epoch': 0.48}\n",
      "\n",
      "\n",
      "                                             \n",
      " 50%|█████     | 3/6 [00:05<00:01,  2.02it/s]\u001b[A\n",
      "                                             \u001b[A\u001b[32m[I 2021-02-01 15:23:12,294]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "[INFO|trainer.py:557] 2021-02-01 15:23:12,295 >> Trial:\n",
      "[INFO|modeling_utils.py:1024] 2021-02-01 15:23:12,588 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1140] 2021-02-01 15:23:30,734 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-02-01 15:23:30,734 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/01/2021 15:23:30 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "[INFO|trainer.py:703] 2021-02-01 15:23:31,070 >> ***** Running training *****\n",
      "/home/ubuntu/s3/fine-tuning/hyperparameters_test\n",
      "[INFO|trainer.py:704] 2021-02-01 15:23:31,070 >>   Num examples = 50\n",
      "[INFO|trainer.py:705] 2021-02-01 15:23:31,070 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:706] 2021-02-01 15:23:31,070 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:707] 2021-02-01 15:23:31,070 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:708] 2021-02-01 15:23:31,070 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:709] 2021-02-01 15:23:31,070 >>   Total optimization steps = 12\n",
      "[INFO|integrations.py:371] 2021-02-01 15:23:31,074 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 7994\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152302-2qz0av63/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152302-2qz0av63/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 48.24683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 0.00155\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612193011\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 15.5178\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 0.59\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 0.68\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 0.59\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 0.68\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 0.59\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 0.68\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 60.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 9.13628\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss ▁█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate █▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/2qz0av63\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/3imp95dn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152331-3imp95dn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 1/12 [00:00<00:03,  3.30it/s]\u001b[A\n",
      "{'loss': 6.2596588134765625, 'learning_rate': 0.0005150403160925196, 'epoch': 0.08}\n",
      "\u001b[A                                           \n",
      "  8%|▊         | 1/12 [00:00<00:03,  3.30it/s]\u001b[A\n",
      "{'loss': 12.52571964263916, 'learning_rate': 0.00046821846917501786, 'epoch': 0.16}\n",
      " 17%|█▋        | 2/12 [00:00<00:02,  3.72it/s]\u001b[A\n",
      "\u001b[A                                           \n",
      " 17%|█▋        | 2/12 [00:00<00:02,  3.72it/s]\u001b[A\n",
      " 25%|██▌       | 3/12 [00:00<00:02,  3.82it/s]\u001b[A\n",
      "{'loss': 13.204557418823242, 'learning_rate': 0.000421396622257516, 'epoch': 0.24}\n",
      "\u001b[A                                           \n",
      " 25%|██▌       | 3/12 [00:00<00:02,  3.82it/s]\u001b[A[INFO|trainer.py:1412] 2021-02-01 15:23:36,251 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:23:36,251 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:23:36,251 >>   Batch size = 1\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.29it/s]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.99it/s]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.26it/s]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.44it/s]\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.72it/s]{'eval_loss': 6.817545413970947, 'eval_rouge1_precision': 6.32, 'eval_rouge1_recall': 14.12, 'eval_rouge1_fmeasure': 8.469999999999999, 'eval_rouge2_precision': 0.0, 'eval_rouge2_recall': 0.0, 'eval_rouge2_fmeasure': 0.0, 'eval_rougeL_precision': 5.2, 'eval_rougeL_recall': 11.379999999999999, 'eval_rougeL_fmeasure': 6.93, 'eval_rougeLsum_precision': 5.2, 'eval_rougeLsum_recall': 11.450000000000001, 'eval_rougeLsum_fmeasure': 6.99, 'eval_gen_len': 60.0, 'eval_sentence_distilroberta_cosine': 12.8572016954422, 'epoch': 0.24}\n",
      "\n",
      "\n",
      "\u001b[A                                           \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      " 25%|██▌       | 3/12 [00:04<00:02,  3.82it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                             \u001b[A\u001b[A\u001b[32m[I 2021-02-01 15:23:40,078]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "[INFO|trainer.py:557] 2021-02-01 15:23:40,079 >> Trial:\n",
      "[INFO|modeling_utils.py:1024] 2021-02-01 15:23:40,363 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1140] 2021-02-01 15:23:59,681 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-02-01 15:23:59,682 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/01/2021 15:23:59 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
      "[INFO|trainer.py:703] 2021-02-01 15:24:00,012 >> ***** Running training *****\n",
      "/home/ubuntu/s3/fine-tuning/hyperparameters_test[INFO|trainer.py:704] 2021-02-01 15:24:00,012 >>   Num examples = 50\n",
      "[INFO|trainer.py:705] 2021-02-01 15:24:00,012 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:706] 2021-02-01 15:24:00,012 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:707] 2021-02-01 15:24:00,012 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:708] 2021-02-01 15:24:00,012 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:709] 2021-02-01 15:24:00,012 >>   Total optimization steps = 12\n",
      "\n",
      "[INFO|integrations.py:371] 2021-02-01 15:24:00,017 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 8146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152331-3imp95dn/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152331-3imp95dn/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 13.20456\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 0.00042\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612193040\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 6.81755\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 6.32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 14.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 8.47\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 5.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 11.38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 6.93\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 5.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 11.45\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 6.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 60.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 12.8572\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss ▁▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate █▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/3imp95dn\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/hfuizhff\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152400-hfuizhff\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 1/12 [00:00<00:03,  3.28it/s]\u001b[A\u001b[A{'loss': 6.2596588134765625, 'learning_rate': 4.712544215291119e-05, 'epoch': 0.08}\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A                                        \n",
      "\n",
      "  8%|▊         | 1/12 [00:00<00:03,  3.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 2/12 [00:00<00:02,  3.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "{'loss': 3.3907713890075684, 'learning_rate': 4.284131104810108e-05, 'epoch': 0.16}\n",
      "\u001b[A\u001b[A                                        \n",
      "\n",
      " 17%|█▋        | 2/12 [00:00<00:02,  3.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 3/12 [00:00<00:02,  3.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "{'loss': 2.6785104274749756, 'learning_rate': 3.855717994329097e-05, 'epoch': 0.24}\n",
      "\u001b[A\u001b[A                                        \n",
      "\n",
      " 25%|██▌       | 3/12 [00:00<00:02,  3.66it/s]\u001b[A\u001b[A[INFO|trainer.py:1412] 2021-02-01 15:24:05,255 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:24:05,255 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:24:05,255 >>   Batch size = 1\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 111.85it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.17it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.84it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.24it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 111.83it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.15it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.63it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 127.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.45it/s]\u001b[A\n",
      "{'eval_loss': 1.7444435358047485, 'eval_rouge1_precision': 37.13, 'eval_rouge1_recall': 59.96, 'eval_rouge1_fmeasure': 44.37, 'eval_rouge2_precision': 20.48, 'eval_rouge2_recall': 34.48, 'eval_rouge2_fmeasure': 24.55, 'eval_rougeL_precision': 29.110000000000003, 'eval_rougeL_recall': 47.48, 'eval_rougeL_fmeasure': 34.89, 'eval_rougeLsum_precision': 32.29, 'eval_rougeLsum_recall': 52.64, 'eval_rougeLsum_fmeasure': 39.03, 'eval_gen_len': 60.0, 'eval_sentence_distilroberta_cosine': 71.77233695983887, 'epoch': 0.24}\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                    \n",
      "\n",
      "\u001b[A\u001b[A                                        \n",
      "\n",
      "\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 3/12 [00:04<00:02,  3.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                             \u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 4/12 [00:04<00:14,  1.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "{'loss': 2.406606912612915, 'learning_rate': 3.427304883848086e-05, 'epoch': 0.32}\n",
      "\u001b[A\u001b[A                                        \n",
      "\n",
      " 33%|███▎      | 4/12 [00:04<00:14,  1.78s/it]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 5/12 [00:05<00:08,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "{'loss': 3.804718255996704, 'learning_rate': 2.9988917733670757e-05, 'epoch': 0.4}\n",
      "\u001b[A\u001b[A                                        \n",
      "\n",
      " 42%|████▏     | 5/12 [00:05<00:08,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 6/12 [00:05<00:05,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "{'loss': 2.9009153842926025, 'learning_rate': 2.5704786628860648e-05, 'epoch': 0.48}\n",
      "\u001b[A\u001b[A                                        \n",
      "\n",
      " 50%|█████     | 6/12 [00:05<00:05,  1.10it/s]\u001b[A\u001b[A[INFO|trainer.py:1412] 2021-02-01 15:24:09,890 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:24:09,890 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:24:09,890 >>   Batch size = 1\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:01<00:00,  2.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 118.88it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 119.68it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.30it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.08it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.86it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.40it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.64it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.12it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.98it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.68it/s]\u001b[A{'eval_loss': 1.6786811351776123, 'eval_rouge1_precision': 32.54, 'eval_rouge1_recall': 52.33, 'eval_rouge1_fmeasure': 38.96, 'eval_rouge2_precision': 16.869999999999997, 'eval_rouge2_recall': 31.64, 'eval_rouge2_fmeasure': 20.9, 'eval_rougeL_precision': 26.919999999999998, 'eval_rougeL_recall': 45.42, 'eval_rougeL_fmeasure': 32.54, 'eval_rougeLsum_precision': 27.169999999999998, 'eval_rougeLsum_recall': 45.410000000000004, 'eval_rougeLsum_fmeasure': 32.74, 'eval_gen_len': 59.4, 'eval_sentence_distilroberta_cosine': 67.2070860862732, 'epoch': 0.48}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                    \n",
      "\n",
      "\u001b[A\u001b[A                                        \n",
      "\n",
      "\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 6/12 [00:09<00:05,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                             \u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 7/12 [00:09<00:09,  1.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "{'loss': 3.7374558448791504, 'learning_rate': 2.142065552405054e-05, 'epoch': 0.56}\n",
      "\u001b[A\u001b[A                                        \n",
      "\n",
      " 58%|█████▊    | 7/12 [00:09<00:09,  1.93s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 8/12 [00:09<00:05,  1.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "{'loss': 2.9691381454467773, 'learning_rate': 1.713652441924043e-05, 'epoch': 0.64}\n",
      "\u001b[A\u001b[A                                        \n",
      "\n",
      " 67%|██████▋   | 8/12 [00:09<00:05,  1.40s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 9/12 [00:10<00:03,  1.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "{'loss': 2.36102557182312, 'learning_rate': 1.2852393314430324e-05, 'epoch': 0.72}\n",
      "\u001b[A\u001b[A                                        \n",
      "\n",
      " 75%|███████▌  | 9/12 [00:10<00:03,  1.05s/it]\u001b[A\u001b[A[INFO|trainer.py:1412] 2021-02-01 15:24:14,454 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:24:14,454 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:24:14,454 >>   Batch size = 1\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 118.88it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.50it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.58it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 127.07it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.60it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.90it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.79it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.34it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.08it/s]\u001b[A{'eval_loss': 1.6184492111206055, 'eval_rouge1_precision': 32.48, 'eval_rouge1_recall': 53.190000000000005, 'eval_rouge1_fmeasure': 39.03, 'eval_rouge2_precision': 17.07, 'eval_rouge2_recall': 30.919999999999998, 'eval_rouge2_fmeasure': 21.21, 'eval_rougeL_precision': 26.63, 'eval_rougeL_recall': 45.42, 'eval_rougeL_fmeasure': 32.48, 'eval_rougeLsum_precision': 28.57, 'eval_rougeLsum_recall': 47.64, 'eval_rougeLsum_fmeasure': 34.62, 'eval_gen_len': 59.4, 'eval_sentence_distilroberta_cosine': 69.03863549232483, 'epoch': 0.72}\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A                                        \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                    \n",
      "\n",
      " 75%|███████▌  | 9/12 [00:13<00:03,  1.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                             \u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 10/12 [00:14<00:03,  1.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         .568262209620216e-06, 'epoch': 0.8}\n",
      "\n",
      "\n",
      " 83%|████████▎ | 10/12 [00:14<00:03,  1.96s/it]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 11/12 [00:14<00:01,  1.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "{'loss': 2.720834732055664, 'learning_rate': 4.284131104810108e-06, 'epoch': 0.88}\n",
      "\u001b[A\u001b[A                                         \n",
      "\n",
      " 92%|█████████▏| 11/12 [00:14<00:01,  1.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 12/12 [00:14<00:00,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "{'loss': 3.040510654449463, 'learning_rate': 0.0, 'epoch': 0.96}\n",
      "\u001b[A\u001b[A                                         \n",
      "\n",
      "100%|██████████| 12/12 [00:14<00:00,  1.09s/it]\u001b[A\u001b[A[INFO|trainer.py:1412] 2021-02-01 15:24:19,002 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-02-01 15:24:19,002 >>   Num examples = 5\n",
      "[INFO|trainer.py:1414] 2021-02-01 15:24:19,002 >>   Batch size = 1\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 4/5 [00:02<00:00,  1.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.55it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.84it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.54it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.73it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.21it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.21it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.04it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.02it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.80it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.67it/s]\u001b[A\n",
      "{'eval_loss': 1.6175823211669922, 'eval_rouge1_precision': 37.3, 'eval_rouge1_recall': 59.78, 'eval_rouge1_fmeasure': 44.72, 'eval_rouge2_precision': 22.689999999999998, 'eval_rouge2_recall': 37.82, 'eval_rouge2_fmeasure': 27.47, 'eval_rougeL_precision': 31.35, 'eval_rougeL_recall': 51.580000000000005, 'eval_rougeL_fmeasure': 37.81, 'eval_rougeLsum_precision': 32.7, 'eval_rougeLsum_recall': 53.71, 'eval_rougeLsum_fmeasure': 39.44, 'eval_gen_len': 58.8, 'eval_sentence_distilroberta_cosine': 71.63667678833008, 'epoch': 0.96}\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                    \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                             \u001b[A\u001b[A\u001b[A[INFO|trainer.py:862] 2021-02-01 15:24:22,787 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "\n",
      "\n",
      "100%|██████████| 12/12 [00:18<00:00,  1.53s/it]\u001b[A\u001b[A\n",
      "\u001b[32m[I 2021-02-01 15:24:22,788]\u001b[0m Trial 7 finished with value: 548.0066767883301 and parameters: {'learning_rate': 5.1409573257721297e-05, 'gradient_accumulation_steps': 4}. Best is trial 7 with value: 548.0066767883301.\u001b[0m\n",
      "[INFO|trainer.py:557] 2021-02-01 15:24:22,788 >> Trial:\n",
      "[INFO|modeling_utils.py:1024] 2021-02-01 15:24:23,071 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1140] 2021-02-01 15:24:41,640 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-02-01 15:24:41,640 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/01/2021 15:24:41 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-02-01 15:24:42,103]\u001b[0m Trial 8 failed because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 14.76 GiB total capacity; 13.66 GiB already allocated; 13.75 MiB free; 13.83 GiB reserved in total by PyTorch)',)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/optuna/_optimize.py\", line 211, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/integrations.py\", line 168, in _objective\n",
      "    trainer.train(model_path=model_path, trial=trial)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/trainer.py\", line 622, in train\n",
      "    self.model = model.to(self.args.device)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 612, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 359, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 359, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 359, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 381, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 610, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 14.76 GiB total capacity; 13.66 GiB already allocated; 13.75 MiB free; 13.83 GiB reserved in total by PyTorch)\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/transformers/examples/seq2seq/finetune_trainer.py\", line 435, in <module>\n",
      "    main()\n",
      "  File \"/home/ubuntu/transformers/examples/seq2seq/finetune_trainer.py\", line 350, in main\n",
      "    backend = \"optuna\")\n",
      "  File \"/home/ubuntu/transformers/src/transformers/trainer.py\", line 1077, in hyperparameter_search\n",
      "    best_run = run_hp_search(self, n_trials, direction, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/integrations.py\", line 178, in run_hp_search_optuna\n",
      "    study.optimize(_objective, n_trials=n_trials, timeout=timeout, n_jobs=n_jobs)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/optuna/study.py\", line 385, in optimize\n",
      "    show_progress_bar=show_progress_bar,\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/optuna/_optimize.py\", line 73, in _optimize\n",
      "    progress_bar=progress_bar,\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/optuna/_optimize.py\", line 164, in _optimize_sequential\n",
      "    trial = _run_trial(study, func, catch)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/optuna/_optimize.py\", line 262, in _run_trial\n",
      "    raise func_err\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/optuna/_optimize.py\", line 211, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/integrations.py\", line 168, in _objective\n",
      "    trainer.train(model_path=model_path, trial=trial)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/trainer.py\", line 622, in train\n",
      "    self.model = model.to(self.args.device)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 612, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 359, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 359, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 359, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 381, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 610, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 14.76 GiB total capacity; 13.66 GiB already allocated; 13.75 MiB free; 13.83 GiB reserved in total by PyTorch)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 8306\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program failed with code 1.  Press ctrl-c to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152400-hfuizhff/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210201_152400-hfuizhff/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 3.04051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 39\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612193082\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 1.61758\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 37.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 59.78\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 44.72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 22.69\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 37.82\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 27.47\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 31.35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 51.58\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 37.81\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 32.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 53.71\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 39.44\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 58.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 71.63668\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos 16767021772800\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss █▃▂▁▄▂▃▂▁▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate █▇▇▆▅▅▄▄▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▂▂▃▄▄▅▅▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▂▂▃▄▄▅▅▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁▂▂▂▃▃▃▄▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁▂▂▂▃▃▃▄▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss █▄▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision █▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall █▁▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure █▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision ▅▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall ▅▂▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure ▅▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision ▅▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall ▃▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure ▄▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision ▇▁▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall ▇▁▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure █▁▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len █▅▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine █▁▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/home/ubuntu/s3/fine-tuning/hyperparameters_test\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/hfuizhff\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 $finetune_script \\\n",
    "--model_name_or_path $model_name_or_path \\\n",
    "--config_name $model_name_or_path \\\n",
    "--tokenizer_name $model_name_or_path \\\n",
    "--cache_dir $cache_dir \\\n",
    "--data_dir $data_dir \\\n",
    "--hyperparameters_search \\\n",
    "--fp16 \\\n",
    "--freeze_embeds --freeze_encoder \\\n",
    "--sortish_sampler \\\n",
    "--task summarization \\\n",
    "--max_source_length 512 \\\n",
    "--max_target_length 60 \\\n",
    "--val_max_target_length 60 \\\n",
    "--num_train_epochs 1 \\\n",
    "--n_train 50 \\\n",
    "--n_val 5 \\\n",
    "--logging_steps 1 --logging_first_step \\\n",
    "--per_device_train_batch_size 1 --per_device_eval_batch_size 1 \\\n",
    "--evaluation_strategy steps --eval_steps 3 \\\n",
    "--predict_with_generate \\\n",
    "--output_dir $output_dir \\\n",
    "--overwrite_output_dir \\\n",
    "--seed $config.SEED \\\n",
    "--run_name $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66ynxjmYEB5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LevExsI7oNF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "L5sXxqeNCtkN"
   ],
   "name": "bart_hyperparameters_search.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0cf5224ab8cb40198d84fa37cfcc68d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "46539ad5e91e4af9860cab29a8200b53": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3095d48314a49d2b81e066c2c3fd389",
      "placeholder": "​",
      "style": "IPY_MODEL_dad549b11e194e53af76c965e3f482a2",
      "value": " 1.52k/1.52k [00:00&lt;00:00, 50.8kB/s]"
     }
    },
    "4dca0f8d944943a7879b3de0a2f8347a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb421fdbad584767bff8da4c882fdfbf",
       "IPY_MODEL_46539ad5e91e4af9860cab29a8200b53"
      ],
      "layout": "IPY_MODEL_c8804e5babea4223b192bca1a9111fac"
     }
    },
    "521e472af5e24843a467a768c042d6c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb421fdbad584767bff8da4c882fdfbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_521e472af5e24843a467a768c042d6c6",
      "max": 1525,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0cf5224ab8cb40198d84fa37cfcc68d5",
      "value": 1525
     }
    },
    "c8804e5babea4223b192bca1a9111fac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3095d48314a49d2b81e066c2c3fd389": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dad549b11e194e53af76c965e3f482a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
