{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bart_hyperparameters_search.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "P95DxvqWi_2Y",
        "L5sXxqeNCtkN"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4dca0f8d944943a7879b3de0a2f8347a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c8804e5babea4223b192bca1a9111fac",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bb421fdbad584767bff8da4c882fdfbf",
              "IPY_MODEL_46539ad5e91e4af9860cab29a8200b53"
            ]
          }
        },
        "c8804e5babea4223b192bca1a9111fac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb421fdbad584767bff8da4c882fdfbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0cf5224ab8cb40198d84fa37cfcc68d5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1525,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1525,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_521e472af5e24843a467a768c042d6c6"
          }
        },
        "46539ad5e91e4af9860cab29a8200b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dad549b11e194e53af76c965e3f482a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.52k/1.52k [00:00&lt;00:00, 50.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d3095d48314a49d2b81e066c2c3fd389"
          }
        },
        "0cf5224ab8cb40198d84fa37cfcc68d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "521e472af5e24843a467a768c042d6c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dad549b11e194e53af76c965e3f482a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d3095d48314a49d2b81e066c2c3fd389": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P95DxvqWi_2Y"
      },
      "source": [
        "#### For Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD_KFnI1H1ip",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a59e5a84-a959-4fb6-928f-951b5e72be11"
      },
      "source": [
        "\"\"\"\n",
        "function ClickConnect(){\n",
        "    console.log(\"Working\");\n",
        "    document.querySelector(\"colab-toolbar-button\").click() \n",
        "}\n",
        "var i = setInterval(ClickConnect, 900000)\n",
        "clearInterval(i)\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfunction ClickConnect(){\\n    console.log(\"Working\");\\n    document.querySelector(\"colab-toolbar-button\").click() \\n}\\nvar i = setInterval(ClickConnect, 900000)\\nclearInterval(i)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBdw_oH-9Dn7",
        "outputId": "f2c41642-dbba-47ac-cba3-af782b8f1255"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb  1 14:49:37 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR-OcN_Wy1jE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dd746a5-fe46-4eb6-d93f-977932616391"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au5Z9XQAC7C-"
      },
      "source": [
        "drive_dir = '/content/drive/My Drive/MAGMA: Summarization/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5sXxqeNCtkN"
      },
      "source": [
        "#### Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6KSIQlYzjEy"
      },
      "source": [
        "%%capture\n",
        "\n",
        "requirements_dir = (drive_dir + 'transformers/examples/seq2seq/').replace(' ', '\\ ')\n",
        "requirements_file = requirements_dir + 'requirements.txt'\n",
        "!cd $requirements_dir; pip install -r $requirements_file\n",
        "\n",
        "!pip install transformers==4.1.1\n",
        "!pip install -U pyarrow\n",
        "!pip install -U wandb\n",
        "!pip install -U sentence-transformers\n",
        "!pip install -U optuna\n",
        "!pip install -U \"ray[tune]\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EddY1WDNsKlS"
      },
      "source": [
        "## **Fine-tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d6M41X9AKBi"
      },
      "source": [
        "finetune_script = '\"'+drive_dir+'transformers/examples/seq2seq/finetune_trainer.py\"'\n",
        "eval_script = '\"'+drive_dir+'transformers/examples/seq2seq/run_eval.py\"'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0FByNNOIRvG"
      },
      "source": [
        "### **Config**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82WSp6khIcua",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "6b5776d2-758b-455f-f3f5-6ff21f1b0a9f"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, drive_dir)\n",
        "import config\n",
        "\n",
        "import torch\n",
        "torch.manual_seed = config.SEED\n",
        "\n",
        "import wandb\n",
        "wandb.login()\n",
        "project_name = 'hp_search_para_wordembed'\n",
        "%env WANDB_PROJECT=$project_name"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "env: WANDB_PROJECT=hp_search_para_wordembed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPZ7A-sBVOam"
      },
      "source": [
        "### HP search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJOYl_g6F1e2"
      },
      "source": [
        "model_name_or_path = 'sshleifer/distilbart-cnn-12-6'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xs_XkbCVOar"
      },
      "source": [
        "data_dir = '\"'+drive_dir+'datasets/xsum\"'\n",
        "\n",
        "output_dir = '\"'+drive_dir+'fine-tuning/hp_search_test\"'\n",
        "\n",
        "log_dir = drive_dir + '/logs'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtLC1O1ZJpU3",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "4dca0f8d944943a7879b3de0a2f8347a",
            "c8804e5babea4223b192bca1a9111fac",
            "bb421fdbad584767bff8da4c882fdfbf",
            "46539ad5e91e4af9860cab29a8200b53",
            "0cf5224ab8cb40198d84fa37cfcc68d5",
            "521e472af5e24843a467a768c042d6c6",
            "dad549b11e194e53af76c965e3f482a2",
            "d3095d48314a49d2b81e066c2c3fd389"
          ]
        },
        "outputId": "317a9e46-1c7b-4fba-c55a-eef2f13d243e"
      },
      "source": [
        "from transformers import AutoConfig\n",
        "model_config = AutoConfig.from_pretrained('facebook/bart-large')\n",
        "model_config.activation_dropout = 0\n",
        "model_config.attention_dropout = 0\n",
        "model_config.classif_dropout = 0\n",
        "model_config.min_length = 11\n",
        "model_config.max_length = 63\n",
        "model_config.no_repeat_ngram_size = 3\n",
        "model_config.num_beams = 2\n",
        "model_config.task_specific_params['summarization']['length_penalty'] = 1\n",
        "model_config.task_specific_params['summarization']['min_length'] = 11\n",
        "model_config.task_specific_params['summarization']['max_length'] = 62\n",
        "model_config.task_specific_params['summarization']['num_beams'] = 2\n",
        "\n",
        "model_config_dir = '\"'+drive_dir+'fine-tuning/'+\\\n",
        "    model_name_or_path.replace('/', '?')+'_config\"'\n",
        "model_config.save_pretrained(model_config_dir[1:-1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4dca0f8d944943a7879b3de0a2f8347a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1525.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K66kY1WOVOaw",
        "scrolled": true,
        "outputId": "c50158bf-7522-42c0-d9e7-164da464ba26"
      },
      "source": [
        "!python3 $finetune_script \\\n",
        "--model_name_or_path $model_name_or_path \\\n",
        "--config_name $model_name_or_path \\\n",
        "--tokenizer_name $model_name_or_path \\\n",
        "--data_dir $data_dir \\\n",
        "--hyperparameters_search \\\n",
        "--fp16 \\\n",
        "--freeze_embeds --freeze_encoder \\\n",
        "--sortish_sampler \\\n",
        "--task summarization \\\n",
        "--max_source_length 256 \\\n",
        "--max_target_length 60 \\\n",
        "--val_max_target_length 60 \\\n",
        "--num_train_epochs 1 \\\n",
        "--n_train 50 \\\n",
        "--n_val 5 \\\n",
        "--logging_steps 1 --logging_first_step \\\n",
        "--per_device_train_batch_size 1 --per_device_eval_batch_size 1 \\\n",
        "--evaluation_strategy steps --eval_steps 3 \\\n",
        "--predict_with_generate \\\n",
        "--output_dir $output_dir \\\n",
        "--overwrite_output_dir \\\n",
        "--seed $config.SEED \\\n",
        "--run_name $output_dir"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-01 14:51:58.937197: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "100% 306M/306M [00:12<00:00, 25.3MB/s]\n",
            "02/01/2021 14:52:22 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
            "02/01/2021 14:52:22 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test', overwrite_output_dir=True, do_train=False, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Feb01_14-52-22_19b1e3cac7e6', logging_first_step=True, logging_steps=1, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=3, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False, label_smoothing=0.0, sortish_sampler=True, predict_with_generate=True, adafactor=False, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear')\n",
            "02/01/2021 14:52:22 - INFO - filelock -   Lock 140600086277536 acquired on /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.9307b6cec4435559ec6e79d5a210a334b17706465329e138f335649d14f27e78.lock\n",
            "[INFO|file_utils.py:1301] 2021-02-01 14:52:22,673 >> https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpthln_mdt\n",
            "Downloading: 100% 1.62k/1.62k [00:00<00:00, 2.36MB/s]\n",
            "[INFO|file_utils.py:1305] 2021-02-01 14:52:22,690 >> storing https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.9307b6cec4435559ec6e79d5a210a334b17706465329e138f335649d14f27e78\n",
            "[INFO|file_utils.py:1308] 2021-02-01 14:52:22,690 >> creating metadata file for /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.9307b6cec4435559ec6e79d5a210a334b17706465329e138f335649d14f27e78\n",
            "02/01/2021 14:52:22 - INFO - filelock -   Lock 140600086277536 released on /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.9307b6cec4435559ec6e79d5a210a334b17706465329e138f335649d14f27e78.lock\n",
            "[INFO|configuration_utils.py:431] 2021-02-01 14:52:22,691 >> loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.9307b6cec4435559ec6e79d5a210a334b17706465329e138f335649d14f27e78\n",
            "[INFO|configuration_utils.py:467] 2021-02-01 14:52:22,691 >> Model config BartConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_blenderbot_90_layernorm\": false,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"extra_pos_embeddings\": 2,\n",
            "  \"force_bos_token_to_be_generated\": true,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \" \",\n",
            "  \"replacing_rate\": 0,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"student_decoder_layers\": null,\n",
            "  \"student_encoder_layers\": null,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:431] 2021-02-01 14:52:22,709 >> loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.9307b6cec4435559ec6e79d5a210a334b17706465329e138f335649d14f27e78\n",
            "[INFO|configuration_utils.py:467] 2021-02-01 14:52:22,709 >> Model config BartConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_blenderbot_90_layernorm\": false,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"extra_pos_embeddings\": 2,\n",
            "  \"force_bos_token_to_be_generated\": true,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 142,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 56,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \" \",\n",
            "  \"replacing_rate\": 0,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"student_decoder_layers\": null,\n",
            "  \"student_encoder_layers\": null,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1721] 2021-02-01 14:52:22,709 >> Model name 'sshleifer/distilbart-cnn-12-6' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'sshleifer/distilbart-cnn-12-6' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/01/2021 14:52:22 - INFO - filelock -   Lock 140600096267568 acquired on /root/.cache/huggingface/transformers/9951e68693b9a7c583ae677e9cb53c02715d9bd0311a78706401372653cdea0a.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05.lock\n",
            "[INFO|file_utils.py:1301] 2021-02-01 14:52:22,732 >> https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp91cgaytp\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 26.2MB/s]\n",
            "[INFO|file_utils.py:1305] 2021-02-01 14:52:22,789 >> storing https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/9951e68693b9a7c583ae677e9cb53c02715d9bd0311a78706401372653cdea0a.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
            "[INFO|file_utils.py:1308] 2021-02-01 14:52:22,789 >> creating metadata file for /root/.cache/huggingface/transformers/9951e68693b9a7c583ae677e9cb53c02715d9bd0311a78706401372653cdea0a.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
            "02/01/2021 14:52:22 - INFO - filelock -   Lock 140600096267568 released on /root/.cache/huggingface/transformers/9951e68693b9a7c583ae677e9cb53c02715d9bd0311a78706401372653cdea0a.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05.lock\n",
            "02/01/2021 14:52:22 - INFO - filelock -   Lock 140600096267568 acquired on /root/.cache/huggingface/transformers/7588c8d398d659b230a038240cc023f67b6848117d2999f06ab625af7bfc7ec1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
            "[INFO|file_utils.py:1301] 2021-02-01 14:52:22,809 >> https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpr3nmjk_7\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 26.1MB/s]\n",
            "[INFO|file_utils.py:1305] 2021-02-01 14:52:22,847 >> storing https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/7588c8d398d659b230a038240cc023f67b6848117d2999f06ab625af7bfc7ec1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1308] 2021-02-01 14:52:22,847 >> creating metadata file for /root/.cache/huggingface/transformers/7588c8d398d659b230a038240cc023f67b6848117d2999f06ab625af7bfc7ec1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "02/01/2021 14:52:22 - INFO - filelock -   Lock 140600096267568 released on /root/.cache/huggingface/transformers/7588c8d398d659b230a038240cc023f67b6848117d2999f06ab625af7bfc7ec1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
            "02/01/2021 14:52:22 - INFO - filelock -   Lock 140600105079416 acquired on /root/.cache/huggingface/transformers/f5316f64f9716436994a7ad76a354dc20ecb2dd74eb61d278f103a9c8b80291f.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8.lock\n",
            "[INFO|file_utils.py:1301] 2021-02-01 14:52:22,913 >> https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptoh9gc9a\n",
            "Downloading: 100% 26.0/26.0 [00:00<00:00, 44.7kB/s]\n",
            "[INFO|file_utils.py:1305] 2021-02-01 14:52:22,929 >> storing https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/f5316f64f9716436994a7ad76a354dc20ecb2dd74eb61d278f103a9c8b80291f.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n",
            "[INFO|file_utils.py:1308] 2021-02-01 14:52:22,929 >> creating metadata file for /root/.cache/huggingface/transformers/f5316f64f9716436994a7ad76a354dc20ecb2dd74eb61d278f103a9c8b80291f.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n",
            "02/01/2021 14:52:22 - INFO - filelock -   Lock 140600105079416 released on /root/.cache/huggingface/transformers/f5316f64f9716436994a7ad76a354dc20ecb2dd74eb61d278f103a9c8b80291f.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8.lock\n",
            "[INFO|tokenization_utils_base.py:1802] 2021-02-01 14:52:22,929 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/9951e68693b9a7c583ae677e9cb53c02715d9bd0311a78706401372653cdea0a.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
            "[INFO|tokenization_utils_base.py:1802] 2021-02-01 14:52:22,929 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/7588c8d398d659b230a038240cc023f67b6848117d2999f06ab625af7bfc7ec1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1802] 2021-02-01 14:52:22,929 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2021-02-01 14:52:22,929 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2021-02-01 14:52:22,929 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2021-02-01 14:52:22,929 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f5316f64f9716436994a7ad76a354dc20ecb2dd74eb61d278f103a9c8b80291f.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n",
            "02/01/2021 14:52:23 - INFO - filelock -   Lock 140600222449616 acquired on /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844.lock\n",
            "[INFO|file_utils.py:1301] 2021-02-01 14:52:23,068 >> https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5vh4l6m1\n",
            "Downloading: 100% 1.22G/1.22G [00:15<00:00, 78.1MB/s]\n",
            "[INFO|file_utils.py:1305] 2021-02-01 14:52:38,850 >> storing https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
            "[INFO|file_utils.py:1308] 2021-02-01 14:52:38,850 >> creating metadata file for /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
            "02/01/2021 14:52:38 - INFO - filelock -   Lock 140600222449616 released on /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844.lock\n",
            "[INFO|modeling_utils.py:1024] 2021-02-01 14:52:38,850 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
            "[INFO|modeling_utils.py:1140] 2021-02-01 14:52:53,893 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1149] 2021-02-01 14:52:53,894 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "02/01/2021 14:52:53 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
            "[INFO|modeling_utils.py:1024] 2021-02-01 14:53:03,989 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
            "[INFO|modeling_utils.py:1140] 2021-02-01 14:53:28,412 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1149] 2021-02-01 14:53:28,412 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "02/01/2021 14:53:28 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
            "02/01/2021 14:53:43 - INFO - __main__ -   *** Hyperparameters Search ***\n",
            "\u001b[32m[I 2021-02-01 14:53:43,595]\u001b[0m A new study created in memory with name: no-name-0cd83979-193e-46c7-8b93-29684ab02eda\u001b[0m\n",
            "[INFO|trainer.py:557] 2021-02-01 14:53:43,603 >> Trial:\n",
            "[INFO|modeling_utils.py:1024] 2021-02-01 14:53:43,634 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
            "[INFO|modeling_utils.py:1140] 2021-02-01 14:53:58,680 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1149] 2021-02-01 14:53:58,680 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "02/01/2021 14:53:58 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
            "[INFO|trainer.py:703] 2021-02-01 14:53:59,298 >> ***** Running training *****\n",
            "[INFO|trainer.py:704] 2021-02-01 14:53:59,298 >>   Num examples = 50\n",
            "[INFO|trainer.py:705] 2021-02-01 14:53:59,298 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:706] 2021-02-01 14:53:59,298 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:707] 2021-02-01 14:53:59,298 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:708] 2021-02-01 14:53:59,298 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:709] 2021-02-01 14:53:59,298 >>   Total optimization steps = 6\n",
            "/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test\n",
            "[INFO|integrations.py:371] 2021-02-01 14:53:59,328 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "2021-02-01 14:54:03.590418: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/f8qvn4h0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210201_145359-f8qvn4h0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "{'loss': 4.3311567306518555, 'learning_rate': 0.0004456380359667223, 'epoch': 0.16}\n",
            "{'loss': 8.735233306884766, 'learning_rate': 0.0003565104287733778, 'epoch': 0.32}\n",
            "{'loss': 7.340000629425049, 'learning_rate': 0.00026738282158003337, 'epoch': 0.48}\n",
            " 50%|█████     | 3/6 [00:14<00:21,  7.11s/it][INFO|trainer.py:1412] 2021-02-01 14:54:21,401 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1413] 2021-02-01 14:54:21,401 >>   Num examples = 5\n",
            "[INFO|trainer.py:1414] 2021-02-01 14:54:21,403 >>   Batch size = 1\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.80it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.15it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:02<00:00,  1.85it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:02<00:00,  1.66it/s]\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.73it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.47it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.86it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.69it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.22it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.82it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.54it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.13it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.87it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.12it/s]\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 7.667572021484375, 'eval_rouge1_precision': 24.349999999999998, 'eval_rouge1_recall': 24.15, 'eval_rouge1_fmeasure': 23.66, 'eval_rouge2_precision': 0.0, 'eval_rouge2_recall': 0.0, 'eval_rouge2_fmeasure': 0.0, 'eval_rougeL_precision': 18.72, 'eval_rougeL_recall': 18.25, 'eval_rougeL_fmeasure': 17.96, 'eval_rougeLsum_precision': 20.57, 'eval_rougeLsum_recall': 20.330000000000002, 'eval_rougeLsum_fmeasure': 19.939999999999998, 'eval_gen_len': 58.2, 'eval_sentence_distilroberta_cosine': 20.29363214969635, 'epoch': 0.48}\n",
            " 50%|█████     | 3/6 [00:19<00:21,  7.11s/it]\n",
            "100%|██████████| 5/5 [00:03<00:00,  1.66it/s]\u001b[A\n",
            "{'loss': 8.744010925292969, 'learning_rate': 0.0001782552143866889, 'epoch': 0.64}\n",
            "{'loss': 4.513834476470947, 'learning_rate': 8.912760719334445e-05, 'epoch': 0.8}\n",
            "                                             {'loss': 3.70139741897583, 'learning_rate': 0.0, 'epoch': 0.96}\n",
            "100%|██████████| 6/6 [00:21<00:00,  3.49s/it][INFO|trainer.py:1412] 2021-02-01 14:54:27,882 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1413] 2021-02-01 14:54:27,882 >>   Num examples = 5\n",
            "[INFO|trainer.py:1414] 2021-02-01 14:54:27,882 >>   Batch size = 1\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.76it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.11it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:02<00:00,  1.83it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:02<00:00,  1.65it/s]\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 103.13it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.10it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.48it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.46it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.69it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.01it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.38it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.02it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.34it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.23it/s]\n",
            "                                             \n",
            "{'eval_loss': 3.2965080738067627, 'eval_rouge1_precision': 12.2, 'eval_rouge1_recall': 34.489999999999995, 'eval_rouge1_fmeasure': 17.68, 'eval_rouge2_precision': 1.26, 'eval_rouge2_recall': 2.9499999999999997, 'eval_rouge2_fmeasure': 1.77, 'eval_rougeL_precision': 9.71, 'eval_rougeL_recall': 28.1, 'eval_rougeL_fmeasure': 14.149999999999999, 'eval_rougeLsum_precision': 9.59, 'eval_rougeLsum_recall': 27.97, 'eval_rougeLsum_fmeasure': 14.11, 'eval_gen_len': 59.8, 'eval_sentence_distilroberta_cosine': 45.097023248672485, 'epoch': 0.96}\n",
            "100%|██████████| 6/6 [00:25<00:00,  3.49s/it]\n",
            "100%|██████████| 5/5 [00:03<00:00,  1.65it/s]\u001b[A\n",
            "                                             \u001b[A[INFO|trainer.py:862] 2021-02-01 14:54:31,798 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'epoch': 0.96}\n",
            "100%|██████████| 6/6 [00:25<00:00,  4.23s/it]\n",
            "\u001b[32m[I 2021-02-01 14:54:31,802]\u001b[0m Trial 0 finished with value: 219.0770232486725 and parameters: {'learning_rate': 0.0005347656431600667, 'gradient_accumulation_steps': 8}. Best is trial 0 with value: 219.0770232486725.\u001b[0m\n",
            "[INFO|trainer.py:557] 2021-02-01 14:54:31,802 >> Trial:\n",
            "[INFO|modeling_utils.py:1024] 2021-02-01 14:54:31,830 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
            "[INFO|modeling_utils.py:1140] 2021-02-01 14:54:54,086 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1149] 2021-02-01 14:54:54,086 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "02/01/2021 14:54:54 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
            "[INFO|trainer.py:703] 2021-02-01 14:54:54,524 >> ***** Running training *****\n",
            "[INFO|trainer.py:704] 2021-02-01 14:54:54,524 >>   Num examples = 50\n",
            "[INFO|trainer.py:705] 2021-02-01 14:54:54,524 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:706] 2021-02-01 14:54:54,524 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:707] 2021-02-01 14:54:54,524 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:708] 2021-02-01 14:54:54,524 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:709] 2021-02-01 14:54:54,524 >>   Total optimization steps = 6\n",
            "[INFO|integrations.py:371] 2021-02-01 14:54:54,534 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 529\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210201_145359-f8qvn4h0/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210201_145359-f8qvn4h0/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 3.7014\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.96\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 55\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612191294\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 3.29651\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 34.49\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 17.68\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 1.26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 2.95\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 1.77\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 9.71\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 28.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 14.15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 9.59\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 27.97\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 14.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 59.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 45.09702\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos 24284410675200\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss ▂█▆█▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate █▇▅▄▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▂▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▂▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁▂▂▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁▂▂▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/f8qvn4h0\u001b[0m\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "2021-02-01 14:55:00.190207: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/gwj55owa\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210201_145454-gwj55owa\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "                                             \n",
            "                                             \n",
            "                                             \n",
            " 50%|█████     | 3/6 [00:01<00:01,  1.78it/s][INFO|trainer.py:1412] 2021-02-01 14:55:04,696 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1413] 2021-02-01 14:55:04,696 >>   Num examples = 5\n",
            "[INFO|trainer.py:1414] 2021-02-01 14:55:04,696 >>   Batch size = 1\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.73it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.11it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:02<00:00,  1.80it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:02<00:00,  1.63it/s]\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.37it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.48it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.35it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.66it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.85it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.22it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.36it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.40it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.92it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 144.47it/s]\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 3.8098273277282715, 'eval_rouge1_precision': 10.620000000000001, 'eval_rouge1_recall': 29.470000000000002, 'eval_rouge1_fmeasure': 15.43, 'eval_rouge2_precision': 1.5699999999999998, 'eval_rouge2_recall': 4.3999999999999995, 'eval_rouge2_fmeasure': 2.3, 'eval_rougeL_precision': 7.42, 'eval_rougeL_recall': 21.33, 'eval_rougeL_fmeasure': 10.85, 'eval_rougeLsum_precision': 8.93, 'eval_rougeLsum_recall': 25.05, 'eval_rougeLsum_fmeasure': 13.07, 'eval_gen_len': 59.6, 'eval_sentence_distilroberta_cosine': 45.705634355545044, 'epoch': 0.48}\n",
            " 50%|█████     | 3/6 [00:05<00:01,  1.78it/s]\n",
            "100%|██████████| 5/5 [00:03<00:00,  1.63it/s]\u001b[A\n",
            "                                             \n",
            "                                             {'loss': 3.6370298862457275, 'learning_rate': 2.492255594963116e-06, 'epoch': 0.8}\n",
            "                                             {'loss': 3.635002851486206, 'learning_rate': 0.0, 'epoch': 0.96}\n",
            "100%|██████████| 6/6 [00:07<00:00,  1.09s/it][INFO|trainer.py:1412] 2021-02-01 14:55:10,064 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1413] 2021-02-01 14:55:10,064 >>   Num examples = 5\n",
            "[INFO|trainer.py:1414] 2021-02-01 14:55:10,064 >>   Batch size = 1\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.74it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.11it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:02<00:00,  1.81it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:02<00:00,  1.64it/s]\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 118.53it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.63it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.45it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.69it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.87it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.99it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.11it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.89it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.86it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.23it/s]\n",
            "                                             \n",
            "\n",
            "100%|██████████| 6/6 [00:11<00:00,  1.09s/it]\n",
            "100%|██████████| 5/5 [00:03<00:00,  1.64it/s]\u001b[A\n",
            "                                             \u001b[A[INFO|trainer.py:862] 2021-02-01 14:55:13,982 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "                                             \n",
            "100%|██████████| 6/6 [00:11<00:00,  1.84s/it]\n",
            "\u001b[32m[I 2021-02-01 14:55:13,986]\u001b[0m Trial 1 finished with value: 196.91563435554502 and parameters: {'learning_rate': 1.4953533569778698e-05, 'gradient_accumulation_steps': 8}. Best is trial 0 with value: 219.0770232486725.\u001b[0m\n",
            "[INFO|trainer.py:557] 2021-02-01 14:55:13,986 >> Trial:\n",
            "[INFO|modeling_utils.py:1024] 2021-02-01 14:55:14,090 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
            "[INFO|modeling_utils.py:1140] 2021-02-01 14:55:28,886 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1149] 2021-02-01 14:55:28,886 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "02/01/2021 14:55:28 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
            "[INFO|trainer.py:703] 2021-02-01 14:55:29,340 >> ***** Running training *****\n",
            "[INFO|trainer.py:704] 2021-02-01 14:55:29,340 >>   Num examples = 50\n",
            "[INFO|trainer.py:705] 2021-02-01 14:55:29,340 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:706] 2021-02-01 14:55:29,340 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:707] 2021-02-01 14:55:29,340 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:708] 2021-02-01 14:55:29,340 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:709] 2021-02-01 14:55:29,340 >>   Total optimization steps = 6\n",
            "/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test\n",
            "[INFO|integrations.py:371] 2021-02-01 14:55:29,351 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 579\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210201_145454-gwj55owa/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210201_145454-gwj55owa/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 3.635\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.96\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 31\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612191329\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 3.61761\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 10.66\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 29.96\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 15.47\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 1.57\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 4.45\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 2.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 7.42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 21.33\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 10.86\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 9.03\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 25.05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 13.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 59.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 45.70563\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos 24284410675200\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss █▃▁▃▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate █▇▅▄▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▂▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▂▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁▂▂▃█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁▂▂▃█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/gwj55owa\u001b[0m\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "2021-02-01 14:55:33.874290: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/q0jhs02g\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210201_145529-q0jhs02g\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "                                             \n",
            " 33%|███▎      | 2/6 [00:01<00:02,  1.92it/s]{'loss': 3.49271559715271, 'learning_rate': 2.9552562942603524e-05, 'epoch': 0.32}\n",
            "                                             \n",
            " 50%|█████     | 3/6 [00:01<00:01,  1.95it/s][INFO|trainer.py:1412] 2021-02-01 14:55:37,244 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1413] 2021-02-01 14:55:37,244 >>   Num examples = 5\n",
            "[INFO|trainer.py:1414] 2021-02-01 14:55:37,244 >>   Batch size = 1\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.71it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.10it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:02<00:00,  1.79it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:02<00:00,  1.63it/s]\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.62it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.31it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.58it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.62it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.15it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.56it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.77it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 103.27it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.04it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.79it/s]\n",
            "{'eval_loss': 3.053359031677246, 'eval_rouge1_precision': 12.29, 'eval_rouge1_recall': 34.11, 'eval_rouge1_fmeasure': 17.89, 'eval_rouge2_precision': 2.04, 'eval_rouge2_recall': 5.8500000000000005, 'eval_rouge2_fmeasure': 3.01, 'eval_rougeL_precision': 9.47, 'eval_rougeL_recall': 26.68, 'eval_rougeL_fmeasure': 13.77, 'eval_rougeLsum_precision': 11.09, 'eval_rougeLsum_recall': 30.990000000000002, 'eval_rougeLsum_fmeasure': 16.14, 'eval_gen_len': 59.6, 'eval_sentence_distilroberta_cosine': 43.56685280799866, 'epoch': 0.48}\n",
            "\n",
            " 50%|█████     | 3/6 [00:05<00:01,  1.95it/s]\n",
            "100%|██████████| 5/5 [00:03<00:00,  1.63it/s]\u001b[A\n",
            "{'loss': 3.225928544998169, 'learning_rate': 1.4776281471301762e-05, 'epoch': 0.64}\n",
            "{'loss': 3.0255231857299805, 'learning_rate': 7.388140735650881e-06, 'epoch': 0.8}\n",
            "{'loss': 2.9681859016418457, 'learning_rate': 0.0, 'epoch': 0.96}\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.07s/it][INFO|trainer.py:1412] 2021-02-01 14:55:42,629 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1413] 2021-02-01 14:55:42,629 >>   Num examples = 5\n",
            "[INFO|trainer.py:1414] 2021-02-01 14:55:42,629 >>   Batch size = 1\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.73it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.11it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:02<00:00,  1.80it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:02<00:00,  1.65it/s]\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 117.89it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.22it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.25it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.27it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.90it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 127.72it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.72it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.98it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.09it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.54it/s]\n",
            "                                             \n",
            "{'eval_loss': 2.9781789779663086, 'eval_rouge1_precision': 12.06, 'eval_rouge1_recall': 32.76, 'eval_rouge1_fmeasure': 17.36, 'eval_rouge2_precision': 0.84, 'eval_rouge2_recall': 2.25, 'eval_rouge2_fmeasure': 1.22, 'eval_rougeL_precision': 8.260000000000002, 'eval_rougeL_recall': 22.400000000000002, 'eval_rougeL_fmeasure': 11.940000000000001, 'eval_rougeLsum_precision': 9.44, 'eval_rougeLsum_recall': 25.52, 'eval_rougeLsum_fmeasure': 13.59, 'eval_gen_len': 60.0, 'eval_sentence_distilroberta_cosine': 40.29236435890198, 'epoch': 0.96}\n",
            "100%|██████████| 6/6 [00:10<00:00,  1.07s/it]\n",
            "100%|██████████| 5/5 [00:03<00:00,  1.65it/s]\u001b[A\n",
            "                                             \u001b[A[INFO|trainer.py:862] 2021-02-01 14:55:46,541 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'epoch': 0.96}\n",
            "100%|██████████| 6/6 [00:10<00:00,  1.80s/it]\n",
            "\u001b[32m[I 2021-02-01 14:55:46,544]\u001b[0m Trial 2 finished with value: 197.932364358902 and parameters: {'learning_rate': 4.432884441390529e-05, 'gradient_accumulation_steps': 8}. Best is trial 0 with value: 219.0770232486725.\u001b[0m\n",
            "[INFO|trainer.py:557] 2021-02-01 14:55:46,544 >> Trial:\n",
            "[INFO|modeling_utils.py:1024] 2021-02-01 14:55:46,648 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
            "[INFO|modeling_utils.py:1140] 2021-02-01 14:56:01,358 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1149] 2021-02-01 14:56:01,358 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "02/01/2021 14:56:01 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
            "[INFO|trainer.py:703] 2021-02-01 14:56:01,796 >> ***** Running training *****\n",
            "[INFO|trainer.py:704] 2021-02-01 14:56:01,796 >>   Num examples = 50\n",
            "[INFO|trainer.py:705] 2021-02-01 14:56:01,796 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:706] 2021-02-01 14:56:01,796 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:707] 2021-02-01 14:56:01,796 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:708] 2021-02-01 14:56:01,796 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:709] 2021-02-01 14:56:01,796 >>   Total optimization steps = 12\n",
            "/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test\n",
            "[INFO|integrations.py:371] 2021-02-01 14:56:01,806 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 618\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210201_145529-q0jhs02g/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210201_145529-q0jhs02g/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 2.96819\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.96\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612191361\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 2.97818\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 12.06\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 32.76\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 17.36\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 0.84\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 2.25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 1.22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 8.26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 22.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 11.94\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 9.44\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 25.52\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 13.59\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 60.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 40.29236\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos 24284410675200\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss █▄▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate █▇▅▄▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▂▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▂▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁▂▃▃█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁▂▃▃█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len ▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/q0jhs02g\u001b[0m\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "2021-02-01 14:56:06.332748: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/3ugiea6a\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210201_145601-3ugiea6a\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "                                              \n",
            "                                              \n",
            "                                              \n",
            " 25%|██▌       | 3/12 [00:00<00:02,  3.23it/s][INFO|trainer.py:1412] 2021-02-01 14:56:08,750 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1413] 2021-02-01 14:56:08,750 >>   Num examples = 5\n",
            "[INFO|trainer.py:1414] 2021-02-01 14:56:08,750 >>   Batch size = 1\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.68it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.09it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:02<00:00,  1.78it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:02<00:00,  1.62it/s]\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 111.59it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.29it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.84it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.45it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.01it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.59it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.94it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.46it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 93.98it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 101.32it/s]\n",
            "\n",
            "{'eval_loss': 3.039475202560425, 'eval_rouge1_precision': 12.76, 'eval_rouge1_recall': 34.39, 'eval_rouge1_fmeasure': 18.310000000000002, 'eval_rouge2_precision': 2.15, 'eval_rouge2_recall': 6.419999999999999, 'eval_rouge2_fmeasure': 3.16, 'eval_rougeL_precision': 8.09, 'eval_rougeL_recall': 22.29, 'eval_rougeL_fmeasure': 11.67, 'eval_rougeLsum_precision': 8.799999999999999, 'eval_rougeLsum_recall': 24.3, 'eval_rougeLsum_fmeasure': 12.6, 'eval_gen_len': 60.0, 'eval_sentence_distilroberta_cosine': 49.48943257331848, 'epoch': 0.24}\n",
            "                                              \n",
            " 25%|██▌       | 3/12 [00:04<00:02,  3.23it/s]\n",
            "{'loss': 2.3418710231781006, 'learning_rate': 5.215619154810592e-05, 'epoch': 0.32}\n",
            "{'loss': 3.0343916416168213, 'learning_rate': 4.563666760459269e-05, 'epoch': 0.4}\n",
            " 50%|█████     | 6/12 [00:05<00:05,  1.15it/s]{'loss': 2.730104446411133, 'learning_rate': 3.911714366107944e-05, 'epoch': 0.48}\n",
            " 50%|█████     | 6/12 [00:05<00:05,  1.15it/s][INFO|trainer.py:1412] 2021-02-01 14:56:13,539 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1413] 2021-02-01 14:56:13,539 >>   Num examples = 5\n",
            "[INFO|trainer.py:1414] 2021-02-01 14:56:13,540 >>   Batch size = 1\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.69it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.09it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:02<00:00,  1.80it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:02<00:00,  1.63it/s]\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 120.10it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.78it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.10it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.71it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.77it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.69it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 104.25it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 107.11it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.55it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.03it/s]\n",
            "{'eval_loss': 2.7215590476989746, 'eval_rouge1_precision': 13.56, 'eval_rouge1_recall': 34.43, 'eval_rouge1_fmeasure': 19.23, 'eval_rouge2_precision': 1.7399999999999998, 'eval_rouge2_recall': 4.25, 'eval_rouge2_fmeasure': 2.4699999999999998, 'eval_rougeL_precision': 8.34, 'eval_rougeL_recall': 21.240000000000002, 'eval_rougeL_fmeasure': 11.82, 'eval_rougeLsum_precision': 9.180000000000001, 'eval_rougeLsum_recall': 23.05, 'eval_rougeLsum_fmeasure': 13.0, 'eval_gen_len': 60.0, 'eval_sentence_distilroberta_cosine': 48.24509024620056, 'epoch': 0.48}\n",
            "\u001b[A\n",
            "                                              \n",
            " 50%|█████     | 6/12 [00:09<00:05,  1.15it/s]\n",
            "                                              {'loss': 2.8990354537963867, 'learning_rate': 3.2597619717566204e-05, 'epoch': 0.56}\n",
            "                                              \n",
            " 75%|███████▌  | 9/12 [00:10<00:03,  1.06s/it]{'loss': 2.923245429992676, 'learning_rate': 1.955857183053972e-05, 'epoch': 0.72}\n",
            " 75%|███████▌  | 9/12 [00:10<00:03,  1.06s/it][INFO|trainer.py:1412] 2021-02-01 14:56:18,308 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1413] 2021-02-01 14:56:18,308 >>   Num examples = 5\n",
            "[INFO|trainer.py:1414] 2021-02-01 14:56:18,308 >>   Batch size = 1\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.73it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.09it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:02<00:00,  1.81it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:02<00:00,  1.66it/s]\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 115.98it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.12it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.08it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.73it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.04it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.66it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 123.53it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.18it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 100.33it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.69it/s]\n",
            "{'eval_loss': 2.6834990978240967, 'eval_rouge1_precision': 10.77, 'eval_rouge1_recall': 27.49, 'eval_rouge1_fmeasure': 15.340000000000002, 'eval_rouge2_precision': 1.28, 'eval_rouge2_recall': 2.9000000000000004, 'eval_rouge2_fmeasure': 1.77, 'eval_rougeL_precision': 8.08, 'eval_rougeL_recall': 20.919999999999998, 'eval_rougeL_fmeasure': 11.67, 'eval_rougeLsum_precision': 9.229999999999999, 'eval_rougeLsum_recall': 23.54, 'eval_rougeLsum_fmeasure': 13.059999999999999, 'eval_gen_len': 59.4, 'eval_sentence_distilroberta_cosine': 40.05434513092041, 'epoch': 0.72}\n",
            "\n",
            "                                              \n",
            " 75%|███████▌  | 9/12 [00:14<00:03,  1.06s/it]\n",
            "{'loss': 2.7018988132476807, 'learning_rate': 1.303904788702648e-05, 'epoch': 0.8}\n",
            " 92%|█████████▏| 11/12 [00:14<00:01,  1.48s/it]{'loss': 2.701615333557129, 'learning_rate': 6.51952394351324e-06, 'epoch': 0.88}\n",
            "100%|██████████| 12/12 [00:15<00:00,  1.12s/it]{'loss': 2.9856154918670654, 'learning_rate': 0.0, 'epoch': 0.96}\n",
            "100%|██████████| 12/12 [00:15<00:00,  1.12s/it][INFO|trainer.py:1412] 2021-02-01 14:56:23,054 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1413] 2021-02-01 14:56:23,054 >>   Num examples = 5\n",
            "[INFO|trainer.py:1414] 2021-02-01 14:56:23,054 >>   Batch size = 1\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.67it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.05it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:02<00:00,  1.77it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:02<00:00,  1.62it/s]\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 123.53it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.31it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.88it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.52it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 147.26it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.40it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.47it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.40it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.94it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.77it/s]\n",
            "\n",
            "                                               \n",
            "\n",
            "100%|██████████| 12/12 [00:19<00:00,  1.12s/it]\n",
            "                                             \u001b[A[INFO|trainer.py:862] 2021-02-01 14:56:27,026 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'epoch': 0.96}\n",
            "100%|██████████| 12/12 [00:19<00:00,  1.60s/it]\n",
            "\u001b[32m[I 2021-02-01 14:56:27,033]\u001b[0m Trial 3 finished with value: 209.97574690818786 and parameters: {'learning_rate': 7.823428732215889e-05, 'gradient_accumulation_steps': 4}. Best is trial 0 with value: 219.0770232486725.\u001b[0m\n",
            "[INFO|trainer.py:557] 2021-02-01 14:56:27,034 >> Trial:\n",
            "[INFO|modeling_utils.py:1024] 2021-02-01 14:56:27,059 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
            "[INFO|modeling_utils.py:1140] 2021-02-01 14:56:49,541 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1149] 2021-02-01 14:56:49,541 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "02/01/2021 14:56:49 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 142, 'min_length': 56, 'no_repeat_ngram_size': 3, 'num_beams': 4}\n",
            "[INFO|trainer.py:703] 2021-02-01 14:56:49,967 >> ***** Running training *****\n",
            "[INFO|trainer.py:704] 2021-02-01 14:56:49,967 >>   Num examples = 50\n",
            "[INFO|trainer.py:705] 2021-02-01 14:56:49,967 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:706] 2021-02-01 14:56:49,967 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:707] 2021-02-01 14:56:49,968 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:708] 2021-02-01 14:56:49,968 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:709] 2021-02-01 14:56:49,968 >>   Total optimization steps = 6\n",
            "/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test\n",
            "[INFO|integrations.py:371] 2021-02-01 14:56:49,981 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 659\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210201_145601-3ugiea6a/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210201_145601-3ugiea6a/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss 2.98562\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch 0.96\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime 44\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp 1612191409\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss 2.65291\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision 12.43\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall 34.37\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure 17.97\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision 1.67\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall 5.42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure 2.51\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision 7.89\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall 21.97\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure 11.42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision 8.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall 23.87\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure 12.57\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len 60.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine 49.18575\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos 24284410675200\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           train/loss █▃▅▁▃▂▃▃▃▂▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  train/learning_rate █▇▇▆▅▅▄▄▃▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          train/epoch ▁▂▂▃▄▄▅▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                _step ▁▂▂▃▄▄▅▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                             _runtime ▁▁▂▂▂▃▃▃▃▃▄█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                           _timestamp ▁▁▂▂▂▃▃▃▃▃▄█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                            eval/loss █▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge1_precision ▆█▁▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge1_recall ██▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge1_fmeasure ▆█▁▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rouge2_precision █▅▁▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rouge2_recall █▄▁▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rouge2_fmeasure █▅▁▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeL_precision ▄█▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/rougeL_recall █▃▁▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeL_fmeasure ▅█▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/rougeLsum_precision ▂▇█▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/rougeLsum_recall █▁▄▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/rougeLsum_fmeasure ▁▇█▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                         eval/gen_len ██▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/sentence_distilroberta_cosine █▇▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/3ugiea6a\u001b[0m\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "2021-02-01 14:56:57.085972: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/drive/My Drive/MAGMA: Summarization/fine-tuning/hp_search_test\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/hp_search_para_wordembed/runs/wxh25vpr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210201_145649-wxh25vpr\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "                                             \n",
            "                                             \n",
            "                                             \n",
            " 50%|█████     | 3/6 [00:01<00:01,  1.92it/s][INFO|trainer.py:1412] 2021-02-01 14:57:01,520 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1413] 2021-02-01 14:57:01,520 >>   Num examples = 5\n",
            "[INFO|trainer.py:1414] 2021-02-01 14:57:01,520 >>   Batch size = 1\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.69it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.08it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:02<00:00,  1.80it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:02<00:00,  1.62it/s]\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 109.38it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.08it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.28it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.57it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 122.66it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.83it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.84it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.36it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.73it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.93it/s]\n",
            "{'eval_loss': 3.3987770080566406, 'eval_rouge1_precision': 12.07, 'eval_rouge1_recall': 34.06, 'eval_rouge1_fmeasure': 17.65, 'eval_rouge2_precision': 3.2, 'eval_rouge2_recall': 8.28, 'eval_rouge2_fmeasure': 4.61, 'eval_rougeL_precision': 10.03, 'eval_rougeL_recall': 28.18, 'eval_rougeL_fmeasure': 14.649999999999999, 'eval_rougeLsum_precision': 9.93, 'eval_rougeLsum_recall': 28.18, 'eval_rougeLsum_fmeasure': 14.62, 'eval_gen_len': 59.8, 'eval_sentence_distilroberta_cosine': 40.8170610666275, 'epoch': 0.48}\n",
            "\u001b[A\n",
            "                                             \n",
            " 50%|█████     | 3/6 [00:05<00:01,  1.92it/s]\n",
            "{'loss': 3.9641778469085693, 'learning_rate': 0.00016420985887089743, 'epoch': 0.64}\n",
            "{'loss': 3.3016607761383057, 'learning_rate': 8.210492943544871e-05, 'epoch': 0.8}\n",
            "{'loss': 3.339073896408081, 'learning_rate': 0.0, 'epoch': 0.96}\n",
            "100%|██████████| 6/6 [00:06<00:00,  1.08s/it][INFO|trainer.py:1412] 2021-02-01 14:57:06,933 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1413] 2021-02-01 14:57:06,933 >>   Num examples = 5\n",
            "[INFO|trainer.py:1414] 2021-02-01 14:57:06,933 >>   Batch size = 1\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.67it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.03it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:02<00:00,  1.77it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:02<00:00,  1.62it/s]\u001b[A\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.63it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.24it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.58it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.59it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.42it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.73it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.09it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.62it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.68it/s]\n",
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.42it/s]\n",
            "{'eval_loss': 3.003383159637451, 'eval_rouge1_precision': 12.520000000000001, 'eval_rouge1_recall': 33.73, 'eval_rouge1_fmeasure': 18.01, 'eval_rouge2_precision': 3.4099999999999997, 'eval_rouge2_recall': 10.440000000000001, 'eval_rouge2_fmeasure': 5.01, 'eval_rougeL_precision': 10.15, 'eval_rougeL_recall': 27.98, 'eval_rougeL_fmeasure': 14.610000000000001, 'eval_rougeLsum_precision': 10.01, 'eval_rougeLsum_recall': 27.6, 'eval_rougeLsum_fmeasure': 14.44, 'eval_gen_len': 59.6, 'eval_sentence_distilroberta_cosine': 45.28793394565582, 'epoch': 0.96}\n",
            "\n",
            "                                             \n",
            "100%|██████████| 6/6 [00:10<00:00,  1.08s/it]\n",
            "                                             \u001b[A[INFO|trainer.py:862] 2021-02-01 14:57:10,935 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "                                             {'epoch': 0.96}\n",
            "100%|██████████| 6/6 [00:10<00:00,  1.83s/it]\n",
            "\u001b[32m[I 2021-02-01 14:57:10,938]\u001b[0m Trial 4 finished with value: 233.19793394565582 and parameters: {'learning_rate': 0.0004926295766126923, 'gradient_accumulation_steps': 8}. Best is trial 4 with value: 233.19793394565582.\u001b[0m\n",
            "[INFO|trainer.py:557] 2021-02-01 14:57:10,938 >> Trial:\n",
            "[INFO|modeling_utils.py:1024] 2021-02-01 14:57:10,966 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66ynxjmYEB5P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LevExsI7oNF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}