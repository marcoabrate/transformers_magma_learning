{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 30194,
     "status": "ok",
     "timestamp": 1610616621856,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "au5Z9XQAC7C-"
   },
   "outputs": [],
   "source": [
    "magma_dir = '/home/ubuntu/magma/'\n",
    "bucket_dir = '/home/ubuntu/s3/'\n",
    "transformers_dir = '/home/ubuntu/transformers/'\n",
    "cache_dir = bucket_dir+'.cache/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EddY1WDNsKlS"
   },
   "source": [
    "## **Fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 85455,
     "status": "ok",
     "timestamp": 1610616677129,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "2d6M41X9AKBi"
   },
   "outputs": [],
   "source": [
    "finetune_script = '\"'+transformers_dir+'examples/seq2seq/finetune_trainer.py\"'\n",
    "eval_script = '\"'+transformers_dir+'examples/seq2seq/run_eval.py\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "executionInfo": {
     "elapsed": 113930,
     "status": "ok",
     "timestamp": 1610616705611,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "82WSp6khIcua",
    "outputId": "54de6794-ef3c-41f4-b0e3-e108cfc4e333"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, magma_dir)\n",
    "import config\n",
    "\n",
    "import torch\n",
    "torch.manual_seed = config.SEED\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPZ7A-sBVOam"
   },
   "source": [
    "### Karger Books Para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 113928,
     "status": "ok",
     "timestamp": 1610616705613,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "vJOYl_g6F1e2"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = 'facebook/bart-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 113926,
     "status": "ok",
     "timestamp": 1610616705616,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "4xs_XkbCVOar"
   },
   "outputs": [],
   "source": [
    "data_dir = '\"'+bucket_dir+'datasets/xsum\"'\n",
    "\n",
    "output_dir = '\"'+bucket_dir+'fine-tuning/'+\\\n",
    "    model_name_or_path.replace('/', '?')+'_xsum\"'\n",
    "\n",
    "log_dir = bucket_dir + '/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db70dec087de4ea6ac4d2783ec4878e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_num_labels\": 3,\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.0,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 1024,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 4096,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 12,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"do_blenderbot_90_layernorm\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 4096,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 12,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"eos_token_ids\": [\n",
       "    2\n",
       "  ],\n",
       "  \"extra_pos_embeddings\": 2,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_length\": 62,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"min_length\": 11,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 6,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"prefix\": \" \",\n",
       "  \"replacing_rate\": 0,\n",
       "  \"scale_embedding\": false,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"student_decoder_layers\": null,\n",
       "  \"student_encoder_layers\": null,\n",
       "  \"task_specific_params\": {},\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50264\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "AutoConfig.from_pretrained('facebook/bart-large-xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_test_dir = data_dir[:-1] + '/test.source\"'\n",
    "reference_test_dir = data_dir[:-1] + '/test.target\"'\n",
    "\n",
    "save_dir = output_dir[:-1] + '/'+model_name_or_path.replace('/', '?')+'_xsum.txt\"'\n",
    "score_dir = output_dir[:-1] + '/'+model_name_or_path.replace('/', '?')+'_xsum.json\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 709/709 [1:05:24<00:00,  5.54s/it]\n",
      "{'rouge1': 45.3773, 'rouge2': 22.3115, 'rougeL': 37.1896, 'rougeLsum': 37.1944, 'n_obs': 11333, 'runtime': 3930, 'seconds_per_sample': 0.3468}\n"
     ]
    }
   ],
   "source": [
    "!python3 $eval_script \\\n",
    "'facebook/bart-large-xsum' \\\n",
    "$source_test_dir \\\n",
    "$save_dir \\\n",
    "--reference_path $reference_test_dir \\\n",
    "--score_path $score_dir \\\n",
    "--fp16 \\\n",
    "--bs 16 \\\n",
    "--dump-args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_num_labels\": 3,\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.0,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 1024,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 4096,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 12,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"do_blenderbot_90_layernorm\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 4096,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 12,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"eos_token_ids\": [\n",
       "    2\n",
       "  ],\n",
       "  \"extra_pos_embeddings\": 2,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_length\": 62,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"min_length\": 11,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 6,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"prefix\": \" \",\n",
       "  \"replacing_rate\": 0,\n",
       "  \"scale_embedding\": false,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"student_decoder_layers\": null,\n",
       "  \"student_encoder_layers\": null,\n",
       "  \"task_specific_params\": {},\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50264\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoConfig.from_pretrained('facebook/bart-large-xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "referenced_widgets": [
      "88734567ccb2435091eeab9f30b2de9b"
     ]
    },
    "executionInfo": {
     "elapsed": 120037,
     "status": "ok",
     "timestamp": 1610616711730,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "wtLC1O1ZJpU3",
    "outputId": "16632326-7ab5-4ef4-aa65-44efbdf45fb3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "model_config = AutoConfig.from_pretrained('facebook/bart-large', cache_dir=cache_dir)\n",
    "model_config.activation_dropout = 0\n",
    "model_config.attention_dropout = 0\n",
    "model_config.classif_dropout = 0\n",
    "model_config.min_length = 11\n",
    "model_config.max_length = 63\n",
    "model_config.no_repeat_ngram_size = 3\n",
    "model_config.num_beams = 6\n",
    "model_config.task_specific_params['summarization']['length_penalty'] = 1\n",
    "model_config.task_specific_params['summarization']['min_length'] = 11\n",
    "model_config.task_specific_params['summarization']['max_length'] = 62\n",
    "model_config.task_specific_params['summarization']['num_beams'] = 6\n",
    "\n",
    "model_config_dir = '\"'+bucket_dir+'fine-tuning/'+\\\n",
    "    'facebook/bart-large'.replace('/', '?')+'_config\"'\n",
    "model_config.save_pretrained(model_config_dir[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M58yiP1yVOav"
   },
   "source": [
    "##### Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: finetune_trainer.py [-h] --model_name_or_path MODEL_NAME_OR_PATH\r\n",
      "                           [--config_name CONFIG_NAME]\r\n",
      "                           [--tokenizer_name TOKENIZER_NAME]\r\n",
      "                           [--cache_dir CACHE_DIR] [--freeze_encoder]\r\n",
      "                           [--freeze_embeds] --data_dir DATA_DIR [--task TASK]\r\n",
      "                           [--max_source_length MAX_SOURCE_LENGTH]\r\n",
      "                           [--max_target_length MAX_TARGET_LENGTH]\r\n",
      "                           [--val_max_target_length VAL_MAX_TARGET_LENGTH]\r\n",
      "                           [--test_max_target_length TEST_MAX_TARGET_LENGTH]\r\n",
      "                           [--n_train N_TRAIN] [--n_val N_VAL]\r\n",
      "                           [--n_test N_TEST] [--src_lang SRC_LANG]\r\n",
      "                           [--tgt_lang TGT_LANG] [--eval_beams EVAL_BEAMS]\r\n",
      "                           [--no_ignore_pad_token_for_loss] --output_dir\r\n",
      "                           OUTPUT_DIR [--overwrite_output_dir] [--do_train]\r\n",
      "                           [--do_eval] [--do_predict] [--model_parallel]\r\n",
      "                           [--evaluation_strategy {EvaluationStrategy.NO,EvaluationStrategy.STEPS,EvaluationStrategy.EPOCH}]\r\n",
      "                           [--prediction_loss_only]\r\n",
      "                           [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\r\n",
      "                           [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\r\n",
      "                           [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\r\n",
      "                           [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\r\n",
      "                           [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\r\n",
      "                           [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\r\n",
      "                           [--learning_rate LEARNING_RATE]\r\n",
      "                           [--weight_decay WEIGHT_DECAY]\r\n",
      "                           [--adam_beta1 ADAM_BETA1] [--adam_beta2 ADAM_BETA2]\r\n",
      "                           [--adam_epsilon ADAM_EPSILON]\r\n",
      "                           [--max_grad_norm MAX_GRAD_NORM]\r\n",
      "                           [--num_train_epochs NUM_TRAIN_EPOCHS]\r\n",
      "                           [--max_steps MAX_STEPS]\r\n",
      "                           [--warmup_steps WARMUP_STEPS]\r\n",
      "                           [--logging_dir LOGGING_DIR] [--logging_first_step]\r\n",
      "                           [--logging_steps LOGGING_STEPS]\r\n",
      "                           [--save_steps SAVE_STEPS]\r\n",
      "                           [--save_total_limit SAVE_TOTAL_LIMIT] [--no_cuda]\r\n",
      "                           [--seed SEED] [--fp16]\r\n",
      "                           [--fp16_opt_level FP16_OPT_LEVEL]\r\n",
      "                           [--local_rank LOCAL_RANK]\r\n",
      "                           [--tpu_num_cores TPU_NUM_CORES]\r\n",
      "                           [--tpu_metrics_debug] [--debug]\r\n",
      "                           [--dataloader_drop_last] [--eval_steps EVAL_STEPS]\r\n",
      "                           [--dataloader_num_workers DATALOADER_NUM_WORKERS]\r\n",
      "                           [--past_index PAST_INDEX] [--run_name RUN_NAME]\r\n",
      "                           [--disable_tqdm DISABLE_TQDM]\r\n",
      "                           [--no_remove_unused_columns]\r\n",
      "                           [--label_names LABEL_NAMES [LABEL_NAMES ...]]\r\n",
      "                           [--load_best_model_at_end]\r\n",
      "                           [--metric_for_best_model METRIC_FOR_BEST_MODEL]\r\n",
      "                           [--greater_is_better GREATER_IS_BETTER]\r\n",
      "                           [--ignore_data_skip]\r\n",
      "                           [--fp16_backend {auto,amp,apex}] [--sharded_ddp]\r\n",
      "                           [--label_smoothing LABEL_SMOOTHING]\r\n",
      "                           [--sortish_sampler] [--predict_with_generate]\r\n",
      "                           [--adafactor]\r\n",
      "                           [--encoder_layerdrop ENCODER_LAYERDROP]\r\n",
      "                           [--decoder_layerdrop DECODER_LAYERDROP]\r\n",
      "                           [--dropout DROPOUT]\r\n",
      "                           [--attention_dropout ATTENTION_DROPOUT]\r\n",
      "                           [--lr_scheduler LR_SCHEDULER]\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --model_name_or_path MODEL_NAME_OR_PATH\r\n",
      "                        Path to pretrained model or model identifier from\r\n",
      "                        huggingface.co/models\r\n",
      "  --config_name CONFIG_NAME\r\n",
      "                        Pretrained config name or path if not the same as\r\n",
      "                        model_name\r\n",
      "  --tokenizer_name TOKENIZER_NAME\r\n",
      "                        Pretrained tokenizer name or path if not the same as\r\n",
      "                        model_name\r\n",
      "  --cache_dir CACHE_DIR\r\n",
      "                        Where do you want to store the pretrained models\r\n",
      "                        downloaded from huggingface.co\r\n",
      "  --freeze_encoder      Whether tp freeze the encoder.\r\n",
      "  --freeze_embeds       Whether to freeze the embeddings.\r\n",
      "  --data_dir DATA_DIR   The input data dir. Should contain the .tsv files (or\r\n",
      "                        other data files) for the task.\r\n",
      "  --task TASK           Task name, summarization (or summarization_{dataset}\r\n",
      "                        for pegasus) or translation\r\n",
      "  --max_source_length MAX_SOURCE_LENGTH\r\n",
      "                        The maximum total input sequence length after\r\n",
      "                        tokenization. Sequences longer than this will be\r\n",
      "                        truncated, sequences shorter will be padded.\r\n",
      "  --max_target_length MAX_TARGET_LENGTH\r\n",
      "                        The maximum total sequence length for target text\r\n",
      "                        after tokenization. Sequences longer than this will be\r\n",
      "                        truncated, sequences shorter will be padded.\r\n",
      "  --val_max_target_length VAL_MAX_TARGET_LENGTH\r\n",
      "                        The maximum total sequence length for validation\r\n",
      "                        target text after tokenization. Sequences longer than\r\n",
      "                        this will be truncated, sequences shorter will be\r\n",
      "                        padded.\r\n",
      "  --test_max_target_length TEST_MAX_TARGET_LENGTH\r\n",
      "                        The maximum total sequence length for test target text\r\n",
      "                        after tokenization. Sequences longer than this will be\r\n",
      "                        truncated, sequences shorter will be padded.\r\n",
      "  --n_train N_TRAIN     # training examples. -1 means use all.\r\n",
      "  --n_val N_VAL         # validation examples. -1 means use all.\r\n",
      "  --n_test N_TEST       # test examples. -1 means use all.\r\n",
      "  --src_lang SRC_LANG   Source language id for translation.\r\n",
      "  --tgt_lang TGT_LANG   Target language id for translation.\r\n",
      "  --eval_beams EVAL_BEAMS\r\n",
      "                        # num_beams to use for evaluation.\r\n",
      "  --no_ignore_pad_token_for_loss\r\n",
      "                        If only pad tokens should be ignored. This assumes\r\n",
      "                        that `config.pad_token_id` is defined.\r\n",
      "  --output_dir OUTPUT_DIR\r\n",
      "                        The output directory where the model predictions and\r\n",
      "                        checkpoints will be written.\r\n",
      "  --overwrite_output_dir\r\n",
      "                        Overwrite the content of the output directory.Use this\r\n",
      "                        to continue training if output_dir points to a\r\n",
      "                        checkpoint directory.\r\n",
      "  --do_train            Whether to run training.\r\n",
      "  --do_eval             Whether to run eval on the dev set.\r\n",
      "  --do_predict          Whether to run predictions on the test set.\r\n",
      "  --model_parallel      If there are more than one devices, whether to use\r\n",
      "                        model parallelism to distribute the model's modules\r\n",
      "                        across devices.\r\n",
      "  --evaluation_strategy {EvaluationStrategy.NO,EvaluationStrategy.STEPS,EvaluationStrategy.EPOCH}\r\n",
      "                        Run evaluation during training at each logging step.\r\n",
      "  --prediction_loss_only\r\n",
      "                        When performing evaluation and predictions, only\r\n",
      "                        returns the loss.\r\n",
      "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\r\n",
      "                        Batch size per GPU/TPU core/CPU for training.\r\n",
      "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\r\n",
      "                        Batch size per GPU/TPU core/CPU for evaluation.\r\n",
      "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\r\n",
      "                        Deprecated, the use of `--per_device_train_batch_size`\r\n",
      "                        is preferred. Batch size per GPU/TPU core/CPU for\r\n",
      "                        training.\r\n",
      "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\r\n",
      "                        Deprecated, the use of `--per_device_eval_batch_size`\r\n",
      "                        is preferred.Batch size per GPU/TPU core/CPU for\r\n",
      "                        evaluation.\r\n",
      "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\r\n",
      "                        Number of updates steps to accumulate before\r\n",
      "                        performing a backward/update pass.\r\n",
      "  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\r\n",
      "                        Number of predictions steps to accumulate before\r\n",
      "                        moving the tensors to the CPU.\r\n",
      "  --learning_rate LEARNING_RATE\r\n",
      "                        The initial learning rate for Adam.\r\n",
      "  --weight_decay WEIGHT_DECAY\r\n",
      "                        Weight decay if we apply some.\r\n",
      "  --adam_beta1 ADAM_BETA1\r\n",
      "                        Beta1 for Adam optimizer\r\n",
      "  --adam_beta2 ADAM_BETA2\r\n",
      "                        Beta2 for Adam optimizer\r\n",
      "  --adam_epsilon ADAM_EPSILON\r\n",
      "                        Epsilon for Adam optimizer.\r\n",
      "  --max_grad_norm MAX_GRAD_NORM\r\n",
      "                        Max gradient norm.\r\n",
      "  --num_train_epochs NUM_TRAIN_EPOCHS\r\n",
      "                        Total number of training epochs to perform.\r\n",
      "  --max_steps MAX_STEPS\r\n",
      "                        If > 0: set total number of training steps to perform.\r\n",
      "                        Override num_train_epochs.\r\n",
      "  --warmup_steps WARMUP_STEPS\r\n",
      "                        Linear warmup over warmup_steps.\r\n",
      "  --logging_dir LOGGING_DIR\r\n",
      "                        Tensorboard log dir.\r\n",
      "  --logging_first_step  Log the first global_step\r\n",
      "  --logging_steps LOGGING_STEPS\r\n",
      "                        Log every X updates steps.\r\n",
      "  --save_steps SAVE_STEPS\r\n",
      "                        Save checkpoint every X updates steps.\r\n",
      "  --save_total_limit SAVE_TOTAL_LIMIT\r\n",
      "                        Limit the total amount of checkpoints.Deletes the\r\n",
      "                        older checkpoints in the output_dir. Default is\r\n",
      "                        unlimited checkpoints\r\n",
      "  --no_cuda             Do not use CUDA even when it is available\r\n",
      "  --seed SEED           random seed for initialization\r\n",
      "  --fp16                Whether to use 16-bit (mixed) precision (through\r\n",
      "                        NVIDIA Apex) instead of 32-bit\r\n",
      "  --fp16_opt_level FP16_OPT_LEVEL\r\n",
      "                        For fp16: Apex AMP optimization level selected in\r\n",
      "                        ['O0', 'O1', 'O2', and 'O3'].See details at\r\n",
      "                        https://nvidia.github.io/apex/amp.html\r\n",
      "  --local_rank LOCAL_RANK\r\n",
      "                        For distributed training: local_rank\r\n",
      "  --tpu_num_cores TPU_NUM_CORES\r\n",
      "                        TPU: Number of TPU cores (automatically passed by\r\n",
      "                        launcher script)\r\n",
      "  --tpu_metrics_debug   Deprecated, the use of `--debug` is preferred. TPU:\r\n",
      "                        Whether to print debug metrics\r\n",
      "  --debug               Whether to print debug metrics on TPU\r\n",
      "  --dataloader_drop_last\r\n",
      "                        Drop the last incomplete batch if it is not divisible\r\n",
      "                        by the batch size.\r\n",
      "  --eval_steps EVAL_STEPS\r\n",
      "                        Run an evaluation every X steps.\r\n",
      "  --dataloader_num_workers DATALOADER_NUM_WORKERS\r\n",
      "                        Number of subprocesses to use for data loading\r\n",
      "                        (PyTorch only). 0 means that the data will be loaded\r\n",
      "                        in the main process.\r\n",
      "  --past_index PAST_INDEX\r\n",
      "                        If >=0, uses the corresponding part of the output as\r\n",
      "                        the past state for next step.\r\n",
      "  --run_name RUN_NAME   An optional descriptor for the run. Notably used for\r\n",
      "                        wandb logging.\r\n",
      "  --disable_tqdm DISABLE_TQDM\r\n",
      "                        Whether or not to disable the tqdm progress bars.\r\n",
      "  --no_remove_unused_columns\r\n",
      "                        Remove columns not required by the model when using an\r\n",
      "                        nlp.Dataset.\r\n",
      "  --label_names LABEL_NAMES [LABEL_NAMES ...]\r\n",
      "                        The list of keys in your dictionary of inputs that\r\n",
      "                        correspond to the labels.\r\n",
      "  --load_best_model_at_end\r\n",
      "                        Whether or not to load the best model found during\r\n",
      "                        training at the end of training.\r\n",
      "  --metric_for_best_model METRIC_FOR_BEST_MODEL\r\n",
      "                        The metric to use to compare two different models.\r\n",
      "  --greater_is_better GREATER_IS_BETTER\r\n",
      "                        Whether the `metric_for_best_model` should be\r\n",
      "                        maximized or not.\r\n",
      "  --ignore_data_skip    When resuming training, whether or not to skip the\r\n",
      "                        first epochs and batches to get to the same training\r\n",
      "                        data.\r\n",
      "  --fp16_backend {auto,amp,apex}\r\n",
      "                        The backend to be used for mixed precision.\r\n",
      "  --sharded_ddp         Whether or not to use sharded DDP training (in\r\n",
      "                        distributed training only).\r\n",
      "  --label_smoothing LABEL_SMOOTHING\r\n",
      "                        The label smoothing epsilon to apply (if not zero).\r\n",
      "  --sortish_sampler     Whether to SortishSamler or not.\r\n",
      "  --predict_with_generate\r\n",
      "                        Whether to use generate to calculate generative\r\n",
      "                        metrics (ROUGE, BLEU).\r\n",
      "  --adafactor           whether to use adafactor\r\n",
      "  --encoder_layerdrop ENCODER_LAYERDROP\r\n",
      "                        Encoder layer dropout probability. Goes into\r\n",
      "                        model.config.\r\n",
      "  --decoder_layerdrop DECODER_LAYERDROP\r\n",
      "                        Decoder layer dropout probability. Goes into\r\n",
      "                        model.config.\r\n",
      "  --dropout DROPOUT     Dropout probability. Goes into model.config.\r\n",
      "  --attention_dropout ATTENTION_DROPOUT\r\n",
      "                        Attention dropout probability. Goes into model.config.\r\n",
      "  --lr_scheduler LR_SCHEDULER\r\n",
      "                        Which lr scheduler to use. Selected in ['constant',\r\n",
      "                        'constant_w_warmup', 'cosine', 'cosine_w_restarts',\r\n",
      "                        'linear', 'polynomial']\r\n"
     ]
    }
   ],
   "source": [
    "!python3 $finetune_script --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2522396,
     "status": "ok",
     "timestamp": 1610555308072,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "K66kY1WOVOaw",
    "outputId": "11421fe6-93e5-461f-8b97-0615b4c2e7af",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/15/2021 18:10:55 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "01/15/2021 18:10:55 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/home/ubuntu/s3/fine-tuning/facebook?bart-large_xsum', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=64, eval_accumulation_steps=None, learning_rate=0.00012, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=5000, warmup_steps=0, logging_dir='runs/Jan15_18-10-55_ip-172-31-39-35', logging_first_step=False, logging_steps=100, save_steps=1000, save_total_limit=3, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name='/home/ubuntu/s3/fine-tuning/facebook?bart-large_xsum', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False, label_smoothing=0.0, sortish_sampler=True, predict_with_generate=True, adafactor=False, encoder_layerdrop=None, decoder_layerdrop=None, dropout=0.1, attention_dropout=0.0, lr_scheduler='linear')\n",
      "[INFO|configuration_utils.py:429] 2021-01-15 18:10:55,887 >> loading configuration file /home/ubuntu/s3/fine-tuning/facebook?bart-large_config/config.json\n",
      "[INFO|configuration_utils.py:467] 2021-01-15 18:10:55,888 >> Model config BartConfig {\n",
      "  \"activation_dropout\": 0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\",\n",
      "    \"BartForConditionalGeneration\",\n",
      "    \"BartForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 63,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 11,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 6,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:431] 2021-01-15 18:10:56,199 >> loading configuration file https://huggingface.co/facebook/bart-large/resolve/main/config.json from cache at /home/ubuntu/s3/.cache/3f12fb71b844fcb7d591fdd4e55027da90d7b5dd6aa5430ad00ec6d76585f26c.01119ad5ed0734de7152ef51ba44fccefe008001bca9a6ddebeec1caf28f6bb8\n",
      "[INFO|configuration_utils.py:467] 2021-01-15 18:10:56,199 >> Model config BartConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\",\n",
      "    \"BartForConditionalGeneration\",\n",
      "    \"BartForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-01-15 18:10:57,066 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /home/ubuntu/s3/.cache/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-01-15 18:10:57,066 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /home/ubuntu/s3/.cache/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-01-15 18:10:57,066 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /home/ubuntu/s3/.cache/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|modeling_utils.py:1024] 2021-01-15 18:10:57,460 >> loading weights file https://huggingface.co/facebook/bart-large/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/d065edfe6954baf0b989a2063b26eb07e8c4d0b19354b5c74af9a51f5518df6e.6ca4df1a6ec59aa763989ceec10dff41dde19f0f0824b9f5d3fcd35a8abffdb2\n",
      "[INFO|modeling_utils.py:1140] 2021-01-15 18:11:22,302 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-01-15 18:11:22,302 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "01/15/2021 18:11:22 - INFO - utils -   using task specific params for summarization: {'length_penalty': 1, 'max_length': 62, 'min_length': 11, 'num_beams': 6}\n",
      "[INFO|trainer.py:280] 2021-01-15 18:11:30,635 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "01/15/2021 18:11:30 - INFO - __main__ -   *** Train ***\n",
      "[INFO|trainer.py:703] 2021-01-15 18:11:30,645 >> ***** Running training *****\n",
      "[INFO|trainer.py:704] 2021-01-15 18:11:30,645 >>   Num examples = 204016\n",
      "[INFO|trainer.py:705] 2021-01-15 18:11:30,645 >>   Num Epochs = 4\n",
      "[INFO|trainer.py:706] 2021-01-15 18:11:30,645 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:707] 2021-01-15 18:11:30,645 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:708] 2021-01-15 18:11:30,645 >>   Gradient Accumulation steps = 64\n",
      "[INFO|trainer.py:709] 2021-01-15 18:11:30,645 >>   Total optimization steps = 5000\n",
      "/home/ubuntu/s3/fine-tuning/facebook?bart-large_xsum\n",
      "[INFO|integrations.py:371] 2021-01-15 18:11:30,650 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/facebook?bart-large_xsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/huggingface/runs/2h4md3hy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/magma/fine-tuning/wandb/run-20210115_181131-2h4md3hy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "  0%|          | 6/5000 [01:33<20:47:13, 14.98s/it]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/transformers/examples/seq2seq/finetune_trainer.py\", line 379, in <module>\n",
      "    main()\n",
      "  File \"/home/ubuntu/transformers/examples/seq2seq/finetune_trainer.py\", line 316, in main\n",
      "    model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
      "  File \"/home/ubuntu/transformers/src/transformers/trainer.py\", line 799, in train\n",
      "    tr_loss += self.training_step(model, inputs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/trainer.py\", line 1137, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/ubuntu/transformers/examples/seq2seq/seq2seq_trainer.py\", line 180, in compute_loss\n",
      "    loss, _ = self._compute_loss(model, inputs, labels)\n",
      "  File \"/home/ubuntu/transformers/examples/seq2seq/seq2seq_trainer.py\", line 166, in _compute_loss\n",
      "    logits = model(**inputs, use_cache=False)[0]\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 1246, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 1097, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 775, in forward\n",
      "    hidden_states, attn = encoder_layer(hidden_states, attention_mask, output_attentions=output_attentions)\n",
      "  File \"/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/transformers/src/transformers/models/bart/modeling_bart.py\", line 374, in forward\n",
      "    if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n",
      "KeyboardInterrupt\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 15410\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program failed with code 255.  Press ctrl-c to abort syncing.\n"
     ]
    }
   ],
   "source": [
    "!python3 $finetune_script \\\n",
    "--model_name_or_path $model_name_or_path \\\n",
    "--config_name $model_config_dir \\\n",
    "--tokenizer_name $model_name_or_path \\\n",
    "--cache_dir $cache_dir \\\n",
    "--data_dir $data_dir \\\n",
    "--fp16 \\\n",
    "--attention_dropout 0 --dropout 0.1 \\\n",
    "--learning_rate 1.2e-4 \\\n",
    "--sortish_sampler --freeze_embeds \\\n",
    "--task summarization \\\n",
    "--max_source_length 1024 \\\n",
    "--max_target_length 60 \\\n",
    "--val_max_target_length 60 \\\n",
    "--do_train \\\n",
    "--max_steps 5000 \\\n",
    "--logging_steps 100 --save_steps 1000 --save_total_limit 3 \\\n",
    "--per_device_train_batch_size 2 --per_device_eval_batch_size 2 \\\n",
    "--gradient_accumulation_steps 64 \\\n",
    "--do_eval --evaluation_strategy steps --eval_steps 1000 \\\n",
    "--predict_with_generate \\\n",
    "--output_dir $output_dir \\\n",
    "--overwrite_output_dir \\\n",
    "--seed $config.SEED \\\n",
    "--run_name $output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeSSMVZwVOa1"
   },
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2YaHOesVOa4"
   },
   "outputs": [],
   "source": [
    "source_test_dir = data_dir[:-1] + '/test.source\"'\n",
    "reference_test_dir = data_dir[:-1] + '/test.target\"'\n",
    "\n",
    "save_dir = huggingface bart large xsum has one more tokenoutput_dir[:-1] + '/'+model_name_or_path.replace('/', '?')+'_test_karger_books_para.txt\"'\n",
    "score_dir = output_dir[:-1] + '/'+model_name_or_path.replace('/', '?')+'_test_karger_books_para.json\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1399,
     "status": "ok",
     "timestamp": 1610471442216,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "AgY3gO94VOa9",
    "outputId": "8436601c-f530-405f-e6c6-c025a7cf5b71"
   },
   "outputs": [],
   "source": [
    "!python3 $eval_script \\\n",
    "$output_dir \\\n",
    "$source_test_dir \\\n",
    "$save_dir \\\n",
    "--reference_path $reference_test_dir \\\n",
    "--score_path $score_dir \\\n",
    "--task summarization \\\n",
    "--bs 2 \\\n",
    "--length_penalty $config.LENAGTH_PENALTY \\\n",
    "--no_repeat_ngram_size $config.NO_REPEAT_NGRAM_SIZE \\\n",
    "--num_beams $config.NUM_BEAMS \\\n",
    "--dump-args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bbRUTtNDHth"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNa0Dk3mImrCcZO3VdQRayI",
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "L5sXxqeNCtkN",
    "S0FByNNOIRvG",
    "GPbOrCLWACbm",
    "siT4m5aYCFSh",
    "Dk1uGO5SCDNa",
    "WdDCBiMOBWiO",
    "pr_0J4xgBWiW",
    "l8hQT6ksBWin",
    "d5V0QCdf04Yx",
    "KQj3gt6s5ACz",
    "ah9sssub5DXX",
    "M58yiP1yVOav",
    "aeSSMVZwVOa1"
   ],
   "name": "bart_finetune.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "88734567ccb2435091eeab9f30b2de9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bd07faa8d1534ff9b1967f18e6fca2c2",
       "IPY_MODEL_43ac5522170c45f7856f7097ff9fee58"
      ],
      "layout": "IPY_MODEL_431d75cb2dd2403e88015f4e009080d4"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
