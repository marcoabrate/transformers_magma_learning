{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 30194,
     "status": "ok",
     "timestamp": 1610616621856,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "au5Z9XQAC7C-"
   },
   "outputs": [],
   "source": [
    "magma_dir = '/home/ubuntu/magma/'\n",
    "bucket_dir = '/home/ubuntu/s3/'\n",
    "transformers_dir = '/home/ubuntu/transformers/'\n",
    "cache_dir = bucket_dir+'.cache/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EddY1WDNsKlS"
   },
   "source": [
    "## **Fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 85455,
     "status": "ok",
     "timestamp": 1610616677129,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "2d6M41X9AKBi"
   },
   "outputs": [],
   "source": [
    "finetune_script = '\"'+transformers_dir+'examples/seq2seq/finetune_trainer.py\"'\n",
    "eval_script = '\"'+transformers_dir+'examples/seq2seq/run_eval.py\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "executionInfo": {
     "elapsed": 113930,
     "status": "ok",
     "timestamp": 1610616705611,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "82WSp6khIcua",
    "outputId": "54de6794-ef3c-41f4-b0e3-e108cfc4e333"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, magma_dir)\n",
    "import config\n",
    "\n",
    "import torch\n",
    "torch.manual_seed = config.SEED\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPZ7A-sBVOam"
   },
   "source": [
    "### Karger Books Para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 113928,
     "status": "ok",
     "timestamp": 1610616705613,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "vJOYl_g6F1e2"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = 'sshleifer/distilbart-cnn-12-6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 113926,
     "status": "ok",
     "timestamp": 1610616705616,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "4xs_XkbCVOar"
   },
   "outputs": [],
   "source": [
    "data_dir = '\"'+bucket_dir+'datasets/karger_books_para/bart\"'\n",
    "\n",
    "output_dir = '\"'+bucket_dir+'fine-tuning/'+\\\n",
    "    model_name_or_path.replace('/', '?')+'_karger_books_para_train\"'\n",
    "\n",
    "log_dir = bucket_dir + '/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "referenced_widgets": [
      "88734567ccb2435091eeab9f30b2de9b"
     ]
    },
    "executionInfo": {
     "elapsed": 120037,
     "status": "ok",
     "timestamp": 1610616711730,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "wtLC1O1ZJpU3",
    "outputId": "16632326-7ab5-4ef4-aa65-44efbdf45fb3"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "model_config = AutoConfig.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "model_config.min_length = config.ONE_BULLET_MIN_LEN\n",
    "model_config.max_length = config.ONE_BULLET_MAX_LEN\n",
    "model_config.length_penalty = config.LENGTH_PENALTY\n",
    "model_config.no_repeat_ngram_size = config.NO_REPEAT_NGRAM_SIZE\n",
    "\n",
    "model_config.task_specific_params['summarization']['min_length'] = config.ONE_BULLET_MIN_LEN\n",
    "model_config.task_specific_params['summarization']['max_length'] = config.ONE_BULLET_MAX_LEN\n",
    "model_config.task_specific_params['summarization']['length_penalty'] = config.LENGTH_PENALTY\n",
    "model_config.task_specific_params['summarization']['no_repeat_ngram_size'] = config.NO_REPEAT_NGRAM_SIZE\n",
    "model_config_dir = '\"'+bucket_dir+'fine-tuning/'+\\\n",
    "    model_name_or_path.replace('/', '?')+'_config\"'\n",
    "model_config.save_pretrained(model_config_dir[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M58yiP1yVOav"
   },
   "source": [
    "##### Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2522396,
     "status": "ok",
     "timestamp": 1610555308072,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "K66kY1WOVOaw",
    "outputId": "11421fe6-93e5-461f-8b97-0615b4c2e7af",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/14/2021 15:57:36 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "01/14/2021 15:57:36 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jan14_15-57-36_ip-172-31-39-35', logging_first_step=False, logging_steps=200, save_steps=1500, save_total_limit=3, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1500, dataloader_num_workers=0, past_index=-1, run_name='/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model='rougeL', greater_is_better='True', ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False, label_smoothing=0.1, sortish_sampler=True, predict_with_generate=True, adafactor=True, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear')\n",
      "[INFO|configuration_utils.py:429] 2021-01-14 15:57:36,496 >> loading configuration file /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_config/config.json\n",
      "[INFO|configuration_utils.py:467] 2021-01-14 15:57:36,497 >> Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 150,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 10,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 5,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 1,\n",
      "      \"max_length\": 150,\n",
      "      \"min_length\": 10,\n",
      "      \"no_repeat_ngram_size\": 5,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:431] 2021-01-14 15:57:36,821 >> loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /home/ubuntu/s3/.cache/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.9307b6cec4435559ec6e79d5a210a334b17706465329e138f335649d14f27e78\n",
      "[INFO|configuration_utils.py:467] 2021-01-14 15:57:36,822 >> Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1721] 2021-01-14 15:57:36,822 >> Model name 'sshleifer/distilbart-cnn-12-6' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'sshleifer/distilbart-cnn-12-6' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-01-14 15:57:38,500 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/vocab.json from cache at /home/ubuntu/s3/.cache/9951e68693b9a7c583ae677e9cb53c02715d9bd0311a78706401372653cdea0a.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-01-14 15:57:38,500 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/merges.txt from cache at /home/ubuntu/s3/.cache/7588c8d398d659b230a038240cc023f67b6848117d2999f06ab625af7bfc7ec1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-01-14 15:57:38,500 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-01-14 15:57:38,500 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-01-14 15:57:38,501 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2021-01-14 15:57:38,501 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer_config.json from cache at /home/ubuntu/s3/.cache/f5316f64f9716436994a7ad76a354dc20ecb2dd74eb61d278f103a9c8b80291f.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n",
      "[INFO|modeling_utils.py:1024] 2021-01-14 15:57:39,114 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/ubuntu/s3/.cache/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n",
      "[INFO|modeling_utils.py:1140] 2021-01-14 15:58:05,235 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2021-01-14 15:58:05,235 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "01/14/2021 15:58:05 - INFO - utils -   using task specific params for summarization: {'early_stopping': True, 'length_penalty': 1, 'max_length': 150, 'min_length': 10, 'no_repeat_ngram_size': 5, 'num_beams': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/14/2021 15:58:08 - INFO - __main__ -   *** Train ***\n",
      "[INFO|trainer.py:703] 2021-01-14 15:58:08,126 >> ***** Running training *****\n",
      "[INFO|trainer.py:704] 2021-01-14 15:58:08,126 >>   Num examples = 2046\n",
      "[INFO|trainer.py:705] 2021-01-14 15:58:08,126 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:706] 2021-01-14 15:58:08,126 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:707] 2021-01-14 15:58:08,126 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:708] 2021-01-14 15:58:08,126 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:709] 2021-01-14 15:58:08,126 >>   Total optimization steps = 5120\n",
      "/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train\n",
      "[INFO|integrations.py:371] 2021-01-14 15:58:08,129 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/huggingface/runs/1eo9kt9t\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/ubuntu/magma/fine-tuning/wandb/run-20210114_155808-1eo9kt9t\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "  0%|          | 0/5120 [00:00<?, ?it/s]/home/ubuntu/miniconda3/envs/magma/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "  0%|          | 9/5120 [00:02<16:05,  5.29it/s]  /home/ubuntu/transformers/src/transformers/optimization.py:506: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370120218/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n",
      "  4%|▍         | 200/5120 [01:01<34:18,  2.39it/s]5e-05, 'epoch': 0.390625}\n",
      "  8%|▊         | 400/5120 [02:04<31:17,  2.51it/s]{'loss': 517.659375, 'learning_rate': 2.765625e-05, 'epoch': 0.78125}\n",
      " 12%|█▏        | 600/5120 [03:06<29:31,  2.55it/s]{'loss': 474.48453125, 'learning_rate': 2.6484375000000002e-05, 'epoch': 1.171875}\n",
      "                                                  {'loss': 449.0977734375, 'learning_rate': 2.5312500000000002e-05, 'epoch': 1.5625}\n",
      " 20%|█▉        | 1000/5120 [05:11<27:58,  2.45it/s]{'loss': 455.7267578125, 'learning_rate': 2.4140625e-05, 'epoch': 1.953125}\n",
      "{'loss': 395.6706640625, 'learning_rate': 2.296875e-05, 'epoch': 2.34375}\n",
      " 27%|██▋       | 1400/5120 [07:16<25:33,  2.43it/s]{'loss': 395.8526953125, 'learning_rate': 2.1796875e-05, 'epoch': 2.734375}\n",
      " 29%|██▉       | 1500/5120 [07:47<18:21,  3.29it/s][INFO|trainer.py:1412] 2021-01-14 16:05:57,193 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-14 16:05:57,194 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-14 16:05:57,194 >>   Batch size = 4\n",
      "\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 2/67 [00:00<00:29,  2.17it/s]\u001b[A\n",
      "  4%|▍         | 3/67 [00:01<00:43,  1.46it/s]\u001b[A\n",
      "  6%|▌         | 4/67 [00:02<00:42,  1.50it/s]\u001b[A\n",
      "  7%|▋         | 5/67 [00:03<00:57,  1.08it/s]\u001b[A\n",
      "  9%|▉         | 6/67 [00:05<01:11,  1.17s/it]\u001b[A\n",
      " 10%|█         | 7/67 [00:06<01:10,  1.17s/it]\u001b[A\n",
      " 12%|█▏        | 8/67 [00:07<01:04,  1.10s/it]\u001b[A\n",
      " 13%|█▎        | 9/67 [00:08<00:58,  1.00s/it]\u001b[A\n",
      " 15%|█▍        | 10/67 [00:09<01:01,  1.07s/it]\u001b[A\n",
      " 16%|█▋        | 11/67 [00:10<00:59,  1.07s/it]\u001b[A\n",
      " 18%|█▊        | 12/67 [00:11<00:56,  1.03s/it]\u001b[A\n",
      " 19%|█▉        | 13/67 [00:13<01:02,  1.16s/it]\u001b[A\n",
      " 21%|██        | 14/67 [00:14<01:04,  1.21s/it]\u001b[A\n",
      " 22%|██▏       | 15/67 [00:15<01:03,  1.23s/it]\u001b[A\n",
      " 24%|██▍       | 16/67 [00:16<00:56,  1.10s/it]\u001b[A\n",
      " 25%|██▌       | 17/67 [00:17<00:49,  1.01it/s]\u001b[A\n",
      " 27%|██▋       | 18/67 [00:18<00:46,  1.04it/s]\u001b[A\n",
      " 28%|██▊       | 19/67 [00:19<00:43,  1.11it/s]\u001b[A\n",
      " 30%|██▉       | 20/67 [00:19<00:42,  1.12it/s]\u001b[A\n",
      " 31%|███▏      | 21/67 [00:21<00:46,  1.00s/it]\u001b[A\n",
      " 33%|███▎      | 22/67 [00:22<00:43,  1.03it/s]\u001b[A\n",
      " 34%|███▍      | 23/67 [00:22<00:42,  1.05it/s]\u001b[A\n",
      " 36%|███▌      | 24/67 [00:23<00:35,  1.20it/s]\u001b[A\n",
      " 37%|███▋      | 25/67 [00:24<00:32,  1.29it/s]\u001b[A\n",
      " 39%|███▉      | 26/67 [00:25<00:38,  1.08it/s]\u001b[A\n",
      " 40%|████      | 27/67 [00:25<00:32,  1.23it/s]\u001b[A\n",
      " 42%|████▏     | 28/67 [00:26<00:32,  1.19it/s]\u001b[A\n",
      " 43%|████▎     | 29/67 [00:27<00:34,  1.10it/s]\u001b[A\n",
      " 45%|████▍     | 30/67 [00:28<00:32,  1.15it/s]\u001b[A\n",
      " 46%|████▋     | 31/67 [00:29<00:32,  1.11it/s]\u001b[A\n",
      " 48%|████▊     | 32/67 [00:30<00:30,  1.13it/s]\u001b[A\n",
      " 49%|████▉     | 33/67 [00:31<00:29,  1.16it/s]\u001b[A\n",
      " 51%|█████     | 34/67 [00:32<00:30,  1.09it/s]\u001b[A\n",
      " 52%|█████▏    | 35/67 [00:33<00:31,  1.02it/s]\u001b[A\n",
      " 54%|█████▎    | 36/67 [00:34<00:31,  1.01s/it]\u001b[A\n",
      " 55%|█████▌    | 37/67 [00:35<00:32,  1.07s/it]\u001b[A\n",
      " 57%|█████▋    | 38/67 [00:36<00:27,  1.04it/s]\u001b[A\n",
      " 58%|█████▊    | 39/67 [00:37<00:23,  1.19it/s]\u001b[A\n",
      " 60%|█████▉    | 40/67 [00:38<00:27,  1.00s/it]\u001b[A\n",
      " 61%|██████    | 41/67 [00:39<00:24,  1.05it/s]\u001b[A\n",
      " 63%|██████▎   | 42/67 [00:39<00:21,  1.19it/s]\u001b[A\n",
      " 64%|██████▍   | 43/67 [00:41<00:24,  1.02s/it]\u001b[A\n",
      " 66%|██████▌   | 44/67 [00:41<00:20,  1.14it/s]\u001b[A\n",
      " 67%|██████▋   | 45/67 [00:43<00:23,  1.06s/it]\u001b[A\n",
      " 69%|██████▊   | 46/67 [00:44<00:24,  1.15s/it]\u001b[A\n",
      " 70%|███████   | 47/67 [00:45<00:21,  1.09s/it]\u001b[A\n",
      " 72%|███████▏  | 48/67 [00:46<00:19,  1.01s/it]\u001b[A\n",
      " 73%|███████▎  | 49/67 [00:47<00:20,  1.14s/it]\u001b[A\n",
      " 75%|███████▍  | 50/67 [00:49<00:20,  1.18s/it]\u001b[A\n",
      " 76%|███████▌  | 51/67 [00:50<00:17,  1.08s/it]\u001b[A\n",
      " 78%|███████▊  | 52/67 [00:50<00:14,  1.03it/s]\u001b[A\n",
      " 79%|███████▉  | 53/67 [00:51<00:13,  1.08it/s]\u001b[A\n",
      " 81%|████████  | 54/67 [00:52<00:12,  1.07it/s]\u001b[A\n",
      " 82%|████████▏ | 55/67 [00:53<00:10,  1.13it/s]\u001b[A\n",
      " 84%|████████▎ | 56/67 [00:54<00:09,  1.13it/s]\u001b[A\n",
      " 85%|████████▌ | 57/67 [00:55<00:10,  1.03s/it]\u001b[A\n",
      " 87%|████████▋ | 58/67 [00:56<00:09,  1.05s/it]\u001b[A\n",
      " 88%|████████▊ | 59/67 [00:57<00:07,  1.05it/s]\u001b[A\n",
      " 90%|████████▉ | 60/67 [00:57<00:05,  1.22it/s]\u001b[A\n",
      " 91%|█████████ | 61/67 [00:58<00:04,  1.22it/s]\u001b[A\n",
      " 93%|█████████▎| 62/67 [00:59<00:04,  1.07it/s]\u001b[A\n",
      " 94%|█████████▍| 63/67 [01:00<00:03,  1.16it/s]\u001b[A\n",
      " 96%|█████████▌| 64/67 [01:01<00:02,  1.17it/s]\u001b[A\n",
      " 97%|█████████▋| 65/67 [01:03<00:02,  1.19s/it]\u001b[A\n",
      " 99%|█████████▊| 66/67 [01:04<00:01,  1.07s/it]\u001b[A\n",
      "                                                   {'eval_loss': 543.5177001953125, 'eval_rouge1': 33.8043, 'eval_rouge2': 15.5749, 'eval_rougeL': 27.5429, 'eval_rougeLsum': 28.1555, 'eval_gen_len': 41.1, 'epoch': 2.9296875}\n",
      " 29%|██▉       | 1500/5120 [08:54<18:21,  3.29it/s]\n",
      "100%|██████████| 67/67 [01:05<00:00,  1.10it/s]\u001b[A\n",
      "                                               \u001b[A\n",
      "[INFO|trainer.py:1226] 2021-01-14 16:07:04,629 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-1500\n",
      "[INFO|configuration_utils.py:289] 2021-01-14 16:07:04,790 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-14 16:07:51,358 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-1500/pytorch_model.bin\n",
      "{'loss': 382.416953125, 'learning_rate': 2.0625e-05, 'epoch': 3.125}\n",
      " 35%|███▌      | 1800/5120 [11:17<21:10,  2.61it/s]{'loss': 358.6586328125, 'learning_rate': 1.9453125e-05, 'epoch': 3.515625}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 2000/5120 [12:19<21:54,  2.37it/s]{'loss': 351.455234375, 'learning_rate': 1.828125e-05, 'epoch': 3.90625}\n",
      " 43%|████▎     | 2200/5120 [13:20<18:40,  2.61it/s]{'loss': 326.3680859375, 'learning_rate': 1.7109375e-05, 'epoch': 4.296875}\n",
      " 47%|████▋     | 2400/5120 [14:23<17:50,  2.54it/s]{'loss': 315.88583984375, 'learning_rate': 1.59375e-05, 'epoch': 4.6875}\n",
      " 51%|█████     | 2600/5120 [15:25<17:24,  2.41it/s]{'loss': 316.39099609375, 'learning_rate': 1.4765625e-05, 'epoch': 5.078125}\n",
      " 55%|█████▍    | 2800/5120 [16:28<15:37,  2.47it/s]{'loss': 299.29072265625, 'learning_rate': 1.359375e-05, 'epoch': 5.46875}\n",
      " 59%|█████▊    | 3000/5120 [17:31<14:58,  2.36it/s][INFO|trainer.py:1412] 2021-01-14 16:15:40,502 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-14 16:15:40,502 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-14 16:15:40,502 >>   Batch size = 4\n",
      "\n",
      "\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 2/67 [00:00<00:31,  2.03it/s]\u001b[A\n",
      "  4%|▍         | 3/67 [00:01<00:37,  1.72it/s]\u001b[A\n",
      "  6%|▌         | 4/67 [00:02<00:39,  1.58it/s]\u001b[A\n",
      "  7%|▋         | 5/67 [00:03<00:45,  1.38it/s]\u001b[A\n",
      "  9%|▉         | 6/67 [00:05<01:05,  1.07s/it]\u001b[A\n",
      " 10%|█         | 7/67 [00:06<01:05,  1.09s/it]\u001b[A\n",
      " 12%|█▏        | 8/67 [00:07<00:59,  1.01s/it]\u001b[A\n",
      " 13%|█▎        | 9/67 [00:08<00:57,  1.00it/s]\u001b[A\n",
      " 15%|█▍        | 10/67 [00:08<00:56,  1.01it/s]\u001b[A\n",
      " 16%|█▋        | 11/67 [00:10<00:56,  1.01s/it]\u001b[A\n",
      " 18%|█▊        | 12/67 [00:11<00:55,  1.01s/it]\u001b[A\n",
      " 19%|█▉        | 13/67 [00:12<00:57,  1.07s/it]\u001b[A\n",
      " 21%|██        | 14/67 [00:13<01:05,  1.23s/it]\u001b[A\n",
      " 22%|██▏       | 15/67 [00:15<01:02,  1.21s/it]\u001b[A\n",
      " 24%|██▍       | 16/67 [00:15<00:57,  1.13s/it]\u001b[A\n",
      " 25%|██▌       | 17/67 [00:16<00:49,  1.01it/s]\u001b[A\n",
      " 27%|██▋       | 18/67 [00:17<00:43,  1.13it/s]\u001b[A\n",
      " 28%|██▊       | 19/67 [00:18<00:42,  1.13it/s]\u001b[A\n",
      " 30%|██▉       | 20/67 [00:18<00:38,  1.23it/s]\u001b[A\n",
      " 31%|███▏      | 21/67 [00:19<00:38,  1.19it/s]\u001b[A\n",
      " 33%|███▎      | 22/67 [00:20<00:40,  1.11it/s]\u001b[A\n",
      " 34%|███▍      | 23/67 [00:21<00:36,  1.19it/s]\u001b[A\n",
      " 36%|███▌      | 24/67 [00:21<00:31,  1.36it/s]\u001b[A\n",
      " 37%|███▋      | 25/67 [00:22<00:31,  1.32it/s]\u001b[A\n",
      " 39%|███▉      | 26/67 [00:24<00:40,  1.02it/s]\u001b[A\n",
      " 40%|████      | 27/67 [00:25<00:36,  1.09it/s]\u001b[A\n",
      " 42%|████▏     | 28/67 [00:25<00:33,  1.15it/s]\u001b[A\n",
      " 43%|████▎     | 29/67 [00:27<00:45,  1.19s/it]\u001b[A\n",
      " 45%|████▍     | 30/67 [00:28<00:39,  1.07s/it]\u001b[A\n",
      " 46%|████▋     | 31/67 [00:29<00:36,  1.01s/it]\u001b[A\n",
      " 48%|████▊     | 32/67 [00:30<00:33,  1.04it/s]\u001b[A\n",
      " 49%|████▉     | 33/67 [00:30<00:30,  1.10it/s]\u001b[A\n",
      " 51%|█████     | 34/67 [00:32<00:35,  1.06s/it]\u001b[A\n",
      " 52%|█████▏    | 35/67 [00:33<00:31,  1.03it/s]\u001b[A\n",
      " 54%|█████▎    | 36/67 [00:33<00:27,  1.11it/s]\u001b[A\n",
      " 55%|█████▌    | 37/67 [00:35<00:28,  1.04it/s]\u001b[A\n",
      " 57%|█████▋    | 38/67 [00:36<00:28,  1.01it/s]\u001b[A\n",
      " 58%|█████▊    | 39/67 [00:36<00:24,  1.14it/s]\u001b[A\n",
      " 60%|█████▉    | 40/67 [00:37<00:26,  1.03it/s]\u001b[A\n",
      " 61%|██████    | 41/67 [00:38<00:24,  1.07it/s]\u001b[A\n",
      " 63%|██████▎   | 42/67 [00:39<00:20,  1.21it/s]\u001b[A\n",
      " 64%|██████▍   | 43/67 [00:40<00:23,  1.01it/s]\u001b[A\n",
      " 66%|██████▌   | 44/67 [00:41<00:19,  1.15it/s]\u001b[A\n",
      " 67%|██████▋   | 45/67 [00:42<00:22,  1.02s/it]\u001b[A\n",
      " 69%|██████▊   | 46/67 [00:43<00:20,  1.03it/s]\u001b[A\n",
      " 70%|███████   | 47/67 [00:44<00:19,  1.01it/s]\u001b[A\n",
      " 72%|███████▏  | 48/67 [00:45<00:18,  1.04it/s]\u001b[A\n",
      " 73%|███████▎  | 49/67 [00:46<00:18,  1.03s/it]\u001b[A\n",
      " 75%|███████▍  | 50/67 [00:47<00:19,  1.12s/it]\u001b[A\n",
      " 76%|███████▌  | 51/67 [00:49<00:19,  1.22s/it]\u001b[A\n",
      " 78%|███████▊  | 52/67 [00:49<00:15,  1.04s/it]\u001b[A\n",
      " 79%|███████▉  | 53/67 [00:50<00:12,  1.08it/s]\u001b[A\n",
      " 81%|████████  | 54/67 [00:51<00:11,  1.12it/s]\u001b[A\n",
      " 82%|████████▏ | 55/67 [00:52<00:10,  1.17it/s]\u001b[A\n",
      " 84%|████████▎ | 56/67 [00:53<00:10,  1.03it/s]\u001b[A\n",
      " 85%|████████▌ | 57/67 [00:54<00:10,  1.03s/it]\u001b[A\n",
      " 87%|████████▋ | 58/67 [00:56<00:11,  1.29s/it]\u001b[A\n",
      " 88%|████████▊ | 59/67 [00:57<00:09,  1.16s/it]\u001b[A\n",
      " 90%|████████▉ | 60/67 [00:57<00:06,  1.02it/s]\u001b[A\n",
      " 91%|█████████ | 61/67 [00:58<00:05,  1.09it/s]\u001b[A\n",
      " 93%|█████████▎| 62/67 [00:59<00:04,  1.01it/s]\u001b[A\n",
      " 94%|█████████▍| 63/67 [01:00<00:03,  1.12it/s]\u001b[A\n",
      " 96%|█████████▌| 64/67 [01:01<00:02,  1.20it/s]\u001b[A\n",
      " 97%|█████████▋| 65/67 [01:02<00:02,  1.00s/it]\u001b[A\n",
      " 99%|█████████▊| 66/67 [01:03<00:00,  1.03it/s]\u001b[A\n",
      "                                                   {'eval_loss': 587.0930786132812, 'eval_rouge1': 32.7734, 'eval_rouge2': 14.9052, 'eval_rougeL': 26.4432, 'eval_rougeLsum': 27.137, 'eval_gen_len': 39.4, 'epoch': 5.859375}\n",
      " 59%|█████▊    | 3000/5120 [18:37<14:58,  2.36it/s]\n",
      "100%|██████████| 67/67 [01:04<00:00,  1.25it/s]\u001b[A\n",
      "                                               \u001b[A\n",
      "[INFO|trainer.py:1226] 2021-01-14 16:16:47,361 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-3000\n",
      "[INFO|configuration_utils.py:289] 2021-01-14 16:16:47,510 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-14 16:17:32,476 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-3000/pytorch_model.bin\n",
      " 62%|██████▎   | 3200/5120 [20:25<12:52,  2.49it/s]{'loss': 281.3114453125, 'learning_rate': 1.125e-05, 'epoch': 6.25}\n",
      " 66%|██████▋   | 3400/5120 [21:27<11:28,  2.50it/s]{'loss': 279.33384765625, 'learning_rate': 1.0078125000000001e-05, 'epoch': 6.640625}\n",
      "                                                   {'loss': 268.0846875, 'learning_rate': 8.90625e-06, 'epoch': 7.03125}\n",
      " 74%|███████▍  | 3800/5120 [23:32<08:55,  2.47it/s]{'loss': 258.96712890625, 'learning_rate': 7.734375e-06, 'epoch': 7.421875}\n",
      " 78%|███████▊  | 4000/5120 [24:36<07:16,  2.56it/s]{'loss': 270.4324609375, 'learning_rate': 6.5625e-06, 'epoch': 7.8125}\n",
      "                                                   {'loss': 253.3472265625, 'learning_rate': 5.390625e-06, 'epoch': 8.203125}\n",
      " 86%|████████▌ | 4400/5120 [26:39<05:02,  2.38it/s]-06, 'epoch': 8.59375}\n",
      " 88%|████████▊ | 4500/5120 [27:10<03:14,  3.18it/s][INFO|trainer.py:1412] 2021-01-14 16:25:19,775 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-14 16:25:19,776 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-14 16:25:19,776 >>   Batch size = 4\n",
      "\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 2/67 [00:00<00:25,  2.50it/s]\u001b[A\n",
      "  4%|▍         | 3/67 [00:01<00:37,  1.70it/s]\u001b[A\n",
      "  6%|▌         | 4/67 [00:02<00:36,  1.72it/s]\u001b[A\n",
      "  7%|▋         | 5/67 [00:03<00:47,  1.31it/s]\u001b[A\n",
      "  9%|▉         | 6/67 [00:04<01:03,  1.04s/it]\u001b[A\n",
      " 10%|█         | 7/67 [00:06<01:04,  1.08s/it]\u001b[A\n",
      " 12%|█▏        | 8/67 [00:06<00:55,  1.06it/s]\u001b[A\n",
      " 13%|█▎        | 9/67 [00:07<00:56,  1.03it/s]\u001b[A\n",
      " 15%|█▍        | 10/67 [00:09<01:01,  1.08s/it]\u001b[A\n",
      " 16%|█▋        | 11/67 [00:10<00:59,  1.06s/it]\u001b[A\n",
      " 18%|█▊        | 12/67 [00:11<00:56,  1.03s/it]\u001b[A\n",
      " 19%|█▉        | 13/67 [00:12<00:57,  1.06s/it]\u001b[A\n",
      " 21%|██        | 14/67 [00:13<00:59,  1.13s/it]\u001b[A\n",
      " 22%|██▏       | 15/67 [00:14<01:04,  1.23s/it]\u001b[A\n",
      " 24%|██▍       | 16/67 [00:15<00:57,  1.12s/it]\u001b[A\n",
      " 25%|██▌       | 17/67 [00:16<00:47,  1.04it/s]\u001b[A\n",
      " 27%|██▋       | 18/67 [00:17<00:42,  1.15it/s]\u001b[A\n",
      " 28%|██▊       | 19/67 [00:18<00:43,  1.09it/s]\u001b[A\n",
      " 30%|██▉       | 20/67 [00:18<00:40,  1.17it/s]\u001b[A\n",
      " 31%|███▏      | 21/67 [00:19<00:39,  1.15it/s]\u001b[A\n",
      " 33%|███▎      | 22/67 [00:20<00:42,  1.06it/s]\u001b[A\n",
      " 34%|███▍      | 23/67 [00:21<00:41,  1.06it/s]\u001b[A\n",
      " 36%|███▌      | 24/67 [00:22<00:36,  1.19it/s]\u001b[A\n",
      " 37%|███▋      | 25/67 [00:22<00:31,  1.32it/s]\u001b[A\n",
      " 39%|███▉      | 26/67 [00:24<00:36,  1.13it/s]\u001b[A\n",
      " 40%|████      | 27/67 [00:24<00:33,  1.19it/s]\u001b[A\n",
      " 42%|████▏     | 28/67 [00:25<00:31,  1.23it/s]\u001b[A\n",
      " 43%|████▎     | 29/67 [00:26<00:36,  1.03it/s]\u001b[A\n",
      " 45%|████▍     | 30/67 [00:27<00:33,  1.12it/s]\u001b[A\n",
      " 46%|████▋     | 31/67 [00:28<00:32,  1.12it/s]\u001b[A\n",
      " 48%|████▊     | 32/67 [00:29<00:30,  1.14it/s]\u001b[A\n",
      " 49%|████▉     | 33/67 [00:30<00:30,  1.12it/s]\u001b[A\n",
      " 51%|█████     | 34/67 [00:31<00:30,  1.09it/s]\u001b[A\n",
      " 52%|█████▏    | 35/67 [00:32<00:29,  1.09it/s]\u001b[A\n",
      " 54%|█████▎    | 36/67 [00:33<00:28,  1.10it/s]\u001b[A\n",
      " 55%|█████▌    | 37/67 [00:33<00:25,  1.19it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 38/67 [00:34<00:25,  1.16it/s]\u001b[A\n",
      " 58%|█████▊    | 39/67 [00:35<00:20,  1.35it/s]\u001b[A\n",
      " 60%|█████▉    | 40/67 [00:36<00:26,  1.03it/s]\u001b[A\n",
      " 61%|██████    | 41/67 [00:37<00:24,  1.08it/s]\u001b[A\n",
      " 63%|██████▎   | 42/67 [00:38<00:20,  1.24it/s]\u001b[A\n",
      " 64%|██████▍   | 43/67 [00:39<00:21,  1.14it/s]\u001b[A\n",
      " 66%|██████▌   | 44/67 [00:39<00:17,  1.28it/s]\u001b[A\n",
      " 67%|██████▋   | 45/67 [00:40<00:21,  1.04it/s]\u001b[A\n",
      " 69%|██████▊   | 46/67 [00:41<00:20,  1.04it/s]\u001b[A\n",
      " 70%|███████   | 47/67 [00:43<00:20,  1.01s/it]\u001b[A\n",
      " 72%|███████▏  | 48/67 [00:43<00:18,  1.03it/s]\u001b[A\n",
      " 73%|███████▎  | 49/67 [00:45<00:19,  1.08s/it]\u001b[A\n",
      " 75%|███████▍  | 50/67 [00:46<00:19,  1.15s/it]\u001b[A\n",
      " 76%|███████▌  | 51/67 [00:47<00:18,  1.16s/it]\u001b[A\n",
      " 78%|███████▊  | 52/67 [00:48<00:15,  1.07s/it]\u001b[A\n",
      " 79%|███████▉  | 53/67 [00:49<00:13,  1.04it/s]\u001b[A\n",
      " 81%|████████  | 54/67 [00:50<00:12,  1.07it/s]\u001b[A\n",
      " 82%|████████▏ | 55/67 [00:50<00:10,  1.14it/s]\u001b[A\n",
      " 84%|████████▎ | 56/67 [00:52<00:10,  1.06it/s]\u001b[A\n",
      " 85%|████████▌ | 57/67 [00:53<00:10,  1.03s/it]\u001b[A\n",
      " 87%|████████▋ | 58/67 [00:54<00:09,  1.07s/it]\u001b[A\n",
      " 88%|████████▊ | 59/67 [00:55<00:08,  1.00s/it]\u001b[A\n",
      " 90%|████████▉ | 60/67 [00:55<00:06,  1.14it/s]\u001b[A\n",
      " 91%|█████████ | 61/67 [00:56<00:05,  1.06it/s]\u001b[A\n",
      " 93%|█████████▎| 62/67 [00:58<00:05,  1.00s/it]\u001b[A\n",
      " 94%|█████████▍| 63/67 [00:58<00:03,  1.07it/s]\u001b[A\n",
      " 96%|█████████▌| 64/67 [00:59<00:02,  1.13it/s]\u001b[A\n",
      " 97%|█████████▋| 65/67 [01:01<00:02,  1.07s/it]\u001b[A\n",
      " 99%|█████████▊| 66/67 [01:01<00:00,  1.05it/s]\u001b[A\n",
      "                                                   {'eval_loss': 607.7008666992188, 'eval_rouge1': 33.2772, 'eval_rouge2': 14.8383, 'eval_rougeL': 26.6318, 'eval_rougeLsum': 27.2894, 'eval_gen_len': 39.7, 'epoch': 8.7890625}\n",
      " 88%|████████▊ | 4500/5120 [28:15<03:14,  3.18it/s]\n",
      "100%|██████████| 67/67 [01:03<00:00,  1.24it/s]\u001b[A\n",
      "                                               \u001b[A\n",
      "[INFO|trainer.py:1226] 2021-01-14 16:26:24,995 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-4500\n",
      "[INFO|configuration_utils.py:289] 2021-01-14 16:26:25,147 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-14 16:27:08,420 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/checkpoint-4500/pytorch_model.bin\n",
      " 90%|████████▉ | 4600/5120 [29:31<03:48,  2.28it/s]e-06, 'epoch': 8.984375}\n",
      " 94%|█████████▍| 4800/5120 [30:33<02:16,  2.34it/s]'epoch': 9.375}\n",
      "                                                   {'loss': 250.45046875, 'learning_rate': 7.03125e-07, 'epoch': 9.765625}\n",
      "100%|██████████| 5120/5120 [32:12<00:00,  3.38it/s]{'epoch': 10.0}[INFO|trainer.py:862] 2021-01-14 16:30:22,207 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 5120/5120 [32:12<00:00,  2.65it/s]\n",
      "\n",
      "[INFO|trainer.py:1226] 2021-01-14 16:30:22,209 >> Saving model checkpoint to /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train\n",
      "[INFO|configuration_utils.py:289] 2021-01-14 16:30:22,397 >> Configuration saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/config.json\n",
      "[INFO|modeling_utils.py:814] 2021-01-14 16:31:08,331 >> Model weights saved in /home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train/pytorch_model.bin\n",
      "01/14/2021 16:31:08 - INFO - __main__ -   ***** train metrics *****\n",
      "01/14/2021 16:31:08 - INFO - __main__ -     train_samples_per_second = -0.001\n",
      "01/14/2021 16:31:08 - INFO - __main__ -     train_runtime = 1934.0888\n",
      "01/14/2021 16:31:08 - INFO - __main__ -     train_n_ojbs = -1\n",
      "01/14/2021 16:31:09 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:1412] 2021-01-14 16:31:09,948 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1413] 2021-01-14 16:31:09,948 >>   Num examples = 266\n",
      "[INFO|trainer.py:1414] 2021-01-14 16:31:09,948 >>   Batch size = 4\n",
      "100%|██████████| 67/67 [01:02<00:00,  1.08it/s]\n",
      "01/14/2021 16:32:14 - INFO - __main__ -   ***** val metrics *****\n",
      "01/14/2021 16:32:14 - INFO - __main__ -     val_loss = 609.4792\n",
      "01/14/2021 16:32:14 - INFO - __main__ -     val_rouge1 = 33.6283\n",
      "01/14/2021 16:32:14 - INFO - __main__ -     val_rouge2 = 14.8871\n",
      "01/14/2021 16:32:14 - INFO - __main__ -     val_rougeL = 26.7906\n",
      "01/14/2021 16:32:14 - INFO - __main__ -     val_rougeLsum = 27.5057\n",
      "01/14/2021 16:32:14 - INFO - __main__ -     val_gen_len = 39.2\n",
      "01/14/2021 16:32:14 - INFO - __main__ -     epoch = 10.0\n",
      "01/14/2021 16:32:14 - INFO - __main__ -     val_samples_per_second = -0.016\n",
      "01/14/2021 16:32:14 - INFO - __main__ -     val_runtime = 64.244\n",
      "01/14/2021 16:32:14 - INFO - __main__ -     val_n_ojbs = -1\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2801\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210114_155808-1eo9kt9t/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/ubuntu/magma/fine-tuning/wandb/run-20210114_155808-1eo9kt9t/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                             _step 5120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                          _runtime 2046\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                        _timestamp 1610641934\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                        train/loss 250.45047\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                               train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                       train/epoch 10.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                         eval/loss 607.70087\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                       eval/rouge1 33.2772\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                       eval/rouge2 14.8383\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                       eval/rougeL 26.6318\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                    eval/rougeLsum 27.2894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                      eval/gen_len 39.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/total_flos 12482920312012800\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                    train/val_loss 609.47919\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/val_rouge1 33.6283\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/val_rouge2 14.8871\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/val_rougeL 26.7906\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                               train/val_rougeLsum 27.5057\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                 train/val_gen_len 39.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▆▆▆▆▆▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▆▆▆▆▆▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▆▆▆▄▄▄▄▃▃▃▃▂▂▂▂▁▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/learning_rate ██▇▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             eval/loss ▁▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rouge1 █▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rouge2 █▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           eval/rougeL █▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/rougeLsum █▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/gen_len █▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        train/val_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rouge1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rouge2 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/val_rougeL ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/val_rougeLsum ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/val_gen_len ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/home/ubuntu/s3/fine-tuning/sshleifer?distilbart-cnn-12-6_karger_books_para_train\u001b[0m: \u001b[34mhttps://wandb.ai/marcoabrate/huggingface/runs/1eo9kt9t\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 $finetune_script \\\n",
    "--model_name_or_path $model_name_or_path \\\n",
    "--config_name $model_config_dir \\\n",
    "--tokenizer_name $model_name_or_path \\\n",
    "--cache_dir $cache_dir \\\n",
    "--data_dir $data_dir \\\n",
    "--fp16 \\\n",
    "--learning_rate 3e-5 --label_smoothing 0.1 \\\n",
    "--sortish_sampler --freeze_embeds --adafactor \\\n",
    "--task summarization \\\n",
    "--max_source_length 1024 \\\n",
    "--max_target_length $config.ONE_BULLET_MAX_LEN \\\n",
    "--val_max_target_length $config.ONE_BULLET_MAX_LEN \\\n",
    "--test_max_target_length $config.ONE_BULLET_MAX_LEN \\\n",
    "--do_train \\\n",
    "--num_train_epochs 10 \\\n",
    "--logging_steps 200 --save_steps 1500 --save_total_limit 3 \\\n",
    "--per_device_train_batch_size 4 --per_device_eval_batch_size 4 \\\n",
    "--do_eval --evaluation_strategy steps --eval_steps 1500 --eval_beams 2 \\\n",
    "--metric_for_best_model rougeL --greater_is_better True \\\n",
    "--predict_with_generate \\\n",
    "--output_dir $output_dir \\\n",
    "--overwrite_output_dir \\\n",
    "--seed $config.SEED \\\n",
    "--run_name $output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeSSMVZwVOa1"
   },
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2YaHOesVOa4"
   },
   "outputs": [],
   "source": [
    "source_test_dir = data_dir[:-1] + '/test.source\"'\n",
    "reference_test_dir = data_dir[:-1] + '/test.target\"'\n",
    "\n",
    "save_dir = output_dir[:-1] + '/'+model_name_or_path.replace('/', '?')+'_test_karger_books_para.txt\"'\n",
    "score_dir = output_dir[:-1] + '/'+model_name_or_path.replace('/', '?')+'_test_karger_books_para.json\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1399,
     "status": "ok",
     "timestamp": 1610471442216,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "AgY3gO94VOa9",
    "outputId": "8436601c-f530-405f-e6c6-c025a7cf5b71"
   },
   "outputs": [],
   "source": [
    "!python3 $eval_script \\\n",
    "$output_dir \\\n",
    "$source_test_dir \\\n",
    "$save_dir \\\n",
    "--reference_path $reference_test_dir \\\n",
    "--score_path $score_dir \\\n",
    "--task summarization \\\n",
    "--bs 2 \\\n",
    "--length_penalty $config.LENAGTH_PENALTY \\\n",
    "--no_repeat_ngram_size $config.NO_REPEAT_NGRAM_SIZE \\\n",
    "--num_beams $config.NUM_BEAMS \\\n",
    "--dump-args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bbRUTtNDHth"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNa0Dk3mImrCcZO3VdQRayI",
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "L5sXxqeNCtkN",
    "S0FByNNOIRvG",
    "GPbOrCLWACbm",
    "siT4m5aYCFSh",
    "Dk1uGO5SCDNa",
    "WdDCBiMOBWiO",
    "pr_0J4xgBWiW",
    "l8hQT6ksBWin",
    "d5V0QCdf04Yx",
    "KQj3gt6s5ACz",
    "ah9sssub5DXX",
    "M58yiP1yVOav",
    "aeSSMVZwVOa1"
   ],
   "name": "bart_finetune.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "88734567ccb2435091eeab9f30b2de9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bd07faa8d1534ff9b1967f18e6fca2c2",
       "IPY_MODEL_43ac5522170c45f7856f7097ff9fee58"
      ],
      "layout": "IPY_MODEL_431d75cb2dd2403e88015f4e009080d4"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
