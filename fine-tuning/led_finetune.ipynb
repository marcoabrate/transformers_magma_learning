{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Hgw3GTXLLw0"
   },
   "source": [
    "## ðŸ¤— Finetune **Longformer Encoder-Decoder (LED)** on 8K Tokens ðŸ¤—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7-QHmRiAMB9"
   },
   "source": [
    "The *Longformer Encoder-Decoder (LED)* was recently added as an extension to [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n",
    "\n",
    "In this notebook we will finetune *LED* for Summarization on [Pubmed](https://huggingface.co/datasets/viewer/?dataset=scientific_papers). *Pubmed* is a long-range summarization dataset, which makes it a good candidate for LED. LED will be finetuned up to an input length of 8K tokens on a single GPU.\n",
    "\n",
    "We will leverage ðŸ¤—`Seq2SeqTrainer`, gradient checkpointing and as usual ðŸ¤—`datasets`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0B19PhgrCHM1"
   },
   "source": [
    "First, let's try to get a GPU with at least 15GB RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JnoCUoL-2Jz"
   },
   "outputs": [],
   "source": [
    "# crash colab to get more RAM\n",
    "# !kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZ11Va-qCp96"
   },
   "source": [
    "To check that we are having enough RAM we can run the following command.\n",
    "If the randomely allocated GPU is too small, the above cells can be run \n",
    "to crash the notebook hoping to get a better GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9IkphgF-90-",
    "outputId": "7384a959-6417-46a5-a5fd-b2aedee80c55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 13 16:05:40 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   58C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHSLE5TdC7RL"
   },
   "source": [
    "Next, we install ðŸ¤—Transformers, ðŸ¤—Datasets, and `rouge_score`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pzh-JNaUK3Y_"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%%capture\n",
    "!pip install datasets==1.2.1\n",
    "!pip install transformers==4.2.0\n",
    "!pip install rouge_score\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0u9twLMqYEzT"
   },
   "source": [
    "Let's start by loading and preprocessing the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ODLQ8MUJfmi4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acronym_identification',\n",
       " 'ade_corpus_v2',\n",
       " 'adversarial_qa',\n",
       " 'aeslc',\n",
       " 'afrikaans_ner_corpus',\n",
       " 'ag_news',\n",
       " 'ai2_arc',\n",
       " 'air_dialogue',\n",
       " 'ajgt_twitter_ar',\n",
       " 'allegro_reviews',\n",
       " 'allocine',\n",
       " 'alt',\n",
       " 'amazon_polarity',\n",
       " 'amazon_reviews_multi',\n",
       " 'amazon_us_reviews',\n",
       " 'ambig_qa',\n",
       " 'amttl',\n",
       " 'anli',\n",
       " 'app_reviews',\n",
       " 'aqua_rat',\n",
       " 'aquamuse',\n",
       " 'ar_cov19',\n",
       " 'ar_res_reviews',\n",
       " 'arabic_billion_words',\n",
       " 'arabic_pos_dialect',\n",
       " 'arcd',\n",
       " 'arsentd_lev',\n",
       " 'art',\n",
       " 'arxiv_dataset',\n",
       " 'aslg_pc12',\n",
       " 'asnq',\n",
       " 'asset',\n",
       " 'assin',\n",
       " 'assin2',\n",
       " 'atomic',\n",
       " 'autshumato',\n",
       " 'bc2gm_corpus',\n",
       " 'best2009',\n",
       " 'bianet',\n",
       " 'bible_para',\n",
       " 'big_patent',\n",
       " 'billsum',\n",
       " 'bing_coronavirus_query_set',\n",
       " 'biomrc',\n",
       " 'blended_skill_talk',\n",
       " 'blimp',\n",
       " 'blog_authorship_corpus',\n",
       " 'bn_hate_speech',\n",
       " 'bookcorpus',\n",
       " 'bookcorpusopen',\n",
       " 'boolq',\n",
       " 'bprec',\n",
       " 'break_data',\n",
       " 'brwac',\n",
       " 'bsd_ja_en',\n",
       " 'bswac',\n",
       " 'c3',\n",
       " 'c4',\n",
       " 'cail2018',\n",
       " 'capes',\n",
       " 'catalonia_independence',\n",
       " 'cawac',\n",
       " 'cc100',\n",
       " 'cdsc',\n",
       " 'cdt',\n",
       " 'cfq',\n",
       " 'chr_en',\n",
       " 'cifar10',\n",
       " 'circa',\n",
       " 'civil_comments',\n",
       " 'clickbait_news_bg',\n",
       " 'climate_fever',\n",
       " 'clinc_oos',\n",
       " 'clue',\n",
       " 'cmrc2018',\n",
       " 'cnn_dailymail',\n",
       " 'coached_conv_pref',\n",
       " 'coarse_discourse',\n",
       " 'codah',\n",
       " 'code_search_net',\n",
       " 'com_qa',\n",
       " 'common_gen',\n",
       " 'commonsense_qa',\n",
       " 'compguesswhat',\n",
       " 'conceptnet5',\n",
       " 'conll2000',\n",
       " 'conll2002',\n",
       " 'conll2003',\n",
       " 'conv_ai',\n",
       " 'conv_ai_2',\n",
       " 'conv_ai_3',\n",
       " 'coqa',\n",
       " 'cornell_movie_dialog',\n",
       " 'cos_e',\n",
       " 'cosmos_qa',\n",
       " 'counter',\n",
       " 'covid_qa_castorini',\n",
       " 'covid_qa_deepset',\n",
       " 'covid_qa_ucsd',\n",
       " 'covid_tweets_japanese',\n",
       " 'craigslist_bargains',\n",
       " 'crawl_domain',\n",
       " 'crd3',\n",
       " 'crime_and_punish',\n",
       " 'crows_pairs',\n",
       " 'cs_restaurants',\n",
       " 'curiosity_dialogs',\n",
       " 'daily_dialog',\n",
       " 'dane',\n",
       " 'danish_political_comments',\n",
       " 'dart',\n",
       " 'datacommons_factcheck',\n",
       " 'dbpedia_14',\n",
       " 'dbrd',\n",
       " 'deal_or_no_dialog',\n",
       " 'definite_pronoun_resolution',\n",
       " 'dengue_filipino',\n",
       " 'dialog_re',\n",
       " 'diplomacy_detection',\n",
       " 'disaster_response_messages',\n",
       " 'discofuse',\n",
       " 'discovery',\n",
       " 'doc2dial',\n",
       " 'docred',\n",
       " 'doqa',\n",
       " 'dream',\n",
       " 'drop',\n",
       " 'dutch_social',\n",
       " 'dyk',\n",
       " 'e2e_nlg',\n",
       " 'e2e_nlg_cleaned',\n",
       " 'ecb',\n",
       " 'ehealth_kd',\n",
       " 'eitb_parcc',\n",
       " 'eli5',\n",
       " 'emea',\n",
       " 'emo',\n",
       " 'emotion',\n",
       " 'emotone_ar',\n",
       " 'empathetic_dialogues',\n",
       " 'enriched_web_nlg',\n",
       " 'eraser_multi_rc',\n",
       " 'esnli',\n",
       " 'eth_py150_open',\n",
       " 'ethos',\n",
       " 'euronews',\n",
       " 'europa_eac_tm',\n",
       " 'europa_ecdc_tm',\n",
       " 'event2Mind',\n",
       " 'evidence_infer_treatment',\n",
       " 'exams',\n",
       " 'factckbr',\n",
       " 'fake_news_english',\n",
       " 'fake_news_filipino',\n",
       " 'farsi_news',\n",
       " 'fever',\n",
       " 'finer',\n",
       " 'flores',\n",
       " 'flue',\n",
       " 'fquad',\n",
       " 'gap',\n",
       " 'generated_reviews_enth',\n",
       " 'generics_kb',\n",
       " 'german_legal_entity_recognition',\n",
       " 'germaner',\n",
       " 'germeval_14',\n",
       " 'giga_fren',\n",
       " 'gigaword',\n",
       " 'glucose',\n",
       " 'glue',\n",
       " 'gnad10',\n",
       " 'go_emotions',\n",
       " 'google_wellformed_query',\n",
       " 'grail_qa',\n",
       " 'great_code',\n",
       " 'guardian_authorship',\n",
       " 'gutenberg_time',\n",
       " 'hans',\n",
       " 'hansards',\n",
       " 'hard',\n",
       " 'harem',\n",
       " 'has_part',\n",
       " 'hate_offensive',\n",
       " 'hate_speech18',\n",
       " 'hate_speech_filipino',\n",
       " 'hate_speech_offensive',\n",
       " 'hate_speech_pl',\n",
       " 'hate_speech_portuguese',\n",
       " 'hatexplain',\n",
       " 'hausa_voa_ner',\n",
       " 'hausa_voa_topics',\n",
       " 'head_qa',\n",
       " 'health_fact',\n",
       " 'hebrew_projectbenyehuda',\n",
       " 'hebrew_sentiment',\n",
       " 'hebrew_this_world',\n",
       " 'hellaswag',\n",
       " 'hind_encorp',\n",
       " 'hindi_discourse',\n",
       " 'hippocorpus',\n",
       " 'hkcancor',\n",
       " 'hope_edi',\n",
       " 'hotpot_qa',\n",
       " 'hover',\n",
       " 'hrenwac_para',\n",
       " 'hrwac',\n",
       " 'humicroedit',\n",
       " 'hybrid_qa',\n",
       " 'hyperpartisan_news_detection',\n",
       " 'id_clickbait',\n",
       " 'id_liputan6',\n",
       " 'id_nergrit_corpus',\n",
       " 'id_newspapers_2018',\n",
       " 'id_panl_bppt',\n",
       " 'id_puisi',\n",
       " 'igbo_english_machine_translation',\n",
       " 'igbo_monolingual',\n",
       " 'igbo_ner',\n",
       " 'ilist',\n",
       " 'imdb',\n",
       " 'imdb_urdu_reviews',\n",
       " 'imppres',\n",
       " 'indic_glue',\n",
       " 'indonlu',\n",
       " 'inquisitive_qg',\n",
       " 'interpress_news_category_tr',\n",
       " 'isixhosa_ner_corpus',\n",
       " 'isizulu_ner_corpus',\n",
       " 'iwslt2017',\n",
       " 'jeopardy',\n",
       " 'jfleg',\n",
       " 'jigsaw_toxicity_pred',\n",
       " 'jnlpba',\n",
       " 'journalists_questions',\n",
       " 'kannada_news',\n",
       " 'kd_conv',\n",
       " 'kde4',\n",
       " 'kelm',\n",
       " 'kilt_tasks',\n",
       " 'kilt_wikipedia',\n",
       " 'kinnews_kirnews',\n",
       " 'kor_3i4k',\n",
       " 'kor_hate',\n",
       " 'kor_ner',\n",
       " 'kor_nli',\n",
       " 'kor_nlu',\n",
       " 'kor_qpair',\n",
       " 'kor_sae',\n",
       " 'kor_sarcasm',\n",
       " 'labr',\n",
       " 'lama',\n",
       " 'lambada',\n",
       " 'large_spanish_corpus',\n",
       " 'lc_quad',\n",
       " 'lener_br',\n",
       " 'liar',\n",
       " 'librispeech_lm',\n",
       " 'limit',\n",
       " 'lince',\n",
       " 'linnaeus',\n",
       " 'liveqa',\n",
       " 'lm1b',\n",
       " 'lst20',\n",
       " 'mac_morpho',\n",
       " 'makhzan',\n",
       " 'math_dataset',\n",
       " 'math_qa',\n",
       " 'matinf',\n",
       " 'mc_taco',\n",
       " 'md_gender_bias',\n",
       " 'med_hop',\n",
       " 'medal',\n",
       " 'medical_dialog',\n",
       " 'medical_questions_pairs',\n",
       " 'menyo20k_mt',\n",
       " 'meta_woz',\n",
       " 'metooma',\n",
       " 'metrec',\n",
       " 'mkb',\n",
       " 'mkqa',\n",
       " 'mlqa',\n",
       " 'mlsum',\n",
       " 'mnist',\n",
       " 'mocha',\n",
       " 'movie_rationales',\n",
       " 'mrqa',\n",
       " 'ms_marco',\n",
       " 'ms_terms',\n",
       " 'msr_genomics_kbcomp',\n",
       " 'msr_sqa',\n",
       " 'msr_text_compression',\n",
       " 'msr_zhen_translation_parity',\n",
       " 'msra_ner',\n",
       " 'mt_eng_vietnamese',\n",
       " 'muchocine',\n",
       " 'multi_booked',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'multi_para_crawl',\n",
       " 'multi_re_qa',\n",
       " 'multi_woz_v22',\n",
       " 'multi_x_science_sum',\n",
       " 'mutual_friends',\n",
       " 'mwsc',\n",
       " 'myanmar_news',\n",
       " 'narrativeqa',\n",
       " 'natural_questions',\n",
       " 'ncbi_disease',\n",
       " 'nchlt',\n",
       " 'ncslgr',\n",
       " 'nell',\n",
       " 'neural_code_search',\n",
       " 'news_commentary',\n",
       " 'newsgroup',\n",
       " 'newsph',\n",
       " 'newsph_nli',\n",
       " 'newsqa',\n",
       " 'newsroom',\n",
       " 'nkjp-ner',\n",
       " 'nli_tr',\n",
       " 'norwegian_ner',\n",
       " 'nq_open',\n",
       " 'nsmc',\n",
       " 'numer_sense',\n",
       " 'numeric_fused_head',\n",
       " 'oclar',\n",
       " 'offcombr',\n",
       " 'offenseval2020_tr',\n",
       " 'offenseval_dravidian',\n",
       " 'ofis_publik',\n",
       " 'ohsumed',\n",
       " 'ollie',\n",
       " 'omp',\n",
       " 'onestop_english',\n",
       " 'open_subtitles',\n",
       " 'openbookqa',\n",
       " 'openwebtext',\n",
       " 'opinosis',\n",
       " 'opus100',\n",
       " 'opus_books',\n",
       " 'opus_dgt',\n",
       " 'opus_dogc',\n",
       " 'opus_elhuyar',\n",
       " 'opus_euconst',\n",
       " 'opus_finlex',\n",
       " 'opus_fiskmo',\n",
       " 'opus_gnome',\n",
       " 'opus_infopankki',\n",
       " 'opus_memat',\n",
       " 'opus_montenegrinsubs',\n",
       " 'opus_openoffice',\n",
       " 'opus_paracrawl',\n",
       " 'opus_rf',\n",
       " 'opus_tedtalks',\n",
       " 'opus_ubuntu',\n",
       " 'opus_wikipedia',\n",
       " 'opus_xhosanavy',\n",
       " 'orange_sum',\n",
       " 'para_crawl',\n",
       " 'para_pat',\n",
       " 'paws',\n",
       " 'paws-x',\n",
       " 'pec',\n",
       " 'peer_read',\n",
       " 'peoples_daily_ner',\n",
       " 'per_sent',\n",
       " 'persian_ner',\n",
       " 'pg19',\n",
       " 'php',\n",
       " 'piaf',\n",
       " 'pib',\n",
       " 'piqa',\n",
       " 'pn_summary',\n",
       " 'poem_sentiment',\n",
       " 'polemo2',\n",
       " 'poleval2019_cyberbullying',\n",
       " 'poleval2019_mt',\n",
       " 'polsum',\n",
       " 'polyglot_ner',\n",
       " 'prachathai67k',\n",
       " 'pragmeval',\n",
       " 'proto_qa',\n",
       " 'psc',\n",
       " 'ptb_text_only',\n",
       " 'pubmed',\n",
       " 'pubmed_qa',\n",
       " 'py_ast',\n",
       " 'qa4mre',\n",
       " 'qa_srl',\n",
       " 'qa_zre',\n",
       " 'qangaroo',\n",
       " 'qanta',\n",
       " 'qasc',\n",
       " 'qed',\n",
       " 'qed_amara',\n",
       " 'quac',\n",
       " 'quail',\n",
       " 'quarel',\n",
       " 'quartz',\n",
       " 'quora',\n",
       " 'quoref',\n",
       " 'race',\n",
       " 're_dial',\n",
       " 'reasoning_bg',\n",
       " 'recipe_nlg',\n",
       " 'reclor',\n",
       " 'reddit',\n",
       " 'reddit_tifu',\n",
       " 'refresd',\n",
       " 'reuters21578',\n",
       " 'roman_urdu',\n",
       " 'ronec',\n",
       " 'ropes',\n",
       " 'rotten_tomatoes',\n",
       " 'samsum',\n",
       " 'sanskrit_classic',\n",
       " 'saudinewsnet',\n",
       " 'scan',\n",
       " 'scb_mt_enth_2020',\n",
       " 'schema_guided_dstc8',\n",
       " 'scicite',\n",
       " 'scielo',\n",
       " 'scientific_papers',\n",
       " 'scifact',\n",
       " 'sciq',\n",
       " 'scitail',\n",
       " 'scitldr',\n",
       " 'search_qa',\n",
       " 'selqa',\n",
       " 'sem_eval_2010_task_8',\n",
       " 'sem_eval_2014_task_1',\n",
       " 'sem_eval_2020_task_11',\n",
       " 'sent_comp',\n",
       " 'senti_lex',\n",
       " 'senti_ws',\n",
       " 'sentiment140',\n",
       " 'sepedi_ner',\n",
       " 'sesotho_ner_corpus',\n",
       " 'setimes',\n",
       " 'setswana_ner_corpus',\n",
       " 'sharc',\n",
       " 'sharc_modified',\n",
       " 'simple_questions_v2',\n",
       " 'siswati_ner_corpus',\n",
       " 'smartdata',\n",
       " 'sms_spam',\n",
       " 'snips_built_in_intents',\n",
       " 'snli',\n",
       " 'snow_simplified_japanese_corpus',\n",
       " 'so_stacksample',\n",
       " 'social_bias_frames',\n",
       " 'social_i_qa',\n",
       " 'sofc_materials_articles',\n",
       " 'sogou_news',\n",
       " 'spanish_billion_words',\n",
       " 'spc',\n",
       " 'species_800',\n",
       " 'spider',\n",
       " 'squad',\n",
       " 'squad_adversarial',\n",
       " 'squad_es',\n",
       " 'squad_it',\n",
       " 'squad_kor_v1',\n",
       " 'squad_kor_v2',\n",
       " 'squad_v1_pt',\n",
       " 'squad_v2',\n",
       " 'squadshifts',\n",
       " 'srwac',\n",
       " 'stereoset',\n",
       " 'stsb_mt_sv',\n",
       " 'style_change_detection',\n",
       " 'super_glue',\n",
       " 'swag',\n",
       " 'swahili',\n",
       " 'swahili_news',\n",
       " 'swda',\n",
       " 'swedish_ner_corpus',\n",
       " 'swedish_reviews',\n",
       " 'tab_fact',\n",
       " 'tamilmixsentiment',\n",
       " 'tanzil',\n",
       " 'tapaco',\n",
       " 'tashkeela',\n",
       " 'taskmaster1',\n",
       " 'taskmaster2',\n",
       " 'taskmaster3',\n",
       " 'tatoeba',\n",
       " 'ted_hrlr',\n",
       " 'ted_iwlst2013',\n",
       " 'ted_multi',\n",
       " 'ted_talks_iwslt',\n",
       " 'telugu_books',\n",
       " 'telugu_news',\n",
       " 'tep_en_fa_para',\n",
       " 'thai_toxicity_tweet',\n",
       " 'thainer',\n",
       " 'thaiqa_squad',\n",
       " 'thaisum',\n",
       " 'tilde_model',\n",
       " 'times_of_india_news_headlines',\n",
       " 'tiny_shakespeare',\n",
       " 'tlc',\n",
       " 'tmu_gfm_dataset',\n",
       " 'totto',\n",
       " 'trec',\n",
       " 'trivia_qa',\n",
       " 'tsac',\n",
       " 'ttc4900',\n",
       " 'tunizi',\n",
       " 'tuple_ie',\n",
       " 'turk',\n",
       " 'turkish_movie_sentiment',\n",
       " 'turkish_ner',\n",
       " 'turkish_product_reviews',\n",
       " 'turkish_shrinked_ner',\n",
       " 'turku_ner_corpus',\n",
       " 'tweet_qa',\n",
       " 'tweets_ar_en_parallel',\n",
       " 'tweets_hate_speech_detection',\n",
       " 'twi_text_c3',\n",
       " 'twi_wordsim353',\n",
       " 'tydiqa',\n",
       " 'ubuntu_dialogs_corpus',\n",
       " 'udhr',\n",
       " 'um005',\n",
       " 'un_ga',\n",
       " 'un_multi',\n",
       " 'un_pc',\n",
       " 'universal_dependencies',\n",
       " 'urdu_fake_news',\n",
       " 'urdu_sentiment_corpus',\n",
       " 'web_nlg',\n",
       " 'web_of_science',\n",
       " 'web_questions',\n",
       " 'weibo_ner',\n",
       " 'wi_locness',\n",
       " 'wiki40b',\n",
       " 'wiki_asp',\n",
       " 'wiki_atomic_edits',\n",
       " 'wiki_auto',\n",
       " 'wiki_bio',\n",
       " 'wiki_dpr',\n",
       " 'wiki_hop',\n",
       " 'wiki_lingua',\n",
       " 'wiki_movies',\n",
       " 'wiki_qa',\n",
       " 'wiki_qa_ar',\n",
       " 'wiki_snippets',\n",
       " 'wiki_source',\n",
       " 'wiki_split',\n",
       " 'wiki_summary',\n",
       " 'wikiann',\n",
       " 'wikicorpus',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wikisql',\n",
       " 'wikitext',\n",
       " 'wikitext_tl39',\n",
       " 'wili_2018',\n",
       " 'wino_bias',\n",
       " 'winograd_wsc',\n",
       " 'winogrande',\n",
       " 'wiqa',\n",
       " 'wisesight1000',\n",
       " 'wisesight_sentiment',\n",
       " 'wmt14',\n",
       " 'wmt15',\n",
       " 'wmt16',\n",
       " 'wmt17',\n",
       " 'wmt18',\n",
       " 'wmt19',\n",
       " 'wmt20_mlqe_task1',\n",
       " 'wmt20_mlqe_task2',\n",
       " 'wmt20_mlqe_task3',\n",
       " 'wmt_t2t',\n",
       " 'wnut_17',\n",
       " 'wongnai_reviews',\n",
       " 'woz_dialogue',\n",
       " 'wrbsc',\n",
       " 'x_stance',\n",
       " 'xcopa',\n",
       " 'xed_en_fi',\n",
       " 'xglue',\n",
       " 'xnli',\n",
       " 'xor_tydi_qa',\n",
       " 'xquad',\n",
       " 'xquad_r',\n",
       " 'xsum',\n",
       " 'xsum_factuality',\n",
       " 'xtreme',\n",
       " 'yahoo_answers_qa',\n",
       " 'yahoo_answers_topics',\n",
       " 'yelp_polarity',\n",
       " 'yelp_review_full',\n",
       " 'yoruba_bbc_topics',\n",
       " 'yoruba_gv_ner',\n",
       " 'yoruba_text_c3',\n",
       " 'yoruba_wordsim353',\n",
       " 'youtube_caption_corrections',\n",
       " 'zest',\n",
       " 'Fraser/mnist-text',\n",
       " 'Fraser/mnist-text-small',\n",
       " 'Fraser/news-category-dataset',\n",
       " 'Fraser/python-lines',\n",
       " 'NbAiLab/norne',\n",
       " 'cdminix/mgb1',\n",
       " 'csv',\n",
       " 'german-nlp-group/german_common_crawl',\n",
       " 'joelito/ler',\n",
       " 'joelito/sem_eval_2010_task_8',\n",
       " 'json',\n",
       " 'k-halid/ar',\n",
       " 'lhoestq/squad',\n",
       " 'mulcyber/europarl-mono',\n",
       " 'pandas',\n",
       " 'patrickvonplaten/librispeech_asr_dummy',\n",
       " 'patrickvonplaten/scientific_papers_dummy',\n",
       " 'piEsposito/br-quad-2.0',\n",
       " 'piEsposito/br_quad_20',\n",
       " 'piEsposito/squad_20_ptbr',\n",
       " 'sshleifer/pseudo_bart_xsum',\n",
       " 'text']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, list_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEZ20fa9DP4W"
   },
   "source": [
    "Next, we download the pubmed train and validation dataset ([click to see on ðŸ¤—Datasets Hub](https://huggingface.co/datasets/scientific_papers)). This can take a couple of minutes **â˜•** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6GRz0rksYb3h",
    "outputId": "599d3c3f-5cde-47f7-f176-21a59d16b94b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scientific_papers (/root/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/043e40ed208b8a66ee9e8228c86874946c99d2fc6155a1daee685795851cfdfc)\n"
     ]
    }
   ],
   "source": [
    "val_dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWM_4AChFnpz"
   },
   "source": [
    "It's always a good idea to take a look at some data samples. Let's do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaz72rzWGBAK"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=4):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrwXZ_LBGLwt"
   },
   "source": [
    "We can see that the input data is the `article` - a scientific report and the target data is the `abstract` - a concise summary of the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkDoz13UB3gY"
   },
   "source": [
    "Cool! Having downloaded the dataset, let's tokenize it.\n",
    "We'll import the convenient `AutoTokenizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-gvC8jiYtS_"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gst7xqXECbRc"
   },
   "source": [
    " and load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpUr9QeebZ-n"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhQeQg3oCcL-"
   },
   "source": [
    "Note that for the sake of this notebook, we finetune the \"smaller\" LED checkpoint [\"allenai/led-base-16384\"](https://huggingface.co/allenai/led-base-16384). Better performance can however be attained by finetuning [\"allenai/led-large-16384\"](https://huggingface.co/allenai/led-large-16384) at the cost of a higher required GPU RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-GnxlXnDK7g"
   },
   "source": [
    "Pubmed's input data has a median token length of 2715 with the 90%-ile token length being 6101. The output data has a media token length of 171 with the 90%-ile token length being 352.${}^1$. \n",
    "\n",
    "Thus, we set the maximum input length to 8192 and the maximum output length to 512 to ensure that the model can attend to almost all input tokens is able to generate up to a large enough number of output tokens.\n",
    "\n",
    "In this notebook, we are only able to train on `batch_size=2` to prevent out-of-memory errors.\n",
    "\n",
    "---\n",
    "${}^1$ The data is taken from page 11 of [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nbb24Uh-Y4xO"
   },
   "outputs": [],
   "source": [
    "max_input_length = 8192\n",
    "max_output_length = 512\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN_SAv1JE40f"
   },
   "source": [
    "Now, let's write down the input data processing function that will be used to map each data sample to the correct model format.\n",
    "As explained earlier `article` represents here our input data and `abstract` is the target data. The datasamples are thus tokenized up to the respective maximum lengths of 8192 and 512.\n",
    "\n",
    "In addition to the usual `attention_mask`, LED can make use of an additional `global_attention_mask` defining which input tokens are attended globally and which are attended only locally, just as it's the case of [Longformer](https://huggingface.co/transformers/model_doc/longformer.html). For more information on Longformer's self-attention, please take a look at the corresponding [docs](https://huggingface.co/transformers/model_doc/longformer.html#longformer-self-attention). For summarization, we follow recommendations of the [paper](https://arxiv.org/abs/2004.05150) and use global attention only for the very first token. Finally, we make sure that no loss is computed on padded tokens by setting their index to `-100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEcAaZhNY8ge"
   },
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    inputs = tokenizer(\n",
    "        batch[\"article\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        batch[\"abstract\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_output_length,\n",
    "    )\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    # create 0 global_attention_mask lists\n",
    "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
    "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
    "    ]\n",
    "\n",
    "    # since above lists are references, the following line changes the 0 index for all samples\n",
    "    batch[\"global_attention_mask\"][0][0] = 1\n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "        for labels in batch[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyseRmC5IcXj"
   },
   "source": [
    "For the sake of this notebook, we will reduce the training and validation data \n",
    "to a dummy dataset of sizes 250 and 25 respectively. For a full training run, those lines should be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMAkJfjCZKeE"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.select(range(250))\n",
    "val_dataset = val_dataset.select(range(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNzZZACtIoCD"
   },
   "source": [
    "Great, having defined the mapping function, let's preprocess the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RClMjZOZCPO",
    "outputId": "c0f16f83-f047-474c-c58d-aa352d2e606c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/043e40ed208b8a66ee9e8228c86874946c99d2fc6155a1daee685795851cfdfc/cache-c2e64b0d21e5b15c.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUtRFmgVIuHG"
   },
   "source": [
    "and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gy_9DCocZD5G",
    "outputId": "49c9e803-9973-4295-a5ba-23e97a81c7ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/043e40ed208b8a66ee9e8228c86874946c99d2fc6155a1daee685795851cfdfc/cache-5a8b56be791ed762.arrow\n"
     ]
    }
   ],
   "source": [
    "val_dataset = val_dataset.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evRRitZaIw9N"
   },
   "source": [
    "Finally, the datasets should be converted into the PyTorch format as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ci2QYHCMZiNO"
   },
   "outputs": [],
   "source": [
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    ")\n",
    "val_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI-QHywgI6Hb"
   },
   "source": [
    "Alright, we're almost ready to start training. Let's load the model via the `AutoModelForSeq2SeqLM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZf_8QXJacIc"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNJQlLDKJHIK"
   },
   "source": [
    "We've decided to stick to the smaller model `\"allenai/led-base-16384\"` for the sake of this notebook. In addition, we directly enable gradient checkpointing and disable the caching mechanism to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-UfEo0Zadpl"
   },
   "outputs": [],
   "source": [
    "led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQOaX6eRJXkM"
   },
   "source": [
    "During training, we want to evaluate the model on Rouge, the most common metric used in summarization, to make sure the model is indeed improving during training. For this, we set fitting generation parameters. We'll use beam search with a small beam of just 2 to save memory. Also, we force the model to generate at least 100 tokens, but no more than 512. In addition, some other generation parameters are set that have been found helpful for generation. For more information on those parameters, please take a look at the [docs](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPnNi_tWaklV"
   },
   "outputs": [],
   "source": [
    "# set generate hyperparameters\n",
    "led.config.num_beams = 2\n",
    "led.config.max_length = 512\n",
    "led.config.min_length = 100\n",
    "led.config.length_penalty = 2.0\n",
    "led.config.early_stopping = True\n",
    "led.config.no_repeat_ngram_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7FTiwHrKSBA"
   },
   "source": [
    "Next, we also have to define the function the will compute the `\"rouge\"` score during evalution.\n",
    "\n",
    "Let's load the `\"rouge\"` metric from ðŸ¤—datasets and define the `compute_metrics(...)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5_rc9wUaBop"
   },
   "outputs": [],
   "source": [
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsytOIWVKqFz"
   },
   "source": [
    "The compute metrics function expects the generation output, called `pred.predictions` as well as the gold label, called `pred.label_ids`.\n",
    "\n",
    "Those tokens are decoded and consequently, the rouge score can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ahmf7mcZzU7"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(\n",
    "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
    "    )[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrX2gBnYLAna"
   },
   "source": [
    "Now, we're ready to start training. Let's import the `Seq2SeqTrainer` and `Seq2SeqTrainingArguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gq7CajIWaUo5"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ga-fCOB4LI4W"
   },
   "source": [
    "In contrast to the usual `Trainer`, the `Seq2SeqTrainer` makes it possible to use the `generate()` function during evaluation. This should be enabled with `predict_with_generate=True`. Because our GPU RAM is limited, we make use of gradient accumulation by setting `gradient_accumulation_steps=4` to have an effective `batch_size` of 2 * 4 = 8.\n",
    "\n",
    "Other training arguments can be read upon in the [docs](https://huggingface.co/transformers/main_classes/trainer.html?highlight=trainingarguments#transformers.TrainingArguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMehwI4QZqB_"
   },
   "outputs": [],
   "source": [
    "# enable fp16 apex training\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,\n",
    "    output_dir=\"./\",\n",
    "    logging_steps=5,\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZCY-C5mLzXY"
   },
   "source": [
    "The training arguments, along with the model, tokenizer, datasets and the `compute_metrics` function can then be passed to the `Seq2SeqTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WPyTYO_JfHW"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=led,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZxSe_afL9TH"
   },
   "source": [
    "and we can start training. This will take about ~35min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "id": "g4zkCpeQa2NN",
    "outputId": "3f5fd141-1d4d-42d9-9d73-f171b4005f3f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py:851: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='11' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/31 04:20 < 09:39, 0.03 it/s, Epoch 0.32/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='4' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/13 07:42 < 23:07, 0.01 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gs1gYYb1MJOr"
   },
   "source": [
    "This completes the fine-tuning tutorial for LED. This training script with some small changes was used to train [this](https://huggingface.co/patrickvonplaten/led-large-16384-pubmed) checkpoint, called `\" patrickvonplaten/led-large-16384-pubmed\"` on a single GPU for ca. 3 days. Evaluating `\" patrickvonplaten/led-large-16384-pubmed\"` on Pubmed's test data gives a Rouge-2 score of **19.33** which is around 1 Rouge-2 point below SOTA performance on Pubmed.\n",
    "\n",
    "In the Appendix below, the condensed training and evaluation scripts that were used locally to finetune `\" patrickvonplaten/led-large-16384-pubmed\"` are attached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_t9sjHha5T8"
   },
   "source": [
    "# **Appendix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uv7edHocbDCD"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U3vpJxnOJiH"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")\n",
    "\n",
    "# load rouge\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "# load pubmed\n",
    "pubmed_train = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"train\")\n",
    "pubmed_val = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"validation[:10%]\")\n",
    "\n",
    "# comment out following lines for a test run\n",
    "# pubmed_train = pubmed_train.select(range(32))\n",
    "# pubmed_val = pubmed_val.select(range(32))\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-large-16384\")\n",
    "\n",
    "\n",
    "# max encoder length is 8192 for PubMed\n",
    "encoder_max_length = 8192\n",
    "decoder_max_length = 512\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    inputs = tokenizer(\n",
    "        batch[\"article\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        batch[\"abstract\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=decoder_max_length,\n",
    "    )\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    # create 0 global_attention_mask lists\n",
    "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
    "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
    "    ]\n",
    "\n",
    "    # since above lists are references, the following line changes the 0 index for all samples\n",
    "    batch[\"global_attention_mask\"][0][0] = 1\n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "        for labels in batch[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "# map train data\n",
    "pubmed_train = pubmed_train.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",
    ")\n",
    "\n",
    "# map val data\n",
    "pubmed_val = pubmed_val.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",
    ")\n",
    "\n",
    "# set Python list to PyTorch tensor\n",
    "pubmed_train.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "# set Python list to PyTorch tensor\n",
    "pubmed_val.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "# enable fp16 apex training\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,\n",
    "    fp16_backend=\"apex\",\n",
    "    output_dir=\"./\",\n",
    "    logging_steps=250,\n",
    "    eval_steps=5000,\n",
    "    save_steps=500,\n",
    "    warmup_steps=1500,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=4,\n",
    ")\n",
    "\n",
    "\n",
    "# compute Rouge score during validation\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(\n",
    "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
    "    )[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }\n",
    "\n",
    "\n",
    "# load model + enable gradient checkpointing & disable cache for checkpointing\n",
    "led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-large-16384\", gradient_checkpointing=True, use_cache=False)\n",
    "\n",
    "# set generate hyperparameters\n",
    "led.config.num_beams = 4\n",
    "led.config.max_length = 512\n",
    "led.config.min_length = 100\n",
    "led.config.length_penalty = 2.0\n",
    "led.config.early_stopping = True\n",
    "led.config.no_repeat_ngram_size = 3\n",
    "\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=led,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=pubmed_train,\n",
    "    eval_dataset=pubmed_val,\n",
    ")\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFtGA4yibG2u"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLM3niQqhEzP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "\n",
    "# load pubmed\n",
    "pubmed_test = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"test\")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = LEDTokenizer.from_pretrained(\"patrickvonplaten/led-large-16384-pubmed\")\n",
    "model = LEDForConditionalGeneration.from_pretrained(\"patrickvonplaten/led-large-16384-pubmed\").to(\"cuda\").half()\n",
    "\n",
    "\n",
    "def generate_answer(batch):\n",
    "  inputs_dict = tokenizer(batch[\"article\"], padding=\"max_length\", max_length=8192, return_tensors=\"pt\", truncation=True)\n",
    "  input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
    "  attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n",
    "  global_attention_mask = torch.zeros_like(attention_mask)\n",
    "  # put global attention on <s> token\n",
    "  global_attention_mask[:, 0] = 1\n",
    "\n",
    "  predicted_abstract_ids = model.generate(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n",
    "  batch[\"predicted_abstract\"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "  return batch\n",
    "\n",
    "\n",
    "result = pubmed_test.map(generate_answer, batched=True, batch_size=4)\n",
    "\n",
    "# load rouge\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "print(\"Result:\", rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge2\"])[\"rouge2\"].mid)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Fine-tune Longformer Encoder-Decoder (LED) for Summarization on pubmed",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
