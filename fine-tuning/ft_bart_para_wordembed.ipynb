{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 30194,
     "status": "ok",
     "timestamp": 1610616621856,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "au5Z9XQAC7C-"
   },
   "outputs": [],
   "source": [
    "magma_dir = '/home/ubuntu/magma/'\n",
    "bucket_dir = '/home/ubuntu/s3/'\n",
    "transformers_dir = '/home/ubuntu/transformers/'\n",
    "cache_dir = bucket_dir+'.cache/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EddY1WDNsKlS"
   },
   "source": [
    "## **Fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 85455,
     "status": "ok",
     "timestamp": 1610616677129,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "2d6M41X9AKBi"
   },
   "outputs": [],
   "source": [
    "finetune_script = '\"'+transformers_dir+'examples/seq2seq/finetune_trainer.py\"'\n",
    "eval_script = '\"'+transformers_dir+'examples/seq2seq/run_eval.py\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FByNNOIRvG"
   },
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "executionInfo": {
     "elapsed": 113930,
     "status": "ok",
     "timestamp": 1610616705611,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "82WSp6khIcua",
    "outputId": "54de6794-ef3c-41f4-b0e3-e108cfc4e333",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=ft_bart_para_wordembed_testw2v\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, magma_dir)\n",
    "import config\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "project_name = 'ft_bart_para_wordembed_testw2v'\n",
    "%env WANDB_PROJECT=$project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPZ7A-sBVOam"
   },
   "source": [
    "### Karger Books Para Wordembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 113928,
     "status": "ok",
     "timestamp": 1610616705613,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "vJOYl_g6F1e2"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = 'sshleifer/distilbart-cnn-12-6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 113926,
     "status": "ok",
     "timestamp": 1610616705616,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "4xs_XkbCVOar"
   },
   "outputs": [],
   "source": [
    "data_dir = '\"'+bucket_dir+'datasets/karger_books_para_wordembed/bart/st/\"'\n",
    "\n",
    "output_dir = '\"'+bucket_dir+'fine-tuning/'+project_name+'\"'\n",
    "\n",
    "log_dir = output_dir + '/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "model_config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "model_config.min_length = config.ONE_BULLET_MIN_LEN\n",
    "model_config.max_length = config.ONE_BULLET_MAX_LEN\n",
    "model_config.num_beams = 2\n",
    "\n",
    "model_config.task_specific_params['summarization']['min_length'] = config.ONE_BULLET_MIN_LEN\n",
    "model_config.task_specific_params['summarization']['max_length'] = config.ONE_BULLET_MAX_LEN\n",
    "model_config.task_specific_params['summarization']['num_beams'] = 2\n",
    "model_config_dir = '\"'+bucket_dir+'fine-tuning/'+\\\n",
    "    model_name_or_path.replace('/', '?')+'_config\"'\n",
    "model_config.save_pretrained(model_config_dir[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M58yiP1yVOav"
   },
   "source": [
    "##### Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_name_or_path = output_dir[:-1] + \\'checkpoint-325\"\\'\\nprint(model_name_or_path)\\n\\nfrom transformers import AutoTokenizer\\ntok = AutoTokenizer.from_pretrained(\\'sshleifer/distilbart-cnn-12-6\\', use_cache=False)\\ntok.save_pretrained(model_name_or_path[1:-1])\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model_name_or_path = output_dir[:-1] + 'checkpoint-325\"'\n",
    "print(model_name_or_path)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6', use_cache=False)\n",
    "tok.save_pretrained(model_name_or_path[1:-1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2522396,
     "status": "ok",
     "timestamp": 1610555308072,
     "user": {
      "displayName": "Marco Pietro Abrate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64",
      "userId": "15422244832836998434"
     },
     "user_tz": -60
    },
    "id": "K66kY1WOVOaw",
    "outputId": "11421fe6-93e5-461f-8b97-0615b4c2e7af",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/15/2021 12:45:54 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "02/15/2021 12:45:54 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/home/marco/epfl/magma/fine-tuning/ft_bart_para_wordembed_testw2v', overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=20.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/Feb15_12-45-54_calcolatore', logging_first_step=True, logging_steps=10, save_steps=98, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1, dataloader_num_workers=0, past_index=-1, run_name='/home/marco/epfl/magma/fine-tuning/ft_bart_para_wordembed_testw2v', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.1, adafactor=False, group_by_length=False, report_to=['tensorboard', 'wandb'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, label_smoothing=0.0, sortish_sampler=True, predict_with_generate=True, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear')\n",
      "[INFO|configuration_utils.py:447] 2021-02-15 12:45:54,749 >> loading configuration file /home/marco/epfl/magma/fine-tuning/sshleifer?distilbart-cnn-12-6_config/config.json\n",
      "[INFO|configuration_utils.py:485] 2021-02-15 12:45:54,750 >> Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 150,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 10,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 2,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 150,\n",
      "      \"min_length\": 10,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 2\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.3.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:449] 2021-02-15 12:45:55,136 >> loading configuration file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/config.json from cache at /home/marco/.cache/huggingface/transformers/adac95cf641be69365b3dd7fe00d4114b3c7c77fb0572931db31a92d4995053b.9307b6cec4435559ec6e79d5a210a334b17706465329e138f335649d14f27e78\n",
      "[INFO|configuration_utils.py:485] 2021-02-15 12:45:55,140 >> Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.3.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1688] 2021-02-15 12:45:55,140 >> Model name 'sshleifer/distilbart-cnn-12-6' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'sshleifer/distilbart-cnn-12-6' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-15 12:45:57,396 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/vocab.json from cache at /home/marco/.cache/huggingface/transformers/9951e68693b9a7c583ae677e9cb53c02715d9bd0311a78706401372653cdea0a.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-15 12:45:57,397 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/merges.txt from cache at /home/marco/.cache/huggingface/transformers/7588c8d398d659b230a038240cc023f67b6848117d2999f06ab625af7bfc7ec1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-15 12:45:57,397 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-15 12:45:57,397 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-15 12:45:57,397 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-15 12:45:57,397 >> loading file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/tokenizer_config.json from cache at /home/marco/.cache/huggingface/transformers/f5316f64f9716436994a7ad76a354dc20ecb2dd74eb61d278f103a9c8b80291f.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n",
      "[INFO|modeling_utils.py:1027] 2021-02-15 12:45:57,956 >> loading weights file https://huggingface.co/sshleifer/distilbart-cnn-12-6/resolve/main/pytorch_model.bin from cache at /home/marco/.cache/huggingface/transformers/b336fa0b874ea92e3e22f07a7e6f8fa9da01221759c33abeb2679d6d98fe7755.585965cf7e82e4536033cd21d76c486af3d6b1c2a34b3a847840d4e7fe9d8844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1143] 2021-02-15 12:46:14,747 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-15 12:46:14,748 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "02/15/2021 12:46:14 - INFO - utils -   setting model.config to task specific params for summarization:\n",
      " {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 150, 'min_length': 10, 'no_repeat_ngram_size': 3, 'num_beams': 2}\n",
      "02/15/2021 12:46:14 - INFO - utils -   note: command line args may override some of these\n",
      "02/15/2021 12:46:15 - INFO - __main__ -   *** Train ***\n",
      "/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/trainer.py:705: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:837] 2021-02-15 12:46:15,109 >> ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-15 12:46:15,109 >>   Num examples = 2046\n",
      "[INFO|trainer.py:839] 2021-02-15 12:46:15,109 >>   Num Epochs = 20\n",
      "[INFO|trainer.py:840] 2021-02-15 12:46:15,109 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:841] 2021-02-15 12:46:15,109 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:842] 2021-02-15 12:46:15,109 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:843] 2021-02-15 12:46:15,109 >>   Total optimization steps = 40920\n",
      "[INFO|integrations.py:546] 2021-02-15 12:46:15,131 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.19 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/marco/epfl/magma/fine-tuning/ft_bart_para_wordembed_testw2v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/ft_bart_para_wordembed_testw2v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/marcoabrate/ft_bart_para_wordembed_testw2v/runs/3r3l3lx6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/marco/epfl/magma/fine-tuning/wandb/run-20210215_124615-3r3l3lx6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "  0%|          | 1/40920 [00:04<47:17:37,  4.16s/it]{'loss': 2.0321, 'learning_rate': 4.9998778103616815e-05, 'epoch': 0.0}\n",
      "[INFO|trainer.py:1600] 2021-02-15 12:46:21,365 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1601] 2021-02-15 12:46:21,366 >>   Num examples = 20\n",
      "[INFO|trainer.py:1602] 2021-02-15 12:46:21,367 >>   Batch size = 2\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:14<00:57,  7.16s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:18<00:44,  6.33s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:23<00:35,  5.86s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [00:28<00:27,  5.50s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [00:34<00:22,  5.68s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:41<00:18,  6.14s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:47<00:11,  5.99s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:53<00:06,  6.00s/it]\u001b[A\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.60it/s]\n",
      "[['immuno', 'oncology', 'is', 'new', 'multi', 'faceted', 'and', 'rapidly', 'evolving', 'collection', 'of', 'treatment', 'strategies', 'aimed', 'at', 'harnessing', 'immune', 'processes', 'to', 'target', 'and', 'destroy', 'tumor', 'cells'], ['adaptive', 'acquired', 'immunity', 'is', 'conferred', 'by', 'the', 'formation', 'of', 'antibodies', 'following', 'exposure', 'to', 'foreign', 'material', 'antigens', 'in', 'general', 'innate', 'immune', 'reactions', 'provide', 'rapid', 'within', 'hours', 'but', 'non', 'specific', 'initial', 'response', 'to', 'infection', 'or', 'injury'], ['cellular', 'or', 'cell', 'mediated', 'immunity', 'is', 'mediated', 'by', 'cells', 'originating', 'in', 'the', 'thymus', 'humoral', 'immunity', 'is', 'mediated', 'by', 'cells', 'derived', 'from', 'bone', 'marrow'], ['surface', 'cell', 'receptors', 'tcrs', 'are', 'specific', 'to', 'individual', 'antigens', 'and', 'are', 'activated', 'when', 'the', 'tcr', 'binds', 'to', 'the', 'appropriate', 'antigen', 'there', 'are', 'two', 'types', 'of', 'cd', 'cell', 'responses', 'th', 'responses', 'directed', 'against', 'intracellular', 'pathogens', 'th', 'responses', 'are', 'effective', 'against', 'extracellular', 'bacteria', 'and', 'parasites', 'reg', 'cells', 'regulate', 'the', 'immune', 'response', 'to', 'common', 'environmental', 'allergens', 'and', 'prevent', 'the', 'development', 'of', 'atopy', 'or', 'undesirable', 'inflammation'], ['humoral', 'immunity', 'involves', 'the', 'production', 'of', 'antibodies', 'immunoglobulins', 'against', 'specific', 'antigens'], ['central', 'tolerance', 'is', 'the', 'principal', 'mechanism', 'by', 'which', 'the', 'immune', 'system', 'learns', 'to', 'distinguish', 'between', 'self', 'and', 'non', 'self'], ['the', 'fraction', 'sizes', 'are', 'typically', 'greater', 'than', 'gy', 'per', 'fraction', 'it', 'is', 'used', 'for', 'the', 'treatment', 'of', 'stage', 'disease', 'and', 'more', 'recently', 'for', 'oligometastatic', 'disease'], ['best', 'outcomes', 'are', 'seen', 'when', 'conventional', 'radical', 'radiotherapy', 'is', 'used', 'concurrently', 'with', 'chemotherapy'], ['best', 'outcomes', 'are', 'seen', 'when', 'conventional', 'radical', 'radiotherapy', 'is', 'used', 'concurrently', 'with', 'chemotherapy'], ['best', 'outcomes', 'are', 'seen', 'when', 'conventional', 'radical', 'radiotherapy', 'is', 'used', 'concurrently', 'with', 'chemotherapy'], ['gy', 'in', 'fractions', 'is', 'equivalent', 'to', 'longer', 'schedules', 'exempli', 'gratia', 'gy', 'in', 'or', 'gy', 'in', 'fractions', 'with', 'no', 'difference', 'in', 'survival'], ['there', 'are', 'almost', 'certainly', 'several', 'mechanisms', 'underlying', 'smoking', 'addiction', 'all', 'involving', 'nicotine', 'in', 'some', 'way', 'one', 'likely', 'mechanism', 'involves', 'the', 'development', 'of', 'powerful', 'habit', 'in', 'which', 'cues', 'associated', 'with', 'smoking', 'trigger', 'an', 'urge', 'to', 'smoke'], ['third', 'mechanism', 'involves', 'learning', 'that', 'smoking', 'cigarette', 'helps', 'to', 'alleviate', 'feelings', 'of', 'anxiety', 'depression', 'irritability', 'restlessness', 'and', 'difficulty', 'concentrating', 'these', 'are', 'all', 'withdrawal', 'symptoms', 'resulting', 'from', 'physiological', 'adaptation', 'to', 'repeated', 'nicotine', 'intake'], ['third', 'mechanism', 'involves', 'learning', 'that', 'smoking', 'cigarette', 'helps', 'to', 'alleviate', 'feelings', 'of', 'anxiety', 'depression', 'irritability', 'restlessness', 'and', 'difficulty', 'concentrating', 'these', 'are', 'all', 'withdrawal', 'symptoms', 'resulting', 'from', 'physiological', 'adaptation', 'to', 'repeated', 'nicotine', 'intake'], ['of', 'efforts', 'fail', 'within', 'week', 'and', 'relapses', 'within', 'months', 'these', 'outcomes', 'parallel', 'those', 'seen', 'in', 'heroin', 'addicts', 'or', 'alcoholics', 'who', 'try', 'to', 'achieve', 'abstinence'], ['there', 'is', 'no', 'in', 'life', 'marker', 'for', 'idiopathic', 'parkinson', 'disease', 'diagnosis', 'can', 'only', 'be', 'made', 'with', 'certainty', 'if', 'lewy', 'bodies', 'are', 'found', 'in', 'the', 'substantia', 'nigra', 'and', 'other', 'brain', 'regions', 'after', 'death'], ['the', 'disease', 'is', 'estimated', 'to', 'affect', 'of', 'year', 'olds', 'but', 'is', 'also', 'seen', 'in', 'younger', 'people', 'with', 'of', 'cases', 'occurring', 'before', 'the', 'age', 'of'], ['the', 'disease', 'is', 'estimated', 'to', 'affect', 'of', 'year', 'olds', 'but', 'is', 'also', 'seen', 'in', 'younger', 'people', 'men', 'are', 'times', 'more', 'likely', 'to', 'develop', 'the', 'condition', 'than', 'women'], ['braak', 'et', 'al', 'suggested', 'that', 'stage', 'of', 'the', 'disease', 'begins', 'at', 'induction', 'sites', 'in', 'the', 'olfactory', 'system', 'and', 'the', 'dorsal', 'vagal', 'nucleus', 'stage', 'reflects', 'progression', 'of', 'the', 'pathological', 'process', 'to', 'the', 'nuclei', 'of', 'the', 'caudal', 'brainstem', 'the', 'locus', 'ceruleus', 'and', 'other', 'nuclei'], ['the', 'susceptible', 'neurons', 'are', 'located', 'in', 'astroglial', 'poor', 'regions', 'such', 'as', 'the', 'ventral', 'tier', 'glia', 'may', 'offer', 'neuroprotection', 'by', 'providing', 'neurotrophic', 'factors', 'that', 'prevent', 'cell', 'death']]\n",
      "[['the', 'immune', 'system', 'has', 'two', 'components', 'innate', 'immunity', 'involving', 'mechanisms', 'present', 'throughout', 'life', 'and', 'adaptive', 'acquired', 'immunity', 'which', 'is', 'conferred', 'by', 'immune', 'responses', 'following', 'exposure', 'to', 'an', 'antigen', 'and', 'is', 'specific', 'to', 'that', 'antigen'], ['innate', 'immunity', 'is', 'primarily', 'conferred', 'by', 'phagocytic', 'cells', 'derived', 'from', 'stem', 'cells', 'in', 'the', 'bone', 'marrow', 'principally', 'macrophages', 'monocytes', 'and', 'neutrophils'], ['there', 'are', 'two', 'forms', 'of', 'adaptive', 'immunity', 'cellular', 'immunity', 'mediated', 'by', 'cells', 'originating', 'in', 'the', 'thymus', 'and', 'humoral', 'immunity', 'mediated', 'by', 'cells', 'originating', 'in', 'the', 'bone', 'marrow'], ['in', 'cellular', 'immunity', 'cells', 'recognize', 'their', 'target', 'antigens', 'as', 'protein', 'sequences', 'presented', 'on', 'the', 'surface', 'of', 'antigen', 'presenting', 'cells', 'apcs', 'in', 'association', 'with', 'major', 'complex', 'mhc', 'molecules', 'activation', 'of', 'cd', 'cells', 'leads', 'to', 'cytokine', 'release', 'that', 'affects', 'multiple', 'immune', 'cells', 'including', 'apcs', 'activation', 'of', 'cd', 'cytotoxic', 'cells', 'triggers', 'clonal', 'selection', 'during', 'which', 'the', 'cells', 'proliferate', 'to', 'produce', 'population', 'of', 'effector', 'cells', 'which', 'release', 'enzymes', 'and', 'toxins', 'that', 'lyse', 'the', 'membrane', 'of', 'antigen', 'bearing', 'cells', 'and', 'induce', 'programmed', 'cell', 'death', 'apoptosis'], ['humoral', 'immunity', 'involves', 'the', 'production', 'by', 'cells', 'of', 'antibodies', 'against', 'specific', 'antigens'], ['immune', 'tolerance', 'is', 'state', 'in', 'which', 'the', 'immune', 'system', 'is', 'unresponsive', 'to', 'stimulus', 'that', 'would', 'normally', 'provoke', 'an', 'immune', 'response', 'this', 'may', 'be', 'central', 'or', 'peripheral', 'depending', 'on', 'where', 'tolerance', 'develops', 'immune', 'tolerance', 'is', 'an', 'important', 'mechanism', 'by', 'which', 'tumor', 'cells', 'evade', 'the', 'immune', 'system'], ['stereotactic', 'body', 'radiotherapy', 'is', 'an', 'alternative', 'to', 'surgery', 'in', 'peripheral', 'stage', 'ii', 'tumors', 'less', 'than', 'cm'], ['surgery', 'is', 'the', 'preferred', 'treatment', 'for', 'all', 'patients', 'with', 'early', 'stage', 'ii', 'nsclc', 'with', 'radiotherapy', 'reserved', 'for', 'those', 'unsuitable', 'for', 'surgery'], ['concurrent', 'is', 'preferred', 'to', 'sequential', 'in', 'stage', 'iib', 'iiib', 'disease'], ['postoperative', 'radiotherapy', 'is', 'recommended', 'when', 'residual', 'disease', 'is', 'present', 'it', 'can', 'also', 'be', 'discussed', 'in', 'patients', 'with', 'complete', 'resections', 'with', 'pn', 'disease'], ['patients', 'with', 'advanced', 'disease', 'and', 'poor', 'performance', 'status', 'should', 'receive', 'gy', 'in', 'fraction', 'for', 'palliation', '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of', 'intrathoracic', 'disease'], ['cigarette', 'addiction', 'involves', 'powerful', 'motivations', 'to', 'smoke', 'that', 'undermine', 'and', 'overwhelm', 'the', 'desire', 'to', 'avoid', 'smoking', 'because', 'of', 'its', 'social', 'financial', 'and', 'health', 'costs'], ['cigarette', 'addiction', 'stems', 'primarily', 'from', 'nicotine', 'dependence', 'repeated', 'rapid', 'intake', 'of', 'nicotine', 'from', 'cigarettes', 'sets', 'up', 'powerful', 'association', 'between', 'smoking', 'and', 'situations', 'in', 'which', 'smoking', 'typically', 'occurs', 'it', 'also', 'creates', 'nicotine', 'hunger', 'so', 'that', 'when', 'brain', 'nicotine', 'levels', 'fall', 'the', 'smoker', 'experiences', 'need', 'to', 'smoke', 'it', 'creates', 'unpleasant', 'withdrawal', 'symptoms', 'including', 'mood', 'disturbance', 'because', 'of', 'physiological', 'adaptation'], ['the', 'withdrawal', 'symptoms', 'are', 'relieved', 'by', 'smoking', 'thus', 'generating', 'the', 'feeling', 'of', 'need', 'to', 'smoke', 'whenever', 'these', 'symptoms', 'are', 'experienced', 'even', 'if', 'they', 'are', 'caused', 'by', 'something', 'else'], ['addiction', 'to', 'cigarettes', 'is', 'demonstrated', 'by', 'the', 'fact', 'that', 'fewer', 'than', 'of', 'serious', 'attempts', 'to', 'stop', 'smoking', 'succeed', 'without', 'behavioral', 'support', 'or', 'pharmacological', 'treatment'], ['parkinson', 'disease', 'is', 'one', 'of', 'the', 'most', 'common', 'diseases', 'with', 'prevalence', 'of', 'approximately', 'per', 'individuals'], ['the', 'incidence', 'and', 'prevalence', 'of', 'parkinson', 'disease', 'increases', 'sharply', 'with', 'age'], ['men', 'are', 'times', 'more', 'likely', 'than', 'women', 'to', 'develop', 'the', 'disease'], ['braak', 'has', 'suggested', 'that', 'the', 'condition', 'may', 'begin', 'in', 'the', 'olfactory', 'bundle', 'and', 'lower', 'brainstem', 'and', 'studies', 'are', 'under', 'way', 'to', 'identify', 'specific', 'non', 'motor', 'biomarkers', 'of', 'the', 'prodromal', 'phase', 'of', 'parkinson', 'disease'], ['the', 'cause', 'of', 'neuronal', 'degeneration', 'is', 'uncertain']]\n",
      "\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.word2vec -   collecting all words and their counts\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.word2vec -   PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.word2vec -   collected 440 word types from a corpus of 1048 raw words and 40 sentences\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.word2vec -   Loading a fresh vocabulary\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.word2vec -   effective_min_count=1 retains 440 unique words (100% of original 440, drops 0)\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.word2vec -   effective_min_count=1 leaves 1048 word corpus (100% of original 1048, drops 0)\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.word2vec -   deleting the raw counts dictionary of 440 items\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.word2vec -   sample=0.001 downsamples 85 most-common words\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.word2vec -   downsampling leaves estimated 737 word corpus (70.4% of prior 1048)\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   estimated required memory for 440 words and 100 dimensions: 572000 bytes\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.word2vec -   resetting layer weights\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   training model with 3 workers on 440 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   EPOCH - 1 : training on 1048 raw words (728 effective words) took 0.0s, 67023 effective words/s\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   EPOCH - 2 : training on 1048 raw words (731 effective words) took 0.0s, 149145 effective words/s\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   EPOCH - 3 : training on 1048 raw words (734 effective words) took 0.0s, 153137 effective words/s\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   EPOCH - 4 : training on 1048 raw words (738 effective words) took 0.0s, 137995 effective words/s\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 2 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 1 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   worker thread finished; awaiting finish of 0 more threads\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   EPOCH - 5 : training on 1048 raw words (745 effective words) took 0.0s, 136476 effective words/s\n",
      "02/15/2021 12:47:32 - INFO - gensim.models.base_any2vec -   training on a 5240 raw words (3676 effective words) took 0.0s, 81996 effective words/s\n",
      "02/15/2021 12:47:32 - WARNING - gensim.models.base_any2vec -   under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "                                                    sion': 31.2, 'eval_rouge1_recall': 33.96, 'eval_rouge1_fmeasure': 30.409999999999997, 'eval_rouge2_precision': 11.99, 'eval_rouge2_recall': 13.15, 'eval_rouge2_fmeasure': 11.700000000000001, 'eval_rougeL_precision': 24.46, 'eval_rougeL_recall': 26.650000000000002, 'eval_rougeL_fmeasure': 23.74, 'eval_rougeLsum_precision': 25.53, 'eval_rougeLsum_recall': 27.77, 'eval_rougeLsum_fmeasure': 24.740000000000002, 'eval_gen_len': 39.8, 'eval_sentence_distilroberta_cosine': 59.43015813827515, 'eval_w2v_cosine': 86.66195869445801, 'eval_runtime': 70.9614, 'eval_samples_per_second': 0.282, 'epoch': 0.0}\n",
      "  0%|          | 1/40920 [01:15<47:17:37,  4.16s/it]\n",
      "100%|██████████| 10/10 [01:03<00:00,  6.51s/it]\u001b[A\n",
      "                                               \u001b[A\n",
      "  0%|          | 2/40920 [01:18<287:22:57, 25.28s/it][INFO|trainer.py:1600] 2021-02-15 12:47:35,761 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1601] 2021-02-15 12:47:35,761 >>   Num examples = 20\n",
      "[INFO|trainer.py:1602] 2021-02-15 12:47:35,761 >>   Batch size = 2\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marco/epfl/transformers/examples/seq2seq/finetune_trainer.py\", line 450, in <module>\n",
      "    main()\n",
      "  File \"/home/marco/epfl/transformers/examples/seq2seq/finetune_trainer.py\", line 388, in main\n",
      "    model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
      "  File \"/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/trainer.py\", line 983, in train\n",
      "    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch)\n",
      "  File \"/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/trainer.py\", line 1058, in _maybe_log_save_evaluate\n",
      "    metrics = self.evaluate()\n",
      "  File \"/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/trainer.py\", line 1513, in evaluate\n",
      "    metric_key_prefix=metric_key_prefix,\n",
      "  File \"/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/trainer.py\", line 1630, in prediction_loop\n",
      "    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
      "  File \"/home/marco/epfl/transformers/examples/seq2seq/seq2seq_trainer.py\", line 232, in prediction_step\n",
      "    loss, logits = self._compute_loss(model, inputs, labels)\n",
      "  File \"/home/marco/epfl/transformers/examples/seq2seq/seq2seq_trainer.py\", line 166, in _compute_loss\n",
      "    logits = model(**inputs, use_cache=False)[0]\n",
      "  File \"/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py\", line 1295, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py\", line 1180, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py\", line 1047, in forward\n",
      "    use_cache=use_cache,\n",
      "  File \"/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py\", line 422, in forward\n",
      "    output_attentions=output_attentions,\n",
      "  File \"/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/marco/miniconda3/envs/magma/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py\", line 195, in forward\n",
      "    if self.is_decoder:\n",
      "KeyboardInterrupt\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 10261\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program failed with code 255.  Press ctrl-c to abort syncing.\n"
     ]
    }
   ],
   "source": [
    "!python3 $finetune_script \\\n",
    "--model_name_or_path $model_name_or_path \\\n",
    "--config_name $model_config_dir \\\n",
    "--tokenizer_name $model_name_or_path \\\n",
    "--cache_dir $cache_dir \\\n",
    "--data_dir $data_dir \\\n",
    "--freeze_embeds \\\n",
    "--do_train \\\n",
    "--learning_rate 5e-5 \\\n",
    "--label_smoothing_factor 0.1 \\\n",
    "--warmup_steps 0 \\\n",
    "--fp16 \\\n",
    "--sortish_sampler \\\n",
    "--task summarization \\\n",
    "--max_source_length 1024 \\\n",
    "--max_target_length $config.ONE_BULLET_MAX_LEN \\\n",
    "--val_max_target_length $config.ONE_BULLET_MAX_LEN \\\n",
    "--num_train_epochs 20 \\\n",
    "--logging_steps 10 --logging_first_step \\\n",
    "--per_device_train_batch_size 2 --per_device_eval_batch_size 8 \\\n",
    "--gradient_accumulation_steps 16 \\\n",
    "--evaluation_strategy steps --eval_steps 7 --eval_beams 2 \\\n",
    "--predict_with_generate \\\n",
    "--save_steps 98 \\\n",
    "--output_dir $output_dir \\\n",
    "--overwrite_output_dir \\\n",
    "--seed $config.SEED \\\n",
    "--run_name $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bbRUTtNDHth"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNa0Dk3mImrCcZO3VdQRayI",
   "collapsed_sections": [
    "P95DxvqWi_2Y",
    "L5sXxqeNCtkN",
    "S0FByNNOIRvG",
    "GPbOrCLWACbm",
    "siT4m5aYCFSh",
    "Dk1uGO5SCDNa",
    "WdDCBiMOBWiO",
    "pr_0J4xgBWiW",
    "l8hQT6ksBWin",
    "d5V0QCdf04Yx",
    "KQj3gt6s5ACz",
    "ah9sssub5DXX",
    "M58yiP1yVOav",
    "aeSSMVZwVOa1"
   ],
   "name": "bart_finetune.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "88734567ccb2435091eeab9f30b2de9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bd07faa8d1534ff9b1967f18e6fca2c2",
       "IPY_MODEL_43ac5522170c45f7856f7097ff9fee58"
      ],
      "layout": "IPY_MODEL_431d75cb2dd2403e88015f4e009080d4"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
