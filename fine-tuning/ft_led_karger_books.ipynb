{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"led_finetune_karger_books.ipynb","provenance":[],"collapsed_sections":["P95DxvqWi_2Y","L5sXxqeNCtkN","Uv7edHocbDCD","CFtGA4yibG2u"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"P95DxvqWi_2Y"},"source":["#### For Colab"]},{"cell_type":"code","metadata":{"id":"GD_KFnI1H1ip","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1611824055179,"user_tz":-60,"elapsed":661,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"a266fdea-b5dc-436b-e4a7-1d77e4b82eda"},"source":["\"\"\"\n","function ClickConnect(){\n","    console.log(\"Working\");\n","    document.querySelector(\"colab-toolbar-button\").click() \n","}\n","var i = setInterval(ClickConnect, 900000)\n","clearInterval(i)\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nfunction ClickConnect(){\\n    console.log(\"Working\");\\n    document.querySelector(\"colab-toolbar-button\").click() \\n}\\nvar i = setInterval(ClickConnect, 900000)\\nclearInterval(i)\\n'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"ja6-0xXdM-RH"},"source":["First, let's try to get a GPU with at least 15GB RAM."]},{"cell_type":"code","metadata":{"id":"iEIBtKCIM55i"},"source":["# crash colab to get more RAM\n","!kill -9 -1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TuKL2zYwNIzA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844609366,"user_tz":-60,"elapsed":891,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"6bec7f16-9102-4fed-e258-c32696ab6f6e"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Thu Jan 28 14:36:48 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   73C    P0    32W /  70W |      0MiB / 15079MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RR-OcN_Wy1jE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844612909,"user_tz":-60,"elapsed":1718,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"2d60135a-3001-4c46-a060-81358b1d4244"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"au5Z9XQAC7C-"},"source":["drive_dir = '/content/drive/My Drive/MAGMA: Summarization/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L5sXxqeNCtkN"},"source":["#### Install Libraries"]},{"cell_type":"code","metadata":{"id":"m6KSIQlYzjEy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844626207,"user_tz":-60,"elapsed":12068,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"5de708dc-7605-4dae-823e-f4fec13071c7"},"source":["!pip install datasets==1.2.1\n","!pip install transformers==4.2.0\n","!pip install rouge_score\n","!pip install -U wandb\n","!pip install -U sentence-transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: datasets==1.2.1 in /usr/local/lib/python3.6/dist-packages (1.2.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (0.70.11.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (0.3.3)\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (3.0.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (3.4.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (1.1.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (1.19.5)\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (4.41.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (2.0.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (0.8)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.2.1) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.2.1) (3.4.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.2.1) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.2.1) (2.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.2.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.2.1) (1.24.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.2.1) (1.15.0)\n","Requirement already satisfied: transformers==4.2.0 in /usr/local/lib/python3.6/dist-packages (4.2.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (0.0.43)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (1.19.5)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (0.8)\n","Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (0.9.4)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (3.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (20.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.0) (1.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.0) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.0) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.2.0) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0) (2.10)\n","Requirement already satisfied: rouge_score in /usr/local/lib/python3.6/dist-packages (0.0.4)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from rouge_score) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rouge_score) (1.19.5)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from rouge_score) (0.10.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge_score) (3.2.5)\n","Requirement already up-to-date: wandb in /usr/local/lib/python3.6/dist-packages (0.10.15)\n","Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.3)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n","Requirement already satisfied, skipping upgrade: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied, skipping upgrade: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.0.1)\n","Requirement already satisfied, skipping upgrade: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.0.1)\n","Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.1.12)\n","Requirement already satisfied, skipping upgrade: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.5.4)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.12.4)\n","Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n","Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied, skipping upgrade: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.19.5)\n","Requirement already satisfied, skipping upgrade: watchdog<0.10.5,>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.10.4)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.0->wandb) (51.3.3)\n","Requirement already satisfied, skipping upgrade: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog<0.10.5,>=0.8.3->wandb) (0.1.2)\n","Requirement already satisfied, skipping upgrade: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n","Requirement already up-to-date: sentence-transformers in /usr/local/lib/python3.6/dist-packages (0.4.1.2)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.19.5)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n","Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.1.95)\n","Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n","Requirement already satisfied, skipping upgrade: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.2.0)\n","Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n","Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.8)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (1.0.0)\n","Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.8)\n","Requirement already satisfied, skipping upgrade: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.9.4)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n","Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n","Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n","Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.43)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S0FByNNOIRvG"},"source":["### **Config**"]},{"cell_type":"code","metadata":{"id":"82WSp6khIcua","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844626669,"user_tz":-60,"elapsed":8866,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"0af4c55a-4adb-4ef9-8083-5f9070ccc804"},"source":["import sys\n","sys.path.insert(0, drive_dir)\n","import config\n","\n","import wandb\n","wandb.login()\n","\n","project_name = 'finetune_led'\n","%env WANDB_PROJECT=$project_name"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcoabrate\u001b[0m (use `wandb login --relogin` to force relogin)\n"],"name":"stderr"},{"output_type":"stream","text":["env: WANDB_PROJECT=finetune_led\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6Hgw3GTXLLw0"},"source":["## ðŸ¤— Finetune **Longformer Encoder-Decoder (LED)** on Karger Books ðŸ¤—"]},{"cell_type":"markdown","metadata":{"id":"W7-QHmRiAMB9"},"source":["The *Longformer Encoder-Decoder (LED)* was recently added as an extension to [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n","\n","We will leverage ðŸ¤—`Seq2SeqTrainer`, gradient checkpointing and as usual ðŸ¤—`datasets`."]},{"cell_type":"markdown","metadata":{"id":"0u9twLMqYEzT"},"source":["Let's start by loading and preprocessing the dataset.\n","\n"]},{"cell_type":"code","metadata":{"id":"ODLQ8MUJfmi4","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844627656,"user_tz":-60,"elapsed":9119,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"e4107fa7-7ecf-4e32-f13c-016083e2e5e6"},"source":["from datasets import load_dataset, load_metric\n","train_dataset = load_dataset('csv', data_files=drive_dir+'datasets/karger_books_base/train.csv', split='train')\n","val_dataset = load_dataset('csv', data_files=drive_dir+'datasets/karger_books_base/val.csv', split='train')\n","test_dataset = load_dataset('csv', data_files=drive_dir+'datasets/karger_books_base/test.csv', split='train')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using custom data configuration default\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-0498b287fc32a431/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2)\n","Using custom data configuration default\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-82fd9fd954b5a267/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2)\n","Using custom data configuration default\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-acb527b34156b505/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"lrwXZ_LBGLwt"},"source":["We can see that the input data is the `text` - a scientific chapter and the target data is the `bullets` - a concise summary."]},{"cell_type":"code","metadata":{"id":"jpUr9QeebZ-n"},"source":["from transformers import AutoTokenizer, AddedToken\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n","\n","print(len(tokenizer))\n","print(tokenizer.additional_special_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vnUtPnElMlpa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844631489,"user_tz":-60,"elapsed":11000,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"d20769fd-9c6f-487a-ccc7-e6a51c537889"},"source":["num_added_toks = tokenizer.add_tokens(AddedToken('<BULL>', single_word=False, lstrip=True, rstrip=True, normalized=False))\n","print('We have added', num_added_toks, 'tokens')\n","print(len(tokenizer))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["We have added 1 tokens\n","50266\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZhQeQg3oCcL-"},"source":["Note that for the sake of this notebook, we finetune the \"smaller\" LED checkpoint [\"allenai/led-base-16384\"](https://huggingface.co/allenai/led-base-16384). Better performance can however be attained by finetuning [\"allenai/led-large-16384\"](https://huggingface.co/allenai/led-large-16384) at the cost of a higher required GPU RAM."]},{"cell_type":"code","metadata":{"id":"_VkTU_K6Mlpb"},"source":["import pandas as pd\n","df_train = pd.read_csv(drive_dir+'datasets/karger_books_base/train.csv').set_index(['book', 'chapter'])\n","df_val = pd.read_csv(drive_dir+'datasets/karger_books_base/val.csv').set_index(['book', 'chapter'])\n","df_test = pd.read_csv(drive_dir+'datasets/karger_books_base/test.csv').set_index(['book', 'chapter'])\n","\n","df = pd.concat([df_train, df_val, df_test])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzT6E_MxMlpc"},"source":["df['bullets_tok'] = df.bullets.map(tokenizer.tokenize)\n","df['bullets_enc'] = df.bullets.map(tokenizer.encode)\n","df['bullets_num_tok'] = df.bullets_enc.map(len)\n","df['text_enc'] = df.text.map(tokenizer.encode)\n","df['text_num_tok'] = df.text_enc.map(len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRmpaNvXQugc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844635256,"user_tz":-60,"elapsed":14015,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"39f63b67-4ca7-48ee-e262-0a2ac95c4300"},"source":["print(len(df.iloc[0].bullets_tok), len(df.iloc[0].bullets_enc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["168 170\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u61KfpMrQ2eX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844635258,"user_tz":-60,"elapsed":13772,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"3b136f9c-f404-4c8a-fd98-82f3f832e835"},"source":["print(tokenizer.bos_token_id, tokenizer.eos_token_id)\n","print(df.iloc[0].bullets_enc[0], df.iloc[0].bullets_enc[-1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 2\n","0 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jK_dewH9Per7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844635261,"user_tz":-60,"elapsed":13545,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"55bfdad7-6feb-4975-db38-46bcac943feb"},"source":["list(zip(df.iloc[0].bullets_tok, df.iloc[0].bullets_enc[1:-1]))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('<BULL> ', 50265),\n"," ('The', 133),\n"," ('Ä four', 237),\n"," ('Ä main', 1049),\n"," ('Ä types', 3505),\n"," ('Ä of', 9),\n"," ('Ä leukemia', 28837),\n"," ('Ä are', 32),\n"," ('Ä acute', 13827),\n"," ('Ä my', 127),\n"," ('el', 523),\n"," ('oid', 12572),\n"," ('Ä leukemia', 28837),\n"," (',', 6),\n"," ('Ä acute', 13827),\n"," ('Ä lymph', 23496),\n"," ('obl', 33449),\n"," ('astic', 11599),\n"," ('Ä leukemia', 28837),\n"," (',', 6),\n"," ('Ä chronic', 7642),\n"," ('Ä my', 127),\n"," ('el', 523),\n"," ('oid', 12572),\n"," ('Ä leukemia', 28837),\n"," ('Ä and', 8),\n"," ('Ä chronic', 7642),\n"," ('Ä lymph', 23496),\n"," ('ocy', 30321),\n"," ('tic', 13240),\n"," ('Ä leukemia', 28837),\n"," ('.', 4),\n"," (' <BULL> ', 50265),\n"," ('The', 133),\n"," ('Ä acute', 13827),\n"," ('Ä le', 2084),\n"," ('uke', 7480),\n"," ('m', 119),\n"," ('ias', 5003),\n"," ('Ä are', 32),\n"," ('Ä predominantly', 15351),\n"," ('Ä characterized', 17407),\n"," ('Ä by', 30),\n"," ('Ä the', 5),\n"," ('Ä uncontrolled', 38411),\n"," ('Ä growth', 434),\n"," ('Ä of', 9),\n"," ('Ä immature', 39001),\n"," ('Ä poorly', 12101),\n"," ('Ä differentiated', 32691),\n"," ('Ä cells', 4590),\n"," ('Ä that', 14),\n"," ('Ä are', 32),\n"," ('Ä blocked', 4953),\n"," ('Ä from', 31),\n"," ('Ä further', 617),\n"," ('Ä differentiation', 37225),\n"," ('.', 4),\n"," (' <BULL> ', 50265),\n"," ('The', 133),\n"," ('Ä chronic', 7642),\n"," ('Ä le', 2084),\n"," ('uke', 7480),\n"," ('m', 119),\n"," ('ias', 5003),\n"," ('Ä are', 32),\n"," ('Ä characterized', 17407),\n"," ('Ä by', 30),\n"," ('Ä mat', 7821),\n"," ('uring', 5206),\n"," ('Ä prolifer', 37168),\n"," ('ative', 3693),\n"," ('Ä later', 423),\n"," ('-', 12),\n"," ('stage', 10850),\n"," ('Ä cells', 4590),\n"," ('.', 4),\n"," (' <BULL> ', 50265),\n"," ('The', 133),\n"," ('Ä main', 1049),\n"," ('Ä reason', 1219),\n"," ('Ä for', 13),\n"," ('Ä the', 5),\n"," ('Ä development', 709),\n"," ('Ä of', 9),\n"," ('Ä leukemia', 28837),\n"," ('Ä is', 16),\n"," ('Ä the', 5),\n"," ('Ä accumulation', 20477),\n"," ('Ä of', 9),\n"," ('Ä gene', 10596),\n"," ('Ä mutations', 28513),\n"," ('Ä over', 81),\n"," ('Ä time', 86),\n"," ('.', 4),\n"," ('Ä Chrom', 26877),\n"," ('os', 366),\n"," ('omal', 38868),\n"," ('Ä transl', 37297),\n"," ('ocations', 29876),\n"," ('Ä are', 32),\n"," ('Ä common', 1537),\n"," ('.', 4),\n"," (' <BULL> ', 50265),\n"," ('The', 133),\n"," ('Ä identification', 10614),\n"," ('Ä of', 9),\n"," ('Ä chromos', 47411),\n"," ('omal', 38868),\n"," ('Ä abnormalities', 39063),\n"," ('Ä aids', 25842),\n"," ('Ä treatment', 1416),\n"," ('Ä strat', 30789),\n"," ('ification', 5000),\n"," ('Ä in', 11),\n"," ('Ä patients', 1484),\n"," ('.', 4),\n"," ('Ä New', 188),\n"," ('-', 12),\n"," ('generation', 11092),\n"," ('Ä sequencing', 32243),\n"," ('Ä technologies', 4233),\n"," ('Ä have', 33),\n"," ('Ä revolution', 7977),\n"," ('ized', 1538),\n"," ('Ä the', 5),\n"," ('Ä discovery', 6953),\n"," ('Ä of', 9),\n"," ('Ä mutations', 28513),\n"," ('Ä in', 11),\n"," ('Ä leukemia', 28837),\n"," ('.', 4),\n"," (' <BULL> ', 50265),\n"," ('Mal', 18764),\n"," ('ignant', 35090),\n"," ('Ä cl', 3741),\n"," ('onal', 20171),\n"," ('Ä sub', 2849),\n"," ('pop', 15076),\n"," ('ulations', 16685),\n"," ('Ä identified', 2006),\n"," ('Ä at', 23),\n"," ('Ä diagnosis', 9726),\n"," ('Ä have', 33),\n"," ('Ä also', 67),\n"," ('Ä been', 57),\n"," ('Ä identified', 2006),\n"," ('Ä at', 23),\n"," ('Ä relapse', 38387),\n"," ('.', 4),\n"," (' <BULL> ', 50265),\n"," ('Pro', 10653),\n"," ('gn', 16993),\n"," ('osis', 13310),\n"," ('Ä depends', 7971),\n"," ('Ä on', 15),\n"," ('Ä the', 5),\n"," ('Ä genetic', 9186),\n"," ('Ä abnormalities', 39063),\n"," ('Ä involved', 963),\n"," (',', 6),\n"," ('Ä as', 25),\n"," ('Ä well', 157),\n"," ('Ä as', 25),\n"," ('Ä other', 97),\n"," ('Ä clinical', 5154),\n"," ('Ä factors', 2433),\n"," ('.', 4)]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Edn3Z0zbMlpc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844635262,"user_tz":-60,"elapsed":13235,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"a811df47-d121-47b2-d8de-a3b9a1dd6833"},"source":["df.text_num_tok.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count      453.000000\n","mean      2957.896247\n","std       1896.892605\n","min        640.000000\n","25%       1680.000000\n","50%       2488.000000\n","75%       3616.000000\n","max      13452.000000\n","Name: text_num_tok, dtype: float64"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"HwEipr3bMlpc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844635264,"user_tz":-60,"elapsed":13039,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"b5d7e636-4ee0-4db6-dde7-5521d18a4d13"},"source":["len(df[df.text_num_tok > 8192])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"XVL8epyaMlpd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844635266,"user_tz":-60,"elapsed":12764,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"8e203980-dfb9-496d-c0b6-3c8b31981a58"},"source":["df.bullets_num_tok.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    453.000000\n","mean     185.512141\n","std       90.921720\n","min       48.000000\n","25%      115.000000\n","50%      170.000000\n","75%      235.000000\n","max      680.000000\n","Name: bullets_num_tok, dtype: float64"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"occi5ifrMlpd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844635267,"user_tz":-60,"elapsed":12544,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"7f9406c2-6f53-4892-f6b8-5c4e40d315d9"},"source":["len(df[df.bullets_num_tok > 512])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"Nbb24Uh-Y4xO"},"source":["max_input_length = 8192\n","max_output_length = 512"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WN_SAv1JE40f"},"source":["Now, let's write down the input data processing function that will be used to map each data sample to the correct model format.\n","As explained earlier `text` represents here our input data and `bullets` is the target data. The datasamples are thus tokenized up to the respective maximum lengths of 8192 and 512.\n","\n","In addition to the usual `attention_mask`, LED can make use of an additional `global_attention_mask` defining which input tokens are attended globally and which are attended only locally, just as it's the case of [Longformer](https://huggingface.co/transformers/model_doc/longformer.html). For more information on Longformer's self-attention, please take a look at the corresponding [docs](https://huggingface.co/transformers/model_doc/longformer.html#longformer-self-attention). For summarization, we follow recommendations of the [paper](https://arxiv.org/abs/2004.05150) and use global attention only for the very first token. Finally, we make sure that no loss is computed on padded tokens by setting their index to `-100`."]},{"cell_type":"code","metadata":{"id":"lEcAaZhNY8ge"},"source":["def process_data_to_model_inputs(batch):\n","    # tokenize the inputs and labels\n","    inputs = tokenizer(\n","        batch[\"text\"],\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=max_input_length,\n","    )\n","    outputs = tokenizer(\n","        batch[\"bullets\"],\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=max_output_length,\n","    )\n","\n","    batch[\"input_ids\"] = inputs.input_ids\n","    batch[\"attention_mask\"] = inputs.attention_mask\n","\n","    # create 0 global_attention_mask lists\n","    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n","        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n","    ]\n","\n","    # since above lists are references, the following line changes the 0 index for all samples\n","    batch[\"global_attention_mask\"][0][0] = 1\n","    batch[\"labels\"] = outputs.input_ids\n","\n","    # We have to make sure that the PAD token is ignored\n","    batch[\"labels\"] = [\n","        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n","        for labels in batch[\"labels\"]\n","    ]\n","\n","    return batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iyseRmC5IcXj"},"source":["For the sake of this notebook, we will reduce the training and validation data \n","to a dummy dataset of sizes 250 and 25 respectively. For a full training run, those lines should be commented out."]},{"cell_type":"code","metadata":{"id":"8DiGsjD4Mlpj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844635273,"user_tz":-60,"elapsed":11478,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"bf7bc51a-bb16-4788-cf4b-46bb28a856be"},"source":["train_dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['book', 'chapter', 'text', 'bullets'],\n","    num_rows: 362\n","})"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"fNzZZACtIoCD"},"source":["Great, having defined the mapping function, let's preprocess the training data"]},{"cell_type":"code","metadata":{"id":"8RClMjZOZCPO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844635275,"user_tz":-60,"elapsed":10999,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"983d47ab-7b05-4e9d-8840-64204c4fffa1"},"source":["train_dataset = train_dataset.map(\n","    process_data_to_model_inputs,\n","    batched=True,\n","    batch_size=batch_size,\n","    remove_columns=['book', 'chapter', 'text', 'bullets'],\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-0498b287fc32a431/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2/cache-d04e131ec9c71489.arrow\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eG9bds3RMlpk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844635277,"user_tz":-60,"elapsed":10794,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"894af0cd-9a4f-417f-e297-5b79cd461c16"},"source":["train_dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['attention_mask', 'global_attention_mask', 'input_ids', 'labels'],\n","    num_rows: 362\n","})"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"RUtRFmgVIuHG"},"source":["and validation data"]},{"cell_type":"code","metadata":{"id":"Gy_9DCocZD5G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844635279,"user_tz":-60,"elapsed":10347,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"6003f5f3-a6e0-423d-8547-76191d0afc72"},"source":["val_dataset = val_dataset.map(\n","    process_data_to_model_inputs,\n","    batched=True,\n","    batch_size=batch_size,\n","    remove_columns=['book', 'chapter', 'text', 'bullets'],\n",")\n","test_dataset = test_dataset.map(\n","    process_data_to_model_inputs,\n","    batched=True,\n","    batch_size=batch_size,\n","    remove_columns=['book', 'chapter', 'text', 'bullets'],\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-82fd9fd954b5a267/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2/cache-5ee4aebeea5df705.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-acb527b34156b505/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2/cache-758d0938dbb23390.arrow\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"evRRitZaIw9N"},"source":["Finally, the datasets should be converted into the PyTorch format as follows."]},{"cell_type":"code","metadata":{"id":"ci2QYHCMZiNO"},"source":["train_dataset.set_format(\n","    type=\"torch\",\n","    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",")\n","val_dataset.set_format(\n","    type=\"torch\",\n","    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dI-QHywgI6Hb"},"source":["Alright, we're almost ready to start training. Let's load the model via the `AutoModelForSeq2SeqLM` class."]},{"cell_type":"code","metadata":{"id":"SZf_8QXJacIc"},"source":["from transformers import AutoModelForSeq2SeqLM"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hNJQlLDKJHIK"},"source":["We've decided to stick to the smaller model `\"allenai/led-base-16384\"` for the sake of this notebook. In addition, we directly enable gradient checkpointing and disable the caching mechanism to save memory."]},{"cell_type":"code","metadata":{"id":"R-UfEo0Zadpl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844645470,"user_tz":-60,"elapsed":18697,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"dc7e4b18-0a98-4065-9a1f-403d4a01a75c"},"source":["model_name_or_path = 'allenai/led-base-16384'\n","led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)\n","led.resize_token_embeddings(len(tokenizer))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(50266, 768)"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"kQOaX6eRJXkM"},"source":["During training, we want to evaluate the model on Rouge, the most common metric used in summarization, to make sure the model is indeed improving during training. For this, we set fitting generation parameters. We'll use beam search with a small beam of just 2 to save memory. Also, we force the model to generate at least 100 tokens, but no more than 512. In addition, some other generation parameters are set that have been found helpful for generation. For more information on those parameters, please take a look at the [docs](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate)."]},{"cell_type":"code","metadata":{"id":"kPnNi_tWaklV"},"source":["# set generate hyperparameters\n","led.config.num_beams = 2\n","led.config.max_length = 512\n","led.config.min_length = 70\n","led.config.length_penalty = 1.0\n","led.config.early_stopping = True\n","led.config.no_repeat_ngram_size = 3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wsytOIWVKqFz"},"source":["The compute metrics function expects the generation output, called `pred.predictions` as well as the gold label, called `pred.label_ids`.\n","\n","Those tokens are decoded and consequently, the rouge score can be computed."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p02Ltd_7xS0c","executionInfo":{"status":"ok","timestamp":1611844647662,"user_tz":-60,"elapsed":19575,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"3324f4e2-383a-4c7f-decb-a37fe9361471"},"source":["from rouge_score import rouge_scorer, scoring\n","import numpy as np\n","import re\n","import nltk\n","nltk.download('punkt')\n","from sentence_transformers import SentenceTransformer\n","sentence_distilroberta = SentenceTransformer('paraphrase-distilroberta-base-v1')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1ahmf7mcZzU7"},"source":["ROUGE_KEYS = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n","\n","def extract_rouge_mid_statistics(dct):\n","    new_dict = {}\n","    for k1, v1 in dct.items():\n","        mid = v1.mid\n","        for stat in [\"precision\", \"recall\", \"fmeasure\"]:\n","            new_dict[k1+'_'+stat] = round(getattr(mid, stat), 4)*100\n","    return new_dict\n","\n","def add_newline_to_end_of_each_sentence(x: str) -> str:\n","    \"\"\"This was added to get rougeLsum scores matching published rougeL scores for BART and PEGASUS.\"\"\"\n","    re.sub(\"<n>\", \"\", x)  # remove pegasus newline char\n","    return \"\\n\".join(nltk.sent_tokenize(x))\n","\n","def calculate_rouge(\n","    pred_lns,\n","    tgt_lns,\n","    use_stemmer=True,\n","    rouge_keys=ROUGE_KEYS,\n","    return_precision_and_recall=True,\n","    bootstrap_aggregation=True,\n","    newline_sep=True):\n","\n","    scorer = rouge_scorer.RougeScorer(rouge_keys, use_stemmer=use_stemmer)\n","    aggregator = scoring.BootstrapAggregator()\n","    for tgt, pred in zip(tgt_lns, pred_lns):\n","        # rougeLsum expects \"\\n\" separated sentences within a summary\n","        if newline_sep:\n","            pred = add_newline_to_end_of_each_sentence(pred)\n","            tgt = add_newline_to_end_of_each_sentence(tgt)\n","        scores = scorer.score(tgt, pred)\n","        aggregator.add_scores(scores)\n","\n","    if bootstrap_aggregation:\n","        result = aggregator.aggregate()\n","        if return_precision_and_recall:\n","            return extract_rouge_mid_statistics(result)  # here we return dict\n","        else:\n","            return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}\n","\n","    else:\n","        return aggregator._scores  # here we return defaultdict(list)\n","\n","def calculate_sentence_trans_cosine(pred_lns, tgt_lns):\n","\n","    cosine_sim = lambda a, b: (np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b)))\n","\n","    return np.mean([\\\n","        cosine_sim(sentence_distilroberta.encode(pred),\n","                   sentence_distilroberta.encode(tgt))\\\n","        for tgt, pred in zip(tgt_lns, pred_lns)])*100\n","\n","def compute_metrics(pred):\n","    labels_ids = pred.label_ids\n","    pred_ids = pred.predictions\n","\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n","    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n","\n","    metrics = calculate_rouge(pred_str, label_str)\n","\n","    cosine_sim = calculate_sentence_trans_cosine(pred_str, label_str)\n","    metrics.update({\"sentence_distilroberta_cosine\": cosine_sim})\n","\n","    return metrics"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rrX2gBnYLAna"},"source":["Now, we're ready to start training. Let's import the `Seq2SeqTrainer` and `Seq2SeqTrainingArguments`."]},{"cell_type":"code","metadata":{"id":"Gq7CajIWaUo5"},"source":["from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ga-fCOB4LI4W"},"source":["In contrast to the usual `Trainer`, the `Seq2SeqTrainer` makes it possible to use the `generate()` function during evaluation. This should be enabled with `predict_with_generate=True`. Because our GPU RAM is limited, we make use of gradient accumulation by setting `gradient_accumulation_steps=4` to have an effective `batch_size` of 2 * 4 = 8.\n","\n","Other training arguments can be read upon in the [docs](https://huggingface.co/transformers/main_classes/trainer.html?highlight=trainingarguments#transformers.TrainingArguments)."]},{"cell_type":"code","metadata":{"id":"yyURQdR3S-6S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611844649142,"user_tz":-60,"elapsed":19979,"user":{"displayName":"Marco Pietro Abrate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh-8YF-8BlnrkN9mLZ0xfVOWfOh7kYncpYRv-Y=s64","userId":"15422244832836998434"}},"outputId":"ace28274-64d4-43af-bba4-10145db5c580"},"source":["output_dir = drive_dir+'fine-tuning/'+\\\n","    model_name_or_path.replace('/', '?')+'_karger_books_base'\n","\n","log_dir = output_dir + '/logs'\n","print(log_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MAGMA: Summarization/fine-tuning/allenai?led-base-16384_karger_books_base/logs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mMehwI4QZqB_"},"source":["# enable fp16 apex training\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=output_dir,\n","    overwrite_output_dir=True,\n","    do_train=True,\n","    num_train_epochs=5,\n","    do_eval=True,\n","    evaluation_strategy='steps',\n","    eval_steps=10,\n","    predict_with_generate=True,\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    gradient_accumulation_steps=8,\n","    fp16=True,\n","    logging_steps=5,\n","    save_steps=20,\n","    save_total_limit=10,\n","    logging_dir=log_dir\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JZCY-C5mLzXY"},"source":["The training arguments, along with the model, tokenizer, datasets and the `compute_metrics` function can then be passed to the `Seq2SeqTrainer`"]},{"cell_type":"code","metadata":{"id":"6WPyTYO_JfHW"},"source":["trainer = Seq2SeqTrainer(\n","    model=led,\n","    tokenizer=tokenizer,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xZxSe_afL9TH"},"source":["and we can start training. This will take about ~35min."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"g4zkCpeQa2NN","outputId":"c6b10664-4a94-4b42-ae8a-5448b142d611"},"source":["trainer.train()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","                Tracking run with wandb version 0.10.15<br/>\n","                Syncing run <strong style=\"color:#cdcd00\">/content/drive/My Drive/MAGMA: Summarization/fine-tuning/allenai?led-base-16384_karger_books_base</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n","                Project page: <a href=\"https://wandb.ai/marcoabrate/finetune_led\" target=\"_blank\">https://wandb.ai/marcoabrate/finetune_led</a><br/>\n","                Run page: <a href=\"https://wandb.ai/marcoabrate/finetune_led/runs/exg9w845\" target=\"_blank\">https://wandb.ai/marcoabrate/finetune_led/runs/exg9w845</a><br/>\n","                Run data is saved locally in <code>/content/wandb/run-20210128_143733-exg9w845</code><br/><br/>\n","            "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py:851: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n","  return torch.tensor(x, **format_kwargs)\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='71' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 71/110 2:37:13 < 1:28:51, 0.01 it/s, Epoch 3.18/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1 Precision</th>\n","      <th>Rouge1 Recall</th>\n","      <th>Rouge1 Fmeasure</th>\n","      <th>Rouge2 Precision</th>\n","      <th>Rouge2 Recall</th>\n","      <th>Rouge2 Fmeasure</th>\n","      <th>Rougel Precision</th>\n","      <th>Rougel Recall</th>\n","      <th>Rougel Fmeasure</th>\n","      <th>Rougelsum Precision</th>\n","      <th>Rougelsum Recall</th>\n","      <th>Rougelsum Fmeasure</th>\n","      <th>Sentence Distilroberta Cosine</th>\n","      <th>Runtime</th>\n","      <th>Samples Per Second</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>2.782400</td>\n","      <td>2.544499</td>\n","      <td>52.350000</td>\n","      <td>30.700000</td>\n","      <td>36.340000</td>\n","      <td>16.220000</td>\n","      <td>9.630000</td>\n","      <td>11.440000</td>\n","      <td>29.670000</td>\n","      <td>17.570000</td>\n","      <td>20.670000</td>\n","      <td>34.870000</td>\n","      <td>20.300000</td>\n","      <td>24.050000</td>\n","      <td>70.823926</td>\n","      <td>701.865200</td>\n","      <td>0.064000</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>2.503200</td>\n","      <td>2.438534</td>\n","      <td>51.050000</td>\n","      <td>34.060000</td>\n","      <td>38.300000</td>\n","      <td>16.780000</td>\n","      <td>11.040000</td>\n","      <td>12.490000</td>\n","      <td>27.930000</td>\n","      <td>18.760000</td>\n","      <td>20.950000</td>\n","      <td>33.790000</td>\n","      <td>21.850000</td>\n","      <td>24.830000</td>\n","      <td>71.699560</td>\n","      <td>825.669300</td>\n","      <td>0.055000</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>2.262200</td>\n","      <td>2.381989</td>\n","      <td>48.050000</td>\n","      <td>36.870000</td>\n","      <td>38.140000</td>\n","      <td>14.960000</td>\n","      <td>11.510000</td>\n","      <td>11.880000</td>\n","      <td>26.360000</td>\n","      <td>20.530000</td>\n","      <td>20.970000</td>\n","      <td>31.640000</td>\n","      <td>24.320000</td>\n","      <td>25.100000</td>\n","      <td>73.104674</td>\n","      <td>1049.603900</td>\n","      <td>0.043000</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>2.249100</td>\n","      <td>2.354266</td>\n","      <td>51.340000</td>\n","      <td>32.780000</td>\n","      <td>37.670000</td>\n","      <td>16.590000</td>\n","      <td>10.760000</td>\n","      <td>12.280000</td>\n","      <td>29.000000</td>\n","      <td>19.080000</td>\n","      <td>21.600000</td>\n","      <td>34.490000</td>\n","      <td>22.020000</td>\n","      <td>25.230000</td>\n","      <td>73.239094</td>\n","      <td>799.889100</td>\n","      <td>0.056000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>2.082800</td>\n","      <td>2.383488</td>\n","      <td>50.490000</td>\n","      <td>34.570000</td>\n","      <td>38.230000</td>\n","      <td>16.150000</td>\n","      <td>11.120000</td>\n","      <td>12.270000</td>\n","      <td>29.240000</td>\n","      <td>20.010000</td>\n","      <td>22.010000</td>\n","      <td>33.820000</td>\n","      <td>22.750000</td>\n","      <td>25.260000</td>\n","      <td>73.963046</td>\n","      <td>900.814400</td>\n","      <td>0.050000</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>2.040600</td>\n","      <td>2.365585</td>\n","      <td>50.290000</td>\n","      <td>38.040000</td>\n","      <td>40.920000</td>\n","      <td>16.200000</td>\n","      <td>12.510000</td>\n","      <td>13.280000</td>\n","      <td>27.510000</td>\n","      <td>20.960000</td>\n","      <td>22.460000</td>\n","      <td>33.060000</td>\n","      <td>24.590000</td>\n","      <td>26.630000</td>\n","      <td>74.976486</td>\n","      <td>1039.729000</td>\n","      <td>0.043000</td>\n","    </tr>\n","  </tbody>\n","</table><p>\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='12' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/23 07:12 < 07:12, 0.03 it/s]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n","/usr/local/lib/python3.6/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n","/usr/local/lib/python3.6/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n","/usr/local/lib/python3.6/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='101' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [101/110 3:57:52 < 21:37, 0.01 it/s, Epoch 4.53/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1 Precision</th>\n","      <th>Rouge1 Recall</th>\n","      <th>Rouge1 Fmeasure</th>\n","      <th>Rouge2 Precision</th>\n","      <th>Rouge2 Recall</th>\n","      <th>Rouge2 Fmeasure</th>\n","      <th>Rougel Precision</th>\n","      <th>Rougel Recall</th>\n","      <th>Rougel Fmeasure</th>\n","      <th>Rougelsum Precision</th>\n","      <th>Rougelsum Recall</th>\n","      <th>Rougelsum Fmeasure</th>\n","      <th>Sentence Distilroberta Cosine</th>\n","      <th>Runtime</th>\n","      <th>Samples Per Second</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>2.782400</td>\n","      <td>2.544499</td>\n","      <td>52.350000</td>\n","      <td>30.700000</td>\n","      <td>36.340000</td>\n","      <td>16.220000</td>\n","      <td>9.630000</td>\n","      <td>11.440000</td>\n","      <td>29.670000</td>\n","      <td>17.570000</td>\n","      <td>20.670000</td>\n","      <td>34.870000</td>\n","      <td>20.300000</td>\n","      <td>24.050000</td>\n","      <td>70.823926</td>\n","      <td>701.865200</td>\n","      <td>0.064000</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>2.503200</td>\n","      <td>2.438534</td>\n","      <td>51.050000</td>\n","      <td>34.060000</td>\n","      <td>38.300000</td>\n","      <td>16.780000</td>\n","      <td>11.040000</td>\n","      <td>12.490000</td>\n","      <td>27.930000</td>\n","      <td>18.760000</td>\n","      <td>20.950000</td>\n","      <td>33.790000</td>\n","      <td>21.850000</td>\n","      <td>24.830000</td>\n","      <td>71.699560</td>\n","      <td>825.669300</td>\n","      <td>0.055000</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>2.262200</td>\n","      <td>2.381989</td>\n","      <td>48.050000</td>\n","      <td>36.870000</td>\n","      <td>38.140000</td>\n","      <td>14.960000</td>\n","      <td>11.510000</td>\n","      <td>11.880000</td>\n","      <td>26.360000</td>\n","      <td>20.530000</td>\n","      <td>20.970000</td>\n","      <td>31.640000</td>\n","      <td>24.320000</td>\n","      <td>25.100000</td>\n","      <td>73.104674</td>\n","      <td>1049.603900</td>\n","      <td>0.043000</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>2.249100</td>\n","      <td>2.354266</td>\n","      <td>51.340000</td>\n","      <td>32.780000</td>\n","      <td>37.670000</td>\n","      <td>16.590000</td>\n","      <td>10.760000</td>\n","      <td>12.280000</td>\n","      <td>29.000000</td>\n","      <td>19.080000</td>\n","      <td>21.600000</td>\n","      <td>34.490000</td>\n","      <td>22.020000</td>\n","      <td>25.230000</td>\n","      <td>73.239094</td>\n","      <td>799.889100</td>\n","      <td>0.056000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>2.082800</td>\n","      <td>2.383488</td>\n","      <td>50.490000</td>\n","      <td>34.570000</td>\n","      <td>38.230000</td>\n","      <td>16.150000</td>\n","      <td>11.120000</td>\n","      <td>12.270000</td>\n","      <td>29.240000</td>\n","      <td>20.010000</td>\n","      <td>22.010000</td>\n","      <td>33.820000</td>\n","      <td>22.750000</td>\n","      <td>25.260000</td>\n","      <td>73.963046</td>\n","      <td>900.814400</td>\n","      <td>0.050000</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>2.040600</td>\n","      <td>2.365585</td>\n","      <td>50.290000</td>\n","      <td>38.040000</td>\n","      <td>40.920000</td>\n","      <td>16.200000</td>\n","      <td>12.510000</td>\n","      <td>13.280000</td>\n","      <td>27.510000</td>\n","      <td>20.960000</td>\n","      <td>22.460000</td>\n","      <td>33.060000</td>\n","      <td>24.590000</td>\n","      <td>26.630000</td>\n","      <td>74.976486</td>\n","      <td>1039.729000</td>\n","      <td>0.043000</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>2.124800</td>\n","      <td>2.347729</td>\n","      <td>51.360000</td>\n","      <td>33.720000</td>\n","      <td>38.050000</td>\n","      <td>16.470000</td>\n","      <td>10.540000</td>\n","      <td>11.980000</td>\n","      <td>29.660000</td>\n","      <td>19.760000</td>\n","      <td>22.040000</td>\n","      <td>34.300000</td>\n","      <td>22.460000</td>\n","      <td>25.200000</td>\n","      <td>73.818028</td>\n","      <td>888.579700</td>\n","      <td>0.051000</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.976300</td>\n","      <td>2.360626</td>\n","      <td>48.910000</td>\n","      <td>37.220000</td>\n","      <td>39.320000</td>\n","      <td>15.520000</td>\n","      <td>11.740000</td>\n","      <td>12.470000</td>\n","      <td>27.600000</td>\n","      <td>21.420000</td>\n","      <td>22.300000</td>\n","      <td>32.270000</td>\n","      <td>24.310000</td>\n","      <td>25.740000</td>\n","      <td>74.243903</td>\n","      <td>1329.364900</td>\n","      <td>0.034000</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>2.012300</td>\n","      <td>2.348964</td>\n","      <td>51.850000</td>\n","      <td>32.740000</td>\n","      <td>37.120000</td>\n","      <td>17.610000</td>\n","      <td>11.550000</td>\n","      <td>12.840000</td>\n","      <td>30.680000</td>\n","      <td>19.550000</td>\n","      <td>22.020000</td>\n","      <td>35.460000</td>\n","      <td>22.160000</td>\n","      <td>25.120000</td>\n","      <td>73.655987</td>\n","      <td>846.959300</td>\n","      <td>0.053000</td>\n","    </tr>\n","  </tbody>\n","</table><p>\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='13' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13/23 08:52 < 07:23, 0.02 it/s]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"gs1gYYb1MJOr"},"source":["This completes the fine-tuning tutorial for LED. This training script with some small changes was used to train [this](https://huggingface.co/patrickvonplaten/led-large-16384-pubmed) checkpoint, called `\" patrickvonplaten/led-large-16384-pubmed\"` on a single GPU for ca. 3 days. Evaluating `\" patrickvonplaten/led-large-16384-pubmed\"` on Pubmed's test data gives a Rouge-2 score of **19.33** which is around 1 Rouge-2 point below SOTA performance on Pubmed.\n","\n","In the Appendix below, the condensed training and evaluation scripts that were used locally to finetune `\" patrickvonplaten/led-large-16384-pubmed\"` are attached."]},{"cell_type":"markdown","metadata":{"id":"g_t9sjHha5T8"},"source":["# **Appendix**"]},{"cell_type":"markdown","metadata":{"id":"Uv7edHocbDCD"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"9U3vpJxnOJiH"},"source":["#!/usr/bin/env python3\n","from datasets import load_dataset, load_metric\n","from transformers import (\n","    Seq2SeqTrainer,\n","    Seq2SeqTrainingArguments,\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n",")\n","\n","# load rouge\n","rouge = load_metric(\"rouge\")\n","\n","# load pubmed\n","pubmed_train = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"train\")\n","pubmed_val = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"validation[:10%]\")\n","\n","# comment out following lines for a test run\n","# pubmed_train = pubmed_train.select(range(32))\n","# pubmed_val = pubmed_val.select(range(32))\n","\n","# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-large-16384\")\n","\n","\n","# max encoder length is 8192 for PubMed\n","encoder_max_length = 8192\n","decoder_max_length = 512\n","batch_size = 2\n","\n","\n","def process_data_to_model_inputs(batch):\n","    # tokenize the inputs and labels\n","    inputs = tokenizer(\n","        batch[\"article\"],\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=encoder_max_length,\n","    )\n","    outputs = tokenizer(\n","        batch[\"abstract\"],\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=decoder_max_length,\n","    )\n","\n","    batch[\"input_ids\"] = inputs.input_ids\n","    batch[\"attention_mask\"] = inputs.attention_mask\n","\n","    # create 0 global_attention_mask lists\n","    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n","        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n","    ]\n","\n","    # since above lists are references, the following line changes the 0 index for all samples\n","    batch[\"global_attention_mask\"][0][0] = 1\n","    batch[\"labels\"] = outputs.input_ids\n","\n","    # We have to make sure that the PAD token is ignored\n","    batch[\"labels\"] = [\n","        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n","        for labels in batch[\"labels\"]\n","    ]\n","\n","    return batch\n","\n","\n","# map train data\n","pubmed_train = pubmed_train.map(\n","    process_data_to_model_inputs,\n","    batched=True,\n","    batch_size=batch_size,\n","    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",")\n","\n","# map val data\n","pubmed_val = pubmed_val.map(\n","    process_data_to_model_inputs,\n","    batched=True,\n","    batch_size=batch_size,\n","    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",")\n","\n","# set Python list to PyTorch tensor\n","pubmed_train.set_format(\n","    type=\"torch\",\n","    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",")\n","\n","# set Python list to PyTorch tensor\n","pubmed_val.set_format(\n","    type=\"torch\",\n","    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",")\n","\n","# enable fp16 apex training\n","training_args = Seq2SeqTrainingArguments(\n","    predict_with_generate=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    fp16=True,\n","    fp16_backend=\"apex\",\n","    output_dir=\"./\",\n","    logging_steps=250,\n","    eval_steps=5000,\n","    save_steps=500,\n","    warmup_steps=1500,\n","    save_total_limit=2,\n","    gradient_accumulation_steps=4,\n",")\n","\n","\n","# compute Rouge score during validation\n","def compute_metrics(pred):\n","    labels_ids = pred.label_ids\n","    pred_ids = pred.predictions\n","\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n","    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n","\n","    rouge_output = rouge.compute(\n","        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n","    )[\"rouge2\"].mid\n","\n","    return {\n","        \"rouge2_precision\": round(rouge_output.precision, 4),\n","        \"rouge2_recall\": round(rouge_output.recall, 4),\n","        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n","    }\n","\n","\n","# load model + enable gradient checkpointing & disable cache for checkpointing\n","led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-large-16384\", gradient_checkpointing=True, use_cache=False)\n","\n","# set generate hyperparameters\n","led.config.num_beams = 4\n","led.config.max_length = 512\n","led.config.min_length = 100\n","led.config.length_penalty = 2.0\n","led.config.early_stopping = True\n","led.config.no_repeat_ngram_size = 3\n","\n","\n","# instantiate trainer\n","trainer = Seq2SeqTrainer(\n","    model=led,\n","    tokenizer=tokenizer,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=pubmed_train,\n","    eval_dataset=pubmed_val,\n",")\n","\n","# start training\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CFtGA4yibG2u"},"source":["## Evaluation"]},{"cell_type":"code","metadata":{"id":"tLM3niQqhEzP"},"source":["import torch\n","\n","from datasets import load_dataset, load_metric\n","from transformers import LEDTokenizer, LEDForConditionalGeneration\n","\n","# load pubmed\n","pubmed_test = load_dataset(\"scientific_papers\", \"pubmed\", ignore_verifications=True, split=\"test\")\n","\n","# load tokenizer\n","tokenizer = LEDTokenizer.from_pretrained(\"patrickvonplaten/led-large-16384-pubmed\")\n","model = LEDForConditionalGeneration.from_pretrained(\"patrickvonplaten/led-large-16384-pubmed\").to(\"cuda\").half()\n","\n","\n","def generate_answer(batch):\n","  inputs_dict = tokenizer(batch[\"article\"], padding=\"max_length\", max_length=8192, return_tensors=\"pt\", truncation=True)\n","  input_ids = inputs_dict.input_ids.to(\"cuda\")\n","  attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n","  global_attention_mask = torch.zeros_like(attention_mask)\n","  # put global attention on <s> token\n","  global_attention_mask[:, 0] = 1\n","\n","  predicted_abstract_ids = model.generate(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n","  batch[\"predicted_abstract\"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n","  return batch\n","\n","\n","result = pubmed_test.map(generate_answer, batched=True, batch_size=4)\n","\n","# load rouge\n","rouge = load_metric(\"rouge\")\n","\n","print(\"Result:\", rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"], rouge_types=[\"rouge2\"])[\"rouge2\"].mid)\n"],"execution_count":null,"outputs":[]}]}